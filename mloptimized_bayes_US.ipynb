{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ksndBerUYL_"
      },
      "source": [
        "# Crop Yield Prediction - Optimized ML Models\n",
        "\n",
        "We use crop growth indicators, weather variables, geographic information, soil data and remote sensing indicators to predict the yield."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX-t0S8wUr5X"
      },
      "source": [
        "## Google Colab Notes\n",
        "\n",
        "**To run the script in Google Colab environment**\n",
        "1. Download the data directory and save it somewhere convenient.\n",
        "2. Open the notebook using Google Colaboratory.\n",
        "3. Create a copy of the notebook for yourself.\n",
        "4. Click connect on the right hand side of the bar below menu items. When you are connected to a machine, you will see a green tick mark and bars showing RAM and disk.\n",
        "5. Click the folder icon on the left sidebar and click upload. Upload the data files you downloaded. Click *Ok* when you see a warning saying the files will be deleted after the session is disconnected.\n",
        "6. Use *Runtime* -> *Run before* option to run all cells before **Set Configuration**.\n",
        "7. Run the remaining cells except **Python Script Main**. The configuration subsection allows you to change configuration and rerun experiments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP2t4BbJGJKZ"
      },
      "source": [
        "## Global Variables and Spark Installation/Initialization\n",
        "\n",
        "Initialize Spark session and global variables. Package installation is required only in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcalFxFgYk6s",
        "outputId": "1769f31c-6b5f-41dd-a74e-5d9c330709d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#%%writefile globals.py\n",
        "test_env = 'notebook'\n",
        "# test_env = 'cluster'\n",
        "# test_env = 'pkg'\n",
        "\n",
        "# change to False to skip tests\n",
        "run_tests = False\n",
        "\n",
        "# debug levels\n",
        "debug_levels = [i for i in range(5)]\n",
        "\n",
        "# Keeping these two mappings inside CYPConfiguration leads to SPARK-5063 error\n",
        "# when lambda functions use them. Therefore, they are defined as globals now.\n",
        "\n",
        "# crop name to id mapping\n",
        "crop_id_dict = {\n",
        "    'grain maize': 2,\n",
        "    'sugar beet' : 6,\n",
        "    'sugarbeet' : 6,\n",
        "    'sugarbeets' : 6,\n",
        "    'sugar beets' : 6,\n",
        "    'total potatoes' : 7,\n",
        "    'potatoes' : 7,\n",
        "    'potato' : 7,\n",
        "    'winter wheat' : 90,\n",
        "    'soft wheat' : 90,\n",
        "    'sunflower' : 93,\n",
        "    'spring barley' : 95,\n",
        "}\n",
        "\n",
        "# crop id to name mapping\n",
        "crop_name_dict = {\n",
        "    2 : 'grain maize',\n",
        "    6 : 'sugarbeet',\n",
        "    7 : 'potatoes',\n",
        "    90 : 'soft wheat',\n",
        "    93 : 'sunflower',\n",
        "    95 : 'spring barley',\n",
        "}\n",
        "\n",
        "if (test_env == 'notebook'):\n",
        "  !pip install pyspark > /dev/null\n",
        "  !sudo apt update > /dev/null\n",
        "  !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "  !pip install joblibspark > /dev/null\n",
        "  !pip install scikit-optimize > /dev/null\n",
        "  !pip install shap > /dev/null\n",
        "\n",
        "  import os\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import functions as SparkF\n",
        "from pyspark.sql import types as SparkT\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "SparkContext.setSystemProperty('spark.executor.memory', '12g')\n",
        "SparkContext.setSystemProperty('spark.driver.memory', '6g')\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "sqlContext = SQLContext(sc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXT68GaJ1jOc"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZtsU16F1jOn"
      },
      "outputs": [],
      "source": [
        "#%%writefile util.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# crop name and id mappings\n",
        "def cropNameToID(crop_id_dict, crop):\n",
        "  \"\"\"\n",
        "  Return id of given crop. Relies on crop_id_dict.\n",
        "  Return 0 if crop name is not in the dictionary.\n",
        "  \"\"\"\n",
        "  crop_lcase = crop.lower()\n",
        "  try:\n",
        "    crop_id = crop_id_dict[crop_lcase]\n",
        "  except KeyError as e:\n",
        "    crop_id = 0\n",
        "\n",
        "  return crop_id\n",
        "\n",
        "def cropIDToName(crop_name_dict, crop_id):\n",
        "  \"\"\"\n",
        "  Return crop name for given crop ID. Relies on crop_name_dict.\n",
        "  Return 'NA' if crop id is not found in the dictionary.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    crop_name = crop_name_dict[crop_id]\n",
        "  except KeyError as e:\n",
        "    crop_name = 'NA'\n",
        "\n",
        "  return crop_name\n",
        "\n",
        "def getYear(date_str):\n",
        "  \"\"\"Extract year from date in yyyyMMdd or dd/MM/yyyy format.\"\"\"\n",
        "  return SparkF.when(SparkF.length(date_str) == 8,\n",
        "                     SparkF.year(SparkF.to_date(date_str, 'yyyyMMdd')))\\\n",
        "                     .otherwise(SparkF.year(SparkF.to_date(date_str, 'dd/MM/yyyy')))\n",
        "\n",
        "def getMonth(date_str):\n",
        "  \"\"\"Extract month from date in yyyyMMdd or dd/MM/yyyy format.\"\"\"\n",
        "  return SparkF.when(SparkF.length(date_str) == 8,\n",
        "                     SparkF.month(SparkF.to_date(date_str, 'yyyyMMdd')))\\\n",
        "                     .otherwise(SparkF.month(SparkF.to_date(date_str, 'dd/MM/yyyy')))\n",
        "\n",
        "def getDay(date_str):\n",
        "  \"\"\"Extract day from date in yyyyMMdd or dd/MM/yyyy format.\"\"\"\n",
        "  return SparkF.when(SparkF.length(date_str) == 8,\n",
        "                     SparkF.dayofmonth(SparkF.to_date(date_str, 'yyyyMMdd')))\\\n",
        "                     .otherwise(SparkF.dayofmonth(SparkF.to_date(date_str, 'dd/MM/yyyy')))\n",
        "\n",
        "# 1-10: Dekad 1\n",
        "# 11-20: Dekad 2\n",
        "# > 20 : Dekad 3\n",
        "def getDekad(date_str):\n",
        "  \"\"\"Extract dekad from date in YYYYMMDD format.\"\"\"\n",
        "  month = getMonth(date_str)\n",
        "  day = getDay(date_str)\n",
        "  return SparkF.when(day < 30, (month - 1)* 3 +\n",
        "                     SparkF.ceil(day/10)).otherwise((month - 1) * 3 + 3)\n",
        "\n",
        "# Machine Learning Utility Functions\n",
        "\n",
        "# Hassanat Distance Metric for KNN\n",
        "# See https://arxiv.org/pdf/1708.04321.pdf\n",
        "# Code based on https://github.com/BrunoGomesCoelho/hassanat-distance-checker/blob/master/Experiments.ipynb\n",
        "def hassanatDistance(a, b):\n",
        "  total = 0\n",
        "  for a_i, b_i in zip(a, b):\n",
        "    min_value = min(a_i, b_i)\n",
        "    max_value = max(a_i, b_i)\n",
        "    total += 1\n",
        "    if min_value >= 0:\n",
        "      total -= (1 + min_value) / (1 + max_value)\n",
        "    else:\n",
        "      total -= (1 + min_value + abs(min_value)) / (1 + max_value + abs(min_value))\n",
        "\n",
        "  return total\n",
        "\n",
        "# This definition is from the suggested answer to:\n",
        "# https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn/294069#294069\n",
        "def meanAbsolutePercentageError(Y_true, Y_pred):\n",
        "  \"\"\"Mean Absolute Percentage Error\"\"\"\n",
        "  Y_true, Y_pred = np.array(Y_true), np.array(Y_pred)\n",
        "  return np.mean(np.abs((Y_true - Y_pred) / Y_true)) * 100\n",
        "\n",
        "def modelRefitMeanVariance(cv_results):\n",
        "  \"\"\"\n",
        "  Custom refit callable for hyperparameter optimization.\n",
        "  Look at mean and variance of validation scores\n",
        "  \"\"\"\n",
        "  # cv_results structure\n",
        "  # {\n",
        "  #   'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n",
        "  #   'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n",
        "  #   'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n",
        "  #   'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n",
        "  #   'rank_test_score'    : [2, 4, 3, 1],\n",
        "  #   'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n",
        "  #   'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n",
        "  #   'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n",
        "  #   'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n",
        "  #    ...\n",
        "  #   'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n",
        "  # }\n",
        "\n",
        "  # For mean_score, higher is better\n",
        "  # For std_score or variance, lower is better\n",
        "  # We combine them by using mean_score - std_score\n",
        "  mean_score = cv_results['mean_test_score']\n",
        "  std_score = cv_results['std_test_score']\n",
        "  refit_score = mean_score - std_score\n",
        "\n",
        "  best_score, best_index = max((val, idx) for (idx, val) in enumerate(refit_score))\n",
        "  return best_index\n",
        "\n",
        "def modelRefitTrainValDiff(cv_results):\n",
        "  \"\"\"\n",
        "  Custom refit callable for hyperparameter optimization.\n",
        "  Look at difference between training and validation errors.\n",
        "  NOTE: Hyperparameter search must be called with return_train_score=True.\n",
        "  \"\"\"\n",
        "  # cv_results structure\n",
        "  # {\n",
        "  #   'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n",
        "  #   'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n",
        "  #   'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n",
        "  #   'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n",
        "  #   'rank_test_score'    : [2, 4, 3, 1],\n",
        "  #   'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n",
        "  #   'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n",
        "  #   'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n",
        "  #   'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n",
        "  #    ...\n",
        "  #   'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n",
        "  # }\n",
        "  mean_test_score = cv_results['mean_test_score']\n",
        "  mean_train_score = cv_results['mean_train_score']\n",
        "  mean_score_diff = mean_test_score - mean_train_score\n",
        "\n",
        "  best_score, best_index = min((val, idx) for (idx, val) in enumerate(mean_score_diff))\n",
        "  return best_index\n",
        "\n",
        "def customFitPredict(args):\n",
        "  \"\"\"\n",
        "  We need this because scikit-learn does not support\n",
        "  cross_val_predict for time series splits.\n",
        "  \"\"\"\n",
        "  X_train = args['X_train']\n",
        "  Y_train = args['Y_train']\n",
        "  X_test = args['X_test']\n",
        "  est = args['estimator']\n",
        "  fit_params = args['fit_params']\n",
        "\n",
        "  est.fit(X_train, Y_train, **fit_params)\n",
        "  return est.predict(X_test)\n",
        "\n",
        "def unionCountryDataPandas(pd_df, year_col, test_years, is_train=False):\n",
        "  \"\"\"\n",
        "  Union training or test data for multiple countries.\n",
        "  Test years must be a dictionary with country specific test years,\n",
        "  e.g. { 'NL' : [2012, 2013],\n",
        "         'DE' : [2013, 2014],\n",
        "       }\n",
        "  \"\"\"\n",
        "  pd_sel_df = None\n",
        "  if ('COUNTRY' not in pd_df.columns):\n",
        "    pd_df['COUNTRY'] = 'US'\n",
        "\n",
        "  for cn in test_years:\n",
        "    cn_test_years = test_years[cn]\n",
        "    if (is_train):\n",
        "      cn_filter = (pd_df['COUNTRY'] == cn) & (~pd_df[year_col].isin(cn_test_years))\n",
        "    else:\n",
        "      cn_filter = (pd_df['COUNTRY'] == cn) & (pd_df[year_col].isin(cn_test_years))\n",
        "\n",
        "    pd_cn_df = pd_df[cn_filter]\n",
        "    if (pd_sel_df is None):\n",
        "      pd_sel_df = pd_cn_df\n",
        "    else:\n",
        "      pd_sel_df = pd_sel_df.append(pd_cn_df)\n",
        "\n",
        "  pd_sel_df = pd_sel_df.drop(columns=['COUNTRY'])\n",
        "  return pd_sel_df\n",
        "\n",
        "def printInGroups(items, indices, item_values=None, log_fh=None):\n",
        "  \"\"\"Print elements at given indices in groups of 5\"\"\"\n",
        "  num_items = len(indices)\n",
        "  groups = int(num_items/5) + 1\n",
        "\n",
        "  items_str = '\\n'\n",
        "  for g in range(groups):\n",
        "    group_start = g * 5\n",
        "    group_end = (g + 1) * 5\n",
        "    if (group_end > num_items):\n",
        "      group_end = num_items\n",
        "\n",
        "    group_indices = indices[group_start:group_end]\n",
        "    for idx in group_indices:\n",
        "      items_str += str(idx+1) + ': ' + items[idx]\n",
        "      if (item_values):\n",
        "        items_str += '=' + item_values[idx]\n",
        "\n",
        "      if (idx != group_indices[-1]):\n",
        "          items_str += ', '\n",
        "\n",
        "    items_str += '\\n'\n",
        "\n",
        "  print(items_str)\n",
        "  if (log_fh is not None):\n",
        "    log_fh.write(items_str)\n",
        "\n",
        "def getPredictionScores(Y_true, Y_predicted, metrics):\n",
        "  \"\"\"Get values of metrics for given Y_predicted and Y_true\"\"\"\n",
        "  pred_scores = {}\n",
        "\n",
        "  for met in metrics:\n",
        "    score_function = metrics[met]\n",
        "    met_score = score_function(Y_true, Y_predicted)\n",
        "    # for RMSE, score_function is mean_squared_error, take square root\n",
        "    # normalize RMSE\n",
        "    if (met == 'RMSE'):\n",
        "      met_score = np.round(100*np.sqrt(met_score)/np.mean(Y_true), 2)\n",
        "      pred_scores['NRMSE'] = met_score\n",
        "    # normalize mean absolute errors except MAPE which is already a percentage\n",
        "    elif ((met == 'MAE') or (met == 'MdAE')):\n",
        "      met_score = np.round(100*met_score/np.mean(Y_true), 2)\n",
        "      pred_scores['N' + met] = met_score\n",
        "    # MAPE, R2, ... : no postprocessing\n",
        "    else:\n",
        "      met_score = np.round(met_score, 2)\n",
        "      pred_scores[met] = met_score\n",
        "\n",
        "  return pred_scores\n",
        "\n",
        "def getFilename(crop, yield_trend, early_season, early_season_end,\n",
        "                country=None, spatial_level=None):\n",
        "  \"\"\"Get filename based on input arguments\"\"\"\n",
        "  suffix = crop.replace(' ', '_')\n",
        "\n",
        "  if (country is not None):\n",
        "    suffix += '_' + country\n",
        "\n",
        "  if (spatial_level is not None):\n",
        "    suffix += '_' + spatial_level\n",
        "\n",
        "  if (yield_trend):\n",
        "    suffix += '_trend'\n",
        "  else:\n",
        "    suffix += '_notrend'\n",
        "\n",
        "  if (early_season):\n",
        "    suffix += '_early' + str(early_season_end)\n",
        "\n",
        "  return suffix\n",
        "\n",
        "def getLogFilename(crop, yield_trend, early_season, early_season_end,\n",
        "                   country=None):\n",
        "  \"\"\"Get filename for experiment log\"\"\"\n",
        "  log_file = getFilename(crop, yield_trend, early_season, early_season_end, country)\n",
        "  return log_file + '.log'\n",
        "\n",
        "def getFeatureFilename(crop, yield_trend, early_season, early_season_end,\n",
        "                       country=None):\n",
        "  \"\"\"Get unique filename for features\"\"\"\n",
        "  feature_file = 'ft_'\n",
        "  suffix = getFilename(crop, yield_trend, early_season, early_season_end, country)\n",
        "  feature_file += suffix\n",
        "  return feature_file\n",
        "\n",
        "def getPredictionFilename(crop, yield_trend, early_season, early_season_end,\n",
        "                          country=None, spatial_level=None):\n",
        "  \"\"\"Get unique filename for predictions\"\"\"\n",
        "  pred_file = 'pred_'\n",
        "  suffix = getFilename(crop, yield_trend, early_season, early_season_end,\n",
        "                       country, spatial_level)\n",
        "  pred_file += suffix\n",
        "  return pred_file\n",
        "\n",
        "def plotTrend(years, actual_values, trend_values, trend_label):\n",
        "  \"\"\"Plot a linear trend and scatter plot of actual values\"\"\"\n",
        "  plt.scatter(years, actual_values, color=\"blue\", marker=\"o\")\n",
        "  plt.plot(years, trend_values, '--')\n",
        "  plt.xticks(np.arange(years[0], years[-1] + 1, step=len(years)/5))\n",
        "  ax = plt.axes()\n",
        "  plt.xlabel(\"YEAR\")\n",
        "  plt.ylabel(trend_label)\n",
        "  plt.title(trend_label + ' Trend by YEAR')\n",
        "  plt.show()\n",
        "\n",
        "def plotTrueVSPredicted(actual, predicted):\n",
        "  \"\"\"Plot actual and predicted values\"\"\"\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.scatter(np.asarray(actual), predicted)\n",
        "  ax.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'k--', lw=4)\n",
        "  ax.set_xlabel('Actual')\n",
        "  ax.set_ylabel('Predicted')\n",
        "  plt.show()\n",
        "\n",
        "def plotCVResultsGroup(pd_results_df, score_cols, param_cols):\n",
        "  \"\"\"Plot training and validation scores of given parameters\"\"\"\n",
        "  # Metrics can be string or functions, skip them.\n",
        "  if ('estimator__metric' in param_cols):\n",
        "    param_cols.remove('estimator__metric')\n",
        "\n",
        "  fig, ax = plt.subplots(1, len(param_cols), sharex='none', sharey='all',figsize=(20,5))\n",
        "  fig.suptitle('Score per parameter')\n",
        "  fig.text(0.04, 0.5, 'MEAN SCORE', va='center', rotation='vertical')\n",
        "  for i, p in enumerate(param_cols):\n",
        "    pd_filtered_df = pd_results_df.copy()\n",
        "    pd_filtered_df = pd_filtered_df.drop_duplicates(subset=[p])\n",
        "    pd_param_df = pd_filtered_df[[p] + score_cols]\n",
        "    if ((pd_filtered_df[p].dtype == 'int64') or (pd_filtered_df[p].dtype == 'float64')):\n",
        "      pd_param_df = pd_param_df.sort_values(by=[p])\n",
        "\n",
        "    x = pd_param_df[p].values\n",
        "    y1 = pd_param_df['MEAN_TEST'].values\n",
        "    y2 = pd_param_df['MEAN_TRAIN'].values\n",
        "    e1 = pd_param_df['STD_TEST'].values\n",
        "    e2 = pd_param_df['STD_TRAIN'].values\n",
        "    if (len(param_cols) > 1):\n",
        "      ax[i].errorbar(x, y1, e1, linestyle='--', marker='o', label='test')\n",
        "      ax[i].errorbar(x, y2, e2, linestyle='solid', marker='o', label='train')\n",
        "      ax[i].set_xlabel(p.upper())\n",
        "    else:\n",
        "      ax.errorbar(x, y1, e1, linestyle='--', marker='o', label='test')\n",
        "      ax.errorbar(x, y2, e2, linestyle='solid', marker='o', label='train')\n",
        "      ax.set_xlabel(p.upper())\n",
        "\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "def plotCVResults(search_cv):\n",
        "  \"\"\"Plot training and validation scores of search parameters\"\"\"\n",
        "  score_cols = ['MEAN_TEST', 'MEAN_TRAIN', 'STD_TEST', 'STD_TRAIN']\n",
        "  pd_results_df = pd.concat([pd.DataFrame(search_cv.cv_results_['params']),\n",
        "                             pd.DataFrame(search_cv.cv_results_['mean_test_score'],\n",
        "                                          columns=['MEAN_TEST']),\n",
        "                             pd.DataFrame(search_cv.cv_results_['std_test_score'],\n",
        "                                          columns=['STD_TEST']),\n",
        "                             pd.DataFrame(search_cv.cv_results_['mean_train_score'],\n",
        "                                          columns=['MEAN_TRAIN']),\n",
        "                             pd.DataFrame(search_cv.cv_results_['std_train_score'],\n",
        "                                          columns=['STD_TRAIN'])\n",
        "                             ], axis=1)\n",
        "\n",
        "  param_cols = list(pd_results_df.columns)[:-len(score_cols)]\n",
        "  # remove parameters with fixed value\n",
        "  del_cols = []\n",
        "  for p in param_cols:\n",
        "    x = set(pd_results_df[p].values)\n",
        "    if (len(x) == 1):\n",
        "      del_cols.append(p)\n",
        "\n",
        "  param_cols = [c for c in param_cols if c not in del_cols]\n",
        "  if (len(param_cols) <= 3):\n",
        "    plotCVResultsGroup(pd_results_df, score_cols, param_cols)\n",
        "  else:\n",
        "    num_groups = int(len(param_cols)/ 3) + 1\n",
        "    for g in range(num_groups):\n",
        "      param_cols_group = param_cols[g * 3 : (g + 1) * 3]\n",
        "      if (not param_cols_group):\n",
        "        break\n",
        "\n",
        "      plotCVResultsGroup(pd_results_df, score_cols, param_cols_group)\n",
        "\n",
        "# Based on\n",
        "# https://stackoverflow.com/questions/39409866/correlation-heatmap\n",
        "def plotCorrelation(df, sel_cols):\n",
        "  corr = df[sel_cols].corr()\n",
        "  mask = np.zeros_like(corr, dtype=np.bool)\n",
        "  mask[np.triu_indices_from(mask)] = True\n",
        "  f, ax = plt.subplots(figsize=(20, 18))\n",
        "  cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "  sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1.0, center=0,\n",
        "              square=True, linewidths=.5, cbar_kws={\"shrink\": .5},\n",
        "              annot=True, fmt='.1g')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8UktClOS9Hs"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjP5SjODS-qS"
      },
      "outputs": [],
      "source": [
        "#%%writefile config.py\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import explained_variance_score\n",
        "from sklearn.metrics import median_absolute_error\n",
        "\n",
        "from sklearn.utils.fixes import loguniform\n",
        "import scipy.stats as stats\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "\n",
        "class CYPConfiguration:\n",
        "  def __init__(self, crop_name='potatoes', country_code='NL', season_cross='N'):\n",
        "    self.config = {\n",
        "        'crop_name' : crop_name,\n",
        "        'crop_id' : cropNameToID(crop_id_dict, crop_name),\n",
        "        'season_crosses_calendar_year' : season_cross,\n",
        "        'country_code' : country_code,\n",
        "        'spatial_level' : 'NUTS2',\n",
        "        'data_sources' : [ 'CSSF', 'METEO', 'SOIL', 'YIELD' ],\n",
        "        'clean_data' : 'N',\n",
        "        'use_yield_trend' : 'N',\n",
        "        'predict_yield_residuals' : 'N',\n",
        "        'find_optimal_trend_window' : 'N',\n",
        "        # set it to a list with one entry for fixed window\n",
        "        'trend_windows' : [5, 7, 10],\n",
        "        'use_centroids' : 'N',\n",
        "        'use_remote_sensing' : 'Y',\n",
        "        'use_per_year_crop_calendar' : 'Y',\n",
        "        'early_season_prediction' : 'N',\n",
        "        'early_season_end_dekad' : 0,\n",
        "        'data_path' : '.',\n",
        "        'output_path' : '.',\n",
        "        'use_features_v2' : 'Y',\n",
        "        'save_features' : 'N',\n",
        "        'use_saved_features' : 'N',\n",
        "        'use_sample_weights' : 'N',\n",
        "        'retrain_per_test_year' : 'N',\n",
        "        'save_predictions' : 'Y',\n",
        "        'use_saved_predictions' : 'N',\n",
        "        'compare_with_mcyfs' : 'N',\n",
        "        'debug_level' : 0,\n",
        "    }\n",
        "\n",
        "    # Description of configuration parameters\n",
        "    # This should be in sync with config above\n",
        "    self.config_desc = {\n",
        "        'crop_name' : 'Crop name',\n",
        "        'crop_id' : 'Crop ID',\n",
        "        'season_crosses_calendar_year' : 'Crop growing season crosses calendar year boundary',\n",
        "        'country_code' : 'Country code (e.g. NL)',\n",
        "        'spatial_level' : 'spatial level for yield prediction',\n",
        "        'data_sources' : 'Input data sources',\n",
        "        'clean_data' : 'Remove data or regions with duplicate or missing values',\n",
        "        'use_yield_trend' : 'Estimate and use yield trend',\n",
        "        'predict_yield_residuals' : 'Predict yield residuals instead of full yield',\n",
        "        'find_optimal_trend_window' : 'Find optimal trend window',\n",
        "        'trend_windows' : 'List of trend window lengths (number of years)',\n",
        "        'use_centroids' : 'Use centroid coordinates and distance to coast',\n",
        "        'use_remote_sensing' : 'Use remote sensing data (FAPAR)',\n",
        "        'use_per_year_crop_calendar' : 'Use per region per year crop calendar',\n",
        "        'early_season_prediction' : 'Predict yield early in the season',\n",
        "        'early_season_end_dekad' : 'Early season end dekad relative to harvest',\n",
        "        'data_path' : 'Path to all input data. Default is current directory.',\n",
        "        'output_path' : 'Path to all output files. Default is current directory.',\n",
        "        'use_features_v2' : 'Use feature design v2',\n",
        "        'save_features' : 'Save features to a CSV file',\n",
        "        'use_saved_features' : 'Use features from a CSV file',\n",
        "        'use_sample_weights' : 'Use data sample weights based on crop area',\n",
        "        'retrain_per_test_year' : 'Retrain a model for every test year',\n",
        "        'save_predictions' : 'Save predictions to a CSV file',\n",
        "        'use_saved_predictions' : 'Use predictions from a CSV file',\n",
        "        'compare_with_mcyfs' : 'Compare predictions with MARS Crop Yield Forecasting System',\n",
        "        'debug_level' : 'Debug level to control amount of debug information',\n",
        "    }\n",
        "\n",
        "    ########### Machine learning configuration ###########\n",
        "    # mutual correlation threshold for features\n",
        "    self.feature_correlation_threshold = 0.9\n",
        "\n",
        "    # test fraction\n",
        "    self.test_fraction = 0.3\n",
        "\n",
        "    # scaler\n",
        "    self.scaler = StandardScaler()\n",
        "\n",
        "    # Feature selection algorithms. Initialized in getFeatureSelectors().\n",
        "    self.feature_selectors = {}\n",
        "\n",
        "    # prediction algorithms\n",
        "    self.estimators = {\n",
        "        # linear model\n",
        "        # 'Ridge' : {\n",
        "        #     'estimator' : Ridge(random_state=42, max_iter=1000, tol=1e-3,\n",
        "        #                         normalize=False, fit_intercept=True),\n",
        "        #     'param_space' : {\n",
        "        #         \"estimator__alpha\": Real(1e-1, 1e+2, prior='log-uniform')\n",
        "        #     }\n",
        "        # },\n",
        "        # 'KNN' : {\n",
        "        #     'estimator' : KNeighborsRegressor(n_jobs=-1),\n",
        "        #     'param_space' : {\n",
        "        #         \"estimator__n_neighbors\": Integer(7, 15),\n",
        "        #         \"estimator__weights\": Categorical(['uniform', 'distance']),\n",
        "        #         \"estimator__metric\" : Categorical(['minkowski', 'manhattan']),# hassanatDistance])\n",
        "        #     }\n",
        "        # },\n",
        "        # SVM regression\n",
        "        # 'SVR' : {\n",
        "        #     'estimator' : SVR(gamma='scale', epsilon=1e-1, tol=1e-3,\n",
        "        #                       max_iter=-1, shrinking=True),\n",
        "        #     'param_space' : {\n",
        "        #         \"estimator__C\": Real(1e-2, 5e+2, prior='log-uniform'),\n",
        "        #         \"estimator__epsilon\": Real(1e-2, 5e-1, prior='log-uniform'),\n",
        "        #         \"estimator__gamma\": Categorical(['auto', 'scale']),\n",
        "        #         \"estimator__kernel\": Categorical(['rbf', 'linear']),\n",
        "        #     }\n",
        "        # },\n",
        "        # gradient boosted decision trees\n",
        "        'GBDT' : {\n",
        "            'estimator' : GradientBoostingRegressor(loss='huber', max_features='log2',\n",
        "                                                    max_depth=10, min_samples_leaf=10,\n",
        "                                                    tol=1e-3, n_iter_no_change=5,\n",
        "                                                    subsample=0.6, ccp_alpha=1e-2,\n",
        "                                                    n_estimators=500), # random_state=42),\n",
        "            'param_space' : {\n",
        "                \"estimator__learning_rate\": Real(1e-3, 1e-1, prior='log-uniform'),\n",
        "                \"estimator__loss\" : Categorical(['huber', 'lad']),\n",
        "                # \"estimator__max_depth\": Integer(5, 10),\n",
        "                \"estimator__min_samples_leaf\": Integer(5, 20),\n",
        "                # \"estimator__n_estimators\": Integer(100, 500),\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # k-fold validation metric for feature selection\n",
        "    self.fs_cv_metric = 'neg_mean_squared_error'\n",
        "    # k-fold validation metric for training\n",
        "    self.est_cv_metric = 'neg_mean_squared_error'\n",
        "\n",
        "    # Performance evaluation metrics:\n",
        "    # sklearn supports these metrics:\n",
        "    # 'explained_variance', 'max_error', 'neg_mean_absolute_error\n",
        "    # 'neg_mean_squared_error', 'neg_root_mean_squared_error'\n",
        "    # 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'r2'\n",
        "    self.eval_metrics = {\n",
        "        # EXP_VAR (y_true, y_obs) = 1 - ( var(y_true - y_obs) / var (y_true) )\n",
        "        # 'EXP_VAR' : explained_variance_score,\n",
        "        # MAE (y_true, y_obs) = ( 1 / n ) * sum_i-n ( | y_true_i - y_obs_i | )\n",
        "        # 'MAE' : mean_absolute_error,\n",
        "        # MdAE (y_true, y_obs) = median ( | y_true_1 - y_obs_1 |, | y_true_2 - y_obs_2 |, ... )\n",
        "        # 'MdAE' : median_absolute_error,\n",
        "        # MAPE (y_true, y_obs) = ( 1 / n ) * sum_i-n ( ( y_true_i - y_obs_i ) / y_true_i )\n",
        "        'MAPE' : meanAbsolutePercentageError,\n",
        "        # MSE (y_true, y_obs) = ( 1 / n ) * sum_i-n ( y_true_i - y_obs_i )^2\n",
        "        'RMSE' : mean_squared_error,\n",
        "        # R2 (y_true, y_obs) = 1 - ( ( sum_i-n ( y_true_i - y_obs_i )^2 )\n",
        "        #                           / sum_i-n ( y_true_i - mean(y_true) )^2)\n",
        "        'R2' : r2_score,\n",
        "    }\n",
        "\n",
        "  ########### Setters and getters ###########\n",
        "  def setCropName(self, crop_name):\n",
        "    \"\"\"Set the crop name\"\"\"\n",
        "    crop = crop_name.lower()\n",
        "    assert crop in crop_id_dict\n",
        "    self.config['crop_name'] = crop\n",
        "    self.config['crop_id'] = cropNameToID(crop_id_dict, crop)\n",
        "\n",
        "  def getCropName(self):\n",
        "    \"\"\"Return the crop name\"\"\"\n",
        "    return self.config['crop_name']\n",
        "\n",
        "  def setCropID(self, crop_id):\n",
        "    \"\"\"Set the crop ID\"\"\"\n",
        "    assert crop_id in crop_name_dict\n",
        "    self.config['crop_id'] = crop_id\n",
        "    self.config['crop_name'] = cropIDToName(crop_name_dict, crop_id)\n",
        "\n",
        "  def getCropID(self):\n",
        "    \"\"\"Return the crop ID\"\"\"\n",
        "    return self.config['crop_id']\n",
        "\n",
        "  def setSeasonCrossesCalendarYear(self, season_crosses):\n",
        "    \"\"\"Set whether the season crosses calendar year boundary\"\"\"\n",
        "    scross = season_crosses.upper()\n",
        "    assert scross in ['Y', 'N']\n",
        "    self.config['season_crosses_calendar_year'] = scross\n",
        "\n",
        "  def seasonCrossesCalendarYear(self):\n",
        "    \"\"\"Return whether the season crosses calendar year boundary\"\"\"\n",
        "    return (self.config['season_crosses_calendar_year'] == 'Y')\n",
        "\n",
        "  def setCountryCode(self, country_code):\n",
        "    \"\"\"Set the country code\"\"\"\n",
        "    if (country_code is None):\n",
        "      self.config['country_code'] = None\n",
        "    else:\n",
        "      ccode = country_code.upper()\n",
        "      assert len(ccode) == 2\n",
        "      assert ccode in countries\n",
        "      self.config['country_code'] = ccode\n",
        "\n",
        "  def getCountryCode(self):\n",
        "    \"\"\"Return the country code\"\"\"\n",
        "    return self.config['country_code']\n",
        "\n",
        "  def setSpatialLevel(self, spatial_level):\n",
        "    \"\"\"Set the NUTS level\"\"\"\n",
        "    splvl = spatial_level.upper()\n",
        "    assert splvl in spatial_level\n",
        "    self.config['spatial_level'] = splvl\n",
        "\n",
        "  def getSpatialLevel(self):\n",
        "    \"\"\"Return the NUTS level\"\"\"\n",
        "    return self.config['spatial_level']\n",
        "\n",
        "  def setDataSources(self, data_sources):\n",
        "    \"\"\"Get the data sources\"\"\"\n",
        "    # TODO: some validation\n",
        "    self.config['data_sources'] = data_sources\n",
        "\n",
        "  def updateDataSources(self, data_src, include_src, spatial_level=None):\n",
        "    \"\"\"add or remove data_src from data sources\"\"\"\n",
        "    src_lvl = self.getSpatialLevel()\n",
        "    if (spatial_level is not None):\n",
        "      src_lvl = spatial_level\n",
        "\n",
        "    data_sources = self.config['data_sources']\n",
        "    # no update required\n",
        "    if (((include_src == 'Y') and (data_src in data_sources)) or\n",
        "        ((include_src == 'N') and (data_src not in data_sources))):\n",
        "      return\n",
        "\n",
        "    if (include_src == 'Y'):\n",
        "      if (isinstance(data_sources, dict)):\n",
        "        data_sources[data_src] = src_lvl\n",
        "      else:\n",
        "        data_sources.append(data_src)\n",
        "    else:\n",
        "      if (isinstance(data_sources, dict)):\n",
        "        del data_sources[data_src]\n",
        "      else:\n",
        "        data_sources.remove(data_src)\n",
        "\n",
        "    self.config['data_sources'] = data_sources\n",
        "\n",
        "  def getDataSources(self):\n",
        "    \"\"\"Return the data sources\"\"\"\n",
        "    return self.config['data_sources']\n",
        "\n",
        "  def setCleanData(self, clean_data):\n",
        "    \"\"\"Set whether to clean data with duplicate or missing values\"\"\"\n",
        "    do_clean = clean_data.upper()\n",
        "    assert do_clean in ['Y', 'N']\n",
        "    self.config['clean_data'] = do_clean\n",
        "\n",
        "  def cleanData(self):\n",
        "    \"\"\"Return whether to clean data with duplicate or missing values\"\"\"\n",
        "    return (self.config['clean_data'] == 'Y')\n",
        "\n",
        "  def setUseYieldTrend(self, use_trend):\n",
        "    \"\"\"Set whether to use yield trend\"\"\"\n",
        "    use_yt = use_trend.upper()\n",
        "    assert use_yt in ['Y', 'N']\n",
        "    self.config['use_yield_trend'] = use_yt\n",
        "\n",
        "  def useYieldTrend(self):\n",
        "    \"\"\"Return whether to use yield trend\"\"\"\n",
        "    return (self.config['use_yield_trend'] == 'Y')\n",
        "\n",
        "  def setPredictYieldResiduals(self, pred_res):\n",
        "    \"\"\"Set whether to use predict yield residuals\"\"\"\n",
        "    pred_yres = pred_res.upper()\n",
        "    assert pred_yres in ['Y', 'N']\n",
        "    self.config['predict_yield_residuals'] = pred_yres\n",
        "\n",
        "  def predictYieldResiduals(self):\n",
        "    \"\"\"Return whether to use predict yield residuals\"\"\"\n",
        "    return (self.config['predict_yield_residuals'] == 'Y')\n",
        "\n",
        "  def setFindOptimalTrendWindow(self, find_opt):\n",
        "    \"\"\"Set whether to find optimal trend window for each year\"\"\"\n",
        "    find_otw = find_opt.upper()\n",
        "    assert find_otw in ['Y', 'N']\n",
        "    self.config['find_optimal_trend_window'] = find_otw\n",
        "\n",
        "  def findOptimalTrendWindow(self):\n",
        "    \"\"\"Return whether to find optimal trend window for each year\"\"\"\n",
        "    return (self.config['find_optimal_trend_window'] == 'Y')\n",
        "\n",
        "  def setTrendWindows(self, trend_windows):\n",
        "    \"\"\"Set trend window lengths (years)\"\"\"\n",
        "    assert isinstance(trend_windows, list)\n",
        "    assert len(trend_windows) > 0\n",
        "\n",
        "    # trend windows less than 2 years do not make sense\n",
        "    for tw in trend_windows:\n",
        "      assert tw > 2\n",
        "\n",
        "    self.config['trend_windows'] = trend_windows\n",
        "\n",
        "  def getTrendWindows(self):\n",
        "    \"\"\"Return trend window lengths (years)\"\"\"\n",
        "    return self.config['trend_windows']\n",
        "\n",
        "  def setUseCentroids(self, use_centroids):\n",
        "    \"\"\"Set whether to use centroid coordinates and distance to coast\"\"\"\n",
        "    use_ct = use_centroids.upper()\n",
        "    assert use_ct in ['Y', 'N']\n",
        "    self.config['use_centroids'] = use_ct\n",
        "    self.updateDataSources('CENTROIDS', use_ct)\n",
        "\n",
        "  def useCentroids(self):\n",
        "    \"\"\"Return whether to use centroid coordinates and distance to coast\"\"\"\n",
        "    return (self.config['use_centroids'] == 'Y')\n",
        "\n",
        "  def setUseRemoteSensing(self, use_remote_sensing):\n",
        "    \"\"\"Set whether to use remote sensing data\"\"\"\n",
        "    use_rs = use_remote_sensing.upper()\n",
        "    assert use_rs in ['Y', 'N']\n",
        "    self.config['use_remote_sensing'] = use_rs\n",
        "    self.updateDataSources('REMOTE_SENSING', use_rs, 'NUTS2')\n",
        "\n",
        "  def useRemoteSensing(self):\n",
        "    \"\"\"Return whether to use remote sensing data\"\"\"\n",
        "    return (self.config['use_remote_sensing'] == 'Y')\n",
        "\n",
        "  def setUsePerYearCropCalendar(self, use_per_year_cc):\n",
        "    \"\"\"Set whether to use per region, per year crop calendar\"\"\"\n",
        "    per_year = use_per_year_cc.upper()\n",
        "    assert per_year in ['Y', 'N']\n",
        "    self.config['use_per_year_crop_calendar'] = per_year\n",
        "\n",
        "  def usePerYearCropCalendar(self):\n",
        "    \"\"\"Return whether to use per region, per year crop calendar\"\"\"\n",
        "    return (self.config['use_per_year_crop_calendar'] == 'Y')\n",
        "\n",
        "  def setEarlySeasonPrediction(self, early_season):\n",
        "    \"\"\"Set whether to do early season prediction\"\"\"\n",
        "    ep = early_season.upper()\n",
        "    assert ep in ['Y', 'N']\n",
        "    self.config['early_season_prediction'] = ep\n",
        "\n",
        "  def earlySeasonPrediction(self):\n",
        "    \"\"\"Return whether to do early season prediction\"\"\"\n",
        "    return (self.config['early_season_prediction'] == 'Y')\n",
        "\n",
        "  def setEarlySeasonEndDekad(self, end_dekad):\n",
        "    \"\"\"Set early season prediction dekad\"\"\"\n",
        "    dekads_range = [dek for dek in range(1, 37)]\n",
        "    assert end_dekad in dekads_range\n",
        "    self.config['early_season_end_dekad'] = end_dekad\n",
        "\n",
        "  def getEarlySeasonEndDekad(self):\n",
        "    \"\"\"Return early season prediction dekad\"\"\"\n",
        "    return self.config['early_season_end_dekad']\n",
        "\n",
        "  def setDataPath(self, data_path):\n",
        "    \"\"\"Set the data path\"\"\"\n",
        "    # TODO: some validation\n",
        "    self.config['data_path'] = data_path\n",
        "\n",
        "  def getDataPath(self):\n",
        "    \"\"\"Return the data path\"\"\"\n",
        "    return self.config['data_path']\n",
        "\n",
        "  def setOutputPath(self, out_path):\n",
        "    \"\"\"Set the path to output files. TODO: some validation.\"\"\"\n",
        "    self.config['output_path'] = out_path\n",
        "\n",
        "  def getOutputPath(self):\n",
        "    \"\"\"Return the path to output files.\"\"\"\n",
        "    return self.config['output_path']\n",
        "\n",
        "  def setUseFeaturesV2(self, use_v2):\n",
        "    \"\"\"Set whether to use features v2\"\"\"\n",
        "    ft_v2 = use_v2.upper()\n",
        "    assert ft_v2 in ['Y', 'N']\n",
        "    self.config['use_features_v2'] = ft_v2\n",
        "\n",
        "  def useFeaturesV2(self):\n",
        "    \"\"\"Return whether to use features v2\"\"\"\n",
        "    return (self.config['use_features_v2'] == 'Y')\n",
        "\n",
        "  def setSaveFeatures(self, save_ft):\n",
        "    \"\"\"Set whether to save features in a CSV file\"\"\"\n",
        "    sft = save_ft.upper()\n",
        "    assert sft in ['Y', 'N']\n",
        "    self.config['save_features'] = sft\n",
        "\n",
        "  def saveFeatures(self):\n",
        "    \"\"\"Return whether to save features in a CSV file\"\"\"\n",
        "    return (self.config['save_features'] == 'Y')\n",
        "\n",
        "  def setUseSavedFeatures(self, use_saved):\n",
        "    \"\"\"Set whether to use features from CSV file\"\"\"\n",
        "    saved = use_saved.upper()\n",
        "    assert saved in ['Y', 'N']\n",
        "    self.config['use_saved_features'] = saved\n",
        "\n",
        "  def useSavedFeatures(self):\n",
        "    \"\"\"Return whether to use to use features from CSV file\"\"\"\n",
        "    return (self.config['use_saved_features'] == 'Y')\n",
        "\n",
        "  def setUseSampleWeights(self, use_weights):\n",
        "    \"\"\"Set whether to use data sample weights\"\"\"\n",
        "    use_sw = use_weights.upper()\n",
        "    assert use_sw in ['Y', 'N']\n",
        "    self.config['use_sample_weights'] = use_sw\n",
        "\n",
        "  def useSampleWeights(self):\n",
        "    \"\"\"Return whether to use data sample weights\"\"\"\n",
        "    return (self.config['use_sample_weights'] == 'Y')\n",
        "\n",
        "  def setRetrainPerTestYear(self, per_test_year):\n",
        "    \"\"\"Set whether to retrain the model for every test year\"\"\"\n",
        "    pty = per_test_year.upper()\n",
        "    assert pty in ['Y', 'N']\n",
        "    self.config['retrain_per_test_year'] = pty\n",
        "\n",
        "  def retrainPerTestYear(self):\n",
        "    \"\"\"Return whether to retrain the model for every test year\"\"\"\n",
        "    return (self.config['retrain_per_test_year'] == 'Y')\n",
        "\n",
        "  def setSavePredictions(self, save_pred):\n",
        "    \"\"\"Set whether to save predictions in a CSV file\"\"\"\n",
        "    spd = save_pred.upper()\n",
        "    assert spd in ['Y', 'N']\n",
        "    self.config['save_predictions'] = spd\n",
        "\n",
        "  def savePredictions(self):\n",
        "    \"\"\"Return whether to save predictions in a CSV file\"\"\"\n",
        "    return (self.config['save_predictions'] == 'Y')\n",
        "\n",
        "  def setUseSavedPredictions(self, use_saved):\n",
        "    \"\"\"Set whether to use predictions from CSV file\"\"\"\n",
        "    saved = use_saved.upper()\n",
        "    assert saved in ['Y', 'N']\n",
        "    self.config['use_saved_predictions'] = saved\n",
        "\n",
        "  def useSavedPredictions(self):\n",
        "    \"\"\"Return whether to use to use predictions from CSV file\"\"\"\n",
        "    return (self.config['use_saved_predictions'] == 'Y')\n",
        "\n",
        "  def setCompareWithMCYFS(self, compare_mcyfs):\n",
        "    \"\"\"Set whether to compare predictions with MCYFS\"\"\"\n",
        "    comp_mcyfs = compare_mcyfs.upper()\n",
        "    assert comp_mcyfs in ['Y', 'N']\n",
        "    self.config['compare_with_mcyfs'] = comp_mcyfs\n",
        "\n",
        "  def compareWithMCYFS(self):\n",
        "    \"\"\"Return whether to compare predictions with MCYFS\"\"\"\n",
        "    return (self.config['compare_with_mcyfs'] == 'Y')\n",
        "\n",
        "  def setDebugLevel(self, debug_level):\n",
        "    \"\"\"Set the debug level\"\"\"\n",
        "    assert debug_level in debug_levels\n",
        "    self.config['debug_level'] = debug_level\n",
        "\n",
        "  def getDebugLevel(self):\n",
        "    \"\"\"Return the debug level\"\"\"\n",
        "    return self.config['debug_level']\n",
        "\n",
        "  def updateConfiguration(self, config_update):\n",
        "    \"\"\"Update configuration\"\"\"\n",
        "    assert isinstance(config_update, dict)\n",
        "    for k in config_update:\n",
        "      assert k in self.config\n",
        "\n",
        "      # keys that need special handling\n",
        "      special_cases = {\n",
        "          'crop_name' : self.setCropName,\n",
        "          'crop_id' : self.setCropID,\n",
        "          'use_centroids' : self.setUseCentroids,\n",
        "          'use_remote_sensing' : self.setUseRemoteSensing,\n",
        "      }\n",
        "\n",
        "      if (k not in special_cases):\n",
        "        self.config[k] = config_update[k]\n",
        "        continue\n",
        "\n",
        "      # special case\n",
        "      special_cases[k](config_update[k])\n",
        "\n",
        "  def printConfig(self, log_fh):\n",
        "    \"\"\"Print current configuration and write configuration to log file.\"\"\"\n",
        "    config_str = '\\nCurrent ML Baseline Configuration'\n",
        "    config_str += '\\n--------------------------------'\n",
        "    for k in self.config:\n",
        "      if (isinstance(self.config[k], dict)):\n",
        "        conf_keys = list(self.config[k].keys())\n",
        "        if (not isinstance(conf_keys[0], str)):\n",
        "          conf_keys = [str(k) for k in conf_keys]\n",
        "\n",
        "        config_str += '\\n' + self.config_desc[k] + ': ' + ', '.join(conf_keys)\n",
        "      elif (isinstance(self.config[k], list)):\n",
        "        conf_vals = self.config[k]\n",
        "        if (not isinstance(conf_vals[0], str)):\n",
        "          conf_vals = [str(k) for k in conf_vals]\n",
        "\n",
        "        config_str += '\\n' + self.config_desc[k] + ': ' + ', '.join(conf_vals)\n",
        "      else:\n",
        "        conf_val = self.config[k]\n",
        "        if (not isinstance(conf_val, str)):\n",
        "          conf_val = str(conf_val)\n",
        "\n",
        "        config_str += '\\n' + self.config_desc[k] + ': ' + conf_val\n",
        "\n",
        "    config_str += '\\n'\n",
        "    log_fh.write(config_str + '\\n')\n",
        "    print(config_str)\n",
        "\n",
        "  # Machine learning configuration\n",
        "  def getFeatureCorrelationThreshold(self):\n",
        "    \"\"\"Return threshold for removing mutually correlated features\"\"\"\n",
        "    return self.feature_correlation_threshold\n",
        "\n",
        "  def setFeatureCorrelationThreshold(self, corr_thresh):\n",
        "    \"\"\"Set threshold for removing mutually correlated features\"\"\"\n",
        "    assert (corr_thresh > 0.0 and corr_thresh < 1.0)\n",
        "    self.feature_correlation_threshold = corr_thresh\n",
        "\n",
        "  def getTestFraction(self):\n",
        "    \"\"\"Return test set fraction (of full dataset)\"\"\"\n",
        "    return self.test_fraction\n",
        "\n",
        "  def setTestFraction(self, test_fraction):\n",
        "    \"\"\"Set test set fraction (of full dataset)\"\"\"\n",
        "    assert (test_fraction > 0.0 and test_fraction < 1.0)\n",
        "    self.test_fraction = test_fraction\n",
        "\n",
        "  def getFeatureScaler(self):\n",
        "    \"\"\"Return feature scaling method\"\"\"\n",
        "    return self.scaler\n",
        "\n",
        "  def setFeatureScaler(self, scaler):\n",
        "    \"\"\"Set feature scaling method\"\"\"\n",
        "    assert (isinstance(scaler, MinMaxScaler) or isinstance(scaler, StandardScaler))\n",
        "    self.scaler = scaler\n",
        "\n",
        "  def getFeatureSelectionCVMetric(self):\n",
        "    \"\"\"Return metric for feature selection using K-fold validation\"\"\"\n",
        "    return self.fs_cv_metric\n",
        "\n",
        "  def setFeatureSelectionCVMetric(self, fs_metric):\n",
        "    \"\"\"Return metric for feature selection using K-fold validation\"\"\"\n",
        "    assert fs_metric in self.eval_metrics\n",
        "    self.fs_cv_metric = fs_metric\n",
        "\n",
        "  def getAlgorithmTrainingCVMetric(self):\n",
        "    \"\"\"Return metric for hyperparameter optimization using K-fold validation\"\"\"\n",
        "    return self.est_cv_metric\n",
        "\n",
        "  def setFeatureSelectionCVMetric(self, est_metric):\n",
        "    \"\"\"Return metric for hyperparameter optimization using K-fold validation\"\"\"\n",
        "    assert est_metric in self.eval_metrics\n",
        "    self.est_cv_metric = est_metric\n",
        "\n",
        "  def getFeatureSelectors(self, num_features):\n",
        "    \"\"\"Feature selection methods\"\"\"\n",
        "    # already defined?\n",
        "    if (len(self.feature_selectors) > 0):\n",
        "      return self.feature_selectors\n",
        "\n",
        "    # Early season prediction can have less than 10 features\n",
        "    min_features = 10\n",
        "    if (num_features < 10):\n",
        "      min_features = num_features - 1\n",
        "\n",
        "    rf = RandomForestRegressor(bootstrap=True, max_features='log2', ccp_alpha=1e-2,\n",
        "                               max_depth=10, min_samples_leaf=10, n_estimators=500,\n",
        "                               random_state=42, oob_score=True)\n",
        "\n",
        "    lasso = Lasso(copy_X=True, fit_intercept=True, normalize=False,\n",
        "                  tol=1e-3, random_state=42, selection='random')\n",
        "\n",
        "    self.feature_selectors = {\n",
        "      # random forest\n",
        "      'random_forest' : {\n",
        "          'selector' : SelectFromModel(rf, threshold='median'),\n",
        "          'param_space' : {\n",
        "              \"selector__estimator__min_samples_leaf\" : Integer(5, 20),\n",
        "              # \"selector__estimator__n_estimators\" : Integer(100, 500),\n",
        "              # \"selector__estimator__max_depth\" : Integer(5, 10),\n",
        "              \"selector__max_features\" : Integer(min_features, num_features),\n",
        "          }\n",
        "      },\n",
        "      # recursive feature elimination using Lasso\n",
        "      'RFE_Lasso' : {\n",
        "          'selector' : RFE(lasso),\n",
        "          'param_space' : {\n",
        "              \"selector__estimator__alpha\" : Real(1e-2, 1e+1, prior='log-uniform'),\n",
        "              \"selector__n_features_to_select\" : Integer(min_features, num_features),\n",
        "          }\n",
        "      },\n",
        "      # # NOTE: Mutual info raises an error when used with spark parallel backend.\n",
        "      # univariate feature selection\n",
        "      # 'mutual_info' : {\n",
        "      #     'selector' : SelectKBest(mutual_info_regression),\n",
        "      #     'param_space' : {\n",
        "      #         \"selector__k\" : Integer(min_features, max_features),\n",
        "      #     }\n",
        "      # },\n",
        "    }\n",
        "\n",
        "    return self.feature_selectors\n",
        "\n",
        "  def setFeatureSelectors(self, ft_sel):\n",
        "    \"\"\"Set feature selection algorithms\"\"\"\n",
        "    assert isinstance(ft_sel, dict)\n",
        "    assert len(ft_sel) > 0\n",
        "    for sel in ft_sel:\n",
        "      assert isinstance(sel, dict)\n",
        "      assert 'selector' in sel\n",
        "      assert 'param_space' in sel\n",
        "      # add cases if other feature selection methods are used\n",
        "      assert (isinstance(sel['selector'], SelectKBest) or\n",
        "              isinstance(sel['selector'], SelectFromModel) or\n",
        "              isinstance(sel['selector'], RFE))\n",
        "      assert isinstance(sel['param_space'], dict)\n",
        "\n",
        "    self.feature_selectors = ft_sel\n",
        "\n",
        "  def getEstimators(self):\n",
        "    \"\"\"Return machine learning algorithms for prediction\"\"\"\n",
        "    return self.estimators\n",
        "  \n",
        "  def setEstimators(self, estimators):\n",
        "    \"\"\"Set machine learning algorithms for prediction\"\"\"\n",
        "    assert isinstance(estimators, dict)\n",
        "    assert len(estimators) > 0\n",
        "    for est in estimators:\n",
        "      assert isinstance(est, dict)\n",
        "      assert 'estimator' in est\n",
        "      assert 'param_space' in est\n",
        "      assert isinstance(est['param_space'], dict)\n",
        "\n",
        "    self.estimators = estimators\n",
        "  \n",
        "  def getEvaluationMetrics(self):\n",
        "    \"\"\"Return metrics to evaluate predictions of algorithms\"\"\"\n",
        "    return self.eval_metrics\n",
        "  \n",
        "  def setEvaluationMetrics(self, metrics):\n",
        "    assert isinstance(metrics, dict)\n",
        "    self.eval_metrics = metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKLbsRZPj_la"
      },
      "source": [
        "## Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJOtnzSYQvPu"
      },
      "source": [
        "### Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDlfa0-RtsA7"
      },
      "source": [
        "#### Data Loader Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nk-0JeNdQvP2"
      },
      "outputs": [],
      "source": [
        "#%%writefile data_loading.py\n",
        "class CYPDataLoader:\n",
        "  def __init__(self, spark, cyp_config):\n",
        "    self.spark = spark\n",
        "    self.data_path = cyp_config.getDataPath()\n",
        "    self.country_code = cyp_config.getCountryCode()\n",
        "    self.spatial_level = cyp_config.getSpatialLevel()\n",
        "    self.data_sources = cyp_config.getDataSources()\n",
        "    self.verbose = cyp_config.getDebugLevel()\n",
        "    self.data_dfs = {}\n",
        "\n",
        "  def loadFromCSVFile(self, data_path, src, spatial_level, country_code):\n",
        "    \"\"\"\n",
        "    The implied filename for each source is:\n",
        "    <data_source>_<spatial_level>_<country_code>.csv\n",
        "    Examples: WOFOST_NUTS2_NL.csv.\n",
        "    Schema is inferred from the file. We might want to specify the schema at some point.\n",
        "    \"\"\"\n",
        "    if (country_code is not None):\n",
        "      datafile = data_path + '/' + src  + '_' + spatial_level + '_' + country_code + '.csv'\n",
        "    elif (spatial_level is not None):\n",
        "      datafile = data_path + '/' + src  + '_' + spatial_level + '.csv'\n",
        "    else:\n",
        "      datafile = data_path + '/' + src  + '.csv'\n",
        "\n",
        "    if (self.verbose > 1):\n",
        "      print('Data file name', '\"' + datafile + '\"')\n",
        "\n",
        "    df = self.spark.read.csv(datafile, header = True, inferSchema = True)\n",
        "    return df\n",
        "\n",
        "  def loadData(self, src, spatial_level):\n",
        "    \"\"\"\n",
        "    Load data for a specific data source.\n",
        "    spatial_level may one level or a list of levels.\n",
        "    \"\"\"\n",
        "    data_path = self.data_path\n",
        "    country_code = self.country_code\n",
        "    assert src in self.data_sources\n",
        "\n",
        "    if (isinstance(spatial_level, list)):\n",
        "      src_dfs = []\n",
        "      for lvl in spatial_level:\n",
        "        df = self.loadFromCSVFile(data_path, src, lvl, country_code)\n",
        "        src_dfs.append(df)\n",
        "\n",
        "    else:\n",
        "      src_dfs = self.loadFromCSVFile(data_path, src, spatial_level, country_code)\n",
        "\n",
        "    return src_dfs\n",
        "\n",
        "  def loadAllData(self):\n",
        "    \"\"\"\n",
        "    NOT SUPPORTED:\n",
        "    1. Schema is not defined.\n",
        "    2. Loading data for multiple countries.\n",
        "    3. Loading data from folders.\n",
        "    Ioannis: Spark has a nice way of loading several files from a folder,\n",
        "    and associating the file name on each record, using the function\n",
        "    input_file_name. This allows to extract medatada from the path\n",
        "    into the dataframe. In your case it could be the country name, etc.\n",
        "    \"\"\"\n",
        "    data_dfs = {}\n",
        "    for src in self.data_sources:\n",
        "      spatial_level = self.spatial_level\n",
        "      if (isinstance(self.data_sources, dict)):\n",
        "        spatial_level = self.data_sources[src]\n",
        "\n",
        "      if ('METEO' in src):\n",
        "        data_dfs['METEO'] = self.loadData(src, spatial_level)\n",
        "      else:\n",
        "        data_dfs[src] = self.loadData(src, spatial_level)\n",
        "\n",
        "    if (self.verbose > 1):\n",
        "      data_sources_str = ''\n",
        "      for src in data_dfs:\n",
        "        data_sources_str = data_sources_str + src + ', '\n",
        "\n",
        "      # remove the comma and space from the end\n",
        "      print('Loaded data:', data_sources_str[:-2])\n",
        "      print('\\n')\n",
        "\n",
        "    return data_dfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30v2QIEh2K9P"
      },
      "source": [
        "#### Data Preprocessor Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qo2OHq0x2ID1"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Window\n",
        "\n",
        "class CYPDataPreprocessor:\n",
        "  def __init__(self, spark, cyp_config):\n",
        "    self.spark = spark\n",
        "    self.verbose = cyp_config.getDebugLevel()\n",
        "\n",
        "  def extractYearDekad(self, df):\n",
        "    \"\"\"Extract year and dekad from date_col in yyyyMMdd format.\"\"\"\n",
        "    # Conversion to string type is required to make getYear(), getMonth() etc. work correctly.\n",
        "    # They use to_date() function to verify valid dates and to_date() expects the date column to be string.\n",
        "    df = df.withColumn('DATE', df['DATE'].cast(\"string\"))\n",
        "    df = df.select('*',\n",
        "                   getYear('DATE').alias('FYEAR'),\n",
        "                   getDekad('DATE').alias('DEKAD'))\n",
        "\n",
        "    # Bring FYEAR, DEKAD to the front\n",
        "    col_order = df.columns[:2] + df.columns[-2:] + df.columns[2:-2]\n",
        "    df = df.select(col_order).drop('DATE')\n",
        "    return df\n",
        "\n",
        "  def preprocessCSSF(self, cssf_df):\n",
        "    cssf_df = cssf_df.withColumnRenamed('year', 'FYEAR')\n",
        "    cssf_df = cssf_df.withColumnRenamed('dekad', 'DEKAD')\n",
        "    cssf_df = cssf_df.drop(*['LONTD', 'LATTD'])\n",
        "\n",
        "    return cssf_df\n",
        "\n",
        "  def preprocessMeteo(self, meteo_df):\n",
        "    \"\"\"\n",
        "    Calculate CWB.\n",
        "    \"\"\"\n",
        "    meteo_df = meteo_df.withColumn('CWB',\n",
        "                                   SparkF.bround(meteo_df['PREC'] - meteo_df['ET0'], 2))\n",
        "\n",
        "    meteo_df = meteo_df.withColumnRenamed('year', 'FYEAR')\n",
        "    meteo_df = meteo_df.withColumnRenamed('dekad', 'DEKAD')\n",
        "    meteo_df = meteo_df.drop(*['LONTD', 'LATTD'])\n",
        "\n",
        "    return meteo_df\n",
        "\n",
        "  def preprocessRemoteSensing(self, rs_df):\n",
        "    rs_df = rs_df.withColumnRenamed('year', 'FYEAR')\n",
        "    rs_df = rs_df.withColumnRenamed('dekad', 'DEKAD')\n",
        "    rs_df = rs_df.drop(*['LONTD', 'LATTD'])\n",
        "\n",
        "    return rs_df\n",
        "\n",
        "  def preprocessSoil(self, soil_df, id_col):\n",
        "    # SM_WHC = water holding capacity\n",
        "    soil_df = soil_df.select([id_col, 'SM_WHC'])\n",
        "\n",
        "    return soil_df\n",
        "\n",
        "  def preprocessCropArea(self, area_df):\n",
        "    area_df = area_df.withColumn(\"FYEAR\", area_df[\"FYEAR\"].cast(SparkT.IntegerType()))\n",
        "    # convert crop area into ha\n",
        "    area_df = area_df.withColumn('CROP_AREA', SparkF.round(area_df['CROP_AREA'] * 0.404686, 3))\n",
        "\n",
        "    return area_df\n",
        "\n",
        "  def preprocessYield(self, yield_df, spatial_level='GRIDS'):\n",
        "    \"\"\"\n",
        "    Convert county yields to t/ha.\n",
        "    See https://www.ndwheat.com/buyers/chartsandstats/\n",
        "    \"\"\"\n",
        "    yield_df = yield_df.withColumn(\"YIELD\", yield_df[\"YIELD\"].cast(SparkT.FloatType()))\n",
        "    if (spatial_level == 'COUNTY'):\n",
        "      yield_df = yield_df.withColumn('YIELD', SparkF.round(yield_df['YIELD'] * 0.06725, 3))\n",
        " \n",
        "    yield_df = yield_df.filter(yield_df['YIELD'] > 0.0)\n",
        "    return yield_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2QR2OmysFd1"
      },
      "source": [
        "#### Run Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ajXCUkjsIfK"
      },
      "outputs": [],
      "source": [
        "#%%writefile run_data_preprocessing.py\n",
        "def printPreprocessingInformation(df, data_source, id_col, order_cols, crop_season=None):\n",
        "  \"\"\"Print preprocessed data and additional debug information\"\"\"\n",
        "  df_regions = [reg[0] for reg in df.select(id_col).distinct().collect()]\n",
        "  print(data_source , 'data available for', len(df_regions), id_col[:-3])\n",
        "  if (crop_season is not None):\n",
        "    print('Season end information')\n",
        "    crop_season.orderBy([id_col, 'FYEAR']).show(10)\n",
        "\n",
        "  print(data_source, 'data')\n",
        "  df.orderBy(order_cols).show(10)\n",
        "\n",
        "def preprocessData(cyp_config, cyp_preprocessor, data_dfs):\n",
        "  crop_id = cyp_config.getCropID()\n",
        "  spatial_level = cyp_config.getSpatialLevel()\n",
        "  season_crosses_calyear = cyp_config.seasonCrossesCalendarYear()\n",
        "  clean_data = cyp_config.cleanData()\n",
        "  use_centroids = cyp_config.useCentroids()\n",
        "  use_remote_sensing = cyp_config.useRemoteSensing()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  id_col = \"COUNTY_ID\" if (spatial_level == \"COUNTY\") else \"GRID_ID\"\n",
        "\n",
        "  # County Grids mapping\n",
        "  # county_grids_df = data_dfs['COUNTY']\n",
        "  # w = Window.partitionBy('GRID_ID')\n",
        "  # county_grids_df = county_grids_df.withColumn('maxSHAPE_AREA', SparkF.max('TOTAL_AREA').over(w))\n",
        "  # county_grids_df = county_grids_df.filter(county_grids_df['TOTAL_AREA'] == county_grids_df['maxSHAPE_AREA'])\n",
        "  # county_grids_df = county_grids_df.drop(*['maxSHAPE_AREA', 'TOTAL_AREA'])\n",
        "  # ['AR', 'MO', 'TN', 'KY', 'IL', 'IN', 'KS', 'OH', 'NE', 'IA', 'PA', 'MI', 'WI', 'SD', 'MN', 'ND']\n",
        "  # county_grids_df = county_grids_df.filter(SparkF.substring(county_grids_df['COUNTY_ID'], 1, 2) == 'ND')\n",
        "  # data_dfs['COUNTY'] = county_grids_df\n",
        "\n",
        "  order_cols = [id_col, 'FYEAR', 'DEKAD']\n",
        "  # CSSF data\n",
        "  cssf_df = data_dfs['CSSF'] #.join(county_grids_df, ['GRID_ID']).drop('COUNTY_ID')\n",
        "  cssf_df = cyp_preprocessor.preprocessCSSF(cssf_df)\n",
        "  data_dfs['CSSF'] = cssf_df\n",
        "  if (debug_level > 1):\n",
        "    printPreprocessingInformation(cssf_df, 'CSSF', id_col, order_cols)\n",
        "\n",
        "  # meteo data\n",
        "  meteo_df = data_dfs['METEO'] #.join(county_grids_df, ['GRID_ID']).drop('COUNTY_ID')\n",
        "  meteo_df = cyp_preprocessor.preprocessMeteo(meteo_df)\n",
        "  assert (meteo_df is not None)\n",
        "  data_dfs['METEO'] = meteo_df\n",
        "  if (debug_level > 1):\n",
        "    printPreprocessingInformation(meteo_df, 'METEO', id_col, order_cols)\n",
        " \n",
        "  # remote sensing data\n",
        "  rs_df = None\n",
        "  if (use_remote_sensing):\n",
        "    rs_df = data_dfs['REMOTE_SENSING'] #.join(county_grids_df, ['GRID_ID']).drop('COUNTY_ID')\n",
        "    rs_df = cyp_preprocessor.preprocessRemoteSensing(rs_df)\n",
        "    assert (rs_df is not None)\n",
        "    data_dfs['REMOTE_SENSING'] = rs_df\n",
        "    if (debug_level > 1):\n",
        "      printPreprocessingInformation(rs_df, 'REMOTE_SENSING', id_col, order_cols)\n",
        "\n",
        "  # soil data\n",
        "  soil_df = data_dfs['SOIL'] #.join(county_grids_df, ['GRID_ID']).drop('COUNTY_ID')\n",
        "  soil_df = cyp_preprocessor.preprocessSoil(soil_df, id_col)\n",
        "  data_dfs['SOIL'] = soil_df\n",
        "  order_cols = [id_col]\n",
        "  if (debug_level > 1):\n",
        "    printPreprocessingInformation(soil_df, 'SOIL', id_col, order_cols)\n",
        "\n",
        "  order_cols = [id_col, 'FYEAR']\n",
        "  # yield_data\n",
        "  yield_df = data_dfs['YIELD']\n",
        "  # if (debug_level > 1):\n",
        "  #   print('Yield before preprocessing')\n",
        "  #   yield_df.show(10)\n",
        "\n",
        "  yield_df = cyp_preprocessor.preprocessYield(yield_df, spatial_level)\n",
        "  # yield_df = yield_df.filter(SparkF.substring(yield_df['COUNTY_ID'], 1, 2) == 'ND')\n",
        "  assert (yield_df is not None)\n",
        "  data_dfs['YIELD'] = yield_df #.drop('COUNTY_ID')\n",
        "\n",
        "  if (debug_level > 1):\n",
        "    # print('Yield after preprocessing')\n",
        "    yield_df.show(10)\n",
        "\n",
        "  return data_dfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krHPVXls8MRd"
      },
      "source": [
        "### Training and Test Split\n",
        "\n",
        "**Custom validation splits**\n",
        "\n",
        "When yield trend is used, custom sliding validation split is used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YPH051S8MRf"
      },
      "source": [
        "#### Training and Test Splitter Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zBSEGkO8MRf"
      },
      "outputs": [],
      "source": [
        "#%%writefile train_test_sets.py\n",
        "import numpy as np\n",
        "\n",
        "class CYPTrainTestSplitter:\n",
        "  def __init__(self, cyp_config):\n",
        "    self.use_yield_trend = cyp_config.useYieldTrend()\n",
        "    self.test_fraction = cyp_config.getTestFraction()\n",
        "    self.verbose = cyp_config.getDebugLevel()\n",
        "\n",
        "  def getTestYears(self, all_years, test_fraction=None, use_yield_trend=None):\n",
        "    num_years = len(all_years)\n",
        "    test_years = []\n",
        "    if (test_fraction is None):\n",
        "      test_fraction = self.test_fraction\n",
        "\n",
        "    if (use_yield_trend is None):\n",
        "      use_yield_trend = self.use_yield_trend\n",
        "\n",
        "    if (use_yield_trend):\n",
        "      # If test_year_start 15, years with index >= 15 are added to the test set\n",
        "      test_year_start = num_years - np.floor(num_years * test_fraction).astype('int')\n",
        "      test_years = all_years[test_year_start:]\n",
        "    else:\n",
        "      # If test_year_pos = 5, every 5th year is added to test set.\n",
        "      # indices start with 0, so test_year_pos'th year has index (test_year_pos - 1)\n",
        "      test_year_pos = np.floor(1/test_fraction).astype('int')\n",
        "      test_years = all_years[test_year_pos - 1::test_year_pos]\n",
        "\n",
        "    return test_years\n",
        "\n",
        "  def trainTestSplit(self, yield_df, test_fraction=None, use_yield_trend=None):\n",
        "    all_years = sorted([yr[0] for yr in yield_df.select('FYEAR').distinct().collect()])\n",
        "    test_years = self.getTestYears(all_years, test_fraction, use_yield_trend)\n",
        "\n",
        "    return test_years\n",
        "\n",
        "  # Returns an array containings tuples (train_idxs, test_idxs) for each fold\n",
        "  # NOTE Y_train should include id_col, FYEAR as first two columns.\n",
        "  def customKFoldValidationSplit(self, Y_train_full, num_folds, log_fh=None):\n",
        "    \"\"\"\n",
        "    Custom K-fold Validation Splits:\n",
        "    When using yield trend, we cannot do k-fold cross-validation. The custom\n",
        "    K-Fold validation splits data in time-ordered fashion. The test data\n",
        "    always comes after the training data.\n",
        "    \"\"\"\n",
        "    all_years = sorted(np.unique(Y_train_full[:, 1]))\n",
        "    num_years = len(all_years)\n",
        "    num_test_years = 1\n",
        "    num_train_years = num_years - (num_test_years * num_folds)\n",
        "\n",
        "    custom_cv = []\n",
        "    custom_split_info = '\\nCustom sliding validation train, test splits'\n",
        "    custom_split_info += '\\n----------------------------------------------'\n",
        "\n",
        "    cv_test_years = []\n",
        "    for k in range(num_folds):\n",
        "      test_years_start = num_train_years + (k * num_test_years)\n",
        "      k_train_years = all_years[:test_years_start]\n",
        "      k_test_years = all_years[test_years_start:test_years_start + num_test_years]\n",
        "      cv_test_years += k_test_years\n",
        "      test_indexes = np.ravel(np.nonzero(np.isin(Y_train_full[:, 1], k_test_years)))\n",
        "      train_indexes = np.ravel(np.nonzero(np.isin(Y_train_full[:, 1], k_train_years)))\n",
        "      custom_cv.append(tuple((train_indexes, test_indexes)))\n",
        "\n",
        "      k_train_years = [str(y) for y in k_train_years]\n",
        "      k_test_years = [str(y) for y in k_test_years]\n",
        "      custom_split_info += '\\nValidation set ' + str(k + 1) + ' training years: ' + ', '.join(k_train_years)\n",
        "      custom_split_info += '\\nValidation set ' + str(k + 1) + ' test years: ' + ', '.join(k_test_years)\n",
        "\n",
        "    custom_split_info += '\\n'\n",
        "    if (log_fh is not None):\n",
        "      log_fh.write(custom_split_info)\n",
        "\n",
        "    if (self.verbose > 1):\n",
        "      print(custom_split_info)\n",
        "\n",
        "    return custom_cv, cv_test_years"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URFR5pOxO3XZ"
      },
      "source": [
        "#### Run Training and Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TO9qzRVO3Xb"
      },
      "outputs": [],
      "source": [
        "#%%writefile run_train_test_split.py\n",
        "def printTrainTestSplits(train_df, test_df, src, order_cols):\n",
        "  \"\"\"Print Training and Test Splits\"\"\"\n",
        "  print('\\n', src, 'training data')\n",
        "  train_df.orderBy(order_cols).show(5)\n",
        "  print('\\n', src, 'test data')\n",
        "  test_df.orderBy(order_cols).show(5)\n",
        "\n",
        "# Training, Test Split\n",
        "# --------------------\n",
        "def splitTrainingTest(df, year_col, test_years):\n",
        "  \"\"\"Splitting given df into training and test dataframes.\"\"\"\n",
        "  train_df = df.filter(~df[year_col].isin(test_years))\n",
        "  test_df = df.filter(df[year_col].isin(test_years))\n",
        "\n",
        "  return [train_df, test_df]\n",
        "\n",
        "def splitDataIntoTrainingTestSets(cyp_config, preprocessed_dfs, log_fh):\n",
        "  \"\"\"\n",
        "  Split preprocessed data into training and test sets based on\n",
        "  availability of yield data.\n",
        "  \"\"\"\n",
        "  country_code = cyp_config.getCountryCode()\n",
        "  spatial_level = cyp_config.getSpatialLevel()\n",
        "  use_centroids = cyp_config.useCentroids()\n",
        "  use_remote_sensing = cyp_config.useRemoteSensing()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  id_col = \"COUNTY_ID\" if (spatial_level == \"COUNTY\") else \"GRID_ID\"\n",
        "\n",
        "  yield_df = preprocessed_dfs['YIELD']\n",
        "  train_test_splitter = CYPTrainTestSplitter(cyp_config)\n",
        "  test_years_info = '\\nTest years:'\n",
        "  test_years = train_test_splitter.trainTestSplit(yield_df)\n",
        "  test_years_info += '\\n' + ', '.join([str(y) for y in sorted(test_years)])\n",
        "\n",
        "  # log_fh.write(test_years_info + '\\n')\n",
        "  print(test_years_info)\n",
        "\n",
        "  # Times series data used for feature design.\n",
        "  ts_data_sources = {\n",
        "      'CSSF' : preprocessed_dfs['CSSF'],\n",
        "      'METEO' : preprocessed_dfs['METEO'],\n",
        "  }\n",
        "\n",
        "  if (use_remote_sensing):\n",
        "    ts_data_sources['REMOTE_SENSING'] = preprocessed_dfs['REMOTE_SENSING']\n",
        "\n",
        "  train_test_dfs = {}\n",
        "  for ts_src in ts_data_sources:\n",
        "    train_test_dfs[ts_src] = splitTrainingTest(ts_data_sources[ts_src], 'FYEAR', test_years)\n",
        "\n",
        "  # SOIL, GAES and CENTROIDS data are static.\n",
        "  train_test_dfs['SOIL'] = [preprocessed_dfs['SOIL'], preprocessed_dfs['SOIL']]\n",
        "\n",
        "  # yield data\n",
        "  yield_df = yield_df.drop('COUNTRY')\n",
        "  train_test_dfs['YIELD'] = splitTrainingTest(yield_df, 'FYEAR', test_years)\n",
        "\n",
        "  if (debug_level > 2):\n",
        "    for src in train_test_dfs:\n",
        "      if (src in ts_data_sources):\n",
        "        order_cols = [id_col, 'FYEAR', 'DEKAD']\n",
        "      elif ((src == 'YIELD') or (src == 'CROP_AREA')):\n",
        "        order_cols = [id_col, 'FYEAR']\n",
        "      else:\n",
        "        order_cols = [id_col]\n",
        "\n",
        "      train_df = train_test_dfs[src][0]\n",
        "      test_df = train_test_dfs[src][1]\n",
        "      printTrainTestSplits(train_df, test_df, src, order_cols)\n",
        "\n",
        "  return train_test_dfs, test_years"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCe_4_v--Ukb"
      },
      "source": [
        "### Data Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaSHuv0du9qf"
      },
      "source": [
        "#### Data Summarizer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TD_QGQJs-Ukl"
      },
      "outputs": [],
      "source": [
        "#%%writefile data_summary.py\n",
        "from pyspark.sql import Window\n",
        "import functools\n",
        "\n",
        "class CYPDataSummarizer:\n",
        "  def __init__(self, cyp_config):\n",
        "    self.verbose = cyp_config.getDebugLevel()\n",
        "\n",
        "  def cssfDVSSummary(self, cssf_df, id_col, early_season_end=None):\n",
        "    \"\"\"\n",
        "    Summary of crop calendar based on DVS.\n",
        "    Early season end is relative to end of the season, hence a negative number.\n",
        "    \"\"\"\n",
        "    join_cols = [id_col, 'FYEAR']\n",
        "    dvs_summary = cssf_df.select(join_cols).distinct()\n",
        "\n",
        "    # cssf_df = cssf_df.withColumn('SEASON_ALIGN', cssf_df['CAMPAIGN_DEKAD'] - cssf_df['DEKAD'])\n",
        "    dvs_summary = dvs_summary.join(cssf_df.filter(cssf_df['DVS'] > 0.0).groupBy(join_cols)\\\n",
        "                                   .agg(SparkF.min('DEKAD').alias('START_DVS')), join_cols)\n",
        "    dvs_summary = dvs_summary.join(cssf_df.filter(cssf_df['DVS'] >= 1.0).groupBy(join_cols)\\\n",
        "                                   .agg(SparkF.min('DEKAD').alias('START_DVS1')), join_cols)\n",
        "    dvs_summary = dvs_summary.join(cssf_df.filter(cssf_df['DVS'] >= 2.0).groupBy(join_cols)\\\n",
        "                                   .agg(SparkF.min('DEKAD').alias('START_DVS2')), join_cols)\n",
        "    # dvs_summary = dvs_summary.join(cssf_df.filter(cssf_df['DVS'] >= 2.0).groupBy(join_cols)\\\n",
        "    #                                .agg(SparkF.min('DEKAD').alias('HARVEST')), join_cols)\n",
        "    # dvs_summary = dvs_summary.join(cssf_df.filter(cssf_df['DVS'] >= 2.0).groupBy(join_cols)\\\n",
        "    #                                .agg(SparkF.max('SEASON_ALIGN').alias('SEASON_ALIGN')), join_cols)\n",
        "\n",
        "    # Calendar year end season and early season dekads for comparing with MCYFS\n",
        "    # Campaign year early season dekad to filter data during feature design\n",
        "    # dvs_summary = dvs_summary.withColumn('CALENDAR_END_SEASON', dvs_summary['HARVEST'] + 1)\n",
        "    dvs_summary = dvs_summary.withColumn('CAMPAIGN_EARLY_SEASON',\n",
        "                                         dvs_summary['START_DVS2'] + 1)\n",
        "    if (early_season_end is not None):\n",
        "      # dvs_summary = dvs_summary.withColumn('CALENDAR_EARLY_SEASON',\n",
        "      #                                    dvs_summary['CALENDAR_END_SEASON'] + early_season_end)\n",
        "      dvs_summary = dvs_summary.withColumn('CAMPAIGN_EARLY_SEASON',\n",
        "                                           dvs_summary['CAMPAIGN_EARLY_SEASON'] + early_season_end)\n",
        "\n",
        "    # dvs_summary = dvs_summary.drop('HARVEST', 'SEASON_ALIGN')\n",
        "\n",
        "    return dvs_summary\n",
        "\n",
        "  def indicatorsSummary(self, df, id_col, min_cols, max_cols, avg_cols):\n",
        "    \"\"\"long term min, max and avg values of selected indicators by region\"\"\"\n",
        "    avgs = []\n",
        "    if (avg_cols[1:]):\n",
        "      avgs = [SparkF.bround(SparkF.avg(x), 2).alias('avg(' + x + ')') for x in avg_cols[1:]]\n",
        "    \n",
        "    if (min_cols[:1]):\n",
        "      summary = df.select(min_cols).groupBy(id_col).min()\n",
        "    else:\n",
        "      summary = df.select(min_cols).groupBy(id_col)\n",
        "\n",
        "    if (max_cols[1:]):\n",
        "      summary = summary.join(df.select(max_cols).groupBy(id_col).max(), id_col)\n",
        "\n",
        "    if (avgs):\n",
        "      summary = summary.join(df.select(avg_cols).groupBy(id_col).agg(*avgs), id_col)\n",
        "    return summary\n",
        "\n",
        "  def yieldSummary(self, yield_df, id_col):\n",
        "    \"\"\"long term min, max and avg values of yield by region\"\"\"\n",
        "    select_cols = [id_col, 'YIELD']\n",
        "    yield_summary = yield_df.select(select_cols).groupBy(id_col).min('YIELD')\n",
        "    yield_summary = yield_summary.join(yield_df.select(select_cols).groupBy(id_col)\\\n",
        "                                       .agg(SparkF.max('YIELD')), id_col)\n",
        "    yield_summary = yield_summary.join(yield_df.select(select_cols).groupBy(id_col)\\\n",
        "                                       .agg(SparkF.bround(SparkF.avg('YIELD'), 2)\\\n",
        "                                            .alias('avg(YIELD)')), id_col)\n",
        "    return yield_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zym9pU3XsPrt"
      },
      "source": [
        "#### Run Data Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CNwR1WAsSLz"
      },
      "outputs": [],
      "source": [
        "#%%writefile run_data_summary.py\n",
        "def printDataSummary(df, data_source, id_col):\n",
        "  \"\"\"Print summary information\"\"\"\n",
        "  if (data_source == 'CSSF_DVS'):\n",
        "    print('Crop calender information based on CSSF data')\n",
        "    max_year = df.select('FYEAR').agg(SparkF.max('FYEAR')).collect()[0][0]\n",
        "    df.filter(df['FYEAR'] == max_year).orderBy(id_col).show(10)\n",
        "  else:\n",
        "    print(data_source, 'indicators summary')\n",
        "    df.orderBy(id_col).show()\n",
        "\n",
        "def getCSSFSummaryCols(id_col):\n",
        "  \"\"\"CSSF columns used for data summary\"\"\"\n",
        "  # only RSM has non-zero min values\n",
        "  min_cols = [id_col]\n",
        "  max_cols = [id_col] + ['TAGP', 'TWSO', 'DVS']\n",
        "  # biomass and DVS values grow over time\n",
        "  avg_cols = [id_col]\n",
        "\n",
        "  return [min_cols, max_cols, avg_cols]\n",
        "\n",
        "def getMeteoSummaryCols(id_col):\n",
        "  \"\"\"Meteo columns used for data summary\"\"\"\n",
        "  col_names = ['TMAX', 'TMIN', 'TAVG', 'PREC', 'ET0', 'CWB']\n",
        "  min_cols = [id_col] + col_names\n",
        "  max_cols = [id_col] + col_names\n",
        "  avg_cols = [id_col] + col_names\n",
        "\n",
        "  return [min_cols, max_cols, avg_cols]\n",
        "\n",
        "def getRemoteSensingSummaryCols(id_col):\n",
        "  \"\"\"Remote Sensing columns used for data summary\"\"\"\n",
        "  col_names = ['FAPAR']\n",
        "  min_cols = [id_col] + col_names\n",
        "  max_cols = [id_col] + col_names\n",
        "  avg_cols = [id_col] + col_names\n",
        "\n",
        "  return [min_cols, max_cols, avg_cols]\n",
        "\n",
        "def summarizeData(cyp_config, cyp_summarizer, train_test_dfs):\n",
        "  \"\"\"\n",
        "  Summarize data. Create DVS summary to infer crop calendar.\n",
        "  Summarize selected indicators for each data source.\n",
        "  \"\"\"\n",
        "  cssf_train_df = train_test_dfs['CSSF'][0]\n",
        "  cssf_test_df = train_test_dfs['CSSF'][1]\n",
        "  meteo_train_df = train_test_dfs['METEO'][0]\n",
        "  yield_train_df = train_test_dfs['YIELD'][0]\n",
        "  spatial_level = cyp_config.getSpatialLevel()\n",
        "  id_col = \"COUNTY_ID\" if (spatial_level == \"COUNTY\") else \"GRID_ID\"\n",
        "\n",
        "  use_remote_sensing = cyp_config.useRemoteSensing()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  early_season = cyp_config.earlySeasonPrediction()\n",
        "  early_season_end = None\n",
        "  if (early_season):\n",
        "    early_season_end = cyp_config.getEarlySeasonEndDekad()\n",
        "\n",
        "  # DVS summary (crop calendar)\n",
        "  # NOTE this summary of crops based on CSSF data should be used with caution\n",
        "  # 1. The summary is per region per year.\n",
        "  # 2. The summary is based on CSSF not real sowing and harvest dates\n",
        "  dvs_summary_train = cyp_summarizer.cssfDVSSummary(cssf_train_df, id_col, early_season_end)\n",
        "  # dvs_summary_train = dvs_summary_train.drop('CALENDAR_END_SEASON', 'CALENDAR_EARLY_SEASON')\n",
        "  dvs_summary_test = cyp_summarizer.cssfDVSSummary(cssf_test_df, id_col, early_season_end)\n",
        "  # dvs_summary_test = dvs_summary_test.drop('CALENDAR_END_SEASON', 'CALENDAR_EARLY_SEASON')\n",
        "  if (debug_level > 1):\n",
        "    printDataSummary(dvs_summary_train, 'CSSF_DVS', id_col)\n",
        "\n",
        "  summary_cols = {\n",
        "      'CSSF' : getCSSFSummaryCols(id_col),\n",
        "      'METEO' : getMeteoSummaryCols(id_col),\n",
        "  }\n",
        "\n",
        "  summary_sources_dfs = {\n",
        "      'CSSF' : cssf_train_df,\n",
        "      'METEO' : meteo_train_df,\n",
        "  }\n",
        "\n",
        "  if (use_remote_sensing):\n",
        "    rs_train_df = train_test_dfs['REMOTE_SENSING'][0]\n",
        "    summary_cols['REMOTE_SENSING'] = getRemoteSensingSummaryCols(id_col)\n",
        "    summary_sources_dfs['REMOTE_SENSING'] = rs_train_df\n",
        "\n",
        "  summary_dfs = {}\n",
        "  for sum_src in summary_sources_dfs:\n",
        "    summary_dfs[sum_src] = cyp_summarizer.indicatorsSummary(summary_sources_dfs[sum_src],\n",
        "                                                            id_col,\n",
        "                                                            summary_cols[sum_src][0],\n",
        "                                                            summary_cols[sum_src][1],\n",
        "                                                            summary_cols[sum_src][2])\n",
        "\n",
        "  for src in summary_dfs:\n",
        "    if (debug_level > 2):\n",
        "      printDataSummary(summary_dfs[src], src, id_col)\n",
        "\n",
        "  yield_summary = cyp_summarizer.yieldSummary(yield_train_df, id_col)\n",
        "  if (debug_level > 2):\n",
        "    printDataSummary(yield_summary, 'YIELD', id_col)\n",
        "\n",
        "  summary_dfs['CSSF_DVS'] = [dvs_summary_train, dvs_summary_test]\n",
        "  summary_dfs['YIELD'] = yield_summary\n",
        "\n",
        "  return summary_dfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWwazE4MJI4v"
      },
      "source": [
        "### Crop Calendar\n",
        "\n",
        "We infer crop calendar using CSSF DVS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBVBgwxSJEIn"
      },
      "outputs": [],
      "source": [
        "#%%writefile crop_calendar.py\n",
        "import numpy as np\n",
        "\n",
        "def getCropCalendarPeriods(df):\n",
        "  \"\"\"Periods for per year crop calendar\"\"\"\n",
        "  # (maximum of 4 months = 12 dekads).\n",
        "  # Subtracting 11 because both ends of the period are included.\n",
        "  # p0 : if CAMPAIGN_EARLY_SEASON > df.START_DVS\n",
        "  #        START_DVS - 11 to START_DVS\n",
        "  #      else\n",
        "  #        START_DVS - 11 to CAMPAIGN_EARLY_SEASON\n",
        "  p0_filter = SparkF.when(df.CAMPAIGN_EARLY_SEASON > df.START_DVS,\n",
        "                          (df.DEKAD >= (df.START_DVS - 11)) &\n",
        "                          (df.DEKAD <= df.START_DVS))\\\n",
        "                          .otherwise((df.DEKAD >= (df.START_DVS - 11)) &\n",
        "                                     (df.DEKAD <= df.CAMPAIGN_EARLY_SEASON))\n",
        "  # p1 : if CAMPAIGN_EARLY_SEASON > (df.START_DVS + 1)\n",
        "  #        (START_DVS - 1) to (START_DVS + 1)\n",
        "  #      else\n",
        "  #        (START_DVS - 1) to CAMPAIGN_EARLY_SEASON\n",
        "  p1_filter = SparkF.when(df.CAMPAIGN_EARLY_SEASON > (df.START_DVS + 1),\n",
        "                          (df.DEKAD >= (df.START_DVS - 1)) &\n",
        "                          (df.DEKAD <= (df.START_DVS + 1)))\\\n",
        "                          .otherwise((df.DEKAD >= (df.START_DVS - 1)) &\n",
        "                                     (df.DEKAD <= df.CAMPAIGN_EARLY_SEASON))\n",
        "  # p2 : if CAMPAIGN_EARLY_SEASON > df.START_DVS1\n",
        "  #        START_DVS to START_DVS1\n",
        "  #      else\n",
        "  #        START_DVS to CAMPAIGN_EARLY_SEASON\n",
        "  p2_filter = SparkF.when(df.CAMPAIGN_EARLY_SEASON > df.START_DVS1,\n",
        "                          (df.DEKAD >= df.START_DVS) &\n",
        "                          (df.DEKAD <= df.START_DVS1))\\\n",
        "                          .otherwise((df.DEKAD >= df.START_DVS) &\n",
        "                                     (df.DEKAD <= df.CAMPAIGN_EARLY_SEASON))\n",
        "  # p3 : if CAMPAIGN_EARLY_SEASON > (df.START_DVS1 + 1)\n",
        "  #        (START_DVS1 - 1) to (START_DVS1 + 1)\n",
        "  #      else\n",
        "  #        (START_DVS1 - 1) to CAMPAIGN_EARLY_SEASON\n",
        "  p3_filter = SparkF.when(df.CAMPAIGN_EARLY_SEASON > (df.START_DVS1 + 1),\n",
        "                          (df.DEKAD >= (df.START_DVS1 - 1)) &\n",
        "                          (df.DEKAD <= (df.START_DVS1 + 1)))\\\n",
        "                          .otherwise((df.DEKAD >= (df.START_DVS1 - 1)) &\n",
        "                                     (df.DEKAD <= df.CAMPAIGN_EARLY_SEASON))\n",
        "  # p4 : if CAMPAIGN_EARLY_SEASON > df.START_DVS2\n",
        "  #        START_DVS1 to START_DVS2\n",
        "  #      else\n",
        "  #        START_DVS1 to CAMPAIGN_EARLY_SEASON\n",
        "  p4_filter = SparkF.when(df.CAMPAIGN_EARLY_SEASON > df.START_DVS2,\n",
        "                          (df.DEKAD >= df.START_DVS1) &\n",
        "                          (df.DEKAD <= df.START_DVS2))\\\n",
        "                          .otherwise((df.DEKAD >= df.START_DVS1) &\n",
        "                                     (df.DEKAD <= df.CAMPAIGN_EARLY_SEASON))\n",
        "  # p5 : if CAMPAIGN_EARLY_SEASON > (df.START_DVS2 + 1)\n",
        "  #        (START_DVS2 - 1) to (START_DVS2 + 1)\n",
        "  #      else\n",
        "  #        (START_DVS2 - 1) to CAMPAIGN_EARLY_SEASON\n",
        "  p5_filter = SparkF.when(df.CAMPAIGN_EARLY_SEASON > (df.START_DVS2 + 1),\n",
        "                          (df.DEKAD >= (df.START_DVS2 - 1)) &\n",
        "                          (df.DEKAD <= (df.START_DVS2 + 1)))\\\n",
        "                          .otherwise((df.DEKAD >= (df.START_DVS2 - 1)) &\n",
        "                                     (df.DEKAD <= df.CAMPAIGN_EARLY_SEASON))\n",
        "\n",
        "  cc_periods = {\n",
        "      'p0' : p0_filter,\n",
        "      'p1' : p1_filter,\n",
        "      'p2' : p2_filter,\n",
        "      'p3' : p3_filter,\n",
        "      'p4' : p4_filter,\n",
        "      'p5' : p5_filter,\n",
        "  }\n",
        "\n",
        "  return cc_periods\n",
        "\n",
        "def getSeasonStartFilter(df):\n",
        "  \"\"\"Filter for dekads after season start\"\"\"\n",
        "  return df.DEKAD >= (df.START_DVS - 11)\n",
        "\n",
        "def getCropCalendar(cyp_config, dvs_summary, log_fh):\n",
        "  \"\"\"Use DVS summary to infer the crop calendar\"\"\"\n",
        "  pd_dvs_summary = dvs_summary.toPandas()\n",
        "  early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "\n",
        "  avg_dvs_start = np.round(pd_dvs_summary['START_DVS'].mean(), 0)\n",
        "  avg_dvs1_start = np.round(pd_dvs_summary['START_DVS1'].mean(), 0)\n",
        "  avg_dvs2_start = np.round(pd_dvs_summary['START_DVS2'].mean(), 0)\n",
        "\n",
        "  # We look at 6 windows\n",
        "  # 0. Preplanting window (maximum of 4 months = 12 dekads).\n",
        "  # Subtracting 11 because both ends of the period are included.\n",
        "  p0_start = 1 if (avg_dvs_start - 11) < 1 else (avg_dvs_start - 11)\n",
        "  p0_end = avg_dvs_start\n",
        "\n",
        "  # 1. Planting window\n",
        "  p1_start = avg_dvs_start - 1\n",
        "  p1_end = avg_dvs_start + 1\n",
        "\n",
        "  # 2. Vegetative phase\n",
        "  p2_start = avg_dvs_start\n",
        "  p2_end = avg_dvs1_start\n",
        "\n",
        "  # 3. Flowering phase\n",
        "  p3_start = avg_dvs1_start - 1\n",
        "  p3_end = avg_dvs1_start + 1\n",
        "\n",
        "  # 4. Yield formation phase\n",
        "  p4_start = avg_dvs1_start\n",
        "  p4_end = avg_dvs2_start\n",
        "\n",
        "  # 5. Harvest window\n",
        "  p5_start = avg_dvs2_start - 1\n",
        "  p5_end = avg_dvs2_start + 1\n",
        "\n",
        "  early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "  early_season_end = 36\n",
        "  if (early_season_prediction):\n",
        "    early_season_end = np.round(pd_dvs_summary['CAMPAIGN_EARLY_SEASON'].mean(), 0)\n",
        "    p0_end = early_season_end if (p0_end > early_season_end) else p0_end\n",
        "    p1_end = early_season_end if (p1_end > early_season_end) else p1_end\n",
        "    p2_end = early_season_end if (p2_end > early_season_end) else p2_end\n",
        "    p3_end = early_season_end if (p3_end > early_season_end) else p3_end\n",
        "    p4_end = early_season_end if (p4_end > early_season_end) else p4_end\n",
        "    p5_end = early_season_end if (p5_end > early_season_end) else p5_end\n",
        "\n",
        "  crop_cal = {}\n",
        "  if (p0_end > p0_start):\n",
        "    crop_cal['p0'] = { 'desc' : 'pre-planting window', 'start' : p0_start, 'end' : p0_end }\n",
        "  if (p1_end > p1_start):\n",
        "    crop_cal['p1'] = { 'desc' : 'planting window', 'start' : p1_start, 'end' : p1_end }\n",
        "  if (p2_end > p2_start):\n",
        "    crop_cal['p2'] = { 'desc' : 'vegetative phase', 'start' : p2_start, 'end' : p2_end }\n",
        "  if (p3_end > p3_start):\n",
        "    crop_cal['p3'] = { 'desc' : 'flowering phase', 'start' : p3_start, 'end' : p3_end }\n",
        "  if (p4_end > p4_start):\n",
        "    crop_cal['p4'] = { 'desc' : 'yield formation phase', 'start' : p4_start, 'end' : p4_end }\n",
        "  if (p5_end > p5_start):\n",
        "    crop_cal['p5'] = { 'desc' : 'harvest window', 'start' : p5_start, 'end' : p5_end }\n",
        "\n",
        "  if (early_season_prediction):\n",
        "    early_season_rel_harvest = cyp_config.getEarlySeasonEndDekad()\n",
        "    early_season_info = '\\nEarly Season Prediction Dekad: ' + str(early_season_rel_harvest)\n",
        "    early_season_info += ', Campaign Dekad: ' + str(early_season_end)\n",
        "    log_fh.write(early_season_info + '\\n')\n",
        "    if (debug_level > 1):\n",
        "      print(early_season_info)\n",
        "\n",
        "  crop_cal_info = '\\nCrop Calendar'\n",
        "  crop_cal_info += '\\n-------------'\n",
        "  for p in crop_cal:\n",
        "    crop_cal_info += '\\nPeriod ' + p + ' (' + crop_cal[p]['desc'] + '): '\n",
        "    crop_cal_info += 'Campaign Dekads ' + str(crop_cal[p]['start']) + '-' + str(crop_cal[p]['end'])\n",
        "\n",
        "  log_fh.write(crop_cal_info + '\\n')\n",
        "  if (debug_level > 1):\n",
        "    print(crop_cal_info)\n",
        "\n",
        "  return crop_cal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QokglosfnQax"
      },
      "source": [
        "### Feature Design\n",
        "\n",
        "For CSSF, Meteo and Remote Sensing features, we aggregate indicators or count days/dekads with indicators above or below some thresholds. We use 4 thresholds: +/- 1 STD and +/- 2STD above or below the average.\n",
        "\n",
        "We determine the start and end dekads using crop calendar inferred from CSSF DVS summary. In the case of early season prediction, end dekad is set to the prediction dekad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHVsi9dzv4Pz"
      },
      "source": [
        "#### Featurizer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e94zGLjVnQa0"
      },
      "outputs": [],
      "source": [
        "#%%writefile feature_design.py\n",
        "from pyspark.sql import Window\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import functools\n",
        "\n",
        "class CYPFeaturizer:\n",
        "  def __init__(self, cyp_config):\n",
        "    self.use_per_year_cc = cyp_config.usePerYearCropCalendar()\n",
        "    self.use_features_v2 = cyp_config.useFeaturesV2()\n",
        "    self.verbose = cyp_config.getDebugLevel()\n",
        "    self.lt_stats = {}\n",
        "\n",
        "  def extractFeatures(self, df, data_source, crop_cal,\n",
        "                      max_cols, avg_cols, cum_avg_cols, extreme_cols,\n",
        "                      join_cols, fit=False):\n",
        "    \"\"\"\n",
        "    Extract aggregate and extreme features.\n",
        "    If fit=True, compute and save long-term stats.\n",
        "    \"\"\"\n",
        "    df = df.join(crop_cal, join_cols)\n",
        "    df = df.withColumn('COUNTRY', SparkF.lit('US'))\n",
        "\n",
        "    # Calculate cumulative sums\n",
        "    if (self.use_features_v2 and cum_avg_cols):\n",
        "      w = Window.partitionBy(join_cols).orderBy('DEKAD')\\\n",
        "                .rangeBetween(Window.unboundedPreceding, 0)\n",
        "      after_season_start = getSeasonStartFilter(df)\n",
        "      for c in cum_avg_cols:\n",
        "        df = df.withColumn(c, SparkF.sum(SparkF.when(after_season_start, df[c])\\\n",
        "                                         .otherwise(0.0)).over(w))\n",
        "\n",
        "    cc_periods = getCropCalendarPeriods(df)\n",
        "    aggrs = []\n",
        "    # max aggregation\n",
        "    for p in max_cols:\n",
        "      if (max_cols[p]):\n",
        "        aggrs += [SparkF.bround(SparkF.max(SparkF.when(cc_periods[p], df[x])), 2)\\\n",
        "                  .alias('max' + x + p) for x in max_cols[p] ]\n",
        "\n",
        "    # avg aggregation\n",
        "    for p in avg_cols:\n",
        "      if (avg_cols[p]):\n",
        "        aggrs += [SparkF.bround(SparkF.avg(SparkF.when(cc_periods[p], df[x])), 2)\\\n",
        "                  .alias('avg' + x + p) for x in avg_cols[p] ]\n",
        "\n",
        "    # if not computing extreme features, we can return\n",
        "    if (not extreme_cols):\n",
        "      ft_df = df.groupBy(join_cols).agg(*aggrs)\n",
        "      return ft_df\n",
        "\n",
        "    # compute long-term stats and save them\n",
        "    if (fit):\n",
        "      stat_aggrs = []\n",
        "      for p in extreme_cols:\n",
        "        if (extreme_cols[p]):\n",
        "          stat_aggrs += [ SparkF.bround(SparkF.avg(SparkF.when(cc_periods[p], df[x])), 2)\\\n",
        "                         .alias('avg' + x + p) for x in extreme_cols[p] ]\n",
        "          stat_aggrs += [ SparkF.bround(SparkF.stddev(SparkF.when(cc_periods[p], df[x])), 2)\\\n",
        "                         .alias('std' + x + p) for x in extreme_cols[p] ]\n",
        "\n",
        "      if (stat_aggrs):\n",
        "        lt_stats = df.groupBy('COUNTRY').agg(*stat_aggrs)\n",
        "        self.lt_stats[data_source] = lt_stats\n",
        "\n",
        "    if (data_source in self.lt_stats):\n",
        "      df = df.join(SparkF.broadcast(self.lt_stats[data_source]), 'COUNTRY')\n",
        "\n",
        "    # features for extreme conditions\n",
        "    for p in extreme_cols:\n",
        "      if (extreme_cols[p]):\n",
        "        if (self.use_features_v2):\n",
        "          # sum zscore for values < long-term average\n",
        "          aggrs += [ SparkF.bround(SparkF.sum(SparkF.when(((df[x] - df['avg' + x + p]) < 0) & cc_periods[p],\n",
        "                                                          (df['avg' + x + p] - df[x]) / df['std' + x + p])), 2)\\\n",
        "                     .alias('Z-' + x + p) for x in extreme_cols[p] ]\n",
        "          # sum zscore for values > long-term average\n",
        "          aggrs += [ SparkF.bround(SparkF.sum(SparkF.when(((df[x] - df['avg' + x + p]) > 0) & cc_periods[p],\n",
        "                                                          (df[x] - df['avg' + x + p]) / df['std' + x + p])), 2)\\\n",
        "                     .alias('Z+' + x + p) for x in extreme_cols[p] ]\n",
        "\n",
        "        else:\n",
        "          # Count of days or dekads with values crossing threshold\n",
        "          for i in range(1, 3):\n",
        "            aggrs += [ SparkF.sum(SparkF.when((df[x] > (df['avg' + x + p] + i * df['std' + x + p])) &\n",
        "                                              cc_periods[p], 1))\\\n",
        "                      .alias(x + p + 'gt' + str(i) + 'STD') for x in extreme_cols[p] ]\n",
        "            aggrs += [ SparkF.sum(SparkF.when((df[x] < (df['avg' + x + p] - i * df['std' + x + p])) &\n",
        "                                              cc_periods[p], 1))\\\n",
        "                      .alias(x + p + 'lt' + str(i) + 'STD') for x in extreme_cols[p] ]\n",
        "\n",
        "    ft_df = df.groupBy(join_cols).agg(*aggrs)\n",
        "    ft_df = ft_df.na.fill(0.0)\n",
        "    return ft_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CZS6KDg8raw"
      },
      "source": [
        "#### Yield Trend Estimator Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2O70-hQ8rax"
      },
      "outputs": [],
      "source": [
        "#%%writefile yield_trend.py\n",
        "import numpy as np\n",
        "import functools\n",
        "from pyspark.sql import Window\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "class CYPYieldTrendEstimator:\n",
        "  def __init__(self, cyp_config):\n",
        "    self.verbose = cyp_config.getDebugLevel()\n",
        "    self.trend_windows = cyp_config.getTrendWindows()\n",
        "    spatial_level = cyp_config.getSpatialLevel()\n",
        "    self.id_col = \"COUNTY_ID\" if (spatial_level == \"COUNTY\") else \"GRID_ID\" \n",
        "\n",
        "  def setTrendWindows(self, trend_windows):\n",
        "    \"\"\"Set trend window lengths\"\"\"\n",
        "    assert isinstance(trend_windows, list)\n",
        "    assert len(trend_windows) > 0\n",
        "    assert isinstance(trend_windows[0], int)\n",
        "    self.trend_windows = trend_windows\n",
        "\n",
        "  def getTrendWindowYields(self, df, trend_window, reg_id=None):\n",
        "    \"\"\"Extract previous years' yield values to separate columns\"\"\"\n",
        "    sel_cols = [self.id_col, 'FYEAR', 'YIELD']\n",
        "    my_window = Window.partitionBy(self.id_col).orderBy('FYEAR')\n",
        "\n",
        "    yield_fts = df.select(sel_cols)\n",
        "    if (reg_id is not None):\n",
        "      yield_fts = yield_fts.filter(yield_fts[self.id_col] == reg_id)\n",
        "\n",
        "    for i in range(trend_window):\n",
        "      yield_fts = yield_fts.withColumn('YIELD-' + str(i+1),\n",
        "                                       SparkF.lag(yield_fts.YIELD, i+1).over(my_window))\n",
        "      yield_fts = yield_fts.withColumn('YEAR-' + str(i+1),\n",
        "                                       SparkF.lag(yield_fts.FYEAR, i+1).over(my_window))\n",
        "\n",
        "    # drop columns withs null values\n",
        "    for i in range(trend_window):\n",
        "      yield_fts = yield_fts.filter(SparkF.col('YIELD-' + str(i+1)).isNotNull())\n",
        "\n",
        "    prev_yields = [ 'YIELD-' + str(i) for i in range(trend_window, 0, -1)]\n",
        "    prev_years = [ 'YEAR-' + str(i) for i in range(trend_window, 0, -1)]\n",
        "    sel_cols = [self.id_col, 'FYEAR'] + prev_years + prev_yields\n",
        "    yield_fts = yield_fts.select(sel_cols)\n",
        "\n",
        "    return yield_fts\n",
        "\n",
        "  # Christos, Ante's suggestion\n",
        "  # - To avoid overfitting, trend estimation could use a window which skips a year in between\n",
        "  # So a window of 3 will use 6 years of data\n",
        "  def printYieldTrendRounds(self, df, reg_id, trend_windows=None):\n",
        "    \"\"\"Print the sequence of years used for yield trend estimation\"\"\"\n",
        "    reg_years = sorted([yr[0] for yr in df.filter(df[self.id_col] == reg_id).select('FYEAR').distinct().collect()])\n",
        "    num_years = len(reg_years)\n",
        "    if (trend_windows is None):\n",
        "      trend_windows = self.trend_windows\n",
        "\n",
        "    for trend_window in trend_windows:\n",
        "      rounds = (num_years - trend_window)\n",
        "      if ((self.verbose > 2) and (trend_window == trend_windows[0])):\n",
        "        print('Trend window', trend_window)\n",
        "    \n",
        "      for rd in range(rounds):\n",
        "        test_year = reg_years[-(rd + 1)]\n",
        "        start_year = reg_years[-(rd + trend_window + 1)]\n",
        "        end_year = reg_years[-(rd + 2)]\n",
        "\n",
        "        if ((self.verbose > 2) and (trend_window == trend_windows[0])):\n",
        "          print('Round:', rd, 'Test year:', test_year,\n",
        "                'Trend Window:', start_year, '-', end_year)\n",
        "\n",
        "  def getLinearYieldTrend(self, window_years, window_yields, pred_year):\n",
        "    \"\"\"Linear yield trend prediction\"\"\"\n",
        "    coefs = np.polyfit(window_years, window_yields, 1)\n",
        "    return float(np.round(coefs[0] * pred_year + coefs[1], 2))\n",
        "\n",
        "  def getFixedWindowTrendFeatures(self, df, trend_window=None, pred_year=None):\n",
        "    \"\"\"Predict the yield trend for each id_col and FYEAR using a fixed window\"\"\"\n",
        "    join_cols = [self.id_col, 'FYEAR']\n",
        "    if (trend_window is None):\n",
        "      trend_window = self.trend_windows[0]\n",
        "\n",
        "    yield_ft_df = self.getTrendWindowYields(df, trend_window)\n",
        "    if (pred_year is not None):\n",
        "      yield_ft_df = yield_ft_df.filter(yield_ft_df['FYEAR'] == pred_year)\n",
        "\n",
        "    pd_yield_ft_df = yield_ft_df.toPandas()\n",
        "    region_years = pd_yield_ft_df[join_cols].values\n",
        "    prev_year_cols = ['YEAR-' + str(i) for i in range(1, trend_window + 1)]\n",
        "    prev_yield_cols = ['YIELD-' + str(i) for i in range(1, trend_window + 1)]\n",
        "    window_years = pd_yield_ft_df[prev_year_cols].values\n",
        "    window_yields = pd_yield_ft_df[prev_yield_cols].values\n",
        "\n",
        "    yield_trend = []\n",
        "    for i in range(region_years.shape[0]):\n",
        "      yield_trend.append(self.getLinearYieldTrend(window_years[i, :],\n",
        "                                                  window_yields[i, :],\n",
        "                                                  region_years[i, 1]))\n",
        "\n",
        "    pd_yield_ft_df['YIELD_TREND'] = yield_trend\n",
        "    return pd_yield_ft_df\n",
        "\n",
        "  def getFixedWindowTrend(self, df, reg_id, pred_year, trend_window=None):\n",
        "    \"\"\"\n",
        "    Return linear trend prediction for given region and year\n",
        "    using fixed trend window.\n",
        "    \"\"\"\n",
        "    if (trend_window is None):\n",
        "      trend_window = self.trend_windows[0]\n",
        "\n",
        "    reg_df = df.filter((df[self.id_col] == reg_id) & (df['FYEAR'] <= pred_year))\n",
        "    pd_yield_ft_df = self.getFixedWindowTrendFeatures(reg_df, self.id_col, trend_window, pred_year)\n",
        "    if (len(pd_yield_ft_df.index) == 0):\n",
        "      print('No data to estimate yield trend')\n",
        "      return None\n",
        "\n",
        "    if (len(pd_yield_ft_df.index) == 0):\n",
        "      return None\n",
        "\n",
        "    reg_year_filter = (df[self.id_col] == reg_id) & (df['FYEAR'] == pred_year)\n",
        "    pd_yield_ft_df['ACTUAL'] = df.filter(reg_year_filter).select('YIELD').collect()[0][0]\n",
        "    pd_yield_ft_df = pd_yield_ft_df.rename(columns={'YIELD_TREND' : 'PREDICTED'})\n",
        "\n",
        "    return pd_yield_ft_df\n",
        "\n",
        "  def getL1OutCVPredictions(self, pd_yield_ft_df, trend_window, join_cols, iter):\n",
        "    \"\"\"1 iteration of leave-one-out cross-validation\"\"\"\n",
        "    iter_year_cols = ['YEAR-' + str(i) for i in range(1, trend_window + 1) if i != iter]\n",
        "    iter_yield_cols = ['YIELD-' + str(i) for i in range(1, trend_window + 1) if i != iter]\n",
        "    window_years = pd_yield_ft_df[iter_year_cols].values\n",
        "    window_yields = pd_yield_ft_df[iter_yield_cols].values\n",
        "\n",
        "    # We are going to predict yield value for YEAR-<iter>.\n",
        "    pred_years = pd_yield_ft_df['YEAR-' + str(iter)].values\n",
        "    predicted_trend = []\n",
        "    for i in range(pred_years.shape[0]):\n",
        "      predicted_trend.append(self.getLinearYieldTrend(window_years[i, :],\n",
        "                                                      window_yields[i, :],\n",
        "                                                      pred_years[i]))\n",
        "\n",
        "    pd_iter_preds = pd_yield_ft_df[join_cols]\n",
        "    pd_iter_preds['YTRUE' + str(iter)] = pd_yield_ft_df['YIELD-' + str(iter)]\n",
        "    pd_iter_preds['YPRED' + str(iter)] = predicted_trend\n",
        "\n",
        "    if (self.verbose > 2):\n",
        "      print('Leave-one-out cross-validation: iteration', iter)\n",
        "      print(pd_iter_preds.head(5))\n",
        "\n",
        "    return pd_iter_preds\n",
        "\n",
        "  def getL1outRMSE(self, cv_actual, cv_predicted):\n",
        "    \"\"\"Compute RMSE for leave-one-out predictions\"\"\"\n",
        "    return float(np.round(np.sqrt(mean_squared_error(cv_actual, cv_predicted)), 2))\n",
        "\n",
        "  def getMinRMSEIndex(self, cv_rmses):\n",
        "    \"\"\"Index of min rmse values\"\"\"\n",
        "    return np.nanargmin(cv_rmses)\n",
        "\n",
        "  def getL1OutCVRMSE(self, df, trend_window, join_cols, pred_year=None):\n",
        "    \"\"\"Run leave-one-out cross-validation and compute RMSE\"\"\"\n",
        "    join_cols = [self.id_col, 'FYEAR']\n",
        "    pd_yield_ft_df = self.getFixedWindowTrendFeatures(df, self.id_col, trend_window, pred_year)\n",
        "    pd_l1out_preds = None\n",
        "    for i in range(1, trend_window + 1):\n",
        "      pd_iter_preds = self.getL1OutCVPredictions(pd_yield_ft_df, trend_window,\n",
        "                                                 join_cols, i)\n",
        "      if (pd_l1out_preds is None):\n",
        "        pd_l1out_preds = pd_iter_preds\n",
        "      else:\n",
        "        pd_l1out_preds = pd_l1out_preds.merge(pd_iter_preds, on=join_cols)\n",
        "\n",
        "    region_years = pd_l1out_preds[join_cols].values\n",
        "    ytrue_cols = ['YTRUE' + str(i) for i in range(1, trend_window + 1)]\n",
        "    ypred_cols = ['YPRED' + str(i) for i in range(1, trend_window + 1)]\n",
        "    l1out_ytrue = pd_l1out_preds[ytrue_cols].values\n",
        "    l1out_ypred = pd_l1out_preds[ypred_cols].values\n",
        "    cv_rmse = []\n",
        "    for i in range(region_years.shape[0]):\n",
        "      cv_rmse.append(self.getL1outRMSE(l1out_ytrue[i, :],\n",
        "                                       l1out_ypred[i, :]))\n",
        "\n",
        "    pd_l1out_rmse = pd_yield_ft_df[join_cols]\n",
        "    pd_l1out_rmse['YIELD_TREND' + str(trend_window)] = pd_yield_ft_df['YIELD_TREND']\n",
        "    pd_l1out_rmse['CV_RMSE' + str(trend_window)] = cv_rmse\n",
        "\n",
        "    return pd_l1out_rmse\n",
        "\n",
        "  def getOptimalTrendWindows(self, df, pred_year=None, trend_windows=None):\n",
        "    \"\"\"\n",
        "    Compute optimal yield trend values based on leave-one-out\n",
        "    cross validation errors for different trend windows.\n",
        "    \"\"\"\n",
        "    join_cols = [self.id_col, 'FYEAR']\n",
        "    if (trend_windows is None):\n",
        "      trend_windows = self.trend_windows\n",
        "\n",
        "    pd_tw_rmses = None\n",
        "    for tw in trend_windows:\n",
        "      pd_l1out_rmse = self.getL1OutCVRMSE(df, tw, join_cols, pred_year)\n",
        "      if (pd_tw_rmses is None):\n",
        "        pd_tw_rmses = pd_l1out_rmse\n",
        "      else:\n",
        "        pd_tw_rmses = pd_tw_rmses.merge(pd_l1out_rmse, on=join_cols, how='left')\n",
        "\n",
        "    if (self.verbose > 2):\n",
        "      print('Leave-one-out cross-validation: RMSE')\n",
        "      print(pd_tw_rmses.sort_values(by=join_cols).head(5))\n",
        "\n",
        "    region_years = pd_tw_rmses[join_cols].values\n",
        "    tw_rmse_cols = ['CV_RMSE' + str(tw) for tw in trend_windows]\n",
        "    tw_trend_cols = ['YIELD_TREND' + str(tw) for tw in trend_windows]\n",
        "    tw_cv_rmses = pd_tw_rmses[tw_rmse_cols].values\n",
        "    tw_yield_trend = pd_tw_rmses[tw_trend_cols].values\n",
        "\n",
        "    opt_windows = []\n",
        "    yield_trend_preds = []\n",
        "    for i in range(region_years.shape[0]):\n",
        "      min_rmse_index = self.getMinRMSEIndex(tw_cv_rmses[i, :])\n",
        "      opt_windows.append(trend_windows[min_rmse_index])\n",
        "      yield_trend_preds.append(tw_yield_trend[i, min_rmse_index])\n",
        "\n",
        "    pd_opt_win_df = pd_tw_rmses[join_cols]\n",
        "    pd_opt_win_df['OPT_TW'] = opt_windows\n",
        "    pd_opt_win_df['YIELD_TREND'] = yield_trend_preds\n",
        "    if (self.verbose > 2):\n",
        "      print('Optimal trend windows')\n",
        "      print(pd_opt_win_df.sort_values(by=join_cols).head(5))\n",
        "\n",
        "    return pd_opt_win_df\n",
        "\n",
        "  def getOptimalWindowTrendFeatures(self, df, trend_windows=None):\n",
        "    \"\"\"\n",
        "    Get previous year yield values and predicted yield trend\n",
        "    by determining optimal trend window for each region and year.\n",
        "    NOTE: We have to select the same number of features, so we\n",
        "    select previous trend_windows[0] yield values.\n",
        "    \"\"\"\n",
        "    join_cols = [self.id_col, 'FYEAR']\n",
        "    if (trend_windows is None):\n",
        "      trend_windows = self.trend_windows\n",
        "\n",
        "    pd_yield_ft_df = self.getTrendWindowYields(df, trend_windows[0]).toPandas()\n",
        "    pd_opt_win_df = self.getOptimalTrendWindows(df, trend_windows=trend_windows)\n",
        "    pd_opt_win_df = pd_opt_win_df.drop(columns=['OPT_TW'])\n",
        "    pd_yield_ft_df = pd_yield_ft_df.merge(pd_opt_win_df, on=join_cols)\n",
        "\n",
        "    return pd_yield_ft_df\n",
        "\n",
        "  def getOptimalWindowTrend(self, df, reg_id, pred_year, trend_windows=None):\n",
        "    \"\"\"\n",
        "    Compute the optimal trend window for given region and year based on\n",
        "    leave-one-out cross validation errors for different trend windows.\n",
        "    \"\"\"\n",
        "    df = df.filter(df[self.id_col] == reg_id)\n",
        "    pd_opt_win_df = self.getOptimalTrendWindows(df, pred_year, trend_windows)\n",
        "    if (len(pd_opt_win_df.index) == 0):\n",
        "      return None\n",
        "\n",
        "    reg_year_filter = (df[self.id_col] == reg_id) & (df['FYEAR'] == pred_year)\n",
        "    pd_opt_win_df['ACTUAL'] = df.filter(reg_year_filter).select('YIELD').collect()[0][0]\n",
        "    pd_opt_win_df = pd_opt_win_df.rename(columns={'YIELD_TREND' : 'PREDICTED'})\n",
        "\n",
        "    return pd_opt_win_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGNO_QZCsYf2"
      },
      "source": [
        "#### Create CSSF, Meteo and Remote Sensing Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NpzcktZse_8"
      },
      "outputs": [],
      "source": [
        "#%%writefile run_feature_design.py\n",
        "def getCumulativeAvgCols(ft_src):\n",
        "  \"\"\"columns or indicators using avg of cumulative values\"\"\"\n",
        "  cum_cols = []\n",
        "  if (ft_src == 'METEO'):\n",
        "    cum_cols = ['CWB']\n",
        "\n",
        "  return cum_cols\n",
        "\n",
        "def cssfMaxFeatureCols():\n",
        "  \"\"\"columns or indicators using max aggregation\"\"\"\n",
        "  # must be in sync with crop calendar periods\n",
        "  max_cols = {\n",
        "      'p0' : [],\n",
        "      'p1' : [],\n",
        "      'p2' : ['TAGP'],\n",
        "      'p3' : [],\n",
        "      'p4' : ['TAGP', 'TWSO'],\n",
        "      'p5' : [],\n",
        "  }\n",
        "\n",
        "  return max_cols\n",
        "\n",
        "def cssfAvgFeatureCols():\n",
        "  \"\"\"columns or indicators using avg aggregation\"\"\"\n",
        "  # must be in sync with crop calendar periods\n",
        "  avg_cols = {\n",
        "      'p0' : [],\n",
        "      'p1' : [],\n",
        "      'p2' : [],\n",
        "      'p3' : [],\n",
        "      'p4' : [],\n",
        "      'p5' : [],\n",
        "  }\n",
        "\n",
        "  return avg_cols\n",
        "\n",
        "def cssfCountFeatureCols():\n",
        "  \"\"\"columns or indicators using count aggregation\"\"\"\n",
        "  # must be in sync with crop calendar periods\n",
        "  count_cols = {\n",
        "      'p0' : [],\n",
        "      'p1' : [],\n",
        "      'p2' : [],\n",
        "      'p3' : [],\n",
        "      'p4' : [],\n",
        "      'p5' : [],\n",
        "  }\n",
        "\n",
        "  return count_cols\n",
        "\n",
        "# Meteo Feature ideas:\n",
        "# Two dry summers caused drop in ground water level:\n",
        "#   rainfall sums going back to second of half of previous year\n",
        "# Previous year: high production, prices low, invest less in crop\n",
        "#   Focus on another crop\n",
        "def meteoMaxFeatureCols():\n",
        "  \"\"\"columns or indicators using max aggregation\"\"\"\n",
        "  # must be in sync with crop calendar periods\n",
        "  max_cols = { 'p0' : [], 'p1' : [], 'p2' : [], 'p3' : [], 'p4' : [], 'p5' : [] }\n",
        "\n",
        "  return max_cols\n",
        "\n",
        "def meteoAvgFeatureCols(features_v2=False):\n",
        "  \"\"\"columns or indicators using avg aggregation\"\"\"\n",
        "  # must be in sync with crop calendar periods\n",
        "  avg_cols = {\n",
        "      'p0' : ['TAVG', 'CWB', 'PREC'],\n",
        "      'p1' : ['TAVG', 'PREC'],\n",
        "      'p2' : ['TAVG', 'CWB'],\n",
        "      'p3' : ['TAVG', 'PREC'],\n",
        "      'p4' : ['CWB'],\n",
        "      'p5' : ['PREC'],\n",
        "  }\n",
        "\n",
        "  return avg_cols\n",
        "\n",
        "def meteoCountFeatureCols():\n",
        "  \"\"\"columns or indicators using count aggregation\"\"\"\n",
        "  # must be in sync with crop calendar periods\n",
        "  count_cols = {\n",
        "      'p0' : [],\n",
        "      'p1' : ['TAVG', 'TMIN', 'PREC'],\n",
        "      'p2' : [],\n",
        "      'p3' : ['TAVG', 'CWB', 'PREC', 'TMAX'],\n",
        "      'p4' : [],\n",
        "      'p5' : ['PREC'],\n",
        "  }\n",
        "\n",
        "  return count_cols\n",
        "\n",
        "def rsMaxFeatureCols():\n",
        "  \"\"\"columns or indicators using max aggregation\"\"\"\n",
        "  # must be in sync with crop calendar periods\n",
        "  max_cols = { 'p0' : [], 'p1' : [], 'p2' : [], 'p3' : [], 'p4' : [], 'p5' : [] }\n",
        "\n",
        "  return max_cols\n",
        "\n",
        "def rsAvgFeatureCols():\n",
        "  \"\"\"columns or indicators using avg aggregation\"\"\"\n",
        "  # must be in sync with crop calendar periods\n",
        "  avg_cols = {\n",
        "      'p0' : [],\n",
        "      'p1' : [],\n",
        "      'p2' : ['FAPAR'],\n",
        "      'p3' : [],\n",
        "      'p4' : ['FAPAR'],\n",
        "      'p5' : [],\n",
        "  }\n",
        "\n",
        "  return avg_cols\n",
        "\n",
        "def convertFeaturesToPandas(ft_dfs, join_cols):\n",
        "  \"\"\"Convert features to pandas and merge\"\"\"\n",
        "  train_ft_df = ft_dfs[0]\n",
        "  test_ft_df = ft_dfs[1]\n",
        "  train_ft_df = train_ft_df.withColumnRenamed('CAMPAIGN_YEAR', 'FYEAR')\n",
        "  test_ft_df = test_ft_df.withColumnRenamed('CAMPAIGN_YEAR', 'FYEAR')\n",
        "  pd_train_df = train_ft_df.toPandas()\n",
        "  pd_test_df = test_ft_df.toPandas()\n",
        "\n",
        "  return [pd_train_df, pd_test_df]\n",
        "\n",
        "def dropZeroColumns(pd_ft_dfs):\n",
        "  \"\"\"Drop columns which have all zeros in training data\"\"\"\n",
        "  pd_train_df = pd_ft_dfs[0]\n",
        "  pd_test_df = pd_ft_dfs[1]\n",
        "\n",
        "  pd_train_df = pd_train_df.loc[:, (pd_train_df != 0.0).any(axis=0)]\n",
        "  pd_train_df = pd_train_df.dropna(axis=1)\n",
        "  pd_test_df = pd_test_df[pd_train_df.columns]\n",
        "\n",
        "  return [pd_train_df, pd_test_df]\n",
        "\n",
        "def printFeatureData(pd_feature_dfs, join_cols):\n",
        "  for src in pd_feature_dfs:\n",
        "    pd_train_fts = pd_feature_dfs[src][0]\n",
        "    if (pd_train_fts is None):\n",
        "      continue\n",
        "\n",
        "    pd_test_fts = pd_feature_dfs[src][1]\n",
        "    all_cols = list(pd_train_fts.columns)\n",
        "    aggr_cols = [ c for c in all_cols if (('avg' in c) or ('max' in c))]\n",
        "    if (len(aggr_cols) > 0):\n",
        "      print('\\n', src, 'Aggregate Features: Training')\n",
        "      print(pd_train_fts[join_cols + aggr_cols].head(5))\n",
        "      print('\\n', src, 'Aggregate Features: Test')\n",
        "      print(pd_test_fts[join_cols + aggr_cols].head(5))\n",
        "\n",
        "    ext_cols = [ c for c in all_cols if (('Z+' in c) or ('Z-' in c) or\n",
        "                                         ('lt' in c) or ('gt' in c))]\n",
        "    if (len(ext_cols) > 0):\n",
        "      print('\\n', src, 'Features for Extreme Conditions: Training')\n",
        "      print(pd_train_fts[join_cols + ext_cols].head(5))\n",
        "      print('\\n', src, 'Features for Extreme Conditions: Test')\n",
        "      print(pd_test_fts[join_cols + ext_cols].head(5))\n",
        "\n",
        "def createFeatures(cyp_config, cyp_featurizer, train_test_dfs,\n",
        "                   summary_dfs, log_fh):\n",
        "  \"\"\"Create CSSF, Meteo and Remote Sensing features\"\"\"\n",
        "  spatial_level = cyp_config.getSpatialLevel()\n",
        "  use_remote_sensing = cyp_config.useRemoteSensing()\n",
        "  use_per_year_cc = cyp_config.usePerYearCropCalendar()\n",
        "  use_features_v2 = cyp_config.useFeaturesV2()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  id_col = \"COUNTY_ID\" if (spatial_level == \"COUNTY\") else \"GRID_ID\"\n",
        "\n",
        "  cssf_train_df = train_test_dfs['CSSF'][0]\n",
        "  cssf_test_df = train_test_dfs['CSSF'][1]\n",
        "  meteo_train_df = train_test_dfs['METEO'][0]\n",
        "  meteo_test_df = train_test_dfs['METEO'][1]\n",
        "  yield_train_df = train_test_dfs['YIELD'][0]\n",
        "\n",
        "  dvs_train = summary_dfs['CSSF_DVS'][0]\n",
        "  dvs_test = summary_dfs['CSSF_DVS'][1]\n",
        "\n",
        "  if (debug_level > 2):\n",
        "    print('CSSF training data size',\n",
        "          cssf_train_df.select([id_col, 'FYEAR']).distinct().count())\n",
        "    print('CSSF test data size',\n",
        "          cssf_test_df.select([id_col, 'FYEAR']).distinct().count())\n",
        "    print('Meteo training data size',\n",
        "          meteo_train_df.select([id_col, 'FYEAR']).distinct().count())\n",
        "    print('Meteo test data size',\n",
        "          meteo_test_df.select([id_col, 'FYEAR']).distinct().count())\n",
        "    print('DVS Summary of training data',\n",
        "          dvs_train.select([id_col, 'FYEAR']).distinct().count())\n",
        "    print('DVS Summary of test data',\n",
        "          dvs_test.select([id_col, 'FYEAR']).distinct().count())\n",
        "\n",
        "  rs_train_df = None\n",
        "  rs_test_df = None\n",
        "  if (use_remote_sensing):\n",
        "    rs_train_df = train_test_dfs['REMOTE_SENSING'][0]\n",
        "    rs_test_df = train_test_dfs['REMOTE_SENSING'][1]\n",
        "    if (debug_level > 2):\n",
        "      print('Remote sensing training data size',\n",
        "            rs_train_df.select([id_col, 'FYEAR']).distinct().count())\n",
        "      print('Remote sensing test data size',\n",
        "            rs_test_df.select([id_col, 'FYEAR']).distinct().count())\n",
        "\n",
        "  join_cols = [id_col, 'FYEAR']\n",
        "  aggr_ft_cols = {\n",
        "      'CSSF' : [cssfMaxFeatureCols(), cssfAvgFeatureCols()],\n",
        "      'METEO' : [meteoMaxFeatureCols(), meteoAvgFeatureCols(use_features_v2)],\n",
        "  }\n",
        "\n",
        "  count_ft_cols = {\n",
        "      'CSSF' : cssfCountFeatureCols(),\n",
        "      'METEO' : meteoCountFeatureCols(),\n",
        "  }\n",
        "\n",
        "  train_ft_src_dfs = {\n",
        "      'CSSF' : cssf_train_df,\n",
        "      'METEO' : meteo_train_df,\n",
        "  }\n",
        "\n",
        "  test_ft_src_dfs = {\n",
        "      'CSSF' : cssf_test_df,\n",
        "      'METEO' : meteo_test_df,\n",
        "  }\n",
        "\n",
        "  if (use_remote_sensing):\n",
        "    train_ft_src_dfs['REMOTE_SENSING'] = rs_train_df\n",
        "    test_ft_src_dfs['REMOTE_SENSING'] = rs_test_df\n",
        "    aggr_ft_cols['REMOTE_SENSING'] = [rsMaxFeatureCols(), rsAvgFeatureCols()]\n",
        "    count_ft_cols['REMOTE_SENSING'] = {}\n",
        "\n",
        "  crop_cal_train = dvs_train\n",
        "  crop_cal_test = dvs_test\n",
        "  if (not use_per_year_cc):\n",
        "    crop_cal_test = dvs_train\n",
        "\n",
        "  train_ft_dfs = {}\n",
        "  test_ft_dfs = {}\n",
        "  for ft_src in train_ft_src_dfs:\n",
        "    cum_avg_cols = []\n",
        "    if (use_features_v2):\n",
        "      cum_avg_cols = getCumulativeAvgCols(ft_src)\n",
        "\n",
        "    train_ft_dfs[ft_src] = cyp_featurizer.extractFeatures(train_ft_src_dfs[ft_src],\n",
        "                                                          ft_src,\n",
        "                                                          crop_cal_train,\n",
        "                                                          aggr_ft_cols[ft_src][0],\n",
        "                                                          aggr_ft_cols[ft_src][1],\n",
        "                                                          cum_avg_cols,\n",
        "                                                          count_ft_cols[ft_src],\n",
        "                                                          join_cols,\n",
        "                                                          True)\n",
        "    test_ft_dfs[ft_src] = cyp_featurizer.extractFeatures(test_ft_src_dfs[ft_src],\n",
        "                                                         ft_src,\n",
        "                                                         crop_cal_test,\n",
        "                                                         aggr_ft_cols[ft_src][0],\n",
        "                                                         aggr_ft_cols[ft_src][1],\n",
        "                                                         cum_avg_cols,\n",
        "                                                         count_ft_cols[ft_src],\n",
        "                                                         join_cols)\n",
        "\n",
        "  pd_conversion_dict = {\n",
        "      'CSSF' : [ train_ft_dfs['CSSF'], test_ft_dfs['CSSF'] ],\n",
        "      'METEO' : [ train_ft_dfs['METEO'], test_ft_dfs['METEO'] ],\n",
        "  }\n",
        "\n",
        "  if (use_remote_sensing):\n",
        "      pd_conversion_dict['REMOTE_SENSING'] = [ train_ft_dfs['REMOTE_SENSING'], test_ft_dfs['REMOTE_SENSING'] ]\n",
        "\n",
        "  pd_feature_dfs = {}\n",
        "  for ft_src in pd_conversion_dict:\n",
        "    pd_feature_dfs[ft_src] = convertFeaturesToPandas(pd_conversion_dict[ft_src], join_cols)\n",
        "\n",
        "  # Check and drop features with all zeros (possible in early season prediction).\n",
        "  for ft_src in pd_feature_dfs:\n",
        "    pd_feature_dfs[ft_src] = dropZeroColumns(pd_feature_dfs[ft_src])\n",
        "\n",
        "  if (debug_level > 1):\n",
        "    join_cols = [id_col, 'FYEAR']\n",
        "    printFeatureData(pd_feature_dfs, join_cols)\n",
        "\n",
        "  return pd_feature_dfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERZ4JYeyss7m"
      },
      "source": [
        "#### Create Trend Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nbqouti4swJ0"
      },
      "outputs": [],
      "source": [
        "#%%writefile run_trend_feature_design.py\n",
        "def createYieldTrendFeatures(cyp_config, cyp_trend_est,\n",
        "                             yield_train_df, yield_test_df, test_years):\n",
        "  \"\"\"Create yield trend features\"\"\"\n",
        "  spatial_level = cyp_config.getSpatialLevel()\n",
        "  id_col = \"COUNTY_ID\" if (spatial_level == \"COUNTY\") else \"GRID_ID\"\n",
        "  join_cols = [id_col, 'FYEAR']\n",
        "  find_optimal = cyp_config.findOptimalTrendWindow()\n",
        "  trend_window = cyp_config.getTrendWindows()[0]\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  yield_df = yield_train_df.union(yield_test_df.select(yield_train_df.columns))\n",
        "\n",
        "  if (find_optimal):\n",
        "    pd_train_features = cyp_trend_est.getOptimalWindowTrendFeatures(yield_train_df)\n",
        "    pd_test_features = cyp_trend_est.getOptimalWindowTrendFeatures(yield_df)\n",
        "  else:\n",
        "    pd_train_features = cyp_trend_est.getFixedWindowTrendFeatures(yield_train_df)\n",
        "    pd_test_features = cyp_trend_est.getFixedWindowTrendFeatures(yield_df)\n",
        "\n",
        "  prev_year_cols = ['YEAR-' + str(i) for i in range(1, trend_window + 1)]\n",
        "  pd_train_features = pd_train_features.drop(columns=prev_year_cols)\n",
        "  pd_test_features = pd_test_features.drop(columns=prev_year_cols)\n",
        "\n",
        "  if (debug_level > 1):\n",
        "    print('\\nYield Trend Features: Train')\n",
        "    print(pd_train_features.sort_values(by=join_cols).head(5))\n",
        "    print('Total', len(pd_train_features.index), 'rows')\n",
        "    print('\\nYield Trend Features: Test')\n",
        "    print(pd_test_features.sort_values(by=join_cols).head(5))\n",
        "    print('Total', len(pd_test_features.index), 'rows')\n",
        "\n",
        "  return pd_train_features, pd_test_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIDmOfx9KKuF"
      },
      "source": [
        "### Combine features and labels\n",
        "\n",
        "Combine CSSF, meteo and soil with remote sensing. Combine with centroids or yield trend both if configured. Combine with yield data in the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsoMBrpIKKuH"
      },
      "outputs": [],
      "source": [
        "#%%writefile combine_features.py\n",
        "def combineFeaturesLabels(cyp_config, sqlCtx,\n",
        "                          prep_train_test_dfs, pd_feature_dfs,\n",
        "                          join_cols, log_fh):\n",
        "  \"\"\"\n",
        "  Combine CSSF, meteo and soil with remote sensing. Combine centroids\n",
        "  and yield trend if configured. Combine with yield data in the end.\n",
        "  If configured, save features to a CSV file.\n",
        "  \"\"\"\n",
        "  pd_soil_df = prep_train_test_dfs['SOIL'][0].toPandas()\n",
        "  pd_yield_train_df = prep_train_test_dfs['YIELD'][0].toPandas()\n",
        "  pd_yield_test_df = prep_train_test_dfs['YIELD'][1].toPandas()\n",
        "\n",
        "  # Feature dataframes have already been converted to pandas\n",
        "  pd_cssf_train_ft = pd_feature_dfs['CSSF'][0]\n",
        "  pd_cssf_test_ft = pd_feature_dfs['CSSF'][1]\n",
        "  pd_meteo_train_ft = pd_feature_dfs['METEO'][0]\n",
        "  pd_meteo_test_ft = pd_feature_dfs['METEO'][1]\n",
        "\n",
        "  crop = cyp_config.getCropName()\n",
        "  country = cyp_config.getCountryCode()\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  use_centroids = cyp_config.useCentroids()\n",
        "  use_remote_sensing = cyp_config.useRemoteSensing()\n",
        "  save_features = cyp_config.saveFeatures()\n",
        "  use_sample_weights = cyp_config.useSampleWeights()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  spatial_level = cyp_config.getSpatialLevel()\n",
        "  id_col = \"COUNTY_ID\" if (spatial_level == \"COUNTY\") else \"GRID_ID\"\n",
        "\n",
        "  combine_info = '\\nCombine Features and Labels'\n",
        "  combine_info += '\\n---------------------------'\n",
        "  yield_min_year = pd_yield_train_df['FYEAR'].min()\n",
        "  combine_info += '\\nYield min year ' + str(yield_min_year) + '\\n'\n",
        "\n",
        "  # start with static SOIL data\n",
        "  pd_train_df = pd_soil_df.copy(deep=True)\n",
        "  pd_test_df = pd_soil_df.copy(deep=True)\n",
        "  combine_info += '\\nData size after including SOIL data: '\n",
        "  combine_info += '\\nTrain ' + str(len(pd_train_df.index)) + ' rows.'\n",
        "  combine_info += '\\nTest ' + str(len(pd_test_df.index)) + ' rows.\\n'\n",
        "\n",
        "  # combine with CSSF features\n",
        "  static_cols = list(pd_train_df.columns)\n",
        "  pd_train_df = pd_train_df.merge(pd_cssf_train_ft, on=[id_col])\n",
        "  pd_test_df = pd_test_df.merge(pd_cssf_test_ft, on=[id_col])\n",
        "  cssf_cols = list(pd_cssf_train_ft.columns)\n",
        "  col_order = [id_col, 'FYEAR'] + static_cols[1:] + cssf_cols[2:]\n",
        "  pd_train_df = pd_train_df[col_order]\n",
        "  pd_test_df = pd_test_df[col_order]\n",
        "  combine_info += '\\nData size after including CSSF features: '\n",
        "  combine_info += '\\nTrain ' + str(len(pd_train_df.index)) + ' rows.'\n",
        "  combine_info += '\\nTest ' + str(len(pd_test_df.index)) + ' rows.\\n'\n",
        "\n",
        "  # combine with METEO features\n",
        "  pd_train_df = pd_train_df.merge(pd_meteo_train_ft, on=join_cols)\n",
        "  pd_test_df = pd_test_df.merge(pd_meteo_test_ft, on=join_cols)\n",
        "  combine_info += '\\nData size after including METEO features: '\n",
        "  combine_info += '\\nTrain ' + str(len(pd_train_df.index)) + ' rows.'\n",
        "  combine_info += '\\nTest ' + str(len(pd_test_df.index)) + ' rows.\\n'\n",
        "\n",
        "  # combine with remote sensing features\n",
        "  if (use_remote_sensing):\n",
        "    pd_rs_train_ft = pd_feature_dfs['REMOTE_SENSING'][0]\n",
        "    pd_rs_test_ft = pd_feature_dfs['REMOTE_SENSING'][1]\n",
        "\n",
        "    pd_train_df = pd_train_df.merge(pd_rs_train_ft, on=join_cols)\n",
        "    pd_test_df = pd_test_df.merge(pd_rs_test_ft, on=join_cols)\n",
        "    combine_info += '\\nData size after including REMOTE_SENSING features: '\n",
        "    combine_info += '\\nTrain ' + str(len(pd_train_df.index)) + ' rows.'\n",
        "    combine_info += '\\nTest ' + str(len(pd_test_df.index)) + ' rows.\\n'\n",
        "\n",
        "  if (use_yield_trend):\n",
        "    # combine with yield trend features\n",
        "    pd_yield_trend_train_ft = pd_feature_dfs['YIELD_TREND'][0]\n",
        "    pd_yield_trend_test_ft = pd_feature_dfs['YIELD_TREND'][1]\n",
        "    pd_train_df = pd_train_df.merge(pd_yield_trend_train_ft, on=join_cols)\n",
        "    pd_test_df = pd_test_df.merge(pd_yield_trend_test_ft, on=join_cols)\n",
        "    combine_info += '\\nData size after including yield trend features: '\n",
        "    combine_info += '\\nTrain ' + str(len(pd_train_df.index)) + ' rows.'\n",
        "    combine_info += '\\nTest ' + str(len(pd_test_df.index)) + ' rows.\\n'\n",
        "\n",
        "  # combine with yield data\n",
        "  pd_train_df = pd_train_df.merge(pd_yield_train_df, on=join_cols)\n",
        "  pd_test_df = pd_test_df.merge(pd_yield_test_df, on=join_cols)\n",
        "  pd_train_df = pd_train_df.sort_values(by=join_cols)\n",
        "  pd_test_df = pd_test_df.sort_values(by=join_cols)\n",
        "  combine_info += '\\nData size after including yield (label) data: '\n",
        "  combine_info += '\\nTrain ' + str(len(pd_train_df.index)) + ' rows.'\n",
        "  combine_info += '\\nTest ' + str(len(pd_test_df.index)) + ' rows.\\n'\n",
        "\n",
        "  log_fh.write(combine_info + '\\n')\n",
        "  if (debug_level > 1):\n",
        "    print(combine_info)\n",
        "    print('\\nAll Features and labels: Training')\n",
        "    print(pd_train_df.head(5))\n",
        "    print('\\nAll Features and labels: Test')\n",
        "    print(pd_test_df.head(5))\n",
        "\n",
        "  if (save_features):\n",
        "    early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "    early_season_end = cyp_config.getEarlySeasonEndDekad()\n",
        "    feature_file_path = cyp_config.getOutputPath()\n",
        "    features_file = getFeatureFilename(crop, use_yield_trend,\n",
        "                                       early_season_prediction, early_season_end,\n",
        "                                       country)\n",
        "    save_ft_path = feature_file_path + '/' + features_file\n",
        "    save_ft_info = '\\nSaving features to: ' + save_ft_path + '[train, test].csv'\n",
        "    log_fh.write(save_ft_info + '\\n')\n",
        "    if (debug_level > 1):\n",
        "      print(save_ft_info)\n",
        "\n",
        "    pd_train_df.to_csv(save_ft_path + '_train.csv', index=False, header=True)\n",
        "    pd_test_df.to_csv(save_ft_path + '_test.csv', index=False, header=True)\n",
        "\n",
        "    # NOTE: In some environments, Spark can write, but pandas cannot.\n",
        "    # In such cases, use the following code.\n",
        "    # spark_train_df = sqlCtx.createDataFrame(pd_train_df)\n",
        "    # spark_train_df.coalesce(1).write.option('header','true').mode('overwrite').csv(save_ft_path + '_train')\n",
        "    # spark_test_df = sqlCtx.createDataFrame(pd_test_df)\n",
        "    # spark_test_df.coalesce(1).write.option('header','true').mode('overwrite').csv(save_ft_path + '_test')\n",
        "\n",
        "  return pd_train_df, pd_test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQb85mf_wgvS"
      },
      "source": [
        "### Load Saved Features and Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5TyGb2iwnFr"
      },
      "outputs": [],
      "source": [
        "#%%writefile load_saved_features.py\n",
        "import pandas as pd\n",
        "\n",
        "def loadSavedFeaturesLabels(cyp_config, spark):\n",
        "  \"\"\"Load saved features from a CSV file\"\"\"\n",
        "  crop = cyp_config.getCropName()\n",
        "  country = cyp_config.getCountryCode()\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "  early_season_end = cyp_config.getEarlySeasonEndDekad()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "\n",
        "  feature_file_path = cyp_config.getOutputPath()\n",
        "  feature_file = getFeatureFilename(crop, use_yield_trend,\n",
        "                                    early_season_prediction, early_season_end,\n",
        "                                    country)\n",
        "\n",
        "  load_ft_path = feature_file_path + '/' + feature_file\n",
        "  pd_train_df = pd.read_csv(load_ft_path + '_train.csv', header=0)\n",
        "  pd_test_df = pd.read_csv(load_ft_path + '_test.csv', header=0)\n",
        "\n",
        "  # NOTE: In some environments, Spark can read, but pandas cannot.\n",
        "  # In such cases, use the following code.\n",
        "  # spark_train_df = spark.read.csv(load_ft_path + '_train.csv', header=True, inferSchema=True)\n",
        "  # spark_test_df = spark.read.csv(load_ft_path + '_test.csv', header=True, inferSchema=True)\n",
        "  # pd_train_df = spark_train_df.toPandas()\n",
        "  # pd_test_df = spark_test_df.toPandas()\n",
        "\n",
        "  if (debug_level > 1):\n",
        "    print('\\nAll Features and labels')\n",
        "    print(pd_train_df.head(5))\n",
        "    print(pd_test_df.head(5))\n",
        "\n",
        "  return pd_train_df, pd_test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PjL0Nv1jEpR"
      },
      "source": [
        "### Machine Learning using scikit learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drvMnHJBuBwK"
      },
      "source": [
        "#### Feature Selector Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBcbtxArL2Ld"
      },
      "outputs": [],
      "source": [
        "#%%writefile feature_selection.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.feature_selection import RFE\n",
        "from skopt import BayesSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.utils import parallel_backend\n",
        "\n",
        "class CYPFeatureSelector:\n",
        "  def __init__(self, cyp_config, X_train, Y_train, all_features,\n",
        "               custom_cv, train_weights=None):\n",
        "    self.X_train = X_train\n",
        "    self.Y_train = Y_train\n",
        "    self.train_weights = train_weights\n",
        "    self.custom_cv = custom_cv\n",
        "    self.all_features = all_features\n",
        "    self.feature_selectors = cyp_config.getFeatureSelectors(len(all_features))\n",
        "    self.scaler = cyp_config.getFeatureScaler()\n",
        "    self.cv_metric = cyp_config.getFeatureSelectionCVMetric()\n",
        "    self.use_sample_weights = cyp_config.useSampleWeights()\n",
        "    self.verbose = cyp_config.getDebugLevel()\n",
        "\n",
        "  def setCustomCV(self, custom_cv):\n",
        "    \"\"\"Set custom K-Fold validation splits\"\"\"\n",
        "    self.custom_cv = custom_cv\n",
        "\n",
        "  def setFeatures(self, features):\n",
        "    \"\"\"Set the list of all features\"\"\"\n",
        "    self.all_features = features\n",
        "\n",
        "  # K-fold validation to find the optimal number of features\n",
        "  # and optimal hyperparameters for estimator.\n",
        "  def featureSelectionParameterSearch(self, sel_name, selector, est, param_space, fit_params):\n",
        "    \"\"\"Use grid search with k-fold validation to optimize the number of features\"\"\"\n",
        "    # 3 values per parameter\n",
        "    # nparams_sampled = pow(3, len(param_space))\n",
        "    # if (nparams_sampled < 100):\n",
        "    #  nparams_sampled = 100\n",
        "\n",
        "    nparams_sampled = 40 + pow(2, len(param_space))\n",
        "    X_train_copy = np.copy(self.X_train)\n",
        "    pipeline = Pipeline([(\"scaler\", self.scaler),\n",
        "                         (\"selector\", selector),\n",
        "                         (\"estimator\", est)])\n",
        "\n",
        "    bayes_search = BayesSearchCV(estimator=pipeline,\n",
        "                                 search_spaces=param_space,\n",
        "                                 n_iter=nparams_sampled,\n",
        "                                 scoring=self.cv_metric,\n",
        "                                 cv=self.custom_cv,\n",
        "                                 n_points=4,\n",
        "                                 refit=modelRefitMeanVariance,\n",
        "                                 return_train_score=True,\n",
        "                                 n_jobs=-1)\n",
        "\n",
        "    # with parallel_backend('spark', n_jobs=-1):\n",
        "    bayes_search.fit(X_train_copy, np.ravel(self.Y_train), **fit_params)\n",
        "\n",
        "    best_params = bayes_search.best_params_\n",
        "    if (self.verbose > 2):\n",
        "      print('\\nFeature selection using', sel_name)\n",
        "      plotCVResults(bayes_search)\n",
        "\n",
        "    best_estimator = bayes_search.best_estimator_\n",
        "    with parallel_backend('spark', n_jobs=-1):\n",
        "      cv_scores = cross_val_score(best_estimator, X_train_copy, self.Y_train,\n",
        "                                  cv=self.custom_cv, scoring=self.cv_metric)\n",
        "    indices = []\n",
        "    # feature selectors should have 'get_support' function\n",
        "    selector = bayes_search.best_estimator_.named_steps['selector']\n",
        "    if ((isinstance(selector, SelectFromModel)) or (isinstance(selector, SelectKBest)) or\n",
        "        (isinstance(selector, RFE))):\n",
        "      indices = selector.get_support(indices=True)\n",
        "\n",
        "    if (self.verbose > 2):\n",
        "      print('\\nSelected Features:')\n",
        "      print('-------------------')\n",
        "      printInGroups(self.all_features, indices)\n",
        "\n",
        "    result = {\n",
        "        'indices' : indices,\n",
        "        'cv_scores' : cv_scores,\n",
        "        'estimator' : best_estimator,\n",
        "        'best_params' : best_params,\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "  # Compare different feature selectors using cross validation score.\n",
        "  # Also compare combined features with the best individual feature selector.\n",
        "  def compareFeatureSelectors(self, est_name, est, est_param_space):\n",
        "    \"\"\"Compare feature selectors based on K-fold validation scores\"\"\"\n",
        "    fs_results = {}\n",
        "    combined_indices = []\n",
        "    for sel_name in self.feature_selectors:\n",
        "      selector = self.feature_selectors[sel_name]['selector']\n",
        "      sel_param_space = self.feature_selectors[sel_name]['param_space']\n",
        "      param_space = sel_param_space.copy()\n",
        "      param_space.update(est_param_space)\n",
        "\n",
        "      fit_params = {}\n",
        "      if (self.use_sample_weights and (est_name != 'KNN')):\n",
        "        fit_params['estimator__sample_weight'] = self.train_weights\n",
        "\n",
        "      result = self.featureSelectionParameterSearch(sel_name, selector, est,\n",
        "                                                    param_space, fit_params)\n",
        "      param_space.clear()\n",
        "\n",
        "      fs_results[sel_name] = {\n",
        "          'indices' : result['indices'],\n",
        "          'mean_score' : np.mean(result['cv_scores']),\n",
        "          'std_score' : np.std(result['cv_scores']),\n",
        "          'estimator' : result['estimator'],\n",
        "          'best_params' : result['best_params']\n",
        "      }\n",
        "\n",
        "    # best selector has the highest score based on mean_score and std_score\n",
        "    # we subtract std_score because lower variance is better.\n",
        "    sel_names = list(fs_results.keys())\n",
        "    mean_scores = np.array([fs_results[s]['mean_score'] for s in sel_names])\n",
        "    std_scores = np.array([fs_results[s]['std_score'] for s in sel_names])\n",
        "    sel_scores = mean_scores - std_scores\n",
        "    best_score, best_index = max((val, idx) for (idx, val) in enumerate(sel_scores))\n",
        "    best_sel_name = sel_names[best_index]\n",
        "    for i in range(len(sel_names)):\n",
        "      s = sel_names[i]\n",
        "      fs_results[s]['sel_score'] = sel_scores[i]\n",
        "\n",
        "    return fs_results, best_sel_name\n",
        "\n",
        "  # Between the optimal sets for each estimator select the set with the higher score.\n",
        "  def selectOptimalFeatures(self, est, est_name, est_param_space, log_fh):\n",
        "    \"\"\"\n",
        "    Select optimal features by comparing individual feature selectors\n",
        "    and combined features\n",
        "    \"\"\"\n",
        "    X_train_selected = None\n",
        "    # set it to a large negative value\n",
        "    fs_summary = {}\n",
        "    row_count = 1\n",
        "\n",
        "    est_info = '\\nEstimator: ' + est_name\n",
        "    est_info += '\\n---------------------------'\n",
        "    log_fh.write(est_info)\n",
        "    print(est_info)\n",
        "\n",
        "    fs_results, best_sel = self.compareFeatureSelectors(est_name, est, est_param_space)\n",
        "\n",
        "    # result includes\n",
        "    # - 'best_selector' : name of the best selector\n",
        "    # - 'best_indices' : indices of features selected\n",
        "    # - 'fs_results' : dict with 'indices', 'mean_score' and 'std_score' for all selectors\n",
        "\n",
        "    for sel_name in fs_results:\n",
        "      mean_score = np.round(fs_results[sel_name]['mean_score'], 2)\n",
        "      std_score = np.round(fs_results[sel_name]['std_score'], 2)\n",
        "      sel_score = np.round(fs_results[sel_name]['sel_score'], 2)\n",
        "      est_sel_row = [est_name, sel_name, mean_score, std_score, sel_score]\n",
        "      fs_summary['row' + str(row_count)] = est_sel_row\n",
        "      row_count += 1\n",
        "\n",
        "    selected_indices = fs_results[best_sel]['indices']\n",
        "    best_estimator = fs_results[best_sel]['estimator']\n",
        "    fs_df_columns = ['estimator', 'selector', 'mean score', 'std score', 'selector score']\n",
        "    fs_df = pd.DataFrame.from_dict(fs_summary, orient='index', columns=fs_df_columns)\n",
        "    best_params = fs_results[best_sel]['best_params']\n",
        "    best_params_info = '\\nbest parameters:'\n",
        "    for c in best_params:\n",
        "      best_params_info += '\\n' + c + '=' + str(best_params[c])\n",
        "\n",
        "    log_fh.write(best_params_info)\n",
        "    print(best_params_info)\n",
        "\n",
        "    ftsel_summary_info = '\\nBest selector: ' + best_sel\n",
        "    ftsel_summary_info += '\\nFeature Selection Summary'\n",
        "    ftsel_summary_info += '\\n---------------------------'\n",
        "    ftsel_summary_info += '\\n' + fs_df.to_string(index=False) + '\\n'\n",
        "    log_fh.write(ftsel_summary_info)\n",
        "    print(ftsel_summary_info)\n",
        "\n",
        "    return selected_indices, best_estimator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWdWViKkuFM1"
      },
      "source": [
        "#### Algorithm Evaluator Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LutX8Ih9MsGx"
      },
      "outputs": [],
      "source": [
        "#%%writefile algorithm_evaluation.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "import multiprocessing as mp\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.utils.fixes import loguniform\n",
        "from sklearn.utils import parallel_backend\n",
        "\n",
        "class CYPAlgorithmEvaluator:\n",
        "  def __init__(self, cyp_config, custom_cv=None,\n",
        "               train_weights=None, test_weights=None):\n",
        "    self.scaler = cyp_config.getFeatureScaler()\n",
        "    self.estimators = cyp_config.getEstimators()\n",
        "    self.custom_cv = custom_cv\n",
        "    self.cv_metric = cyp_config.getAlgorithmTrainingCVMetric()\n",
        "    self.train_weights = train_weights\n",
        "    self.test_weights = test_weights\n",
        "    self.metrics = cyp_config.getEvaluationMetrics()\n",
        "    self.verbose = cyp_config.getDebugLevel()\n",
        "    self.use_yield_trend = cyp_config.useYieldTrend()\n",
        "    self.trend_windows = cyp_config.getTrendWindows()\n",
        "    self.predict_residuals = cyp_config.predictYieldResiduals()\n",
        "    self.retrain_per_test_year = cyp_config.retrainPerTestYear()\n",
        "    self.use_sample_weights = cyp_config.useSampleWeights()\n",
        "    spatial_level = cyp_config.getSpatialLevel()\n",
        "    self.id_col = \"COUNTY_ID\" if (spatial_level == \"COUNTY\") else \"GRID_ID\"\n",
        "\n",
        "  def setCustomCV(self, custom_cv):\n",
        "    \"\"\"Set custom K-Fold validation splits\"\"\"\n",
        "    self.custom_cv = custom_cv\n",
        "\n",
        "  # Nash-Sutcliffe Model Efficiency\n",
        "  def nse(self, Y_true, Y_pred):\n",
        "    \"\"\"\n",
        "        Nash Sutcliffe efficiency coefficient\n",
        "        input:\n",
        "          Y_pred: predicted\n",
        "          Y_true: observed\n",
        "        output:\n",
        "          nse: Nash Sutcliffe efficient coefficient\n",
        "        \"\"\"\n",
        "    return (1 - np.sum(np.square(Y_pred - Y_true))/np.sum(np.square(Y_true - np.mean(Y_true))))\n",
        "\n",
        "  def updateAlgorithmsSummary(self, alg_summary, alg_name, scores_list):\n",
        "    \"\"\"Update algorithms summary with scores for given algorithm\"\"\"\n",
        "    alg_row = [alg_name]\n",
        "    alg_index = len(alg_summary)\n",
        "    assert (len(scores_list) > 0)\n",
        "    for met in scores_list[0]:\n",
        "      for pred_scores in scores_list:\n",
        "        alg_row.append(pred_scores[met])\n",
        "\n",
        "    alg_summary['row' + str(alg_index)] = alg_row\n",
        "\n",
        "  def createPredictionDataFrames(self, Y_pred_arrays, data_cols):\n",
        "    \"\"\"\"Create pandas data frames from true and predicted values\"\"\"\n",
        "    pd_pred_dfs = []\n",
        "    for ar in Y_pred_arrays:\n",
        "      pd_df = pd.DataFrame(data=ar, columns=data_cols)\n",
        "      pd_pred_dfs.append(pd_df)\n",
        "\n",
        "    return pd_pred_dfs\n",
        "\n",
        "  def printPredictionDataFrames(self, pd_pred_dfs, pred_set_info, log_fh):\n",
        "    \"\"\"\"Print true and predicted values from pandas data frames\"\"\"\n",
        "    for i in range(len(pd_pred_dfs)):\n",
        "      pd_df = pd_pred_dfs[i]\n",
        "      set_info = pred_set_info[i]\n",
        "      df_info = '\\n Yield Predictions ' + set_info\n",
        "      df_info += '\\n--------------------------------'\n",
        "      df_info += '\\n' + pd_df.head(6).to_string(index=False)\n",
        "      log_fh.write(df_info + '\\n')\n",
        "      print(df_info)\n",
        "\n",
        "  def getNullMethodPredictions(self, Y_train_full, Y_test_full, cv_test_years, log_fh):\n",
        "    \"\"\"\n",
        "    The Null method or poor man's prediction. Y_*_full includes id_col, FYEAR.\n",
        "    If using yield trend, Y_*_full also include YIELD_TREND.\n",
        "    The null method predicts the YIELD_TREND or the average of the training set.\n",
        "    \"\"\"\n",
        "    Y_train = Y_train_full[:, 2]\n",
        "    if (self.use_yield_trend):\n",
        "      Y_train = Y_train_full[:, 3]\n",
        "\n",
        "    min_yield = np.round(np.min(Y_train), 2)\n",
        "    max_yield = np.round(np.max(Y_train), 2)\n",
        "    avg_yield = np.round(np.mean(Y_train), 2)\n",
        "    median_yield = np.round(np.median(np.ravel(Y_train)), 2)\n",
        "    cv_test_years = np.array(cv_test_years)\n",
        "\n",
        "    null_method_label = 'Null Method: '\n",
        "    if (self.use_yield_trend):\n",
        "      null_method_label += 'Predicting linear yield trend:'\n",
        "      data_cols = [self.id_col, 'FYEAR', 'YIELD_TREND', 'YIELD']\n",
        "      Y_cv_full = Y_train_full[np.in1d(Y_train_full[:, 1], cv_test_years)]\n",
        "      Y_pred_arrays = [Y_train_full, Y_cv_full, Y_test_full]\n",
        "    else:\n",
        "      Y_train_full_n = np.insert(Y_train_full, 2, avg_yield, axis=1)\n",
        "      Y_test_full_n = np.insert(Y_test_full, 2, avg_yield, axis=1)\n",
        "      null_method_label += 'Predicting average of the training set:'\n",
        "      data_cols = [self.id_col, 'FYEAR', 'YIELD_PRED', 'YIELD']\n",
        "      Y_cv_full = Y_train_full_n[np.in1d(Y_train_full_n[:, 1], cv_test_years)]\n",
        "      Y_pred_arrays = [Y_train_full_n, Y_cv_full, Y_test_full_n]\n",
        "\n",
        "    pd_pred_dfs = self.createPredictionDataFrames(Y_pred_arrays, data_cols)\n",
        "    null_method_info = '\\n' + null_method_label\n",
        "    null_method_info += '\\nMin Yield: ' + str(min_yield) + ', Max Yield: ' + str(max_yield)\n",
        "    null_method_info += '\\nMedian Yield: ' + str(median_yield) + ', Mean Yield: ' + str(avg_yield)\n",
        "    log_fh.write(null_method_info + '\\n')\n",
        "    print(null_method_info)\n",
        "    pred_set_info = ['Training Set', 'Validation Test Set', 'Test Set']\n",
        "    self.printPredictionDataFrames(pd_pred_dfs, pred_set_info, log_fh)\n",
        "\n",
        "    result = {\n",
        "        'train' : pd_pred_dfs[0],\n",
        "        'custom_cv' : pd_pred_dfs[1],\n",
        "        'test' : pd_pred_dfs[2],\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "  def evaluateNullMethodPredictions(self, pd_pred_dfs, alg_summary):\n",
        "    \"\"\"Evaluate predictions of the Null method\"\"\"\n",
        "    if (self.use_yield_trend):\n",
        "      alg_name = 'trend'\n",
        "      pred_col_name = 'YIELD_TREND'\n",
        "    else:\n",
        "      alg_name = 'average'\n",
        "      pred_col_name = 'YIELD_PRED'\n",
        "\n",
        "    scores_list = []\n",
        "    for pred_set in pd_pred_dfs:\n",
        "      pred_df = pd_pred_dfs[pred_set]\n",
        "      Y_true = pred_df['YIELD'].values\n",
        "      Y_pred = pred_df[pred_col_name].values\n",
        "      pred_scores = getPredictionScores(Y_true, Y_pred, self.metrics)\n",
        "      scores_list.append(pred_scores)\n",
        "\n",
        "    self.updateAlgorithmsSummary(alg_summary, alg_name, scores_list)\n",
        "\n",
        "  def getYieldTrendML(self, X_train, Y_train, X_test):\n",
        "    \"\"\"\n",
        "    Predict yield trend using a linear model.\n",
        "    No need to scale features. They are all yield values.\n",
        "    \"\"\"\n",
        "    est = Ridge(alpha=1, random_state=42, max_iter=1000,\n",
        "                copy_X=True, fit_intercept=True)\n",
        "    param_space = dict(estimator__alpha=loguniform(1e-1, 1e+2))\n",
        "    pipeline = Pipeline([(\"scaler\", self.scaler), (\"estimator\", est)])\n",
        "    nparams_sampled = pow(3, len(param_space))\n",
        "\n",
        "    X_train_copy = np.copy(X_train)\n",
        "    rand_search = RandomizedSearchCV(estimator=pipeline,\n",
        "                                     param_distributions=param_space,\n",
        "                                     n_iter=nparams_sampled,\n",
        "                                     scoring=self.cv_metric,\n",
        "                                     cv=self.custom_cv,\n",
        "                                     return_train_score=True,\n",
        "                                     refit=modelRefitMeanVariance)\n",
        "\n",
        "    fit_params = {}\n",
        "    if (self.use_sample_weights):\n",
        "      fit_params = { 'estimator__sample_weight' : self.train_weights }\n",
        "\n",
        "    with parallel_backend('spark', n_jobs=-1):\n",
        "      rand_search.fit(X_train_copy, np.ravel(Y_train), **fit_params)\n",
        "\n",
        "    best_params = rand_search.best_params_\n",
        "    best_estimator = rand_search.best_estimator_\n",
        "    if (self.verbose > 1):\n",
        "      print('\\nYield Trend: Ridge best parameters:')\n",
        "      for param in param_space:\n",
        "        print(param + '=', best_params[param])\n",
        "\n",
        "    Y_pred_train = np.reshape(best_estimator.predict(X_train), (X_train.shape[0], 1))\n",
        "    Y_pred_test = np.reshape(best_estimator.predict(X_test), (X_test.shape[0], 1))\n",
        "\n",
        "    return Y_pred_train, Y_pred_test\n",
        "\n",
        "  def estimateYieldTrendAndDetrend(self, X_train, Y_train, X_test, Y_test, features):\n",
        "    \"\"\"Estimate yield trend using machine learning and detrend\"\"\"\n",
        "    trend_window = self.trend_windows[0]\n",
        "    # NOTE assuming previous years' yield values are at the end\n",
        "    X_train_trend = X_train[:, -trend_window:]\n",
        "    X_test_trend = X_test[:, -trend_window:]\n",
        "\n",
        "    Y_train_trend, Y_test_trend = self.getYieldTrendML(X_train_trend, Y_train, X_test_trend)\n",
        "    # New features exclude previous years' yield and include YIELD_TREND\n",
        "    features_n = features[:-trend_window] + ['YIELD_TREND']\n",
        "    X_train_n = np.append(X_train[:, :-trend_window], Y_train_trend, axis=1)\n",
        "    X_test_n = np.append(X_test[:, :-trend_window], Y_test_trend, axis=1)\n",
        "    Y_train_res = np.reshape(Y_train, (X_train.shape[0], 1)) - Y_train_trend\n",
        "    Y_test_res = np.reshape(Y_test, (X_test.shape[0], 1)) - Y_test_trend\n",
        "\n",
        "    result =  {\n",
        "        'X_train' : X_train_n,\n",
        "        'Y_train' : Y_train_res,\n",
        "        'Y_train_trend' : Y_train_trend,\n",
        "        'X_test' : X_test_n,\n",
        "        'Y_test' : Y_test_res,\n",
        "        'Y_test_trend' : Y_test_trend,\n",
        "        'features' : features_n,\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "  def yieldPredictionsFromResiduals(self, pd_train_df, Y_train, pd_test_df, Y_test,\n",
        "                                    pd_cv_df, cv_test_years):\n",
        "    \"\"\"Predictions are residuals. Add trend back to get yield predictions.\"\"\"\n",
        "    pd_train_df['YIELD_RES'] = pd_train_df['YIELD']\n",
        "    pd_train_df['YIELD'] = Y_train\n",
        "    Y_custom_cv = pd_train_df[pd_train_df['FYEAR'].isin(cv_test_years)]['YIELD'].values\n",
        "    pd_cv_df['YIELD_RES'] = pd_cv_df['YIELD']\n",
        "    pd_cv_df['YIELD'] = Y_custom_cv\n",
        "    pd_test_df['YIELD_RES'] = pd_test_df['YIELD']\n",
        "    pd_test_df['YIELD'] = Y_test\n",
        "\n",
        "    for alg in self.estimators:\n",
        "      pd_train_df['YIELD_RES_PRED_' + alg] = pd_train_df['YIELD_PRED_' + alg]\n",
        "      pd_train_df['YIELD_PRED_' + alg] = pd_train_df['YIELD_RES_PRED_' + alg] + pd_train_df['YIELD_TREND']\n",
        "      pd_test_df['YIELD_RES_PRED_' + alg] = pd_test_df['YIELD_PRED_' + alg]\n",
        "      pd_test_df['YIELD_PRED_' + alg] = pd_test_df['YIELD_RES_PRED_' + alg] + pd_test_df['YIELD_TREND']\n",
        "      pd_cv_df['YIELD_RES_PRED_' + alg] = pd_cv_df['YIELD_PRED_' + alg]\n",
        "      pd_cv_df['YIELD_PRED_' + alg] = pd_cv_df['YIELD_RES_PRED_' + alg] + pd_cv_df['YIELD_TREND']\n",
        "\n",
        "    sel_cols = [self.id_col, 'FYEAR', 'YIELD_TREND', 'YIELD_RES']\n",
        "    for alg in self.estimators:\n",
        "      sel_cols += ['YIELD_RES_PRED_' + alg, 'YIELD_PRED_' + alg]\n",
        "\n",
        "    sel_cols.append('YIELD')\n",
        "    result = {\n",
        "        'train' : pd_train_df[sel_cols],\n",
        "        'custom_cv' : pd_cv_df[sel_cols],\n",
        "        'test' : pd_test_df[sel_cols],\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "  def updateFeatureSelectionInfo(self, est_name, features, selected_indices,\n",
        "                                 ft_selection_counts, ft_importances, log_fh):\n",
        "    \"\"\"\n",
        "    Update feature selection counts.\n",
        "    Print selected features and importance.\n",
        "    \"\"\"\n",
        "    # update feature selection counts\n",
        "    for idx in selected_indices:\n",
        "      ft_count = 0\n",
        "      ft = features[idx]\n",
        "      ft_period = 'static'\n",
        "      for p in ft_selection_counts:\n",
        "        if p in ft:\n",
        "          ft_period = p\n",
        "\n",
        "      if (ft in ft_selection_counts[ft_period]):\n",
        "        ft_count = ft_selection_counts[ft_period][ft]\n",
        "\n",
        "      ft_selection_counts[ft_period][ft] = ft_count + 1\n",
        "\n",
        "    if (ft_importances is not None):\n",
        "      ft_importance_indices = []\n",
        "      ft_importance_values = [0.0 for i in range(len(features))]\n",
        "      for idx in reversed(np.argsort(ft_importances)):\n",
        "        ft_importance_indices.append(selected_indices[idx])\n",
        "        ft_importance_values[selected_indices[idx]] = str(np.round(ft_importances[idx], 2))\n",
        "\n",
        "      ft_importance_info = '\\nSelected features with importance:'\n",
        "      ft_importance_info += '\\n----------------------------------'\n",
        "      log_fh.write(ft_importance_info)\n",
        "      print(ft_importance_info)\n",
        "      printInGroups(features, ft_importance_indices, ft_importance_values, log_fh)\n",
        "    else:\n",
        "      sel_fts_info = '\\nSelected Features:'\n",
        "      sel_fts_info += '\\n-------------------'\n",
        "      log_fh.write(sel_fts_info)\n",
        "      print(sel_fts_info)\n",
        "      printInGroups(features, selected_indices, log_fh=log_fh)\n",
        "\n",
        "  def getCustomCVPredictions(self, est_name, best_est,\n",
        "                             X_train, Y_train, Y_cv_full):\n",
        "    \"\"\"Get predictions for custom cv test years\"\"\"\n",
        "    Y_pred_cv = np.zeros(Y_cv_full.shape[0])\n",
        "    fit_predict_args = []\n",
        "    for i in range(len(self.custom_cv)):\n",
        "      cv_train_idxs, cv_test_idxs = self.custom_cv[i]\n",
        "      sample_weights = None\n",
        "      if (self.use_sample_weights):\n",
        "        sample_weights = np.copy(self.train_weights[cv_train_idxs])\n",
        "\n",
        "      fit_params = {}\n",
        "      if (self.use_sample_weights and (est_name != 'KNN')):\n",
        "        fit_params = { 'estimator__sample_weight' : sample_weights }\n",
        "\n",
        "      fit_predict_args.append(\n",
        "          {\n",
        "              'X_train' : np.copy(X_train[cv_train_idxs, :]),\n",
        "              'Y_train' : np.copy(Y_train[cv_train_idxs]),\n",
        "              'X_test' : np.copy(X_train[cv_test_idxs, :]),\n",
        "              'fit_params' : fit_params,\n",
        "              'estimator' : deepcopy(best_est),\n",
        "          }\n",
        "      )\n",
        "\n",
        "    pool = mp.Pool(len(self.custom_cv))\n",
        "    Y_preds = pool.map(customFitPredict, fit_predict_args)\n",
        "    for i in range(len(self.custom_cv)):\n",
        "      cv_train_idxs, cv_test_idxs = self.custom_cv[i]\n",
        "      Y_pred_cv[cv_test_idxs] = Y_preds[i]\n",
        "\n",
        "    # clean up\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    return Y_pred_cv\n",
        "\n",
        "  def getPerTestYearPredictions(self, est_name, best_est,\n",
        "                                X_train, Y_train_full,\n",
        "                                X_test, Y_test_full, test_years):\n",
        "    \"\"\"For each test year, fit best_est on all previous years and predict\"\"\"\n",
        "    Y_train = Y_train_full[:, -1]\n",
        "    Y_test = Y_test_full[:, -1]\n",
        "    Y_pred_test = np.zeros(Y_test_full.shape[0])\n",
        "\n",
        "    # For the first test year, X_train and Y_train do not change. No need to refit.\n",
        "    test_indexes = np.where(Y_test_full[:, 1] == test_years[0])[0]\n",
        "    Y_pred_first_yr = best_est.predict(X_test[test_indexes, :])\n",
        "    Y_pred_test[test_indexes] = Y_pred_first_yr\n",
        "\n",
        "    if (est_name == 'GBDT'):\n",
        "      best_est.named_steps['estimator'].set_params(**{ 'warm_start' : True })\n",
        "\n",
        "    fit_predict_args = []\n",
        "    for i in range(1, len(test_years)):\n",
        "      extra_train_years = test_years[:i]\n",
        "      test_indexes = np.where(Y_test_full[:, 1] == test_years[i])[0]\n",
        "      sample_weights = None\n",
        "      if (self.use_sample_weights):\n",
        "        sample_weights = self.train_weights\n",
        "\n",
        "      train_indexes_n = np.ravel(np.nonzero(np.isin(Y_test_full[:, 1], extra_train_years)))\n",
        "      X_train_n = np.append(X_train, X_test[train_indexes_n, :], axis=0)\n",
        "      Y_train_n = np.append(Y_train, Y_test[train_indexes_n])\n",
        "      if (self.use_sample_weights):\n",
        "        sample_weights = np.append(sample_weights, self.test_weights[train_indexes_n], axis=0)\n",
        "\n",
        "      fit_params = {}\n",
        "      if (self.use_sample_weights and (est_name != 'KNN')):\n",
        "        fit_params['estimator__sample_weight'] = sample_weights\n",
        "\n",
        "      fit_predict_args.append(\n",
        "          {\n",
        "              'X_train' : np.copy(X_train_n),\n",
        "              'Y_train' : np.copy(Y_train_n),\n",
        "              'X_test' : np.copy(X_test[test_indexes, :]),\n",
        "              'fit_params' : fit_params,\n",
        "              'estimator' : deepcopy(best_est),\n",
        "          }\n",
        "      )\n",
        "\n",
        "    pool = mp.Pool(len(test_years) - 1)\n",
        "    Y_preds = pool.map(customFitPredict, fit_predict_args)\n",
        "    for i in range(1, len(test_years)):\n",
        "      test_indexes = np.where(Y_test_full[:, 1] == test_years[i])[0]\n",
        "      Y_pred_test[test_indexes] = Y_preds[i-1]\n",
        "\n",
        "    # clean up\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    return Y_pred_test\n",
        "\n",
        "  def combineAlgorithmPredictions(self, pd_ml_predictions, pd_alg_predictions, alg):\n",
        "    \"\"\"Combine predictions of ML algorithms.\"\"\"\n",
        "    join_cols = [self.id_col, 'FYEAR']\n",
        "\n",
        "    if (pd_ml_predictions is None):\n",
        "      pd_ml_predictions = pd_alg_predictions\n",
        "      pd_ml_predictions = pd_ml_predictions.rename(columns={'YIELD_PRED': 'YIELD_PRED_' + alg })\n",
        "    else:\n",
        "      pd_alg_predictions = pd_alg_predictions[join_cols + ['YIELD_PRED']]\n",
        "      pd_ml_predictions = pd_ml_predictions.merge(pd_alg_predictions, on=join_cols)\n",
        "      pd_ml_predictions = pd_ml_predictions.rename(columns={'YIELD_PRED': 'YIELD_PRED_' + alg })\n",
        "      # Put YIELD at the end\n",
        "      all_cols = list(pd_ml_predictions.columns)\n",
        "      col_order = all_cols[:-2] + ['YIELD_PRED_' + alg, 'YIELD']\n",
        "      pd_ml_predictions = pd_ml_predictions[col_order]\n",
        "\n",
        "    return pd_ml_predictions\n",
        "\n",
        "  def getMLPredictions(self, X_train, Y_train_full, X_test, Y_test_full,\n",
        "                       cv_test_years, cyp_ftsel, features, log_fh):\n",
        "    \"\"\"Train and evaluate crop yield prediction algorithms\"\"\"\n",
        "    # Y_*_full\n",
        "    # GRID_ID, FYEAR, YIELD_TREND, YIELD_PRED_Ridge, ..., YIELD_PRED_GBDT, YIELD\n",
        "    # NL11, NL12, NL13 (some regions can have missing values)\n",
        "    # 1999, ..., 2011 => training\n",
        "    # 2012, ..., 2018 => Test\n",
        "    # We need to aggregate to national level. Need predictions from all regions.\n",
        "    # cv_test_years\n",
        "    # 1999, ..., 2006 => 2007\n",
        "    # 1999, ..., 2007 => 2008\n",
        "    # ...\n",
        "    # cv_test_years : [2007, 2008, ..., 2011]\n",
        "\n",
        "    Y_train = Y_train_full[:, -1]\n",
        "    train_years = sorted(np.unique(Y_train_full[:, 1]))\n",
        "    Y_cv_full = np.copy(Y_train_full)\n",
        "    Y_test = Y_test_full[:, -1]\n",
        "    test_years = sorted(np.unique(Y_test_full[:, 1]))\n",
        "\n",
        "    pd_test_predictions = None\n",
        "    pd_cv_predictions = None\n",
        "    pd_train_predictions = None\n",
        "\n",
        "    # feature selection frequency\n",
        "    # NOTE must be in sync with crop calendar periods\n",
        "    ft_selection_counts = {\n",
        "        'static' : {}, 'p0' : {}, 'p1' : {}, 'p2' : {}, 'p3' : {}, 'p4' : {}, 'p5' : {},\n",
        "    }\n",
        "\n",
        "    for est_name in self.estimators:\n",
        "      # feature selection\n",
        "      est = self.estimators[est_name]['estimator']\n",
        "      param_space = self.estimators[est_name]['param_space']\n",
        "      sel_indices, best_est = cyp_ftsel.selectOptimalFeatures(est, est_name, param_space, log_fh)\n",
        "\n",
        "      # feature importance\n",
        "      ft_importances = None\n",
        "      if ((est_name == 'Ridge') or (est_name == 'Lasso')):\n",
        "        ft_importances = best_est.named_steps['estimator'].coef_\n",
        "      elif (est_name == 'SVR'):\n",
        "        try:\n",
        "          ft_importances = np.ravel(best_est.named_steps['estimator'].coef_)\n",
        "        except AttributeError as e:\n",
        "          ft_importances = None\n",
        "      elif ((est_name == 'GBDT') or (est_name == 'RF') or (est_name == 'ERT')):\n",
        "        ft_importances = best_est.named_steps['estimator'].feature_importances_\n",
        "\n",
        "      self.updateFeatureSelectionInfo(est_name, features, sel_indices, ft_selection_counts,\n",
        "                                      ft_importances, log_fh)\n",
        "\n",
        "      # Extract selected features\n",
        "      # X_train_sel = X_train[:, sel_indices]\n",
        "      # X_test_sel = X_test[:, sel_indices]\n",
        "      # fit_params = {}\n",
        "      # if (self.use_sample_weights and (est_name != 'KNN')):\n",
        "      #   fit_params['estimator__sample_weight'] = self.train_weights\n",
        "\n",
        "      # best_est.fit(X_train_sel, Y_train, **fit_params)\n",
        "\n",
        "      # Predictions\n",
        "      Y_pred_train = best_est.predict(X_train)\n",
        "      Y_pred_test = best_est.predict(X_test)\n",
        "\n",
        "      # custom cv predictions for cv metrics\n",
        "      Y_pred_cv = self.getCustomCVPredictions(est_name, best_est,\n",
        "                                              X_train, Y_train, Y_cv_full)\n",
        "\n",
        "      # per test year predictions\n",
        "      # 1999, ..., 2011 => 2012\n",
        "      # 1999, ..., 2012 => 2013\n",
        "      # ...\n",
        "      if (self.retrain_per_test_year):\n",
        "        Y_pred_test = self.getPerTestYearPredictions(est_name, best_est,\n",
        "                                                     X_train, Y_train_full,\n",
        "                                                     X_test, Y_test_full, test_years)\n",
        "\n",
        "      data_cols = [self.id_col, 'FYEAR']\n",
        "      if (self.use_yield_trend):\n",
        "        data_cols.append('YIELD_TREND')\n",
        "        Y_train_full_n = np.insert(Y_train_full, 3, Y_pred_train, axis=1)\n",
        "        Y_cv_full_n = np.insert(Y_cv_full, 3, Y_pred_cv, axis=1)\n",
        "        Y_test_full_n = np.insert(Y_test_full, 3, Y_pred_test, axis=1)\n",
        "      else:\n",
        "        Y_train_full_n = np.insert(Y_train_full, 2, Y_pred_train, axis=1)\n",
        "        Y_cv_full_n = np.insert(Y_cv_full, 2, Y_pred_cv, axis=1)\n",
        "        Y_test_full_n = np.insert(Y_test_full, 2, Y_pred_test, axis=1)\n",
        "\n",
        "      data_cols += ['YIELD_PRED', 'YIELD']\n",
        "      Y_cv_full_n = Y_cv_full_n[np.in1d(Y_cv_full_n[:, 1], cv_test_years)]\n",
        "      Y_pred_arrays = [Y_train_full_n, Y_cv_full_n, Y_test_full_n]\n",
        "      pd_pred_dfs = self.createPredictionDataFrames(Y_pred_arrays, data_cols)\n",
        "      pd_train_predictions = self.combineAlgorithmPredictions(pd_train_predictions,\n",
        "                                                              pd_pred_dfs[0], est_name)\n",
        "      pd_cv_predictions = self.combineAlgorithmPredictions(pd_cv_predictions,\n",
        "                                                           pd_pred_dfs[1], est_name)\n",
        "      pd_test_predictions = self.combineAlgorithmPredictions(pd_test_predictions,\n",
        "                                                             pd_pred_dfs[2], est_name)\n",
        "\n",
        "    ft_counts_info = '\\nFeature Selection Frequencies'\n",
        "    ft_counts_info += '\\n-------------------------------'\n",
        "    for ft_period in ft_selection_counts:\n",
        "      ft_count_str = ft_period + ': '\n",
        "      for ft in sorted(ft_selection_counts[ft_period],\n",
        "                       key=ft_selection_counts[ft_period].get, reverse=True):\n",
        "        ft_count_str += ft + '(' + str(ft_selection_counts[ft_period][ft]) + '), '\n",
        "\n",
        "      if (len(ft_selection_counts[ft_period]) > 0):\n",
        "        # drop ', ' from the end\n",
        "        ft_count_str = ft_count_str[:-2]\n",
        "\n",
        "      ft_counts_info += '\\n' + ft_count_str\n",
        "\n",
        "    ft_counts_info += '\\n'\n",
        "    log_fh.write(ft_counts_info)\n",
        "    if (self.verbose > 1):\n",
        "      print(ft_counts_info)\n",
        "\n",
        "    result = {\n",
        "        'train' : pd_train_predictions,\n",
        "        'custom_cv' : pd_cv_predictions,\n",
        "        'test' : pd_test_predictions,\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "  def evaluateMLPredictions(self, pd_pred_dfs, alg_summary):\n",
        "    \"\"\"Evaluate predictions of ML algorithms and add entries to alg_summary.\"\"\"\n",
        "    for alg in self.estimators:\n",
        "      pred_col = 'YIELD_PRED_' + alg\n",
        "      scores_list = []\n",
        "      for pred_set in pd_pred_dfs:\n",
        "        pred_df = pd_pred_dfs[pred_set]\n",
        "        Y_true = pred_df['YIELD'].values\n",
        "        Y_pred = pred_df[pred_col].values\n",
        "        pred_scores = getPredictionScores(Y_true, Y_pred, self.metrics)\n",
        "        scores_list.append(pred_scores)\n",
        "\n",
        "      self.updateAlgorithmsSummary(alg_summary, alg, scores_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpxrWSjsxMdx"
      },
      "source": [
        "#### Run Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HbC8rSxjEpT"
      },
      "outputs": [],
      "source": [
        "#%%writefile run_machine_learning.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from joblibspark import register_spark\n",
        "\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "\n",
        "def dropHighlyCorrelatedFeatures(cyp_config, pd_train_df, pd_test_df,\n",
        "                                 log_fh, corr_method='pearson', corr_thresh=0.95):\n",
        "  \"\"\"Plot correlations. Drop columns that are highly correlated.\"\"\"\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  all_cols = list(pd_train_df.columns)[2:]\n",
        "  avg_cols = [c for c in all_cols if 'avg' in c] + ['YIELD']\n",
        "  max_cols = [c for c in all_cols if 'max' in c] + ['YIELD']\n",
        "  lt_th_cols = [c for c in all_cols if 'Z-' in c] + ['YIELD']\n",
        "  gt_th_cols = [c for c in all_cols if 'Z+' in c] + ['YIELD']\n",
        "  yt_cols = ['YIELD-' + str(i) for i in range(1, 6)]  + ['YIELD']\n",
        "\n",
        "  if (debug_level > 2):\n",
        "    plotCorrelation(pd_train_df, avg_cols)\n",
        "    plotCorrelation(pd_train_df, max_cols)\n",
        "    plotCorrelation(pd_train_df, lt_th_cols)\n",
        "    plotCorrelation(pd_train_df, gt_th_cols)\n",
        "    plotCorrelation(pd_train_df, yt_cols)\n",
        "\n",
        "  # drop highly correlated features\n",
        "  # Based on https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/\n",
        "\n",
        "  corr_columns = [c for c in all_cols if ((c != 'YIELD') and (c != 'YIELD_TREND'))]\n",
        "  corr_matrix = pd_train_df[corr_columns].corr(method=corr_method).abs()\n",
        "  ut_mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "  ut_matrix = corr_matrix.mask(ut_mask)\n",
        "  to_drop = [c for c in ut_matrix.columns if any(ut_matrix[c] > corr_thresh)]\n",
        "  drop_info = '\\nDropping highly correlated features'\n",
        "  drop_info += '\\n' + ', '.join(to_drop)\n",
        "\n",
        "  log_fh.write(drop_info + '\\n')\n",
        "  if ((debug_level > 1) and (to_drop)):\n",
        "    print(drop_info)\n",
        "\n",
        "  pd_train_df = pd_train_df.drop(columns=to_drop)\n",
        "  pd_test_df = pd_test_df.drop(columns=to_drop)\n",
        "\n",
        "  return pd_train_df, pd_test_df\n",
        "\n",
        "def getValidationSplits(cyp_config, pd_train_df, pd_test_df, log_fh):\n",
        "  \"\"\"Split features and label into training and test sets\"\"\"\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  use_sample_weights = cyp_config.useSampleWeights()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  spatial_level = cyp_config.getSpatialLevel()\n",
        "  id_col = \"COUNTY_ID\" if (spatial_level == \"COUNTY\") else \"GRID_ID\"\n",
        "\n",
        "  regions = [reg for reg in pd_train_df[id_col].unique()]\n",
        "  num_regions = len(regions)\n",
        "\n",
        "  original_headers = list(pd_train_df.columns.values)\n",
        "  features = []\n",
        "  labels = []\n",
        "  if (use_yield_trend):\n",
        "    if (use_sample_weights):\n",
        "      features = original_headers[2:-3]\n",
        "      labels = original_headers[:2] + original_headers[-3:-1]\n",
        "    else:\n",
        "      features = original_headers[2:-2]\n",
        "      labels = original_headers[:2] + original_headers[-2:]\n",
        "  else:\n",
        "    if (use_sample_weights):\n",
        "      features = original_headers[2:-2]\n",
        "      labels = original_headers[:2] + original_headers[-2:-1]\n",
        "    else:\n",
        "      features = original_headers[2:-1]\n",
        "      labels = original_headers[:2] + original_headers[-1:]\n",
        "\n",
        "  X_train = pd_train_df[features].values\n",
        "  Y_train = pd_train_df[labels].values\n",
        "  train_weights = None\n",
        "  if (use_sample_weights):\n",
        "    train_weights = pd_train_df['SAMPLE_WEIGHT'].values\n",
        "\n",
        "  train_info = '\\nTraining Data Size: ' + str(len(pd_train_df.index)) + ' rows'\n",
        "  train_info += '\\nX cols: ' + str(X_train.shape[1]) + ', Y cols: ' + str(Y_train.shape[1])\n",
        "  train_info += '\\n' + pd_train_df.head(5).to_string(index=False)\n",
        "  log_fh.write(train_info + '\\n')\n",
        "  if (debug_level > 1):\n",
        "    print(train_info)\n",
        "\n",
        "  X_test = pd_test_df[features].values\n",
        "  Y_test = pd_test_df[labels].values\n",
        "  test_weights = None\n",
        "  if (use_sample_weights):\n",
        "    test_weights = pd_test_df['SAMPLE_WEIGHT'].values\n",
        "\n",
        "  test_info = '\\nTest Data Size: ' + str(len(pd_test_df.index)) + ' rows'\n",
        "  test_info += '\\nX cols: ' + str(X_test.shape[1]) + ', Y cols: ' + str(Y_test.shape[1])\n",
        "  test_info += '\\n' + pd_test_df.head(5).to_string(index=False)\n",
        "  log_fh.write(test_info + '\\n')\n",
        "  if (debug_level > 1):\n",
        "    print(test_info)\n",
        "\n",
        "  # print feature names\n",
        "  num_features = len(features)\n",
        "  indices = [idx for idx in range(num_features)]\n",
        "  feature_info = '\\nAll features'\n",
        "  feature_info += '\\n-------------'\n",
        "  log_fh.write(feature_info)\n",
        "  print(feature_info)\n",
        "  printInGroups(features, indices, log_fh=log_fh)\n",
        "\n",
        "  # num_folds for k-fold cv\n",
        "  num_folds = 5\n",
        "  custom_cv = num_folds\n",
        "  cv_test_years = []\n",
        "  if (use_yield_trend):\n",
        "    cyp_cv_splitter = CYPTrainTestSplitter(cyp_config)\n",
        "    custom_cv, cv_test_years = cyp_cv_splitter.customKFoldValidationSplit(Y_train, num_folds, log_fh)\n",
        "\n",
        "  result = {\n",
        "      'X_train' : X_train,\n",
        "      'Y_train_full' : Y_train,\n",
        "      'train_weights' : train_weights,\n",
        "      'X_test' : X_test,\n",
        "      'Y_test_full' : Y_test,\n",
        "      'test_weights' : test_weights,\n",
        "      'custom_cv' : custom_cv,\n",
        "      'cv_test_years' : cv_test_years,\n",
        "      'features' : features,\n",
        "  }\n",
        "\n",
        "  return result\n",
        "\n",
        "def printAlgorithmsEvaluationSummary(cyp_config, null_preds, ml_preds,\n",
        "                                     log_fh, country_code=None):\n",
        "  \"\"\"Print summary of algorithm evaluation\"\"\"\n",
        "  metrics = cyp_config.getEvaluationMetrics()\n",
        "  country = country_code\n",
        "  if (country_code is None):\n",
        "    country = cyp_config.getCountryCode()\n",
        "\n",
        "  alg_summary = {}\n",
        "  cyp_algeval = CYPAlgorithmEvaluator(cyp_config)\n",
        "  cyp_algeval.evaluateNullMethodPredictions(null_preds, alg_summary)\n",
        "  cyp_algeval.evaluateMLPredictions(ml_preds, alg_summary)\n",
        "  pd_pred_dfs = [ml_preds['train'], ml_preds['custom_cv'], ml_preds['test']]\n",
        "  pred_sets_info = ['Training Set', 'Validation Test Set', 'Test Set']\n",
        "  cyp_algeval.printPredictionDataFrames(pd_pred_dfs, pred_sets_info, log_fh)\n",
        "\n",
        "  alg_df_columns = ['algorithm']\n",
        "  for met in metrics:\n",
        "    alg_df_columns += ['train_' + met, 'cv_' + met, 'test_' + met]\n",
        "\n",
        "  alg_df = pd.DataFrame.from_dict(alg_summary, orient='index', columns=alg_df_columns)\n",
        "\n",
        "  eval_summary_info = '\\nAlgorithm Evaluation Summary for ' + country\n",
        "  eval_summary_info += '\\n-----------------------------------------'\n",
        "  eval_summary_info += '\\n' + alg_df.to_string(index=False) + '\\n'\n",
        "  log_fh.write(eval_summary_info)\n",
        "  print(eval_summary_info)\n",
        "\n",
        "def getMLPredictionsOneModel(cyp_config, pd_train_df, pd_test_df, log_fh):\n",
        "  \"\"\"Build one ML model and return predictions\"\"\"\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  predict_residuals = cyp_config.predictYieldResiduals()\n",
        "  use_sample_weights = cyp_config.useSampleWeights()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  spatial_level = cyp_config.getSpatialLevel()\n",
        "  id_col = \"COUNTY_ID\" if (spatial_level == \"COUNTY\") else \"GRID_ID\"\n",
        "\n",
        "  data_splits = getValidationSplits(cyp_config, pd_train_df, pd_test_df, log_fh)\n",
        "  X_train = data_splits['X_train']\n",
        "  Y_train_full = data_splits['Y_train_full']\n",
        "  X_test = data_splits['X_test']\n",
        "  Y_test_full = data_splits['Y_test_full']\n",
        "  features = data_splits['features']\n",
        "  custom_cv = data_splits['custom_cv']\n",
        "  cv_test_years = data_splits['cv_test_years']\n",
        "\n",
        "  train_weights = None\n",
        "  test_weights = None\n",
        "  if (use_sample_weights):\n",
        "    train_weights = data_splits['train_weights']\n",
        "    test_weights = data_splits['test_weights']\n",
        "\n",
        "  cyp_algeval = CYPAlgorithmEvaluator(cyp_config, custom_cv, train_weights, test_weights)\n",
        "  null_preds = cyp_algeval.getNullMethodPredictions(Y_train_full, Y_test_full,\n",
        "                                                    cv_test_years, log_fh)\n",
        "\n",
        "  Y_train_full_n = Y_train_full\n",
        "  Y_test_full_n = Y_test_full\n",
        "  if (use_yield_trend and predict_residuals):\n",
        "    result = cyp_algeval.estimateYieldTrendAndDetrend(X_train, Y_train_full[:, -1],\n",
        "                                                      X_test, Y_test_full[:, -1], features)\n",
        "    X_train = result['X_train']\n",
        "    Y_train_full_n = np.copy(Y_train_full)\n",
        "    Y_train_full_n[:, -1] = result['Y_train'][:, 0]\n",
        "    Y_train_full_n[:, -2] = result['Y_train_trend'][:, 0]\n",
        "    X_test = result['X_test']\n",
        "    Y_test_full_n = np.copy(Y_test_full)\n",
        "    Y_test_full_n[:, -1] = result['Y_test'][:,0]\n",
        "    Y_test_full_n[:, -2] = result['Y_test_trend'][:, 0]\n",
        "    features = result['features']\n",
        "\n",
        "  cyp_ftsel = CYPFeatureSelector(cyp_config, X_train, Y_train_full_n[:, -1], features,\n",
        "                                 custom_cv, train_weights)\n",
        "  ml_preds = cyp_algeval.getMLPredictions(X_train, Y_train_full_n, X_test, Y_test_full_n,\n",
        "                                          cv_test_years, cyp_ftsel, features, log_fh)\n",
        "\n",
        "  if (use_yield_trend and predict_residuals):\n",
        "    ml_preds = cyp_algeval.yieldPredictionsFromResiduals(ml_preds['train'],\n",
        "                                                         Y_train_full[:, -1],\n",
        "                                                         ml_preds['test'],\n",
        "                                                         Y_test_full[:, -1],\n",
        "                                                         ml_preds['custom_cv'],\n",
        "                                                         cv_test_years,\n",
        "                                                         id_col)\n",
        "\n",
        "  return null_preds, ml_preds\n",
        "\n",
        "def getMachineLearningPredictions(cyp_config, pd_train_df, pd_test_df, log_fh):\n",
        "  \"\"\"Train and evaluate algorithms\"\"\"\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  alg_names = list(cyp_config.getEstimators().keys())\n",
        "  country_code = cyp_config.getCountryCode()\n",
        "  spatial_level = cyp_config.getSpatialLevel()\n",
        "  id_col = \"COUNTY_ID\" if (spatial_level == \"COUNTY\") else \"GRID_ID\"\n",
        "\n",
        "  # register spark parallel backend\n",
        "  register_spark()\n",
        "\n",
        "  eval_info = '\\nTraining and Evaluation'\n",
        "  eval_info += '\\n-------------------------'\n",
        "  log_fh.write(eval_info)\n",
        "  if (debug_level > 1):\n",
        "    print(eval_info)\n",
        "\n",
        "  null_preds, ml_preds = getMLPredictionsOneModel(cyp_config, pd_train_df, pd_test_df, log_fh)\n",
        "\n",
        "  if (debug_level > 1):\n",
        "    sel_cols = [id_col, 'FYEAR', 'YIELD'] + ['YIELD_PRED_' + alg for alg in alg_names]\n",
        "    print('\\n', ml_preds['test'][sel_cols].head(5))\n",
        "\n",
        "  null_preds_train = null_preds['train']\n",
        "  null_preds_cv = null_preds['custom_cv']\n",
        "  null_preds_test = null_preds['test']\n",
        "  ml_preds_train = ml_preds['train']\n",
        "  ml_preds_cv = ml_preds['custom_cv']\n",
        "  ml_preds_test = ml_preds['test']\n",
        "\n",
        "  printAlgorithmsEvaluationSummary(cyp_config, null_preds, ml_preds, log_fh)\n",
        "  ml_preds_test['COUNTRY'] = country_code\n",
        "\n",
        "  return ml_preds_test\n",
        "\n",
        "def saveMLPredictions(cyp_config, sqlCtx, pd_ml_predictions, iter=None):\n",
        "  \"\"\"Save ML predictions to a CSV file\"\"\"\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  crop = cyp_config.getCropName()\n",
        "  country = cyp_config.getCountryCode()\n",
        "  spatial_level = cyp_config.getSpatialLevel()\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "  early_season_end = cyp_config.getEarlySeasonEndDekad()\n",
        "  ml_algs = cyp_config.getEstimators()\n",
        "\n",
        "  output_path = cyp_config.getOutputPath()\n",
        "  output_file = getPredictionFilename(crop, use_yield_trend,\n",
        "                                      early_season_prediction, early_season_end,\n",
        "                                      country, spatial_level)\n",
        "\n",
        "  save_pred_path = output_path + '/' + output_file\n",
        "  if (iter is not None):\n",
        "    save_pred_path += '-' + str(iter)\n",
        "\n",
        "  if (debug_level > 1):\n",
        "    print('\\nSaving predictions to', save_pred_path + '.csv')\n",
        "    print(pd_ml_predictions.head(5))\n",
        "\n",
        "  pd_ml_predictions.to_csv(save_pred_path + '.csv', index=False, header=True)\n",
        "\n",
        "  # NOTE: In some environments, Spark can write, but pandas cannot.\n",
        "  # In such cases, use the following code.\n",
        "  # spark_predictions_df = sqlCtx.createDataFrame(pd_ml_predictions)\n",
        "  # spark_predictions_df.coalesce(1)\\\n",
        "  #                     .write.option('header','true')\\\n",
        "  #                     .mode(\"overwrite\").csv(save_pred_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jDs9IO_xfPu"
      },
      "source": [
        "### Load Saved Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOFLzsa8xjT-"
      },
      "outputs": [],
      "source": [
        "#%%writefile load_saved_predictions.py\n",
        "import pandas as pd\n",
        "\n",
        "def loadSavedPredictions(cyp_config, spark):\n",
        "  \"\"\"Load machine learning predictions from saved CSV file\"\"\"\n",
        "  crop = cyp_config.getCropName()\n",
        "  country = cyp_config.getCountryCode()\n",
        "  nuts_level = cyp_config.getNUTSLevel()\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "  early_season_end = cyp_config.getEarlySeasonEndDekad()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "\n",
        "  pred_file_path = cyp_config.getOutputPath()\n",
        "  pred_file = getPredictionFilename(crop, use_yield_trend,\n",
        "                                    early_season_prediction, early_season_end,\n",
        "                                    country, nuts_level)\n",
        "  pred_file += '.csv'\n",
        "  pd_ml_predictions = pd.read_csv(pred_file_path + '/' + pred_file, header=0)\n",
        "\n",
        "  # NOTE: In some environments, Spark can read, but pandas cannot.\n",
        "  # In such cases, use the following code.\n",
        "  # all_pred_df = spark.read.csv(pred_file_path + '/' + pred_file, header=True, inferSchema=True)\n",
        "  # pd_ml_predictions = all_pred_df.toPandas()\n",
        "\n",
        "  if (debug_level > 1):\n",
        "    print(pd_ml_predictions.head(5))\n",
        "\n",
        "  return pd_ml_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NrPZSmEViO5"
      },
      "source": [
        "## Run Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3_hCaxR0GQ"
      },
      "source": [
        "### Unzip the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhXPNZgmR2Rc",
        "outputId": "7ab8b9d7-0161-4ed5-d8ce-e38f7afbb610"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  county-data.zip\n",
            "  inflating: county-data/SOIL_COUNTY_US.csv  \n",
            "  inflating: county-data/YIELD_COUNTY_US.csv  \n",
            "  inflating: county-data/CROP_AREA_COUNTY_US.csv  \n",
            "  inflating: county-data/CSSF_COUNTY_US.csv  \n",
            "  inflating: county-data/REMOTE_SENSING_COUNTY_US.csv  \n",
            "  inflating: county-data/METEO_COUNTY_US.csv  \n"
          ]
        }
      ],
      "source": [
        "! unzip county-data.zip\n",
        "! mv county-data/*.csv ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWnqPIU4cM_z"
      },
      "source": [
        "### Set Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2r5-pmuVgS1",
        "outputId": "e8edc4e3-4d18-4321-e55f-b64f20373680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "##################\n",
            "# Configuration  #\n",
            "##################\n",
            "\n",
            "Current ML Baseline Configuration\n",
            "--------------------------------\n",
            "Crop name: grain maize\n",
            "Crop ID: 2\n",
            "Crop growing season crosses calendar year boundary: N\n",
            "Country code (e.g. NL): US\n",
            "spatial level for yield prediction: COUNTY\n",
            "Input data sources: CSSF, METEO, SOIL, YIELD, REMOTE_SENSING\n",
            "Remove data or regions with duplicate or missing values: N\n",
            "Estimate and use yield trend: Y\n",
            "Predict yield residuals instead of full yield: N\n",
            "Find optimal trend window: N\n",
            "List of trend window lengths (number of years): 5, 7, 10\n",
            "Use centroid coordinates and distance to coast: N\n",
            "Use remote sensing data (FAPAR): Y\n",
            "Use per region per year crop calendar: Y\n",
            "Predict yield early in the season: Y\n",
            "Early season end dekad relative to harvest: -6\n",
            "Path to all input data. Default is current directory.: .\n",
            "Path to all output files. Default is current directory.: .\n",
            "Use feature design v2: Y\n",
            "Save features to a CSV file: Y\n",
            "Use features from a CSV file: N\n",
            "Use data sample weights based on crop area: N\n",
            "Retrain a model for every test year: N\n",
            "Save predictions to a CSV file: Y\n",
            "Use predictions from a CSV file: N\n",
            "Compare predictions with MARS Crop Yield Forecasting System: N\n",
            "Debug level to control amount of debug information: 2\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if (test_env == 'notebook'):\n",
        "  cyp_config = CYPConfiguration()\n",
        "\n",
        "  my_config = {\n",
        "      'crop_name' : 'grain maize',\n",
        "      'season_crosses_calendar_year' : 'N',\n",
        "      'country_code' : 'US',\n",
        "      'data_sources' : [ 'CSSF', 'METEO', 'SOIL', 'YIELD'], #'COUNTY'],\n",
        "      'clean_data' : 'N',\n",
        "      'data_path' : '.',\n",
        "      'output_path' : '.',\n",
        "      'spatial_level' : 'COUNTY',\n",
        "      'use_yield_trend' : 'Y',\n",
        "      'predict_yield_residuals' : 'N',\n",
        "      'trend_windows' : [5, 7, 10],\n",
        "      'find_optimal_trend_window' : 'N',\n",
        "      'use_centroids' : 'N',\n",
        "      'use_remote_sensing' : 'Y',\n",
        "      'use_per_year_crop_calendar' : 'Y',\n",
        "      'early_season_prediction' : 'Y',\n",
        "      'early_season_end_dekad' : -6,\n",
        "      'use_features_v2' : 'Y',\n",
        "      'save_features' : 'Y',\n",
        "      'use_saved_features' : 'N',\n",
        "      'use_sample_weights' : 'N',\n",
        "      'retrain_per_test_year' : 'N',\n",
        "      'save_predictions' : 'Y',\n",
        "      'use_saved_predictions' : 'N',\n",
        "      'compare_with_mcyfs' : 'N',\n",
        "      'debug_level' : 2,\n",
        "  }\n",
        "\n",
        "  cyp_config.updateConfiguration(my_config)\n",
        "  crop = cyp_config.getCropName()\n",
        "  country = cyp_config.getCountryCode()\n",
        "  spatial_level = cyp_config.getSpatialLevel()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  use_saved_predictions = cyp_config.useSavedPredictions()\n",
        "  use_saved_features = cyp_config.useSavedFeatures()\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "  early_season_end = cyp_config.getEarlySeasonEndDekad()\n",
        "\n",
        "  print('##################')\n",
        "  print('# Configuration  #')\n",
        "  print('##################')\n",
        "  output_path = cyp_config.getOutputPath()\n",
        "  log_file = getLogFilename(crop, use_yield_trend,\n",
        "                            early_season_prediction, early_season_end,\n",
        "                            country)\n",
        "  log_fh = open(output_path + '/' + log_file, 'w+')\n",
        "  cyp_config.printConfig(log_fh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cytBAKTO2SfR"
      },
      "source": [
        "### Load and Preprocess Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpkFFtVi2SfU",
        "outputId": "fc55b6f4-500a-4a93-9e7e-a5776c6f56f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#################\n",
            "# Data Loading  #\n",
            "#################\n",
            "Data file name \"./CSSF_COUNTY_US.csv\"\n",
            "Data file name \"./METEO_COUNTY_US.csv\"\n",
            "Data file name \"./SOIL_COUNTY_US.csv\"\n",
            "Data file name \"./YIELD_COUNTY_US.csv\"\n",
            "Data file name \"./REMOTE_SENSING_COUNTY_US.csv\"\n",
            "Loaded data: CSSF, METEO, SOIL, YIELD, REMOTE_SENSING\n",
            "\n",
            "\n",
            "#######################\n",
            "# Data Preprocessing  #\n",
            "#######################\n",
            "CSSF data available for 917 grids\n",
            "CSSF data\n",
            "+----------+-----+-----+-----------------+------------------+-------------------+\n",
            "| COUNTY_ID|FYEAR|DEKAD|             TAGP|              TWSO|                DVS|\n",
            "+----------+-----+-----+-----------------+------------------+-------------------+\n",
            "|AL_CALHOUN| 2002|   14|507.0769958496094|               0.0|0.02313069999217987|\n",
            "|AL_CALHOUN| 2002|   15|2411.719970703125|               0.0|0.13162100315093994|\n",
            "|AL_CALHOUN| 2002|   16| 4187.97021484375|               0.0|0.26501700282096863|\n",
            "|AL_CALHOUN| 2002|   17|   6021.669921875|               0.0| 0.3900670111179352|\n",
            "|AL_CALHOUN| 2002|   18| 7480.35009765625|               0.0| 0.5123569965362549|\n",
            "|AL_CALHOUN| 2002|   19|   9424.580078125|               0.0| 0.6496300101280212|\n",
            "|AL_CALHOUN| 2002|   20| 11209.2001953125|               0.0| 0.7823020219802856|\n",
            "|AL_CALHOUN| 2002|   21|  12893.099609375|               0.0| 0.9245690107345581|\n",
            "|AL_CALHOUN| 2002|   22|  14769.900390625|233.31300354003906| 1.0565099716186523|\n",
            "|AL_CALHOUN| 2002|   23|  16409.599609375|1191.4000244140625| 1.1881300210952759|\n",
            "+----------+-----+-----+-----------------+------------------+-------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "METEO data available for 917 grids\n",
            "METEO data\n",
            "+----------+-----+-----+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+-------------+------+\n",
            "| COUNTY_ID|FYEAR|DEKAD|              TMAX|               TMIN|              TAVG|             VPRES|              WSPD|              PREC|               ET0|          RAD|   CWB|\n",
            "+----------+-----+-----+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+-------------+------+\n",
            "|AL_CALHOUN| 2002|    1|15.639800071716309| -5.023540019989014|2.2167699337005615| 5.574100017547607|3.4766900539398193|25.097299575805664|11.003499984741211|      99294.0| 14.09|\n",
            "|AL_CALHOUN| 2002|    2| 14.64579963684082| -0.802344024181366| 6.540070056915283| 7.733429908752441| 3.000469923019409|48.150699615478516|13.465100288391113|96445.8984375| 34.69|\n",
            "|AL_CALHOUN| 2002|    3|22.743000030517578|  1.434380054473877|12.588000297546387|12.689900398254395|3.3685600757598877|33.131500244140625| 18.23979949951172|     120627.0| 14.89|\n",
            "|AL_CALHOUN| 2002|    4| 16.33989906311035|-1.5956499576568604| 5.341859817504883| 6.803679943084717|  3.43533992767334| 36.47520065307617|16.141399383544922|     101123.0| 20.33|\n",
            "|AL_CALHOUN| 2002|    5|17.557899475097656|    -2.180419921875| 8.032520294189453| 6.745520114898682|3.3750500679016113| 6.620500087738037| 23.07699966430664|     153740.0|-16.46|\n",
            "|AL_CALHOUN| 2002|    6|18.878700256347656| -6.520090103149414| 5.581769943237305| 5.532189846038818|3.4475700855255127|1.6923600435256958|20.929399490356445|     144723.0|-19.24|\n",
            "|AL_CALHOUN| 2002|    7|22.176599502563477| -6.045149803161621| 7.294280052185059| 6.556039810180664|3.8734400272369385|31.584999084472656|27.041500091552734|     166222.0|  4.54|\n",
            "|AL_CALHOUN| 2002|    8|25.349599838256836| 1.1826399564743042|16.159299850463867|14.959699630737305|3.3362998962402344|62.597198486328125|26.411100387573242|     136295.0| 36.19|\n",
            "|AL_CALHOUN| 2002|    9|24.909799575805664| -2.231980085372925|12.565699577331543| 10.19219970703125|3.5987300872802734|27.976900100708008|37.157798767089844|     205995.0| -9.18|\n",
            "|AL_CALHOUN| 2002|   10|25.821500778198242|  4.640460014343262|14.912599563598633|10.645700454711914| 3.742140054702759|21.283000946044922| 40.42879867553711|     209530.0|-19.15|\n",
            "+----------+-----+-----+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+-------------+------+\n",
            "only showing top 10 rows\n",
            "\n",
            "REMOTE_SENSING data available for 917 grids\n",
            "REMOTE_SENSING data\n",
            "+----------+-----+-----+-------------------+\n",
            "| COUNTY_ID|FYEAR|DEKAD|              FAPAR|\n",
            "+----------+-----+-----+-------------------+\n",
            "|AL_CALHOUN| 2002|    1|0.31575000286102295|\n",
            "|AL_CALHOUN| 2002|    2|0.32774999737739563|\n",
            "|AL_CALHOUN| 2002|    3| 0.3502500057220459|\n",
            "|AL_CALHOUN| 2002|    4|0.35199999809265137|\n",
            "|AL_CALHOUN| 2002|    5| 0.3409999907016754|\n",
            "|AL_CALHOUN| 2002|    6|0.33149999380111694|\n",
            "|AL_CALHOUN| 2002|    7| 0.3434999883174896|\n",
            "|AL_CALHOUN| 2002|    8|  0.367249995470047|\n",
            "|AL_CALHOUN| 2002|    9|0.41475000977516174|\n",
            "|AL_CALHOUN| 2002|   10|0.47325000166893005|\n",
            "+----------+-----+-----+-------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "SOIL data available for 917 grids\n",
            "SOIL data\n",
            "+-------------+------------------+\n",
            "|    COUNTY_ID|            SM_WHC|\n",
            "+-------------+------------------+\n",
            "|   AL_CALHOUN|11.104715807403592|\n",
            "|   AL_COLBERT|11.548404485791965|\n",
            "|AL_LAUDERDALE| 11.96848916802795|\n",
            "|  AL_LAWRENCE|11.810489732526587|\n",
            "|    AL_SHELBY|11.886477786203775|\n",
            "| AL_TALLADEGA|11.580875739552617|\n",
            "|    AR_ASHLEY| 16.93890876725048|\n",
            "|    AR_CHICOT|21.104000091552734|\n",
            "|      AR_CLAY| 19.29786764675809|\n",
            "|    AR_GREENE|16.539827209107834|\n",
            "+-------------+------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+-----------+-----+------+\n",
            "|  COUNTY_ID|FYEAR| YIELD|\n",
            "+-----------+-----+------+\n",
            "| AL_AUTAUGA| 2018|11.137|\n",
            "|  AL_DALLAS| 2018| 9.233|\n",
            "|  AL_ELMORE| 2018|11.466|\n",
            "|   AL_PERRY| 2018| 9.765|\n",
            "| AL_BALDWIN| 2018|10.222|\n",
            "| AL_CONECUH| 2018|10.088|\n",
            "|AL_ESCAMBIA| 2018| 11.89|\n",
            "|  AL_MONROE| 2018|10.599|\n",
            "| AL_CALHOUN| 2018|10.504|\n",
            "| AL_CULLMAN| 2018|12.213|\n",
            "+-----------+-----+------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if ((test_env == 'notebook') and\n",
        "    (not use_saved_predictions) and\n",
        "    (not use_saved_features)):\n",
        "\n",
        "  print('#################')\n",
        "  print('# Data Loading  #')\n",
        "  print('#################')\n",
        "\n",
        "  cyp_loader = CYPDataLoader(spark, cyp_config)\n",
        "  data_dfs = cyp_loader.loadAllData()\n",
        "\n",
        "  print('#######################')\n",
        "  print('# Data Preprocessing  #')\n",
        "  print('#######################')\n",
        "\n",
        "  cyp_preprocessor = CYPDataPreprocessor(spark, cyp_config)\n",
        "  data_dfs = preprocessData(cyp_config, cyp_preprocessor, data_dfs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePa1-zrp-Vay"
      },
      "source": [
        "### Split Data into Training and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtLALA3gtbiQ",
        "outputId": "16cce57c-218d-441d-c102-c99fe880a602"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "###########################\n",
            "# Training and Test Split #\n",
            "###########################\n",
            "\n",
            "Test years:\n",
            "2012, 2013, 2014, 2015, 2016, 2017, 2018\n"
          ]
        }
      ],
      "source": [
        "if ((test_env == 'notebook') and\n",
        "    (not use_saved_predictions) and\n",
        "    (not use_saved_features)):\n",
        "\n",
        "  print('###########################')\n",
        "  print('# Training and Test Split #')\n",
        "  print('###########################')\n",
        "\n",
        "  prep_train_test_dfs, test_years = splitDataIntoTrainingTestSets(cyp_config, data_dfs, log_fh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7w13GLYExv1"
      },
      "source": [
        "### Summarize Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxYQ_BnPExv3",
        "outputId": "4de3e28b-824b-400f-d6b9-3795cf2cabb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#################\n",
            "# Data Summary  #\n",
            "#################\n",
            "Crop calender information based on CSSF data\n",
            "+---------------+-----+---------+----------+----------+---------------------+\n",
            "|      COUNTY_ID|FYEAR|START_DVS|START_DVS1|START_DVS2|CAMPAIGN_EARLY_SEASON|\n",
            "+---------------+-----+---------+----------+----------+---------------------+\n",
            "|     AL_CALHOUN| 2011|       14|        21|        31|                   26|\n",
            "|     AL_COLBERT| 2011|       14|        21|        31|                   26|\n",
            "|  AL_LAUDERDALE| 2011|       14|        21|        31|                   26|\n",
            "|    AL_LAWRENCE| 2011|       14|        21|        31|                   26|\n",
            "|      AL_SHELBY| 2011|       11|        20|        27|                   22|\n",
            "|   AL_TALLADEGA| 2011|       11|        20|        31|                   26|\n",
            "|      AR_CHICOT| 2011|        5|        15|        21|                   16|\n",
            "|        AR_CLAY| 2011|       14|        21|        31|                   26|\n",
            "|      AR_GREENE| 2011|       14|        21|        31|                   26|\n",
            "|AR_INDEPENDENCE| 2011|       14|        21|        30|                   25|\n",
            "+---------------+-----+---------+----------+----------+---------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if ((test_env == 'notebook') and\n",
        "    (not use_saved_predictions) and\n",
        "    (not use_saved_features)):\n",
        "\n",
        "  print('#################')\n",
        "  print('# Data Summary  #')\n",
        "  print('#################')\n",
        "\n",
        "  cyp_summarizer = CYPDataSummarizer(cyp_config)\n",
        "  summary_dfs = summarizeData(cyp_config, cyp_summarizer, prep_train_test_dfs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvVoVMRgcejF"
      },
      "source": [
        "### Create Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucxoQlMXwKEg",
        "outputId": "52925ac7-2c48-48aa-fe24-0a09ac27694b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "###################\n",
            "# Feature Design  #\n",
            "###################\n",
            "\n",
            " CSSF Aggregate Features: Training\n",
            "    COUNTY_ID  FYEAR  maxTAGPp2  maxTAGPp4  maxTWSOp4\n",
            "0  AL_CALHOUN   2002   14769.90   21873.50    6645.69\n",
            "1  AL_CALHOUN   2004   16727.40   24398.70    7631.92\n",
            "2  AL_CALHOUN   2007   13201.30   19447.50    6415.20\n",
            "3  AL_COLBERT   2004   14392.82   23858.19    8997.34\n",
            "4  AL_COLBERT   2007   11587.56   14694.56    3472.08\n",
            "\n",
            " CSSF Aggregate Features: Test\n",
            "     COUNTY_ID  FYEAR  maxTAGPp2  maxTAGPp4  maxTWSOp4\n",
            "0      IL_CLAY   2016   11776.74   19689.74    6474.60\n",
            "1      IL_PIKE   2012   14996.53   22043.02    7564.81\n",
            "2  IL_RICHLAND   2012   10711.39   17085.95    6598.70\n",
            "3    KS_KEARNY   2013    4429.71    4429.71     757.81\n",
            "4     MI_EATON   2017   11779.96   16543.15    4533.88\n",
            "\n",
            " METEO Aggregate Features: Training\n",
            "    COUNTY_ID  FYEAR  avgTAVGp0  avgCWBp0  avgPRECp0  avgTAVGp1  avgPRECp1  \\\n",
            "0  AL_CALHOUN   2002      13.81     -3.45      25.58      21.22      21.48   \n",
            "1  AL_CALHOUN   2004      13.19      8.54      27.84      22.38      31.47   \n",
            "2  AL_CALHOUN   2007      13.10    -50.12      19.23      22.36       7.14   \n",
            "3  AL_COLBERT   2004      11.95    105.32      35.31      21.93      36.68   \n",
            "4  AL_COLBERT   2007      12.65    -63.89      17.69      22.83      11.24   \n",
            "\n",
            "   avgTAVGp2  avgCWBp2  avgTAVGp3  avgPRECp3  avgCWBp4  \n",
            "0      24.45   -157.73      26.00      58.03   -113.29  \n",
            "1      24.75   -100.58      25.04      39.88   -102.58  \n",
            "2      25.29   -344.59      28.50      20.35   -464.67  \n",
            "3      24.31     61.24      24.34      28.26     13.41  \n",
            "4      25.50   -344.65      28.38      21.64   -476.01  \n",
            "\n",
            " METEO Aggregate Features: Test\n",
            "       COUNTY_ID  FYEAR  avgTAVGp0  avgCWBp0  avgPRECp0  avgTAVGp1  avgPRECp1  \\\n",
            "0     AL_COLBERT   2015      11.33     73.41      32.28      21.14      29.13   \n",
            "1     AL_COLBERT   2016      12.83    146.77      38.93      20.58      22.00   \n",
            "2     AL_COLBERT   2017      14.61     41.01      35.26      20.36      47.04   \n",
            "3     AL_COLBERT   2018      13.23    251.07      58.16      22.91      42.84   \n",
            "4  AL_LAUDERDALE   2012      14.57     79.30      34.00      22.55      29.15   \n",
            "\n",
            "   avgTAVGp2  avgCWBp2  avgTAVGp3  avgPRECp3  avgCWBp4  \n",
            "0      25.48    -73.87      26.60      22.99   -290.27  \n",
            "1      25.78    -75.38      27.57      41.98   -207.60  \n",
            "2      24.64     26.95      26.24      33.15    -27.58  \n",
            "3      25.78    300.77      26.51      34.16    206.19  \n",
            "4      25.64    -86.10      26.53      64.82   -174.44  \n",
            "\n",
            " METEO Features for Extreme Conditions: Training\n",
            "    COUNTY_ID  FYEAR  Z-TAVGp1  Z-TMINp1  Z-PRECp1  Z+TAVGp1  Z+TMINp1  \\\n",
            "0  AL_CALHOUN   2002       0.0       0.0      0.89      4.31      4.37   \n",
            "1  AL_CALHOUN   2004       0.0       0.0      0.97      5.03      5.68   \n",
            "2  AL_CALHOUN   2007       0.0       0.0      2.81      5.02      4.02   \n",
            "3  AL_COLBERT   2004       0.0       0.0      0.85      4.75      5.60   \n",
            "4  AL_COLBERT   2007       0.0       0.0      2.26      5.32      4.94   \n",
            "\n",
            "   Z+PRECp1  Z-TAVGp3  Z-CWBp3  Z-PRECp3  Z-TMAXp3  Z+TAVGp3  Z+CWBp3  \\\n",
            "0      0.00      0.27     0.00      0.12      2.08      0.65     1.57   \n",
            "1      1.41      1.12     0.00      0.07      0.65      0.20     1.79   \n",
            "2      0.00      0.00     2.58      0.87      0.57      3.76     0.00   \n",
            "3      2.00      1.87     0.00      0.74      0.96      0.00     3.74   \n",
            "4      0.00      0.10     2.65      0.67      0.30      3.69     0.00   \n",
            "\n",
            "   Z+PRECp3  Z+TMAXp3  \n",
            "0      5.32      0.01  \n",
            "1      2.62      0.08  \n",
            "2      0.58      3.48  \n",
            "3      1.60      0.21  \n",
            "4      0.56      4.18  \n",
            "\n",
            " METEO Features for Extreme Conditions: Test\n",
            "       COUNTY_ID  FYEAR  Z-TAVGp1  Z-TMINp1  Z-PRECp1  Z+TAVGp1  Z+TMINp1  \\\n",
            "0     AL_COLBERT   2015       0.0       0.0      1.25      4.25      3.93   \n",
            "1     AL_COLBERT   2016       0.0       0.0      1.18      3.90      3.84   \n",
            "2     AL_COLBERT   2017       0.0       0.0      0.15      3.77      4.07   \n",
            "3     AL_COLBERT   2018       0.0       0.0      0.00      5.37      7.48   \n",
            "4  AL_LAUDERDALE   2012       0.0       0.0      1.03      5.14      5.29   \n",
            "\n",
            "   Z+PRECp1  Z-TAVGp3  Z-CWBp3  Z-PRECp3  Z-TMAXp3  Z+TAVGp3  Z+CWBp3  \\\n",
            "0      1.38      0.26     0.08      0.43      0.91      1.45     0.24   \n",
            "1      0.36      0.00     0.00      0.15      0.55      2.50     0.52   \n",
            "2      2.68      0.23     0.00      0.00      1.18      0.93     3.09   \n",
            "3      1.97      0.00     0.00      0.10      1.01      1.07     6.63   \n",
            "4      1.17      0.13     0.00      0.00      1.30      1.22     1.54   \n",
            "\n",
            "   Z+PRECp3  Z+TMAXp3  \n",
            "0      0.52      0.85  \n",
            "1      3.01      1.97  \n",
            "2      1.57      0.55  \n",
            "3      1.83      0.00  \n",
            "4      6.19      1.30  \n",
            "\n",
            " REMOTE_SENSING Aggregate Features: Training\n",
            "    COUNTY_ID  FYEAR  avgFAPARp2\n",
            "0  AL_CALHOUN   2002        0.68\n",
            "1  AL_CALHOUN   2004        0.77\n",
            "2  AL_CALHOUN   2007        0.60\n",
            "3  AL_COLBERT   2004        0.63\n",
            "4  AL_COLBERT   2007        0.49\n",
            "\n",
            " REMOTE_SENSING Aggregate Features: Test\n",
            "       COUNTY_ID  FYEAR  avgFAPARp2\n",
            "0     AL_COLBERT   2015        0.61\n",
            "1     AL_COLBERT   2016        0.56\n",
            "2     AL_COLBERT   2017        0.60\n",
            "3     AL_COLBERT   2018        0.59\n",
            "4  AL_LAUDERDALE   2012        0.55\n",
            "\n",
            "Yield Trend Features: Train\n",
            "    COUNTY_ID  FYEAR  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND\n",
            "0  AL_AUTAUGA   1999    5.380    3.363    2.556    5.044    2.219         2.32\n",
            "1  AL_AUTAUGA   2000    3.363    2.556    5.044    2.219    5.111         4.61\n",
            "2  AL_AUTAUGA   2001    2.556    5.044    2.219    5.111    2.085         3.14\n",
            "3  AL_AUTAUGA   2002    5.044    2.219    5.111    2.085    4.775         3.65\n",
            "4  AL_AUTAUGA   2003    2.219    5.111    2.085    4.775    3.497         4.20\n",
            "Total 23358 rows\n",
            "\n",
            "Yield Trend Features: Test\n",
            "    COUNTY_ID  FYEAR  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND\n",
            "0  AL_AUTAUGA   1999    5.380    3.363    2.556    5.044    2.219         2.32\n",
            "1  AL_AUTAUGA   2000    3.363    2.556    5.044    2.219    5.111         4.61\n",
            "2  AL_AUTAUGA   2001    2.556    5.044    2.219    5.111    2.085         3.14\n",
            "3  AL_AUTAUGA   2002    5.044    2.219    5.111    2.085    4.775         3.65\n",
            "4  AL_AUTAUGA   2003    2.219    5.111    2.085    4.775    3.497         4.20\n",
            "Total 33821 rows\n"
          ]
        }
      ],
      "source": [
        "if ((test_env == 'notebook') and\n",
        "    (not use_saved_predictions) and\n",
        "    (not use_saved_features)):\n",
        "\n",
        "  print('###################')\n",
        "  print('# Feature Design  #')\n",
        "  print('###################')\n",
        "  id_col = \"COUNTY_ID\" if (spatial_level == \"COUNTY\") else \"GRID_ID\"\n",
        "\n",
        "  # CSSF, Meteo and Remote Sensing Features\n",
        "  cyp_featurizer = CYPFeaturizer(cyp_config)\n",
        "  pd_feature_dfs = createFeatures(cyp_config, cyp_featurizer,\n",
        "                                  prep_train_test_dfs, summary_dfs, log_fh)\n",
        "\n",
        "  # trend features\n",
        "  join_cols = [id_col, 'FYEAR']\n",
        "  if (use_yield_trend):\n",
        "    yield_train_df = prep_train_test_dfs['YIELD'][0]\n",
        "    yield_test_df = prep_train_test_dfs['YIELD'][1]\n",
        "\n",
        "    # Trend features from label data\n",
        "    cyp_trend_est = CYPYieldTrendEstimator(cyp_config)\n",
        "    pd_yield_train_ft, pd_yield_test_ft = createYieldTrendFeatures(cyp_config, cyp_trend_est,\n",
        "                                                                   yield_train_df, yield_test_df,\n",
        "                                                                   test_years)\n",
        "    pd_feature_dfs['YIELD_TREND'] = [pd_yield_train_ft, pd_yield_test_ft]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOy8NcAtfnhu"
      },
      "source": [
        "### Combine Features and Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1brZTniBfpaO",
        "outputId": "f4a7c83f-8708-4fc3-9cd0-a2a9aad135c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Combine Features and Labels\n",
            "---------------------------\n",
            "Yield min year 1994\n",
            "\n",
            "Data size after including SOIL data: \n",
            "Train 917 rows.\n",
            "Test 917 rows.\n",
            "\n",
            "Data size after including CSSF features: \n",
            "Train 7185 rows.\n",
            "Test 4137 rows.\n",
            "\n",
            "Data size after including METEO features: \n",
            "Train 7185 rows.\n",
            "Test 4137 rows.\n",
            "\n",
            "Data size after including REMOTE_SENSING features: \n",
            "Train 7185 rows.\n",
            "Test 4137 rows.\n",
            "\n",
            "Data size after including yield trend features: \n",
            "Train 7170 rows.\n",
            "Test 4137 rows.\n",
            "\n",
            "Data size after including yield (label) data: \n",
            "Train 7170 rows.\n",
            "Test 4137 rows.\n",
            "\n",
            "\n",
            "All Features and labels: Training\n",
            "       COUNTY_ID  FYEAR     SM_WHC  maxTAGPp2  maxTAGPp4  maxTWSOp4  \\\n",
            "5388  AL_CALHOUN   2002  11.104716    14769.9    21873.5    6645.69   \n",
            "5389  AL_CALHOUN   2004  11.104716    16727.4    24398.7    7631.92   \n",
            "5391  AL_CALHOUN   2006  11.104716    12858.0    19501.2    5652.52   \n",
            "5390  AL_CALHOUN   2007  11.104716    13201.3    19447.5    6415.20   \n",
            "5392  AL_CALHOUN   2010  11.104716    12979.9    21260.1    6902.48   \n",
            "\n",
            "      avgTAVGp0  avgCWBp0  avgPRECp0  avgTAVGp1  ...  Z+PRECp3  Z+TMAXp3  \\\n",
            "5388      13.81     -3.45      25.58      21.22  ...      5.32      0.01   \n",
            "5389      13.19      8.54      27.84      22.38  ...      2.62      0.08   \n",
            "5391      13.59    -17.22      23.10      21.44  ...      0.32      3.06   \n",
            "5390      13.10    -50.12      19.23      22.36  ...      0.58      3.48   \n",
            "5392      12.07    102.05      34.43      22.48  ...      7.33      0.00   \n",
            "\n",
            "      avgFAPARp2  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  \\\n",
            "5388        0.68    7.976    3.901    6.120    6.053    5.649         4.44   \n",
            "5389        0.77    6.120    6.053    5.649    6.456    7.868         7.46   \n",
            "5391        0.65    6.053    5.649    6.456    7.868    9.684         9.45   \n",
            "5390        0.60    5.649    6.456    7.868    9.684    4.102         6.85   \n",
            "5392        0.72    7.868    9.684    4.102    6.053    6.927         5.23   \n",
            "\n",
            "      YIELD  \n",
            "5388  6.456  \n",
            "5389  9.684  \n",
            "5391  4.102  \n",
            "5390  6.053  \n",
            "5392  7.956  \n",
            "\n",
            "[5 rows x 38 columns]\n",
            "\n",
            "All Features and labels: Test\n",
            "       COUNTY_ID  FYEAR     SM_WHC  maxTAGPp2  maxTAGPp4  maxTWSOp4  \\\n",
            "3115  AL_CALHOUN   2012  11.104716   14125.00   25495.70    9918.09   \n",
            "3116  AL_CALHOUN   2014  11.104716   17637.60   28036.60    9693.25   \n",
            "3114  AL_CALHOUN   2018  11.104716   16362.80   22891.30    6472.97   \n",
            "2668  AL_COLBERT   2012  11.548404   10751.20   20776.49    8960.94   \n",
            "2665  AL_COLBERT   2015  11.548404   13971.85   22237.25    8834.71   \n",
            "\n",
            "      avgTAVGp0  avgCWBp0  avgPRECp0  avgTAVGp1  ...  Z+PRECp3  Z+TMAXp3  \\\n",
            "3115      15.71     33.37      31.67      22.80  ...      6.16      1.61   \n",
            "3116      11.67     45.47      35.21      21.06  ...      1.50      1.08   \n",
            "3114      13.83    127.54      38.89      22.33  ...      5.08      0.00   \n",
            "2668      14.68     81.60      34.31      22.59  ...      6.02      1.45   \n",
            "2665      11.33     73.41      32.28      21.14  ...      0.52      0.85   \n",
            "\n",
            "      avgFAPARp2  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  \\\n",
            "3115        0.72    4.102    6.053    6.927    7.956    8.043         9.16   \n",
            "3116        0.77    6.927    7.956    8.043    6.638    7.895         7.68   \n",
            "3114        0.81    7.956    8.043    6.638    7.895    9.159         9.29   \n",
            "2668        0.53    6.053    5.716    8.474    9.603    8.830        10.33   \n",
            "2665        0.61    9.603    8.830    6.886   12.253   12.878        13.08   \n",
            "\n",
            "       YIELD  \n",
            "3115   6.638  \n",
            "3116   9.159  \n",
            "3114  10.504  \n",
            "2668   6.886  \n",
            "2665  10.699  \n",
            "\n",
            "[5 rows x 38 columns]\n",
            "\n",
            "Saving features to: ./ft_grain_maize_US_trend_early-6[train, test].csv\n"
          ]
        }
      ],
      "source": [
        "if ((test_env == 'notebook') and\n",
        "    (not use_saved_predictions) and\n",
        "    (not use_saved_features)):\n",
        "\n",
        "  pd_train_df, pd_test_df = combineFeaturesLabels(cyp_config, sqlContext,\n",
        "                                                  prep_train_test_dfs, pd_feature_dfs,\n",
        "                                                  join_cols, log_fh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lzdenOWYxYr"
      },
      "source": [
        "### Apply Machine Learning using scikit learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1AKH4dbtR1d"
      },
      "outputs": [],
      "source": [
        "def loadAndCombineFeaturesLabels(crop, country, states, yield_trend,\n",
        "                                 early_season_end=0):\n",
        "  pd_train_df = None\n",
        "  pd_test_df = None\n",
        "  for st in states:\n",
        "    early_season = False\n",
        "    if (early_season_end < 0):\n",
        "      early_season = True\n",
        "\n",
        "    ft_file = getFeatureFilename(crop, yield_trend,\n",
        "                                 early_season, early_season_end,\n",
        "                                 country=country)\n",
        "\n",
        "    train_ft_file = ft_file + '_train' + st + '.csv'\n",
        "    test_ft_file = ft_file + '_test' + st + '.csv'\n",
        "    pd_train_cn_df = pd.read_csv(train_ft_file, header=0)\n",
        "    pd_test_cn_df = pd.read_csv(test_ft_file, header=0)\n",
        "\n",
        "    if (pd_train_df is None):\n",
        "      pd_train_df = pd_train_cn_df\n",
        "      pd_test_df = pd_test_cn_df\n",
        "    else:\n",
        "      common_cols = [ c for c in pd_train_df.columns if (c in pd_train_cn_df.columns)]\n",
        "      pd_train_df = pd.concat([pd_train_df[common_cols], pd_train_cn_df[common_cols]], axis=0)\n",
        "      pd_test_df = pd.concat([pd_test_df[common_cols], pd_test_cn_df[common_cols]], axis=0)\n",
        "\n",
        "    print(st, 'train', len(pd_train_df.index))\n",
        "    print(st, 'test', len(pd_test_df.index))\n",
        "\n",
        "  return pd_train_df, pd_test_df\n",
        "\n",
        "# states = ['AR', 'MO', 'TN', 'KY', 'IL', 'IN', 'KS', 'OH', 'NE', 'IA', 'PA', 'MI', 'WI', 'SD', 'MN', 'ND']\n",
        "# pd_train_df, pd_test_df = loadAndCombineFeaturesLabels(crop, country, states,\n",
        "#                                                        use_yield_trend, early_season_end)\n",
        "\n",
        "# pd_train_df.to_csv('ft_grain_maize_US_trend_early-6_train.csv', index=False)\n",
        "# pd_test_df.to_csv('ft_grain_maize_US_trend_early-6_test.csv', index=False)\n",
        "\n",
        "use_saved_features = True\n",
        "if ((test_env == 'notebook') and\n",
        "    (not use_saved_predictions)):\n",
        "\n",
        "  if (use_saved_features):\n",
        "    pd_train_df, pd_test_df = loadSavedFeaturesLabels(cyp_config, spark)\n",
        "    pd_train_df = pd_train_df[pd_train_df['YIELD'] > 0.0]\n",
        "    pd_test_df = pd_test_df[pd_test_df['YIELD'] > 0.0]\n",
        "\n",
        "  print('###################################')\n",
        "  print('# Machine Learning using sklearn  #')\n",
        "  print('###################################')\n",
        "\n",
        "num_iters = 10\n",
        "for i in range(1, num_iters + 1):\n",
        "  pd_ml_predictions = getMachineLearningPredictions(cyp_config, pd_train_df, pd_test_df, log_fh)\n",
        "\n",
        "  # save machine learning predictions\n",
        "  save_predictions = cyp_config.savePredictions()\n",
        "  if (save_predictions):\n",
        "    saveMLPredictions(cyp_config, sqlContext, pd_ml_predictions, i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYTG01XmFgYw"
      },
      "source": [
        "### SHAP interpretability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqxh4tLXFox3"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "\n",
        "if (use_saved_features):\n",
        "    pd_train_df, pd_test_df = loadSavedFeaturesLabels(cyp_config, spark)\n",
        "\n",
        "all_cols = list(pd_train_df.columns)\n",
        "ft_cols = all_cols[2:-2]\n",
        "label_cols = all_cols[:2] + all_cols[-2:]\n",
        "\n",
        "X_train = pd_train_df[ft_cols].values\n",
        "Y_train_full = pd_train_df[label_cols].values\n",
        "X_test = pd_test_df[ft_cols].values\n",
        "Y_test_full = pd_test_df[label_cols].values\n",
        "\n",
        "Y_train = Y_train_full[:, -1]\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "model = GradientBoostingRegressor(loss='huber', max_features='log2',\n",
        "                                  max_depth=10, min_samples_leaf=5,\n",
        "                                  tol=1e-3, n_iter_no_change=5,\n",
        "                                  subsample=0.6, ccp_alpha=1e-2,\n",
        "                                  learning_rate=0.1,\n",
        "                                  n_estimators=500, random_state=42)\n",
        "model.fit(X_train, Y_train)\n",
        "# model = xgboost.XGBRegressor().fit(X_train, Y_train)\n",
        "\n",
        "# explain the model's predictions using SHAP\n",
        "explainer = shap.Explainer(model, feature_names=ft_cols)\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "shap.plots.bar(shap_values, max_display=20)\n",
        "shap.plots.beeswarm(shap_values, max_display=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY5PSCFaySoJ"
      },
      "outputs": [],
      "source": [
        "log_fh.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcGOfk-6a1O0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}