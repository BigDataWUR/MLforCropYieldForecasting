{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ksndBerUYL_"
      },
      "source": [
        "# Crop Yield Prediction - Optimized ML Models\n",
        "\n",
        "We use WOFOST crop growth indicators, weather variables, geographic information, soil data and remote sensing indicators to predict the yield."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX-t0S8wUr5X"
      },
      "source": [
        "## Google Colab Notes\n",
        "\n",
        "**To run the script in Google Colab environment**\n",
        "1. Download the data directory and save it somewhere convenient.\n",
        "2. Open the notebook using Google Colaboratory.\n",
        "3. Create a copy of the notebook for yourself.\n",
        "4. Click connect on the right hand side of the bar below menu items. When you are connected to a machine, you will see a green tick mark and bars showing RAM and disk.\n",
        "5. Click the folder icon on the left sidebar and click upload. Upload the data files you downloaded. Click *Ok* when you see a warning saying the files will be deleted after the session is disconnected.\n",
        "6. Use *Runtime* -> *Run before* option to run all cells before **Set Configuration**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP2t4BbJGJKZ"
      },
      "source": [
        "## Global Variables and Spark Installation/Initialization\n",
        "\n",
        "Initialize Spark session and global variables. Package installation is required only in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcalFxFgYk6s",
        "outputId": "49d8eaea-a64a-48e5-a954-f0538949cbe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:114: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "#%%writefile globals.py\n",
        "test_env = 'notebook'\n",
        "# test_env = 'cluster'\n",
        "# test_env = 'pkg'\n",
        "\n",
        "# change to False to skip tests\n",
        "run_tests = False\n",
        "\n",
        "# NUTS levels\n",
        "nuts_levels = ['NUTS' + str(i) for i in range(4)]\n",
        "\n",
        "# country codes\n",
        "countries = ['BG', 'DE', 'ES', 'FR', 'HU', 'IT', 'NL', 'PL', 'RO']\n",
        "\n",
        "# debug levels\n",
        "debug_levels = [i for i in range(5)]\n",
        "\n",
        "# Keeping these two mappings inside CYPConfiguration leads to SPARK-5063 error\n",
        "# when lambda functions use them. Therefore, they are defined as globals now.\n",
        "\n",
        "# crop name to id mapping\n",
        "crop_id_dict = {\n",
        "    'grain maize': 2,\n",
        "    'sugar beet' : 6,\n",
        "    'sugarbeet' : 6,\n",
        "    'sugarbeets' : 6,\n",
        "    'sugar beets' : 6,\n",
        "    'total potatoes' : 7,\n",
        "    'potatoes' : 7,\n",
        "    'potato' : 7,\n",
        "    'winter wheat' : 90,\n",
        "    'soft wheat' : 90,\n",
        "    'sunflower' : 93,\n",
        "    'spring barley' : 95,\n",
        "}\n",
        "\n",
        "# crop id to name mapping\n",
        "crop_name_dict = {\n",
        "    2 : 'grain maize',\n",
        "    6 : 'sugarbeet',\n",
        "    7 : 'potatoes',\n",
        "    90 : 'soft wheat',\n",
        "    93 : 'sunflower',\n",
        "    95 : 'spring barley',\n",
        "}\n",
        "\n",
        "if (test_env == 'notebook'):\n",
        "  !pip install pyspark > /dev/null\n",
        "  !sudo apt update > /dev/null\n",
        "  !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "  !pip install joblibspark > /dev/null\n",
        "  !pip install scikit-optimize > /dev/null\n",
        "  !pip install shap > /dev/null\n",
        "\n",
        "  import os\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import functions as SparkF\n",
        "from pyspark.sql import types as SparkT\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "SparkContext.setSystemProperty('spark.executor.memory', '12g')\n",
        "SparkContext.setSystemProperty('spark.driver.memory', '6g')\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "sqlContext = SQLContext(sc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXT68GaJ1jOc"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZtsU16F1jOn"
      },
      "outputs": [],
      "source": [
        "#%%writefile util.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# crop name and id mappings\n",
        "def cropNameToID(crop_id_dict, crop):\n",
        "  \"\"\"\n",
        "  Return id of given crop. Relies on crop_id_dict.\n",
        "  Return 0 if crop name is not in the dictionary.\n",
        "  \"\"\"\n",
        "  crop_lcase = crop.lower()\n",
        "  try:\n",
        "    crop_id = crop_id_dict[crop_lcase]\n",
        "  except KeyError as e:\n",
        "    crop_id = 0\n",
        "\n",
        "  return crop_id\n",
        "\n",
        "def cropIDToName(crop_name_dict, crop_id):\n",
        "  \"\"\"\n",
        "  Return crop name for given crop ID. Relies on crop_name_dict.\n",
        "  Return 'NA' if crop id is not found in the dictionary.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    crop_name = crop_name_dict[crop_id]\n",
        "  except KeyError as e:\n",
        "    crop_name = 'NA'\n",
        "\n",
        "  return crop_name\n",
        "\n",
        "def getYear(date_str):\n",
        "  \"\"\"Extract year from date in yyyyMMdd or dd/MM/yyyy format.\"\"\"\n",
        "  return SparkF.when(SparkF.length(date_str) == 8,\n",
        "                     SparkF.year(SparkF.to_date(date_str, 'yyyyMMdd')))\\\n",
        "                     .otherwise(SparkF.year(SparkF.to_date(date_str, 'dd/MM/yyyy')))\n",
        "\n",
        "def getMonth(date_str):\n",
        "  \"\"\"Extract month from date in yyyyMMdd or dd/MM/yyyy format.\"\"\"\n",
        "  return SparkF.when(SparkF.length(date_str) == 8,\n",
        "                     SparkF.month(SparkF.to_date(date_str, 'yyyyMMdd')))\\\n",
        "                     .otherwise(SparkF.month(SparkF.to_date(date_str, 'dd/MM/yyyy')))\n",
        "\n",
        "def getDay(date_str):\n",
        "  \"\"\"Extract day from date in yyyyMMdd or dd/MM/yyyy format.\"\"\"\n",
        "  return SparkF.when(SparkF.length(date_str) == 8,\n",
        "                     SparkF.dayofmonth(SparkF.to_date(date_str, 'yyyyMMdd')))\\\n",
        "                     .otherwise(SparkF.dayofmonth(SparkF.to_date(date_str, 'dd/MM/yyyy')))\n",
        "\n",
        "# 1-10: Dekad 1\n",
        "# 11-20: Dekad 2\n",
        "# > 20 : Dekad 3\n",
        "def getDekad(date_str):\n",
        "  \"\"\"Extract dekad from date in YYYYMMDD format.\"\"\"\n",
        "  month = getMonth(date_str)\n",
        "  day = getDay(date_str)\n",
        "  return SparkF.when(day < 30, (month - 1)* 3 +\n",
        "                     SparkF.ceil(day/10)).otherwise((month - 1) * 3 + 3)\n",
        "\n",
        "# Machine Learning Utility Functions\n",
        "\n",
        "# Hassanat Distance Metric for KNN\n",
        "# See https://arxiv.org/pdf/1708.04321.pdf\n",
        "# Code based on https://github.com/BrunoGomesCoelho/hassanat-distance-checker/blob/master/Experiments.ipynb\n",
        "def hassanatDistance(a, b):\n",
        "  total = 0\n",
        "  for a_i, b_i in zip(a, b):\n",
        "    min_value = min(a_i, b_i)\n",
        "    max_value = max(a_i, b_i)\n",
        "    total += 1\n",
        "    if min_value >= 0:\n",
        "      total -= (1 + min_value) / (1 + max_value)\n",
        "    else:\n",
        "      total -= (1 + min_value + abs(min_value)) / (1 + max_value + abs(min_value))\n",
        "\n",
        "  return total\n",
        "\n",
        "# This definition is from the suggested answer to:\n",
        "# https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn/294069#294069\n",
        "def meanAbsolutePercentageError(Y_true, Y_pred):\n",
        "  \"\"\"Mean Absolute Percentage Error\"\"\"\n",
        "  Y_true, Y_pred = np.array(Y_true), np.array(Y_pred)\n",
        "  return np.mean(np.abs((Y_true - Y_pred) / Y_true)) * 100\n",
        "\n",
        "def modelRefitMeanVariance(cv_results):\n",
        "  \"\"\"\n",
        "  Custom refit callable for hyperparameter optimization.\n",
        "  Look at mean and variance of validation scores\n",
        "  \"\"\"\n",
        "  # cv_results structure\n",
        "  # {\n",
        "  #   'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n",
        "  #   'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n",
        "  #   'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n",
        "  #   'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n",
        "  #   'rank_test_score'    : [2, 4, 3, 1],\n",
        "  #   'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n",
        "  #   'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n",
        "  #   'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n",
        "  #   'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n",
        "  #    ...\n",
        "  #   'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n",
        "  # }\n",
        "\n",
        "  # For mean_score, higher is better\n",
        "  # For std_score or variance, lower is better\n",
        "  # We combine them by using mean_score - std_score\n",
        "  mean_score = cv_results['mean_test_score']\n",
        "  std_score = cv_results['std_test_score']\n",
        "  refit_score = mean_score - std_score\n",
        "\n",
        "  best_score, best_index = max((val, idx) for (idx, val) in enumerate(refit_score))\n",
        "  return best_index\n",
        "\n",
        "def modelRefitTrainValDiff(cv_results):\n",
        "  \"\"\"\n",
        "  Custom refit callable for hyperparameter optimization.\n",
        "  Look at difference between training and validation errors.\n",
        "  NOTE: Hyperparameter search must be called with return_train_score=True.\n",
        "  \"\"\"\n",
        "  # cv_results structure\n",
        "  # {\n",
        "  #   'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n",
        "  #   'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n",
        "  #   'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n",
        "  #   'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n",
        "  #   'rank_test_score'    : [2, 4, 3, 1],\n",
        "  #   'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n",
        "  #   'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n",
        "  #   'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n",
        "  #   'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n",
        "  #    ...\n",
        "  #   'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n",
        "  # }\n",
        "  mean_test_score = cv_results['mean_test_score']\n",
        "  mean_train_score = cv_results['mean_train_score']\n",
        "  mean_score_diff = mean_test_score - mean_train_score\n",
        "\n",
        "  best_score, best_index = min((val, idx) for (idx, val) in enumerate(mean_score_diff))\n",
        "  return best_index\n",
        "\n",
        "def customFitPredict(args):\n",
        "  \"\"\"\n",
        "  We need this because scikit-learn does not support\n",
        "  cross_val_predict for time series splits.\n",
        "  \"\"\"\n",
        "  X_train = args['X_train']\n",
        "  Y_train = args['Y_train']\n",
        "  X_test = args['X_test']\n",
        "  est = args['estimator']\n",
        "  fit_params = args['fit_params']\n",
        "\n",
        "  est.fit(X_train, Y_train, **fit_params)\n",
        "  return est.predict(X_test)\n",
        "\n",
        "def unionCountryDataPandas(pd_df, year_col, test_years, is_train=False):\n",
        "  \"\"\"\n",
        "  Union training or test data for multiple countries.\n",
        "  Test years must be a dictionary with country specific test years,\n",
        "  e.g. { 'NL' : [2012, 2013],\n",
        "         'DE' : [2013, 2014],\n",
        "       }\n",
        "  \"\"\"\n",
        "  pd_sel_df = None\n",
        "  if ('COUNTRY' not in pd_df.columns):\n",
        "    pd_df['COUNTRY'] = pd_df['IDREGION'].str[:2]\n",
        "\n",
        "  for cn in test_years:\n",
        "    cn_test_years = test_years[cn]\n",
        "    if (is_train):\n",
        "      cn_filter = (pd_df['COUNTRY'] == cn) & (~pd_df[year_col].isin(cn_test_years))\n",
        "    else:\n",
        "      cn_filter = (pd_df['COUNTRY'] == cn) & (pd_df[year_col].isin(cn_test_years))\n",
        "\n",
        "    pd_cn_df = pd_df[cn_filter]\n",
        "    if (pd_sel_df is None):\n",
        "      pd_sel_df = pd_cn_df\n",
        "    else:\n",
        "      pd_sel_df = pd_sel_df.append(pd_cn_df)\n",
        "\n",
        "  pd_sel_df = pd_sel_df.drop(columns=['COUNTRY'])\n",
        "  return pd_sel_df\n",
        "\n",
        "def unionCountryDataSpark(df, year_col, test_years, is_train=False):\n",
        "  \"\"\"\n",
        "  Union training or test data for multiple countries.\n",
        "  Test years must be a dictionary with country specific test years,\n",
        "  e.g. { 'NL' : [2012, 2013],\n",
        "         'DE' : [2013, 2014],\n",
        "       }\n",
        "  \"\"\"\n",
        "  sel_df = None\n",
        "  if ('COUNTRY' not in df.columns):\n",
        "    df = df.withColumn('COUNTRY', SparkF.substring(df['IDREGION'], 0, 2))\n",
        "\n",
        "  for cn in test_years:\n",
        "    cn_test_years = test_years[cn]\n",
        "    if (is_train):\n",
        "      cn_filter = (df['COUNTRY'] == cn) & (~df[year_col].isin(cn_test_years))\n",
        "    else:\n",
        "      cn_filter = (df['COUNTRY'] == cn) & (df[year_col].isin(cn_test_years))\n",
        "\n",
        "    cn_df = df.filter(cn_filter)\n",
        "    if (sel_df is None):\n",
        "      sel_df = cn_df\n",
        "    else:\n",
        "      sel_df = sel_df.union(cn_df)\n",
        "\n",
        "  sel_df = sel_df.drop('COUNTRY')\n",
        "  return sel_df\n",
        "\n",
        "def printInGroups(items, indices, item_values=None, log_fh=None):\n",
        "  \"\"\"Print elements at given indices in groups of 5\"\"\"\n",
        "  num_items = len(indices)\n",
        "  groups = int(num_items/5) + 1\n",
        "\n",
        "  items_str = '\\n'\n",
        "  for g in range(groups):\n",
        "    group_start = g * 5\n",
        "    group_end = (g + 1) * 5\n",
        "    if (group_end > num_items):\n",
        "      group_end = num_items\n",
        "\n",
        "    group_indices = indices[group_start:group_end]\n",
        "    for idx in group_indices:\n",
        "      items_str += str(idx+1) + ': ' + items[idx]\n",
        "      if (item_values):\n",
        "        items_str += '=' + item_values[idx]\n",
        "\n",
        "      if (idx != group_indices[-1]):\n",
        "          items_str += ', '\n",
        "\n",
        "    items_str += '\\n'\n",
        "\n",
        "  print(items_str)\n",
        "  if (log_fh is not None):\n",
        "    log_fh.write(items_str)\n",
        "\n",
        "def getPredictionScores(Y_true, Y_predicted, metrics):\n",
        "  \"\"\"Get values of metrics for given Y_predicted and Y_true\"\"\"\n",
        "  pred_scores = {}\n",
        "\n",
        "  for met in metrics:\n",
        "    score_function = metrics[met]\n",
        "    met_score = score_function(Y_true, Y_predicted)\n",
        "    # for RMSE, score_function is mean_squared_error, take square root\n",
        "    # normalize RMSE\n",
        "    if (met == 'RMSE'):\n",
        "      met_score = np.round(100*np.sqrt(met_score)/np.mean(Y_true), 2)\n",
        "      pred_scores['NRMSE'] = met_score\n",
        "    # normalize mean absolute errors except MAPE which is already a percentage\n",
        "    elif ((met == 'MAE') or (met == 'MdAE')):\n",
        "      met_score = np.round(100*met_score/np.mean(Y_true), 2)\n",
        "      pred_scores['N' + met] = met_score\n",
        "    # MAPE, R2, ... : no postprocessing\n",
        "    else:\n",
        "      met_score = np.round(met_score, 2)\n",
        "      pred_scores[met] = met_score\n",
        "\n",
        "  return pred_scores\n",
        "\n",
        "def getFilename(crop, yield_trend, early_season, early_season_end,\n",
        "                country=None, nuts_level=None):\n",
        "  \"\"\"Get filename based on input arguments\"\"\"\n",
        "  suffix = crop.replace(' ', '_')\n",
        "\n",
        "  if (country is not None):\n",
        "    suffix += '_' + country\n",
        "\n",
        "  if (nuts_level is not None):\n",
        "    suffix += '_' + nuts_level\n",
        "\n",
        "  if (yield_trend):\n",
        "    suffix += '_trend'\n",
        "  else:\n",
        "    suffix += '_notrend'\n",
        "\n",
        "  if (early_season):\n",
        "    suffix += '_early' + str(early_season_end)\n",
        "\n",
        "  return suffix\n",
        "\n",
        "def getLogFilename(crop, yield_trend, early_season, early_season_end,\n",
        "                   country=None):\n",
        "  \"\"\"Get filename for experiment log\"\"\"\n",
        "  log_file = getFilename(crop, yield_trend, early_season, early_season_end, country)\n",
        "  return log_file + '.log'\n",
        "\n",
        "def getFeatureFilename(crop, yield_trend, early_season, early_season_end,\n",
        "                       country=None):\n",
        "  \"\"\"Get unique filename for features\"\"\"\n",
        "  feature_file = 'ft_'\n",
        "  suffix = getFilename(crop, yield_trend, early_season, early_season_end, country)\n",
        "  feature_file += suffix\n",
        "  return feature_file\n",
        "\n",
        "def getPredictionFilename(crop, yield_trend, early_season, early_season_end,\n",
        "                          country=None, nuts_level=None):\n",
        "  \"\"\"Get unique filename for predictions\"\"\"\n",
        "  pred_file = 'pred_'\n",
        "  suffix = getFilename(crop, yield_trend, early_season, early_season_end,\n",
        "                       country, nuts_level)\n",
        "  pred_file += suffix\n",
        "  return pred_file\n",
        "\n",
        "def plotTrend(years, actual_values, trend_values, trend_label):\n",
        "  \"\"\"Plot a linear trend and scatter plot of actual values\"\"\"\n",
        "  plt.scatter(years, actual_values, color=\"blue\", marker=\"o\")\n",
        "  plt.plot(years, trend_values, '--')\n",
        "  plt.xticks(np.arange(years[0], years[-1] + 1, step=len(years)/5))\n",
        "  ax = plt.axes()\n",
        "  plt.xlabel(\"YEAR\")\n",
        "  plt.ylabel(trend_label)\n",
        "  plt.title(trend_label + ' Trend by YEAR')\n",
        "  plt.show()\n",
        "\n",
        "def plotTrueVSPredicted(actual, predicted):\n",
        "  \"\"\"Plot actual and predicted values\"\"\"\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.scatter(np.asarray(actual), predicted)\n",
        "  ax.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'k--', lw=4)\n",
        "  ax.set_xlabel('Actual')\n",
        "  ax.set_ylabel('Predicted')\n",
        "  plt.show()\n",
        "\n",
        "def plotCVResultsGroup(pd_results_df, score_cols, param_cols):\n",
        "  \"\"\"Plot training and validation scores of given parameters\"\"\"\n",
        "  # Metrics can be string or functions, skip them.\n",
        "  if ('estimator__metric' in param_cols):\n",
        "    param_cols.remove('estimator__metric')\n",
        "\n",
        "  fig, ax = plt.subplots(1, len(param_cols), sharex='none', sharey='all',figsize=(20,5))\n",
        "  fig.suptitle('Score per parameter')\n",
        "  fig.text(0.04, 0.5, 'MEAN SCORE', va='center', rotation='vertical')\n",
        "  for i, p in enumerate(param_cols):\n",
        "    pd_filtered_df = pd_results_df.copy()\n",
        "    pd_filtered_df = pd_filtered_df.drop_duplicates(subset=[p])\n",
        "    pd_param_df = pd_filtered_df[[p] + score_cols]\n",
        "    if ((pd_filtered_df[p].dtype == 'int64') or (pd_filtered_df[p].dtype == 'float64')):\n",
        "      pd_param_df = pd_param_df.sort_values(by=[p])\n",
        "\n",
        "    x = pd_param_df[p].values\n",
        "    y1 = pd_param_df['MEAN_TEST'].values\n",
        "    y2 = pd_param_df['MEAN_TRAIN'].values\n",
        "    e1 = pd_param_df['STD_TEST'].values\n",
        "    e2 = pd_param_df['STD_TRAIN'].values\n",
        "    if (len(param_cols) > 1):\n",
        "      ax[i].errorbar(x, y1, e1, linestyle='--', marker='o', label='test')\n",
        "      ax[i].errorbar(x, y2, e2, linestyle='solid', marker='o', label='train')\n",
        "      ax[i].set_xlabel(p.upper())\n",
        "    else:\n",
        "      ax.errorbar(x, y1, e1, linestyle='--', marker='o', label='test')\n",
        "      ax.errorbar(x, y2, e2, linestyle='solid', marker='o', label='train')\n",
        "      ax.set_xlabel(p.upper())\n",
        "\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "def plotCVResults(search_cv):\n",
        "  \"\"\"Plot training and validation scores of search parameters\"\"\"\n",
        "  score_cols = ['MEAN_TEST', 'MEAN_TRAIN', 'STD_TEST', 'STD_TRAIN']\n",
        "  pd_results_df = pd.concat([pd.DataFrame(search_cv.cv_results_['params']),\n",
        "                             pd.DataFrame(search_cv.cv_results_['mean_test_score'],\n",
        "                                          columns=['MEAN_TEST']),\n",
        "                             pd.DataFrame(search_cv.cv_results_['std_test_score'],\n",
        "                                          columns=['STD_TEST']),\n",
        "                             pd.DataFrame(search_cv.cv_results_['mean_train_score'],\n",
        "                                          columns=['MEAN_TRAIN']),\n",
        "                             pd.DataFrame(search_cv.cv_results_['std_train_score'],\n",
        "                                          columns=['STD_TRAIN'])\n",
        "                             ], axis=1)\n",
        "\n",
        "  param_cols = list(pd_results_df.columns)[:-len(score_cols)]\n",
        "  # remove parameters with fixed value\n",
        "  del_cols = []\n",
        "  for p in param_cols:\n",
        "    x = set(pd_results_df[p].values)\n",
        "    if (len(x) == 1):\n",
        "      del_cols.append(p)\n",
        "\n",
        "  param_cols = [c for c in param_cols if c not in del_cols]\n",
        "  if (len(param_cols) <= 3):\n",
        "    plotCVResultsGroup(pd_results_df, score_cols, param_cols)\n",
        "  else:\n",
        "    num_groups = int(len(param_cols)/ 3) + 1\n",
        "    for g in range(num_groups):\n",
        "      param_cols_group = param_cols[g * 3 : (g + 1) * 3]\n",
        "      if (not param_cols_group):\n",
        "        break\n",
        "\n",
        "      plotCVResultsGroup(pd_results_df, score_cols, param_cols_group)\n",
        "\n",
        "# Based on\n",
        "# https://stackoverflow.com/questions/39409866/correlation-heatmap\n",
        "def plotCorrelation(df, sel_cols):\n",
        "  corr = df[sel_cols].corr()\n",
        "  mask = np.zeros_like(corr, dtype=np.bool)\n",
        "  mask[np.triu_indices_from(mask)] = True\n",
        "  f, ax = plt.subplots(figsize=(20, 18))\n",
        "  cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "  sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1.0, center=0,\n",
        "              square=True, linewidths=.5, cbar_kws={\"shrink\": .5},\n",
        "              annot=True, fmt='.1g')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8UktClOS9Hs"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjP5SjODS-qS"
      },
      "outputs": [],
      "source": [
        "#%%writefile config.py\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import explained_variance_score\n",
        "from sklearn.metrics import median_absolute_error\n",
        "\n",
        "from sklearn.utils.fixes import loguniform\n",
        "import scipy.stats as stats\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "\n",
        "class CYPConfiguration:\n",
        "  def __init__(self, crop_name='potatoes', country_code='NL', season_cross='N'):\n",
        "    self.config = {\n",
        "        'crop_name' : crop_name,\n",
        "        'crop_id' : cropNameToID(crop_id_dict, crop_name),\n",
        "        'season_crosses_calendar_year' : season_cross,\n",
        "        'country_code' : country_code,\n",
        "        'nuts_level' : 'NUTS2',\n",
        "        'data_sources' : [ 'WOFOST', 'METEO_DAILY', 'SOIL', 'YIELD' ],\n",
        "        'clean_data' : 'Y',\n",
        "        'use_yield_trend' : 'N',\n",
        "        'predict_yield_residuals' : 'N',\n",
        "        'find_optimal_trend_window' : 'N',\n",
        "        # set it to a list with one entry for fixed window\n",
        "        'trend_windows' : [5, 7, 10],\n",
        "        'use_centroids' : 'N',\n",
        "        'use_remote_sensing' : 'Y',\n",
        "        'use_gaes' : 'Y',\n",
        "        'use_per_year_crop_calendar' : 'Y',\n",
        "        'early_season_prediction' : 'N',\n",
        "        'early_season_end_dekad' : 0,\n",
        "        'data_path' : '.',\n",
        "        'output_path' : '.',\n",
        "        'use_features_v2' : 'Y',\n",
        "        'save_features' : 'N',\n",
        "        'use_saved_features' : 'N',\n",
        "        'use_sample_weights' : 'N',\n",
        "        'retrain_per_test_year' : 'N',\n",
        "        'save_predictions' : 'Y',\n",
        "        'use_saved_predictions' : 'N',\n",
        "        'compare_with_mcyfs' : 'N',\n",
        "        'debug_level' : 0,\n",
        "    }\n",
        "\n",
        "    # Description of configuration parameters\n",
        "    # This should be in sync with config above\n",
        "    self.config_desc = {\n",
        "        'crop_name' : 'Crop name',\n",
        "        'crop_id' : 'Crop ID',\n",
        "        'season_crosses_calendar_year' : 'Crop growing season crosses calendar year boundary',\n",
        "        'country_code' : 'Country code (e.g. NL)',\n",
        "        'nuts_level' : 'NUTS level for yield prediction',\n",
        "        'data_sources' : 'Input data sources',\n",
        "        'clean_data' : 'Remove data or regions with duplicate or missing values',\n",
        "        'use_yield_trend' : 'Estimate and use yield trend',\n",
        "        'predict_yield_residuals' : 'Predict yield residuals instead of full yield',\n",
        "        'find_optimal_trend_window' : 'Find optimal trend window',\n",
        "        'trend_windows' : 'List of trend window lengths (number of years)',\n",
        "        'use_centroids' : 'Use centroid coordinates and distance to coast',\n",
        "        'use_remote_sensing' : 'Use remote sensing data (FAPAR)',\n",
        "        'use_gaes' : 'Use agro-environmental zones data',\n",
        "        'use_per_year_crop_calendar' : 'Use per region per year crop calendar',\n",
        "        'early_season_prediction' : 'Predict yield early in the season',\n",
        "        'early_season_end_dekad' : 'Early season end dekad relative to harvest',\n",
        "        'data_path' : 'Path to all input data. Default is current directory.',\n",
        "        'output_path' : 'Path to all output files. Default is current directory.',\n",
        "        'use_features_v2' : 'Use feature design v2',\n",
        "        'save_features' : 'Save features to a CSV file',\n",
        "        'use_saved_features' : 'Use features from a CSV file',\n",
        "        'use_sample_weights' : 'Use data sample weights based on crop area',\n",
        "        'retrain_per_test_year' : 'Retrain a model for every test year',\n",
        "        'save_predictions' : 'Save predictions to a CSV file',\n",
        "        'use_saved_predictions' : 'Use predictions from a CSV file',\n",
        "        'compare_with_mcyfs' : 'Compare predictions with MARS Crop Yield Forecasting System',\n",
        "        'debug_level' : 'Debug level to control amount of debug information',\n",
        "    }\n",
        "\n",
        "    ########### Machine learning configuration ###########\n",
        "    # mutual correlation threshold for features\n",
        "    self.feature_correlation_threshold = 0.9\n",
        "\n",
        "    # test fraction\n",
        "    self.test_fraction = 0.3\n",
        "\n",
        "    # scaler\n",
        "    self.scaler = StandardScaler()\n",
        "\n",
        "    # Feature selection algorithms. Initialized in getFeatureSelectors().\n",
        "    self.feature_selectors = {}\n",
        "\n",
        "    # prediction algorithms\n",
        "    self.estimators = {\n",
        "        # linear model\n",
        "        # 'Ridge' : {\n",
        "        #     'estimator' : Ridge(random_state=42, max_iter=1000, tol=1e-3,\n",
        "        #                         normalize=False, fit_intercept=True),\n",
        "        #     'param_space' : {\n",
        "        #         \"estimator__alpha\": Real(1e-1, 1e+2, prior='log-uniform')\n",
        "        #     }\n",
        "        # },\n",
        "        # 'KNN' : {\n",
        "        #     'estimator' : KNeighborsRegressor(n_jobs=-1),\n",
        "        #     'param_space' : {\n",
        "        #         \"estimator__n_neighbors\": Integer(7, 15),\n",
        "        #         \"estimator__weights\": Categorical(['uniform', 'distance']),\n",
        "        #         \"estimator__metric\" : Categorical(['minkowski', 'manhattan']),# hassanatDistance])\n",
        "        #     }\n",
        "        # },\n",
        "        # SVM regression\n",
        "        # 'SVR' : {\n",
        "        #     'estimator' : SVR(gamma='scale', epsilon=1e-1, tol=1e-3,\n",
        "        #                       max_iter=-1, shrinking=True),\n",
        "        #     'param_space' : {\n",
        "        #         \"estimator__C\": Real(1e-2, 5e+2, prior='log-uniform'),\n",
        "        #         \"estimator__epsilon\": Real(1e-2, 5e-1, prior='log-uniform'),\n",
        "        #         \"estimator__gamma\": Categorical(['auto', 'scale']),\n",
        "        #         \"estimator__kernel\": Categorical(['rbf', 'linear']),\n",
        "        #     }\n",
        "        # },\n",
        "        # gradient boosted decision trees\n",
        "        'GBDT' : {\n",
        "            'estimator' : GradientBoostingRegressor(loss='huber', max_features='log2',\n",
        "                                                    max_depth=10, min_samples_leaf=10,\n",
        "                                                    tol=1e-3, n_iter_no_change=5,\n",
        "                                                    subsample=0.6, ccp_alpha=1e-2,\n",
        "                                                    n_estimators=500), #, random_state=42),\n",
        "            'param_space' : {\n",
        "                \"estimator__learning_rate\": Real(1e-3, 1e-1, prior='log-uniform'),\n",
        "                \"estimator__loss\" : Categorical(['huber', 'lad']),\n",
        "                # \"estimator__max_depth\": Integer(5, 10),\n",
        "                \"estimator__min_samples_leaf\": Integer(5, 20),\n",
        "                # \"estimator__n_estimators\": Integer(100, 500),\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # k-fold validation metric for feature selection\n",
        "    self.fs_cv_metric = 'neg_mean_squared_error'\n",
        "    # k-fold validation metric for training\n",
        "    self.est_cv_metric = 'neg_mean_squared_error'\n",
        "\n",
        "    # Performance evaluation metrics:\n",
        "    # sklearn supports these metrics:\n",
        "    # 'explained_variance', 'max_error', 'neg_mean_absolute_error\n",
        "    # 'neg_mean_squared_error', 'neg_root_mean_squared_error'\n",
        "    # 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'r2'\n",
        "    self.eval_metrics = {\n",
        "        # EXP_VAR (y_true, y_obs) = 1 - ( var(y_true - y_obs) / var (y_true) )\n",
        "        # 'EXP_VAR' : explained_variance_score,\n",
        "        # MAE (y_true, y_obs) = ( 1 / n ) * sum_i-n ( | y_true_i - y_obs_i | )\n",
        "        # 'MAE' : mean_absolute_error,\n",
        "        # MdAE (y_true, y_obs) = median ( | y_true_1 - y_obs_1 |, | y_true_2 - y_obs_2 |, ... )\n",
        "        # 'MdAE' : median_absolute_error,\n",
        "        # MAPE (y_true, y_obs) = ( 1 / n ) * sum_i-n ( ( y_true_i - y_obs_i ) / y_true_i )\n",
        "        'MAPE' : meanAbsolutePercentageError,\n",
        "        # MSE (y_true, y_obs) = ( 1 / n ) * sum_i-n ( y_true_i - y_obs_i )^2\n",
        "        'RMSE' : mean_squared_error,\n",
        "        # R2 (y_true, y_obs) = 1 - ( ( sum_i-n ( y_true_i - y_obs_i )^2 )\n",
        "        #                           / sum_i-n ( y_true_i - mean(y_true) )^2)\n",
        "        'R2' : r2_score,\n",
        "    }\n",
        "\n",
        "  ########### Setters and getters ###########\n",
        "  def setCropName(self, crop_name):\n",
        "    \"\"\"Set the crop name\"\"\"\n",
        "    crop = crop_name.lower()\n",
        "    assert crop in crop_id_dict\n",
        "    self.config['crop_name'] = crop\n",
        "    self.config['crop_id'] = cropNameToID(crop_id_dict, crop)\n",
        "\n",
        "  def getCropName(self):\n",
        "    \"\"\"Return the crop name\"\"\"\n",
        "    return self.config['crop_name']\n",
        "\n",
        "  def setCropID(self, crop_id):\n",
        "    \"\"\"Set the crop ID\"\"\"\n",
        "    assert crop_id in crop_name_dict\n",
        "    self.config['crop_id'] = crop_id\n",
        "    self.config['crop_name'] = cropIDToName(crop_name_dict, crop_id)\n",
        "\n",
        "  def getCropID(self):\n",
        "    \"\"\"Return the crop ID\"\"\"\n",
        "    return self.config['crop_id']\n",
        "\n",
        "  def setSeasonCrossesCalendarYear(self, season_crosses):\n",
        "    \"\"\"Set whether the season crosses calendar year boundary\"\"\"\n",
        "    scross = season_crosses.upper()\n",
        "    assert scross in ['Y', 'N']\n",
        "    self.config['season_crosses_calendar_year'] = scross\n",
        "\n",
        "  def seasonCrossesCalendarYear(self):\n",
        "    \"\"\"Return whether the season crosses calendar year boundary\"\"\"\n",
        "    return (self.config['season_crosses_calendar_year'] == 'Y')\n",
        "\n",
        "  def setCountryCode(self, country_code):\n",
        "    \"\"\"Set the country code\"\"\"\n",
        "    if (country_code is None):\n",
        "      self.config['country_code'] = None\n",
        "    else:\n",
        "      ccode = country_code.upper()\n",
        "      assert len(ccode) == 2\n",
        "      assert ccode in countries\n",
        "      self.config['country_code'] = ccode\n",
        "\n",
        "  def getCountryCode(self):\n",
        "    \"\"\"Return the country code\"\"\"\n",
        "    return self.config['country_code']\n",
        "\n",
        "  def setNUTSLevel(self, nuts_level):\n",
        "    \"\"\"Set the NUTS level\"\"\"\n",
        "    nuts = nuts_level.upper()\n",
        "    assert nuts in nuts_levels\n",
        "    self.config['nuts_level'] = nuts\n",
        "\n",
        "  def getNUTSLevel(self):\n",
        "    \"\"\"Return the NUTS level\"\"\"\n",
        "    return self.config['nuts_level']\n",
        "\n",
        "  def setDataSources(self, data_sources):\n",
        "    \"\"\"Get the data sources\"\"\"\n",
        "    # TODO: some validation\n",
        "    self.config['data_sources'] = data_sources\n",
        "\n",
        "  def updateDataSources(self, data_src, include_src, nuts_level=None):\n",
        "    \"\"\"add or remove data_src from data sources\"\"\"\n",
        "    src_nuts = self.getNUTSLevel()\n",
        "    if (nuts_level is not None):\n",
        "      src_nuts = nuts_level\n",
        "\n",
        "    data_sources = self.config['data_sources']\n",
        "    # no update required\n",
        "    if (((include_src == 'Y') and (data_src in data_sources)) or\n",
        "        ((include_src == 'N') and (data_src not in data_sources))):\n",
        "      return\n",
        "\n",
        "    if (include_src == 'Y'):\n",
        "      if (isinstance(data_sources, dict)):\n",
        "        data_sources[data_src] = src_nuts\n",
        "      else:\n",
        "        data_sources.append(data_src)\n",
        "    else:\n",
        "      if (isinstance(data_sources, dict)):\n",
        "        del data_sources[data_src]\n",
        "      else:\n",
        "        data_sources.remove(data_src)\n",
        "\n",
        "    self.config['data_sources'] = data_sources\n",
        "\n",
        "  def getDataSources(self):\n",
        "    \"\"\"Return the data sources\"\"\"\n",
        "    return self.config['data_sources']\n",
        "\n",
        "  def setCleanData(self, clean_data):\n",
        "    \"\"\"Set whether to clean data with duplicate or missing values\"\"\"\n",
        "    do_clean = clean_data.upper()\n",
        "    assert do_clean in ['Y', 'N']\n",
        "    self.config['clean_data'] = do_clean\n",
        "\n",
        "  def cleanData(self):\n",
        "    \"\"\"Return whether to clean data with duplicate or missing values\"\"\"\n",
        "    return (self.config['clean_data'] == 'Y')\n",
        "\n",
        "  def setUseYieldTrend(self, use_trend):\n",
        "    \"\"\"Set whether to use yield trend\"\"\"\n",
        "    use_yt = use_trend.upper()\n",
        "    assert use_yt in ['Y', 'N']\n",
        "    self.config['use_yield_trend'] = use_yt\n",
        "\n",
        "  def useYieldTrend(self):\n",
        "    \"\"\"Return whether to use yield trend\"\"\"\n",
        "    return (self.config['use_yield_trend'] == 'Y')\n",
        "\n",
        "  def setPredictYieldResiduals(self, pred_res):\n",
        "    \"\"\"Set whether to use predict yield residuals\"\"\"\n",
        "    pred_yres = pred_res.upper()\n",
        "    assert pred_yres in ['Y', 'N']\n",
        "    self.config['predict_yield_residuals'] = pred_yres\n",
        "\n",
        "  def predictYieldResiduals(self):\n",
        "    \"\"\"Return whether to use predict yield residuals\"\"\"\n",
        "    return (self.config['predict_yield_residuals'] == 'Y')\n",
        "\n",
        "  def setFindOptimalTrendWindow(self, find_opt):\n",
        "    \"\"\"Set whether to find optimal trend window for each year\"\"\"\n",
        "    find_otw = find_opt.upper()\n",
        "    assert find_otw in ['Y', 'N']\n",
        "    self.config['find_optimal_trend_window'] = find_otw\n",
        "\n",
        "  def findOptimalTrendWindow(self):\n",
        "    \"\"\"Return whether to find optimal trend window for each year\"\"\"\n",
        "    return (self.config['find_optimal_trend_window'] == 'Y')\n",
        "\n",
        "  def setTrendWindows(self, trend_windows):\n",
        "    \"\"\"Set trend window lengths (years)\"\"\"\n",
        "    assert isinstance(trend_windows, list)\n",
        "    assert len(trend_windows) > 0\n",
        "\n",
        "    # trend windows less than 2 years do not make sense\n",
        "    for tw in trend_windows:\n",
        "      assert tw > 2\n",
        "\n",
        "    self.config['trend_windows'] = trend_windows\n",
        "\n",
        "  def getTrendWindows(self):\n",
        "    \"\"\"Return trend window lengths (years)\"\"\"\n",
        "    return self.config['trend_windows']\n",
        "\n",
        "  def setUseCentroids(self, use_centroids):\n",
        "    \"\"\"Set whether to use centroid coordinates and distance to coast\"\"\"\n",
        "    use_ct = use_centroids.upper()\n",
        "    assert use_ct in ['Y', 'N']\n",
        "    self.config['use_centroids'] = use_ct\n",
        "    self.updateDataSources('CENTROIDS', use_ct)\n",
        "\n",
        "  def useCentroids(self):\n",
        "    \"\"\"Return whether to use centroid coordinates and distance to coast\"\"\"\n",
        "    return (self.config['use_centroids'] == 'Y')\n",
        "\n",
        "  def setUseRemoteSensing(self, use_remote_sensing):\n",
        "    \"\"\"Set whether to use remote sensing data\"\"\"\n",
        "    use_rs = use_remote_sensing.upper()\n",
        "    assert use_rs in ['Y', 'N']\n",
        "    self.config['use_remote_sensing'] = use_rs\n",
        "    self.updateDataSources('REMOTE_SENSING', use_rs, 'NUTS2')\n",
        "\n",
        "  def useRemoteSensing(self):\n",
        "    \"\"\"Return whether to use remote sensing data\"\"\"\n",
        "    return (self.config['use_remote_sensing'] == 'Y')\n",
        "\n",
        "  def setUseGAES(self, use_gaes):\n",
        "    \"\"\"Set whether to use GAES data\"\"\"\n",
        "    use_aez = use_gaes.upper()\n",
        "    assert use_aez in ['Y', 'N']\n",
        "    self.config['use_gaes'] = use_aez\n",
        "    self.updateDataSources('GAES', use_aez)\n",
        "    self.updateDataSources('CROP_AREA', use_aez)\n",
        "\n",
        "  def useGAES(self):\n",
        "    \"\"\"Return whether to use GAES data\"\"\"\n",
        "    return (self.config['use_gaes'] == 'Y')\n",
        "\n",
        "  def setUsePerYearCropCalendar(self, use_per_year_cc):\n",
        "    \"\"\"Set whether to use per region, per year crop calendar\"\"\"\n",
        "    per_year = use_per_year_cc.upper()\n",
        "    assert per_year in ['Y', 'N']\n",
        "    self.config['use_per_year_crop_calendar'] = per_year\n",
        "\n",
        "  def usePerYearCropCalendar(self):\n",
        "    \"\"\"Return whether to use per region, per year crop calendar\"\"\"\n",
        "    return (self.config['use_per_year_crop_calendar'] == 'Y')\n",
        "\n",
        "  def setEarlySeasonPrediction(self, early_season):\n",
        "    \"\"\"Set whether to do early season prediction\"\"\"\n",
        "    ep = early_season.upper()\n",
        "    assert ep in ['Y', 'N']\n",
        "    self.config['early_season_prediction'] = ep\n",
        "\n",
        "  def earlySeasonPrediction(self):\n",
        "    \"\"\"Return whether to do early season prediction\"\"\"\n",
        "    return (self.config['early_season_prediction'] == 'Y')\n",
        "\n",
        "  def setEarlySeasonEndDekad(self, end_dekad):\n",
        "    \"\"\"Set early season prediction dekad\"\"\"\n",
        "    dekads_range = [dek for dek in range(1, 37)]\n",
        "    assert end_dekad in dekads_range\n",
        "    self.config['early_season_end_dekad'] = end_dekad\n",
        "\n",
        "  def getEarlySeasonEndDekad(self):\n",
        "    \"\"\"Return early season prediction dekad\"\"\"\n",
        "    return self.config['early_season_end_dekad']\n",
        "\n",
        "  def setDataPath(self, data_path):\n",
        "    \"\"\"Set the data path\"\"\"\n",
        "    # TODO: some validation\n",
        "    self.config['data_path'] = data_path\n",
        "\n",
        "  def getDataPath(self):\n",
        "    \"\"\"Return the data path\"\"\"\n",
        "    return self.config['data_path']\n",
        "\n",
        "  def setOutputPath(self, out_path):\n",
        "    \"\"\"Set the path to output files. TODO: some validation.\"\"\"\n",
        "    self.config['output_path'] = out_path\n",
        "\n",
        "  def getOutputPath(self):\n",
        "    \"\"\"Return the path to output files.\"\"\"\n",
        "    return self.config['output_path']\n",
        "\n",
        "  def setUseFeaturesV2(self, use_v2):\n",
        "    \"\"\"Set whether to use features v2\"\"\"\n",
        "    ft_v2 = use_v2.upper()\n",
        "    assert ft_v2 in ['Y', 'N']\n",
        "    self.config['use_features_v2'] = ft_v2\n",
        "\n",
        "  def useFeaturesV2(self):\n",
        "    \"\"\"Return whether to use features v2\"\"\"\n",
        "    return (self.config['use_features_v2'] == 'Y')\n",
        "\n",
        "  def setSaveFeatures(self, save_ft):\n",
        "    \"\"\"Set whether to save features in a CSV file\"\"\"\n",
        "    sft = save_ft.upper()\n",
        "    assert sft in ['Y', 'N']\n",
        "    self.config['save_features'] = sft\n",
        "\n",
        "  def saveFeatures(self):\n",
        "    \"\"\"Return whether to save features in a CSV file\"\"\"\n",
        "    return (self.config['save_features'] == 'Y')\n",
        "\n",
        "  def setUseSavedFeatures(self, use_saved):\n",
        "    \"\"\"Set whether to use features from CSV file\"\"\"\n",
        "    saved = use_saved.upper()\n",
        "    assert saved in ['Y', 'N']\n",
        "    self.config['use_saved_features'] = saved\n",
        "\n",
        "  def useSavedFeatures(self):\n",
        "    \"\"\"Return whether to use to use features from CSV file\"\"\"\n",
        "    return (self.config['use_saved_features'] == 'Y')\n",
        "\n",
        "  def setUseSampleWeights(self, use_weights):\n",
        "    \"\"\"Set whether to use data sample weights\"\"\"\n",
        "    use_sw = use_weights.upper()\n",
        "    assert use_sw in ['Y', 'N']\n",
        "    self.config['use_sample_weights'] = use_sw\n",
        "\n",
        "  def useSampleWeights(self):\n",
        "    \"\"\"Return whether to use data sample weights\"\"\"\n",
        "    return (self.config['use_sample_weights'] == 'Y')\n",
        "\n",
        "  def setRetrainPerTestYear(self, per_test_year):\n",
        "    \"\"\"Set whether to retrain the model for every test year\"\"\"\n",
        "    pty = per_test_year.upper()\n",
        "    assert pty in ['Y', 'N']\n",
        "    self.config['retrain_per_test_year'] = pty\n",
        "\n",
        "  def retrainPerTestYear(self):\n",
        "    \"\"\"Return whether to retrain the model for every test year\"\"\"\n",
        "    return (self.config['retrain_per_test_year'] == 'Y')\n",
        "\n",
        "  def setSavePredictions(self, save_pred):\n",
        "    \"\"\"Set whether to save predictions in a CSV file\"\"\"\n",
        "    spd = save_pred.upper()\n",
        "    assert spd in ['Y', 'N']\n",
        "    self.config['save_predictions'] = spd\n",
        "\n",
        "  def savePredictions(self):\n",
        "    \"\"\"Return whether to save predictions in a CSV file\"\"\"\n",
        "    return (self.config['save_predictions'] == 'Y')\n",
        "\n",
        "  def setUseSavedPredictions(self, use_saved):\n",
        "    \"\"\"Set whether to use predictions from CSV file\"\"\"\n",
        "    saved = use_saved.upper()\n",
        "    assert saved in ['Y', 'N']\n",
        "    self.config['use_saved_predictions'] = saved\n",
        "\n",
        "  def useSavedPredictions(self):\n",
        "    \"\"\"Return whether to use to use predictions from CSV file\"\"\"\n",
        "    return (self.config['use_saved_predictions'] == 'Y')\n",
        "\n",
        "  def setCompareWithMCYFS(self, compare_mcyfs):\n",
        "    \"\"\"Set whether to compare predictions with MCYFS\"\"\"\n",
        "    comp_mcyfs = compare_mcyfs.upper()\n",
        "    assert comp_mcyfs in ['Y', 'N']\n",
        "    self.config['compare_with_mcyfs'] = comp_mcyfs\n",
        "\n",
        "  def compareWithMCYFS(self):\n",
        "    \"\"\"Return whether to compare predictions with MCYFS\"\"\"\n",
        "    return (self.config['compare_with_mcyfs'] == 'Y')\n",
        "\n",
        "  def setDebugLevel(self, debug_level):\n",
        "    \"\"\"Set the debug level\"\"\"\n",
        "    assert debug_level in debug_levels\n",
        "    self.config['debug_level'] = debug_level\n",
        "\n",
        "  def getDebugLevel(self):\n",
        "    \"\"\"Return the debug level\"\"\"\n",
        "    return self.config['debug_level']\n",
        "\n",
        "  def updateConfiguration(self, config_update):\n",
        "    \"\"\"Update configuration\"\"\"\n",
        "    assert isinstance(config_update, dict)\n",
        "    for k in config_update:\n",
        "      assert k in self.config\n",
        "\n",
        "      # keys that need special handling\n",
        "      special_cases = {\n",
        "          'crop_name' : self.setCropName,\n",
        "          'crop_id' : self.setCropID,\n",
        "          'use_centroids' : self.setUseCentroids,\n",
        "          'use_remote_sensing' : self.setUseRemoteSensing,\n",
        "          'use_gaes' : self.setUseGAES,\n",
        "      }\n",
        "\n",
        "      if (k not in special_cases):\n",
        "        self.config[k] = config_update[k]\n",
        "        continue\n",
        "\n",
        "      # special case\n",
        "      special_cases[k](config_update[k])\n",
        "\n",
        "  def printConfig(self, log_fh):\n",
        "    \"\"\"Print current configuration and write configuration to log file.\"\"\"\n",
        "    config_str = '\\nCurrent ML Baseline Configuration'\n",
        "    config_str += '\\n--------------------------------'\n",
        "    for k in self.config:\n",
        "      if (isinstance(self.config[k], dict)):\n",
        "        conf_keys = list(self.config[k].keys())\n",
        "        if (not isinstance(conf_keys[0], str)):\n",
        "          conf_keys = [str(k) for k in conf_keys]\n",
        "\n",
        "        config_str += '\\n' + self.config_desc[k] + ': ' + ', '.join(conf_keys)\n",
        "      elif (isinstance(self.config[k], list)):\n",
        "        conf_vals = self.config[k]\n",
        "        if (not isinstance(conf_vals[0], str)):\n",
        "          conf_vals = [str(k) for k in conf_vals]\n",
        "\n",
        "        config_str += '\\n' + self.config_desc[k] + ': ' + ', '.join(conf_vals)\n",
        "      else:\n",
        "        conf_val = self.config[k]\n",
        "        if (not isinstance(conf_val, str)):\n",
        "          conf_val = str(conf_val)\n",
        "\n",
        "        config_str += '\\n' + self.config_desc[k] + ': ' + conf_val\n",
        "\n",
        "    config_str += '\\n'\n",
        "    log_fh.write(config_str + '\\n')\n",
        "    print(config_str)\n",
        "\n",
        "  # Machine learning configuration\n",
        "  def getFeatureCorrelationThreshold(self):\n",
        "    \"\"\"Return threshold for removing mutually correlated features\"\"\"\n",
        "    return self.feature_correlation_threshold\n",
        "\n",
        "  def setFeatureCorrelationThreshold(self, corr_thresh):\n",
        "    \"\"\"Set threshold for removing mutually correlated features\"\"\"\n",
        "    assert (corr_thresh > 0.0 and corr_thresh < 1.0)\n",
        "    self.feature_correlation_threshold = corr_thresh\n",
        "\n",
        "  def getTestFraction(self):\n",
        "    \"\"\"Return test set fraction (of full dataset)\"\"\"\n",
        "    return self.test_fraction\n",
        "\n",
        "  def setTestFraction(self, test_fraction):\n",
        "    \"\"\"Set test set fraction (of full dataset)\"\"\"\n",
        "    assert (test_fraction > 0.0 and test_fraction < 1.0)\n",
        "    self.test_fraction = test_fraction\n",
        "\n",
        "  def getFeatureScaler(self):\n",
        "    \"\"\"Return feature scaling method\"\"\"\n",
        "    return self.scaler\n",
        "\n",
        "  def setFeatureScaler(self, scaler):\n",
        "    \"\"\"Set feature scaling method\"\"\"\n",
        "    assert (isinstance(scaler, MinMaxScaler) or isinstance(scaler, StandardScaler))\n",
        "    self.scaler = scaler\n",
        "\n",
        "  def getFeatureSelectionCVMetric(self):\n",
        "    \"\"\"Return metric for feature selection using K-fold validation\"\"\"\n",
        "    return self.fs_cv_metric\n",
        "\n",
        "  def setFeatureSelectionCVMetric(self, fs_metric):\n",
        "    \"\"\"Return metric for feature selection using K-fold validation\"\"\"\n",
        "    assert fs_metric in self.eval_metrics\n",
        "    self.fs_cv_metric = fs_metric\n",
        "\n",
        "  def getAlgorithmTrainingCVMetric(self):\n",
        "    \"\"\"Return metric for hyperparameter optimization using K-fold validation\"\"\"\n",
        "    return self.est_cv_metric\n",
        "\n",
        "  def setFeatureSelectionCVMetric(self, est_metric):\n",
        "    \"\"\"Return metric for hyperparameter optimization using K-fold validation\"\"\"\n",
        "    assert est_metric in self.eval_metrics\n",
        "    self.est_cv_metric = est_metric\n",
        "\n",
        "  def getFeatureSelectors(self, num_features):\n",
        "    \"\"\"Feature selection methods\"\"\"\n",
        "    # already defined?\n",
        "    if (len(self.feature_selectors) > 0):\n",
        "      return self.feature_selectors\n",
        "\n",
        "    # Early season prediction can have less than 10 features\n",
        "    min_features = 10\n",
        "    if (num_features < 10):\n",
        "      min_features = num_features - 1\n",
        "\n",
        "    rf = RandomForestRegressor(bootstrap=True, max_features='log2', ccp_alpha=1e-2,\n",
        "                               max_depth=10, min_samples_leaf=10, n_estimators=500,\n",
        "                               random_state=42, oob_score=True)\n",
        "\n",
        "    lasso = Lasso(copy_X=True, fit_intercept=True, normalize=False,\n",
        "                  tol=1e-3, random_state=42, selection='random')\n",
        "\n",
        "    self.feature_selectors = {\n",
        "      # random forest\n",
        "      'random_forest' : {\n",
        "          'selector' : SelectFromModel(rf, threshold='median'),\n",
        "          'param_space' : {\n",
        "              \"selector__estimator__min_samples_leaf\" : Integer(5, 20),\n",
        "              # \"selector__estimator__n_estimators\" : Integer(100, 500),\n",
        "              # \"selector__estimator__max_depth\" : Integer(5, 10),\n",
        "              \"selector__max_features\" : Integer(min_features, num_features),\n",
        "          }\n",
        "      },\n",
        "      # recursive feature elimination using Lasso\n",
        "      'RFE_Lasso' : {\n",
        "          'selector' : RFE(lasso),\n",
        "          'param_space' : {\n",
        "              \"selector__estimator__alpha\" : Real(1e-2, 1e+1, prior='log-uniform'),\n",
        "              \"selector__n_features_to_select\" : Integer(min_features, num_features),\n",
        "          }\n",
        "      },\n",
        "      # # NOTE: Mutual info raises an error when used with spark parallel backend.\n",
        "      # univariate feature selection\n",
        "      # 'mutual_info' : {\n",
        "      #     'selector' : SelectKBest(mutual_info_regression),\n",
        "      #     'param_space' : {\n",
        "      #         \"selector__k\" : Integer(min_features, max_features),\n",
        "      #     }\n",
        "      # },\n",
        "    }\n",
        "\n",
        "    return self.feature_selectors\n",
        "\n",
        "  def setFeatureSelectors(self, ft_sel):\n",
        "    \"\"\"Set feature selection algorithms\"\"\"\n",
        "    assert isinstance(ft_sel, dict)\n",
        "    assert len(ft_sel) > 0\n",
        "    for sel in ft_sel:\n",
        "      assert isinstance(sel, dict)\n",
        "      assert 'selector' in sel\n",
        "      assert 'param_space' in sel\n",
        "      # add cases if other feature selection methods are used\n",
        "      assert (isinstance(sel['selector'], SelectKBest) or\n",
        "              isinstance(sel['selector'], SelectFromModel) or\n",
        "              isinstance(sel['selector'], RFE))\n",
        "      assert isinstance(sel['param_space'], dict)\n",
        "\n",
        "    self.feature_selectors = ft_sel\n",
        "\n",
        "  def getEstimators(self):\n",
        "    \"\"\"Return machine learning algorithms for prediction\"\"\"\n",
        "    return self.estimators\n",
        "  \n",
        "  def setEstimators(self, estimators):\n",
        "    \"\"\"Set machine learning algorithms for prediction\"\"\"\n",
        "    assert isinstance(estimators, dict)\n",
        "    assert len(estimators) > 0\n",
        "    for est in estimators:\n",
        "      assert isinstance(est, dict)\n",
        "      assert 'estimator' in est\n",
        "      assert 'param_space' in est\n",
        "      assert isinstance(est['param_space'], dict)\n",
        "\n",
        "    self.estimators = estimators\n",
        "  \n",
        "  def getEvaluationMetrics(self):\n",
        "    \"\"\"Return metrics to evaluate predictions of algorithms\"\"\"\n",
        "    return self.eval_metrics\n",
        "  \n",
        "  def setEvaluationMetrics(self, metrics):\n",
        "    assert isinstance(metrics, dict)\n",
        "    self.eval_metrics = metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKLbsRZPj_la"
      },
      "source": [
        "## Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJOtnzSYQvPu"
      },
      "source": [
        "### Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDlfa0-RtsA7"
      },
      "source": [
        "#### Data Loader Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nk-0JeNdQvP2"
      },
      "outputs": [],
      "source": [
        "#%%writefile data_loading.py\n",
        "class CYPDataLoader:\n",
        "  def __init__(self, spark, cyp_config):\n",
        "    self.spark = spark\n",
        "    self.data_path = cyp_config.getDataPath()\n",
        "    self.country_code = cyp_config.getCountryCode()\n",
        "    self.nuts_level = cyp_config.getNUTSLevel()\n",
        "    self.data_sources = cyp_config.getDataSources()\n",
        "    self.verbose = cyp_config.getDebugLevel()\n",
        "    self.data_dfs = {}\n",
        "\n",
        "  def loadFromCSVFile(self, data_path, src, nuts, country_code):\n",
        "    \"\"\"\n",
        "    The implied filename for each source is:\n",
        "    <data_source>_<nuts_level>_<country_code>.csv\n",
        "    Examples: CENTROIDS_NUTS2_NL.csv, WOFOST_NUTS2_NL.csv.\n",
        "    Schema is inferred from the file. We might want to specify the schema at some point.\n",
        "    \"\"\"\n",
        "    if (country_code is not None):\n",
        "      datafile = data_path + '/' + src  + '_' + nuts + '_' + country_code + '.csv'\n",
        "    elif (nuts is not None):\n",
        "      datafile = data_path + '/' + src  + '_' + nuts + '.csv'\n",
        "    else:\n",
        "      datafile = data_path + '/' + src  + '.csv'\n",
        "\n",
        "    if (self.verbose > 1):\n",
        "      print('Data file name', '\"' + datafile + '\"')\n",
        "\n",
        "    df = self.spark.read.csv(datafile, header = True, inferSchema = True)\n",
        "    return df\n",
        "\n",
        "  def loadData(self, src, nuts_level):\n",
        "    \"\"\"\n",
        "    Load data for a specific data source.\n",
        "    nuts_level may one level or a list of levels.\n",
        "    \"\"\"\n",
        "    data_path = self.data_path\n",
        "    country_code = self.country_code\n",
        "    assert src in self.data_sources\n",
        "\n",
        "    if (isinstance(nuts_level, list)):\n",
        "      src_dfs = []\n",
        "      for nuts in nuts_level:\n",
        "        df = self.loadFromCSVFile(data_path, src, nuts, country_code)\n",
        "        src_dfs.append(df)\n",
        "\n",
        "    else:\n",
        "      src_dfs = self.loadFromCSVFile(data_path, src, nuts_level, country_code)\n",
        "\n",
        "    return src_dfs\n",
        "\n",
        "  def loadAllData(self):\n",
        "    \"\"\"\n",
        "    NOT SUPPORTED:\n",
        "    1. Schema is not defined.\n",
        "    2. Loading data for multiple countries.\n",
        "    3. Loading data from folders.\n",
        "    Ioannis: Spark has a nice way of loading several files from a folder,\n",
        "    and associating the file name on each record, using the function\n",
        "    input_file_name. This allows to extract medatada from the path\n",
        "    into the dataframe. In your case it could be the country name, etc.\n",
        "    \"\"\"\n",
        "    data_dfs = {}\n",
        "    for src in self.data_sources:\n",
        "      nuts_level = self.nuts_level\n",
        "      if (isinstance(self.data_sources, dict)):\n",
        "        nuts_level = self.data_sources[src]\n",
        "      # REMOTE_SENSING data is at NUTS2. If nuts_level is None, leave as is.\n",
        "      # elif ((src == 'REMOTE_SENSING') and (nuts_level is not None)):\n",
        "      #   nuts_level = 'NUTS2'\n",
        "\n",
        "      if ('METEO' in src):\n",
        "        data_dfs['METEO'] = self.loadData(src, nuts_level)\n",
        "      else:\n",
        "        data_dfs[src] = self.loadData(src, nuts_level)\n",
        "\n",
        "    if (self.verbose > 1):\n",
        "      data_sources_str = ''\n",
        "      for src in data_dfs:\n",
        "        data_sources_str = data_sources_str + src + ', '\n",
        "\n",
        "      # remove the comma and space from the end\n",
        "      print('Loaded data:', data_sources_str[:-2])\n",
        "      print('\\n')\n",
        "\n",
        "    return data_dfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30v2QIEh2K9P"
      },
      "source": [
        "#### Data Preprocessor Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qo2OHq0x2ID1"
      },
      "outputs": [],
      "source": [
        "#%%writefile data_preprocessing.py\n",
        "from pyspark.sql import Window\n",
        "\n",
        "class CYPDataPreprocessor:\n",
        "  def __init__(self, spark, cyp_config):\n",
        "    self.spark = spark\n",
        "    self.verbose = cyp_config.getDebugLevel()\n",
        "\n",
        "  def extractYearDekad(self, df):\n",
        "    \"\"\"Extract year and dekad from date_col in yyyyMMdd format.\"\"\"\n",
        "    # Conversion to string type is required to make getYear(), getMonth() etc. work correctly.\n",
        "    # They use to_date() function to verify valid dates and to_date() expects the date column to be string.\n",
        "    df = df.withColumn('DATE', df['DATE'].cast(\"string\"))\n",
        "    df = df.select('*',\n",
        "                   getYear('DATE').alias('FYEAR'),\n",
        "                   getDekad('DATE').alias('DEKAD'))\n",
        "\n",
        "    # Bring FYEAR, DEKAD to the front\n",
        "    col_order = df.columns[:2] + df.columns[-2:] + df.columns[2:-2]\n",
        "    df = df.select(col_order).drop('DATE')\n",
        "    return df\n",
        "\n",
        "  def getCropSeasonInformation(self, wofost_df, season_crosses_calyear):\n",
        "    \"\"\"Crop season information based on WOFOST DVS\"\"\"\n",
        "    join_cols = ['IDREGION', 'FYEAR']\n",
        "    if (('DATE' in wofost_df.columns) and ('FYEAR' not in wofost_df.columns)):\n",
        "      wofost_df = self.extractYearDekad(wofost_df)\n",
        "\n",
        "    crop_season = wofost_df.select(join_cols).distinct()\n",
        "    diff_window = Window.partitionBy(join_cols).orderBy('DEKAD')\n",
        "    cs_window = Window.partitionBy('IDREGION').orderBy('FYEAR')\n",
        "\n",
        "    wofost_df = wofost_df.withColumn('VALUE', wofost_df['DVS'])\n",
        "    wofost_df = wofost_df.withColumn('PREV', SparkF.lag(wofost_df['VALUE']).over(diff_window))\n",
        "    wofost_df = wofost_df.withColumn('DIFF', SparkF.when(SparkF.isnull(wofost_df['PREV']), 0)\\\n",
        "                                     .otherwise(wofost_df['VALUE'] - wofost_df['PREV']))\n",
        "    # calculate end of season dekad\n",
        "    dvs_nochange_filter = ((wofost_df['VALUE'] >= 200) & (wofost_df['DIFF'] == 0.0))\n",
        "    year_end_filter = (wofost_df['DEKAD'] == 36)\n",
        "    if (season_crosses_calyear):\n",
        "      value_zero_filter =  (wofost_df['VALUE'] == 0)\n",
        "    else:\n",
        "      value_zero_filter =  ((wofost_df['PREV'] >= 200) & (wofost_df['VALUE'] == 0))\n",
        "\n",
        "    end_season_filter = (dvs_nochange_filter | value_zero_filter | year_end_filter)\n",
        "    crop_season = crop_season.join(wofost_df.filter(end_season_filter).groupBy(join_cols)\\\n",
        "                                   .agg(SparkF.min('DEKAD').alias('SEASON_END_DEKAD')), join_cols)\n",
        "    wofost_df = wofost_df.drop('VALUE', 'PREV', 'DIFF')\n",
        "\n",
        "    # We take the max of SEASON_END_DEKAD for current campaign and next campaign\n",
        "    # to determine which dekads go to next campaign year.\n",
        "    max_year = crop_season.agg(SparkF.max('FYEAR')).collect()[0][0]\n",
        "    min_year = crop_season.agg(SparkF.min('FYEAR')).collect()[0][0]\n",
        "    crop_season = crop_season.withColumn('NEXT_SEASON_END', SparkF.when(crop_season['FYEAR'] == max_year,\n",
        "                                                                        crop_season['SEASON_END_DEKAD'])\\\n",
        "                                         .otherwise(SparkF.lead(crop_season['SEASON_END_DEKAD']).over(cs_window)))\n",
        "    crop_season = crop_season.withColumn('SEASON_END',\n",
        "                                         SparkF.when(crop_season['SEASON_END_DEKAD'] > crop_season['NEXT_SEASON_END'],\n",
        "                                                     crop_season['SEASON_END_DEKAD'])\\\n",
        "                                         .otherwise(crop_season['NEXT_SEASON_END']))\n",
        "    crop_season = crop_season.withColumn('PREV_SEASON_END', SparkF.when(crop_season['FYEAR'] == min_year, 0)\\\n",
        "                                         .otherwise(SparkF.lag(crop_season['SEASON_END']).over(cs_window)))\n",
        "    crop_season = crop_season.select(join_cols + ['PREV_SEASON_END', 'SEASON_END'])\n",
        "\n",
        "    return crop_season\n",
        "\n",
        "  def alignDataToCropSeason(self, df, crop_season, season_crosses_calyear):\n",
        "    \"\"\"Calculate CAMPAIGN_YEAR, CAMPAIGN_DEKAD based on crop_season\"\"\"\n",
        "    join_cols = ['IDREGION', 'FYEAR']\n",
        "    max_year = crop_season.agg(SparkF.max('FYEAR')).collect()[0][0]\n",
        "    min_year = crop_season.agg(SparkF.min('FYEAR')).collect()[0][0]\n",
        "    df = df.join(crop_season, join_cols)\n",
        "\n",
        "    # Dekads > SEASON_END belong to next campaign year\n",
        "    df = df.withColumn('CAMPAIGN_YEAR',\n",
        "                       SparkF.when(df['DEKAD'] > df['SEASON_END'], df['FYEAR'] + 1)\\\n",
        "                       .otherwise(df['FYEAR']))\n",
        "    # min_year has no previous season information. We align CAMPAIGN_DEKAD to end in 36.\n",
        "    # For other years, dekads < SEASON_END are adjusted based on PREV_SEASON_END.\n",
        "    # Dekads > SEASON_END get renumbered from 1 (for next campaign).\n",
        "    df = df.withColumn('CAMPAIGN_DEKAD',\n",
        "                       SparkF.when(df['CAMPAIGN_YEAR'] == min_year, df['DEKAD'] + 36 - df['SEASON_END'])\\\n",
        "                       .otherwise(SparkF.when(df['DEKAD'] > df['SEASON_END'], df['DEKAD'] - df['SEASON_END'])\\\n",
        "                                  .otherwise(df['DEKAD'] + 36 - df['PREV_SEASON_END'])))\n",
        "\n",
        "    # Columns should be IDREGION, FYEAR, DEKAD, ..., CAMPAIGN_YEAR, CAMPAIGN_DEKAD.\n",
        "    # Bring CAMPAIGN_YEAR and CAMPAIGN_DEKAD to the front.\n",
        "    col_order = df.columns[:3] + df.columns[-2:] + df.columns[3:-2]\n",
        "    df = df.select(col_order)\n",
        "    if (season_crosses_calyear):\n",
        "      # For crop with two seasons, remove the first year. Data from the first year\n",
        "      # only contributes to the second year and we have already moved useful data\n",
        "      # to the second year (or first campaign year).\n",
        "      df = df.filter(df['CAMPAIGN_YEAR'] > min_year)\n",
        "\n",
        "    # In both cases, remove extra rows beyond max campaign year\n",
        "    df = df.filter(df['CAMPAIGN_YEAR'] <= max_year)\n",
        "    return df\n",
        "\n",
        "  def preprocessWofost(self, wofost_df, crop_season, season_crosses_calyear):\n",
        "    \"\"\"\n",
        "    Extract year and dekad from date. Use crop_season to compute\n",
        "    CAMPAIGN_YEAR and CAMPAIGN_DEKAD.\n",
        "    \"\"\"\n",
        "    if (('DATE' in wofost_df.columns) and ('FYEAR' not in wofost_df.columns)):\n",
        "      wofost_df = self.extractYearDekad(wofost_df)\n",
        "\n",
        "    join_cols = ['IDREGION', 'CAMPAIGN_YEAR']\n",
        "    wofost_df = self.alignDataToCropSeason(wofost_df, crop_season, season_crosses_calyear)\n",
        "\n",
        "    # Remove spurious values from SEASON_END to when DVS becomes zero again\n",
        "    # wofost_df.filter(wofost_df['FYEAR'] == 1980).orderBy(join_cols + ['CAMPAIGN_DEKAD']).show(40)\n",
        "    start_season_df = wofost_df.select(join_cols).distinct()\n",
        "    # left join to keep all regions and years\n",
        "    start_season_df = start_season_df.join(wofost_df.filter((wofost_df['DEKAD'] > wofost_df['SEASON_END']) &\n",
        "                                                            (wofost_df['DVS'] == 0.0)).groupBy(join_cols)\\\n",
        "                                           .agg(SparkF.min('CAMPAIGN_DEKAD').alias('START_SEASON')), join_cols, \"left\")\n",
        "    # for years with no spurious data, START_SEASON is 1\n",
        "    start_season_df = start_season_df.na.fill(1.0)\n",
        "    # start_season_df.orderBy(join_cols).show(20)\n",
        "    wofost_df = wofost_df.join(start_season_df, join_cols)\n",
        "\n",
        "    # trailing columns: crop_season cols except join_cols, START_SEASON\n",
        "    drop_cols = crop_season.columns[2:] + ['START_SEASON']\n",
        "    num_trailing_cols = len(crop_season.columns) - 2 + 1\n",
        "    wofost_inds = wofost_df.columns[5:-num_trailing_cols]\n",
        "    # set indicators values for dekads after end of season to zero.\n",
        "    # TODO - Dilli: Find a way to avoid the for loop.\n",
        "    for ind in wofost_inds:\n",
        "        wofost_df = wofost_df.withColumn(ind,\n",
        "                                         SparkF.when(wofost_df['CAMPAIGN_DEKAD'] >= wofost_df['START_SEASON'],\n",
        "                                                     wofost_df[ind]).otherwise(0))\n",
        "\n",
        "    wofost_df = wofost_df.drop(*drop_cols)\n",
        "    return wofost_df\n",
        "\n",
        "  def preprocessMeteo(self, meteo_df, crop_season, season_crosses_calyear):\n",
        "    \"\"\"\n",
        "    Extract year and dekad from date, calculate CWB.\n",
        "    Use crop_season to compute CAMPAIGN_YEAR and CAMPAIGN_DEKAD.\n",
        "    \"\"\"\n",
        "    join_cols = ['IDREGION', 'FYEAR']\n",
        "    drop_cols = crop_season.columns[2:]\n",
        "    meteo_df = meteo_df.drop('IDCOVER')\n",
        "    meteo_df = meteo_df.withColumn('CWB',\n",
        "                                   SparkF.bround(meteo_df['PREC'] - meteo_df['ET0'], 2))\n",
        "    if (('DATE' in meteo_df.columns) and ('FYEAR' not in meteo_df.columns)):\n",
        "      meteo_df = self.extractYearDekad(meteo_df)\n",
        "\n",
        "    meteo_df = self.alignDataToCropSeason(meteo_df, crop_season, season_crosses_calyear)\n",
        "    meteo_df = meteo_df.drop(*drop_cols)\n",
        "    return meteo_df\n",
        "\n",
        "  def preprocessMeteoDaily(self, meteo_df):\n",
        "    \"\"\"\n",
        "    Convert daily meteo data to dekadal. Takes avg for all indicators\n",
        "    except TMAX (take max), TMIN (take min), PREC (take sum), ET0 (take sum), CWB (take sum).\n",
        "    \"\"\"\n",
        "    self.spark.catalog.dropTempView('meteo_daily')\n",
        "    meteo_df.createOrReplaceTempView('meteo_daily')\n",
        "    join_cols = ['IDREGION', 'CAMPAIGN_YEAR', 'CAMPAIGN_DEKAD']\n",
        "    join_df = meteo_df.select(join_cols + ['FYEAR', 'DEKAD']).distinct()\n",
        "\n",
        "    # We are ignoring VPRES, WSPD and RELH at the moment\n",
        "    # avg(VPRES) as VPRES1, avg(WSPD) as WSPD1, avg(RELH) as RELH1,\n",
        "    # TMAX| TMIN| TAVG| VPRES| WSPD| PREC| ET0| RAD| RELH| CWB\n",
        "    #\n",
        "    # It seems keeping same name after aggregation is fine. We are using a\n",
        "    # different name just to be sure nothing untoward happens.\n",
        "    query = 'select IDREGION, CAMPAIGN_YEAR, CAMPAIGN_DEKAD, '\n",
        "    query = query + ' max(TMAX) as TMAX1, min(TMIN) as TMIN1, '\n",
        "    query = query + ' bround(avg(TAVG), 2) as TAVG1, bround(sum(PREC), 2) as PREC1, '\n",
        "    query = query + ' bround(sum(ET0), 2) as ET01, bround(avg(RAD), 2) as RAD1, '\n",
        "    query = query + ' bround(sum(CWB), 2) as CWB1 '\n",
        "    query = query + ' from meteo_daily group by IDREGION, CAMPAIGN_YEAR, CAMPAIGN_DEKAD '\n",
        "    query = query + ' order by IDREGION, CAMPAIGN_YEAR, CAMPAIGN_DEKAD'\n",
        "    meteo_df = self.spark.sql(query).cache()\n",
        "\n",
        "    # rename the columns\n",
        "    selected_cols = ['TMAX', 'TMIN', 'TAVG', 'PREC', 'ET0', 'RAD', 'CWB']\n",
        "    for scol in selected_cols:\n",
        "      meteo_df = meteo_df.withColumnRenamed(scol + '1', scol)\n",
        "\n",
        "    meteo_df = meteo_df.join(join_df, join_cols)\n",
        "    # Bring FYEAR, DEKAD to the front\n",
        "    col_order = meteo_df.columns[:1] + meteo_df.columns[-2:] + meteo_df.columns[1:-2]\n",
        "    meteo_df = meteo_df.select(col_order)\n",
        "\n",
        "    return meteo_df\n",
        "\n",
        "  def remoteSensingNUTS2ToNUTS3(self, rs_df, nuts3_regions):\n",
        "    \"\"\"\n",
        "    Convert NUTS2 remote sensing data to NUTS3.\n",
        "    Remote sensing values for NUTS3 regions are inherited from parent regions.\n",
        "    NOTE this function is called before preprocessRemoteSensing.\n",
        "    preprocessRemoteSensing expects crop_season and rs_df to be at the same NUTS level.\n",
        "    \"\"\"\n",
        "    NUTS3_dict = {}\n",
        "\n",
        "    for nuts3 in nuts3_regions:\n",
        "      nuts2 = nuts3[:4]\n",
        "      try:\n",
        "        existing = NUTS3_dict[nuts2]\n",
        "      except KeyError as e:\n",
        "        existing = []\n",
        "\n",
        "      NUTS3_dict[nuts2] = existing + [nuts3]\n",
        "\n",
        "    rs_NUTS3 = rs_df.rdd.map(lambda r: (NUTS3_dict[r[0]] if r[0] in NUTS3_dict else [], r[1], r[2]))\n",
        "    rs_NUTS3_df = rs_NUTS3.toDF(['NUTS3_REG', 'DATE', 'FAPAR'])\n",
        "    rs_NUTS3_df = rs_NUTS3_df.withColumn('IDREGION', SparkF.explode('NUTS3_REG')).drop('NUTS3_REG')\n",
        "    rs_NUTS3_df = rs_NUTS3_df.select('IDREGION', 'DATE', 'FAPAR')\n",
        "\n",
        "    return rs_NUTS3_df\n",
        "\n",
        "  def preprocessRemoteSensing(self, rs_df, crop_season, season_crosses_calyear):\n",
        "    \"\"\"\n",
        "    Extract year and dekad from date.\n",
        "    Use crop_season to compute CAMPAIGN_YEAR and CAMPAIGN_DEKAD.\n",
        "    NOTE crop_season and rs_df must be at the same NUTS level.\n",
        "    \"\"\"\n",
        "    join_cols = ['IDREGION', 'FYEAR']\n",
        "    drop_cols = crop_season.columns[2:]\n",
        "    if (('DATE' in rs_df.columns) and ('FYEAR' not in rs_df.columns)):\n",
        "      rs_df = self.extractYearDekad(rs_df)\n",
        "\n",
        "    rs_df = self.alignDataToCropSeason(rs_df, crop_season, season_crosses_calyear)\n",
        "    rs_df = rs_df.drop(*drop_cols)\n",
        "    return rs_df\n",
        "\n",
        "  def preprocessCentroids(self, centroids_df):\n",
        "    df_cols = centroids_df.columns\n",
        "    centroids_df = centroids_df.withColumn('CENTROID_X', SparkF.bround('CENTROID_X', 2))\n",
        "    centroids_df = centroids_df.withColumn('CENTROID_Y', SparkF.bround('CENTROID_Y', 2))\n",
        "\n",
        "    return centroids_df\n",
        "\n",
        "  def preprocessSoil(self, soil_df):\n",
        "    # SM_WC = water holding capacity\n",
        "    soil_df = soil_df.withColumn('SM_WHC', SparkF.bround(soil_df.SM_FC - soil_df.SM_WP, 2))\n",
        "    soil_df = soil_df.select(['IDREGION', 'SM_WHC'])\n",
        "\n",
        "    return soil_df\n",
        "\n",
        "  def preprocessAreaFractions(self, af_df, crop_id):\n",
        "    \"\"\"Filter area fractions data by crop id\"\"\"\n",
        "    af_df = af_df.withColumn(\"FYEAR\", af_df[\"FYEAR\"].cast(SparkT.IntegerType()))\n",
        "    af_df = af_df.filter(af_df[\"CROP_ID\"] == crop_id).drop('CROP_ID')\n",
        "\n",
        "    return af_df\n",
        "\n",
        "  def preprocessCropArea(self, area_df, crop_id):\n",
        "    \"\"\"Filter area fractions data by crop id\"\"\"\n",
        "    area_df = area_df.withColumn(\"FYEAR\", area_df[\"FYEAR\"].cast(SparkT.IntegerType()))\n",
        "    area_df = area_df.filter(area_df[\"CROP_ID\"] == crop_id).drop('CROP_ID')\n",
        "    area_df = area_df.filter(area_df[\"CROP_AREA\"].isNotNull())\n",
        "    area_df = area_df.drop('FRACTION')\n",
        "\n",
        "    return area_df\n",
        "\n",
        "  def preprocessGAES(self, gaes_df, crop_id):\n",
        "    \"\"\"Select irrigated crop area by crop id\"\"\"\n",
        "    sel_cols = [ c for c in gaes_df.columns if 'IRRIG' not in c]\n",
        "    sel_cols += ['IRRIG_AREA_ALL', 'IRRIG_AREA' + str(crop_id)]\n",
        "\n",
        "    return gaes_df.select(sel_cols)\n",
        "\n",
        "  def removeDuplicateYieldData(self, yield_df, short_seq_len=2, long_seq_len=5,\n",
        "                               max_short_seqs=1, max_long_seqs=0):\n",
        "    \"\"\"\n",
        "    Find and remove duplicate sequences of yield values.\n",
        "    Missing values are replaced with 0.0. So missing values are also handled.\n",
        "    Using some ideas from\n",
        "    https://stackoverflow.com/questions/51291226/finding-length-of-continuous-ones-in-list-in-a-pyspark-column\n",
        "    \"\"\"\n",
        "    w = Window.partitionBy('IDREGION').orderBy('FYEAR')\n",
        "    # check if value changes from one year to next\n",
        "    yield_df = yield_df.select('*',\n",
        "                               (yield_df['YIELD'] != SparkF.lag(yield_df['YIELD'], default=0)\\\n",
        "                                .over(w)).cast(\"int\").alias(\"YIELD_CHANGE\"),\n",
        "                               (yield_df['FYEAR'] - SparkF.lag(yield_df['FYEAR'], default=0)\\\n",
        "                                .over(w)).cast(\"int\").alias(\"FYEAR_CHANGE\"))\n",
        "    # group set of years with the same value\n",
        "    yield_df = yield_df.select('*',\n",
        "                               SparkF.sum(SparkF.col(\"YIELD_CHANGE\"))\\\n",
        "                               .over(w.rangeBetween(Window.unboundedPreceding, 0)).alias(\"YIELD_GROUP\"))\n",
        "\n",
        "    w2 = Window.partitionBy(['IDREGION', 'YIELD_GROUP'])\n",
        "    # compute the start, end and length of duplicate sequence\n",
        "    yield_df = yield_df.select('*',\n",
        "                               SparkF.min(\"FYEAR\").over(w2).alias(\"SEQ_START\"),\n",
        "                               SparkF.max(\"FYEAR\").over(w2).alias(\"SEQ_END\"),\n",
        "                               SparkF.count(\"*\").over(w2).alias(\"SEQ_LEN\"))\n",
        "\n",
        "    w3 = Window.partitionBy('IDREGION')\n",
        "    # compute max year\n",
        "    yield_df = yield_df.select('*',\n",
        "                               SparkF.min(\"FYEAR\").over(w3).alias(\"MIN_FYEAR\"),\n",
        "                               SparkF.max(\"FYEAR\").over(w3).alias(\"MAX_FYEAR\"))\n",
        "    # For sequences ending in max year, we remove data points only.\n",
        "    # So such sequences are not counted here.\n",
        "    yield_df = yield_df.select('*',\n",
        "                               # count number of short sequences except those ending at max(FYEAR)\n",
        "                               SparkF.sum(SparkF.when((yield_df['SEQ_LEN'] > short_seq_len) &\n",
        "                                                      # (yield_df['SEQ_END'] != yield_df['MAX_FYEAR']) &\n",
        "                                                      (yield_df['FYEAR'] == yield_df['SEQ_END']), 1)\\\n",
        "                                          .otherwise(0)).over(w3).alias('COUNT_SHORT_SEQ'),\n",
        "                               # count number of long sequences except those ending at max(FYEAR)\n",
        "                               SparkF.sum(SparkF.when((yield_df['SEQ_LEN'] > long_seq_len) &\n",
        "                                                      (yield_df['SEQ_END'] != yield_df['MAX_FYEAR']) &\n",
        "                                                      (yield_df['FYEAR'] == yield_df['SEQ_END']), 1)\\\n",
        "                                          .otherwise(0)).over(w3).alias('COUNT_LONG_SEQ'),\n",
        "                               # count missing years\n",
        "                               SparkF.sum(SparkF.when((yield_df['FYEAR'] != yield_df['MIN_FYEAR']) &\n",
        "                                                       (yield_df['FYEAR_CHANGE'] > short_seq_len), 1)\\\n",
        "                                          .otherwise(0)).over(w3).alias('COUNT_SHORT_GAPS'),\n",
        "                               SparkF.sum(SparkF.when((yield_df['FYEAR'] != yield_df['MIN_FYEAR']) &\n",
        "                                                        (yield_df['FYEAR_CHANGE'] > long_seq_len), 1)\\\n",
        "                                          .otherwise(0)).over(w3).alias('COUNT_LONG_GAPS'))\n",
        "\n",
        "    if (self.verbose > 2):\n",
        "      print('Data with duplicate sequences')\n",
        "      yield_df.filter(yield_df['COUNT_SHORT_SEQ'] > max_short_seqs).show()\n",
        "      yield_df.filter(yield_df['COUNT_LONG_SEQ'] > max_long_seqs).show()\n",
        "\n",
        "    # remove regions with many short sequences\n",
        "    yield_df = yield_df.filter(yield_df['COUNT_SHORT_SEQ'] <= max_short_seqs)\n",
        "    yield_df = yield_df.filter(yield_df['COUNT_SHORT_GAPS'] <= max_short_seqs)\n",
        "\n",
        "    # remove regions with long sequences\n",
        "    yield_df = yield_df.filter(yield_df['COUNT_LONG_SEQ'] <= max_long_seqs)\n",
        "    yield_df = yield_df.filter(yield_df['COUNT_LONG_GAPS'] <= max_long_seqs)\n",
        "\n",
        "    # remove data points, except SEQ_START, for remaining sequences\n",
        "    yield_df = yield_df.filter((yield_df['FYEAR'] == yield_df['SEQ_START']) |\n",
        "                               (yield_df['SEQ_LEN'] <= short_seq_len))\n",
        "\n",
        "    return yield_df.select(['IDREGION', 'FYEAR', 'YIELD'])\n",
        "\n",
        "  def preprocessYield(self, yield_df, crop_id, clean_data=False):\n",
        "    \"\"\"\n",
        "    Yield preprocessing depends on the data format.\n",
        "    Here we cover preprocessing for France (NUTS3), Germany (NUTS3) and the Netherlands (NUTS2).\n",
        "    \"\"\"\n",
        "    # Delete trailing empty columns\n",
        "    empty_cols = [ c for c in yield_df.columns if c.startswith('_c') ]\n",
        "    for c in empty_cols:\n",
        "      yield_df = yield_df.drop(c)\n",
        "\n",
        "    # Special case for Netherlands and Germany: convert yield columns into rows\n",
        "    years = [int(c) for c in yield_df.columns if c[0].isdigit()]\n",
        "    if (len(years) > 0):\n",
        "      yield_by_year = yield_df.rdd.map(lambda x: (x[0], cropNameToID(crop_id_dict, x[0]), x[1],\n",
        "                                                  [(years[i], x[i+2]) for i in range(len(years))]))\n",
        "\n",
        "      yield_df = yield_by_year.toDF(['CROP', 'CROP_ID', 'IDREGION', 'YIELD'])\n",
        "      yield_df = yield_df.withColumn('YR_YIELD', SparkF.explode('YIELD')).drop('YIELD')\n",
        "      yield_by_year = yield_df.rdd.map(lambda x: (x[0], x[1], x[2], x[3][0], x[3][1]))\n",
        "      yield_df = yield_by_year.toDF(['CROP', 'CROP_ID', 'IDREGION', 'FYEAR', 'YIELD'])\n",
        "    else:\n",
        "      yield_by_year = yield_df.rdd.map(lambda x: (x[0], cropNameToID(crop_id_dict, x[0]), x[1], x[2], x[3]))\n",
        "      yield_df = yield_by_year.toDF(['CROP', 'CROP_ID', 'IDREGION', 'FYEAR', 'YIELD'])\n",
        "\n",
        "    yield_df = yield_df.filter(yield_df.CROP_ID == crop_id).drop('CROP', 'CROP_ID')\n",
        "    if (yield_df.count() == 0):\n",
        "      return None\n",
        "\n",
        "    yield_df = yield_df.filter(yield_df.YIELD.isNotNull())\n",
        "    yield_df = yield_df.withColumn(\"YIELD\", yield_df[\"YIELD\"].cast(SparkT.FloatType()))\n",
        "    if (clean_data):\n",
        "      yield_df = self.removeDuplicateYieldData(yield_df)\n",
        "\n",
        "    yield_df = yield_df.filter(yield_df['YIELD'] > 0.0)\n",
        "\n",
        "    return yield_df\n",
        "\n",
        "  def preprocessYieldMCYFS(self, mcyfs_df, crop_id):\n",
        "    \"\"\"Preprocess MCYFS NUTS0 level yield predictions\"\"\"\n",
        "    # the input columns are IDREGION, CROP, PREDICTION_DATE, FYEAR, YIELD_PRED\n",
        "    mcyfs_df = mcyfs_df.withColumn('PRED_DEKAD', getDekad('PREDICTION_DATE'))\n",
        "    # the columns should now be IDREGION, CROP, PREDICTION_DATE, FYEAR, YIELD_PRED, PRED_DEKAD\n",
        "    yield_by_year = mcyfs_df.rdd.map(lambda x: (x[1], cropNameToID(crop_id_dict, x[1]),\n",
        "                                                x[0], x[3], x[2], x[4], x[5]))\n",
        "    mcyfs_df = yield_by_year.toDF(['CROP', 'CROP_ID', 'IDREGION', 'FYEAR',\n",
        "                                         'PRED_DATE', 'YIELD_PRED', 'PRED_DEKAD'])\n",
        "    mcyfs_df = mcyfs_df.filter(mcyfs_df.CROP_ID == crop_id).drop('CROP', 'CROP_ID')\n",
        "    if (mcyfs_df.count() == 0):\n",
        "      return None\n",
        "\n",
        "    mcyfs_df = mcyfs_df.withColumn(\"YIELD_PRED\", mcyfs_df[\"YIELD_PRED\"].cast(SparkT.FloatType()))\n",
        "\n",
        "    return mcyfs_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2QR2OmysFd1"
      },
      "source": [
        "#### Run Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ajXCUkjsIfK"
      },
      "outputs": [],
      "source": [
        "#%%writefile run_data_preprocessing.py\n",
        "def printPreprocessingInformation(df, data_source, order_cols, crop_season=None):\n",
        "  \"\"\"Print preprocessed data and additional debug information\"\"\"\n",
        "  df_regions = [reg[0] for reg in df.select('IDREGION').distinct().collect()]\n",
        "  print(data_source , 'data available for', len(df_regions), 'region(s)')\n",
        "  if (crop_season is not None):\n",
        "    print('Season end information')\n",
        "    crop_season.orderBy(['IDREGION', 'FYEAR']).show(10)\n",
        "\n",
        "  print(data_source, 'data')\n",
        "  df.orderBy(order_cols).show(10)\n",
        "\n",
        "def preprocessData(cyp_config, cyp_preprocessor, data_dfs):\n",
        "  crop_id = cyp_config.getCropID()\n",
        "  nuts_level = cyp_config.getNUTSLevel()\n",
        "  season_crosses_calyear = cyp_config.seasonCrossesCalendarYear()\n",
        "  clean_data = cyp_config.cleanData()\n",
        "  use_centroids = cyp_config.useCentroids()\n",
        "  use_remote_sensing = cyp_config.useRemoteSensing()\n",
        "  use_gaes = cyp_config.useGAES()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "\n",
        "  order_cols = ['IDREGION', 'CAMPAIGN_YEAR', 'CAMPAIGN_DEKAD']\n",
        "  # wofost data\n",
        "  wofost_df = data_dfs['WOFOST']\n",
        "  wofost_df = wofost_df.filter(wofost_df['CROP_ID'] == crop_id).drop('CROP_ID')\n",
        "  crop_season = cyp_preprocessor.getCropSeasonInformation(wofost_df, season_crosses_calyear)\n",
        "  wofost_df = cyp_preprocessor.preprocessWofost(wofost_df, crop_season, season_crosses_calyear)\n",
        "  wofost_regions = [reg[0] for reg in wofost_df.select('IDREGION').distinct().collect()]\n",
        "  data_dfs['WOFOST'] = wofost_df\n",
        "  if (debug_level > 1):\n",
        "    printPreprocessingInformation(wofost_df, 'WOFOST', order_cols, crop_season)\n",
        "\n",
        "  # meteo data\n",
        "  meteo_df = data_dfs['METEO']\n",
        "  meteo_df = cyp_preprocessor.preprocessMeteo(meteo_df, crop_season, season_crosses_calyear)\n",
        "  assert (meteo_df is not None)\n",
        "  data_dfs['METEO'] = meteo_df\n",
        "  if (debug_level > 1):\n",
        "    printPreprocessingInformation(meteo_df, 'METEO', order_cols)\n",
        "\n",
        "  # remote sensing data\n",
        "  rs_df = None\n",
        "  if (use_remote_sensing):\n",
        "    rs_df = data_dfs['REMOTE_SENSING']\n",
        "    rs_df = rs_df.drop('IDCOVER')\n",
        "\n",
        "    # if other data is at NUTS3, convert rs_df to NUTS3 using parent region data\n",
        "    # if (nuts_level == 'NUTS3'):\n",
        "    #   rs_df = cyp_preprocessor.remoteSensingNUTS2ToNUTS3(rs_df, wofost_regions)\n",
        "\n",
        "    rs_df = cyp_preprocessor.preprocessRemoteSensing(rs_df, crop_season, season_crosses_calyear)\n",
        "    assert (rs_df is not None)\n",
        "    data_dfs['REMOTE_SENSING'] = rs_df\n",
        "    if (debug_level > 1):\n",
        "      printPreprocessingInformation(rs_df, 'REMOTE_SENSING', order_cols)\n",
        "\n",
        "  order_cols = ['IDREGION']\n",
        "  # centroids and distance to coast\n",
        "  centroids_df = None\n",
        "  if (use_centroids):\n",
        "    centroids_df = data_dfs['CENTROIDS']\n",
        "    centroids_df = cyp_preprocessor.preprocessCentroids(centroids_df)\n",
        "    data_dfs['CENTROIDS'] = centroids_df\n",
        "    if (debug_level > 1):\n",
        "      printPreprocessingInformation(centroids_df, 'CENTROIDS', order_cols)\n",
        "\n",
        "  # soil data\n",
        "  soil_df = data_dfs['SOIL']\n",
        "  soil_df = cyp_preprocessor.preprocessSoil(soil_df)\n",
        "  data_dfs['SOIL'] = soil_df\n",
        "  if (debug_level > 1):\n",
        "    printPreprocessingInformation(soil_df, 'SOIL', order_cols)\n",
        "\n",
        "  # agro-environmental zones\n",
        "  if (use_gaes):\n",
        "    aez_df = data_dfs['GAES']\n",
        "    aez_df = cyp_preprocessor.preprocessGAES(aez_df, crop_id)\n",
        "    data_dfs['GAES'] = aez_df\n",
        "    if (debug_level > 1):\n",
        "      printPreprocessingInformation(aez_df, 'GAES', order_cols)\n",
        "\n",
        "    # crop area data\n",
        "    order_cols = ['IDREGION', 'FYEAR']\n",
        "    crop_area_df = data_dfs['CROP_AREA']\n",
        "    crop_area_df = cyp_preprocessor.preprocessCropArea(crop_area_df, crop_id)\n",
        "    data_dfs['CROP_AREA'] = crop_area_df\n",
        "    if (debug_level > 1):\n",
        "      printPreprocessingInformation(crop_area_df, 'CROP_AREA', order_cols)\n",
        "\n",
        "  order_cols = ['IDREGION', 'FYEAR']\n",
        "  # yield_data\n",
        "  yield_df = data_dfs['YIELD']\n",
        "  if (debug_level > 1):\n",
        "    print('Yield before preprocessing')\n",
        "    yield_df.show(10)\n",
        "\n",
        "  yield_df = cyp_preprocessor.preprocessYield(yield_df, crop_id, clean_data)\n",
        "  assert (yield_df is not None)\n",
        "  data_dfs['YIELD'] = yield_df\n",
        "  if (debug_level > 1):\n",
        "    print('Yield after preprocessing')\n",
        "    yield_df.show(10)\n",
        "\n",
        "  return data_dfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krHPVXls8MRd"
      },
      "source": [
        "### Training and Test Split\n",
        "\n",
        "**Custom validation splits**\n",
        "\n",
        "When yield trend is used, custom sliding validation split is used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YPH051S8MRf"
      },
      "source": [
        "#### Training and Test Splitter Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zBSEGkO8MRf"
      },
      "outputs": [],
      "source": [
        "#%%writefile train_test_sets.py\n",
        "import numpy as np\n",
        "\n",
        "class CYPTrainTestSplitter:\n",
        "  def __init__(self, cyp_config):\n",
        "    self.use_yield_trend = cyp_config.useYieldTrend()\n",
        "    self.test_fraction = cyp_config.getTestFraction()\n",
        "    self.verbose = cyp_config.getDebugLevel()\n",
        "\n",
        "  def getTestYears(self, all_years, test_fraction=None, use_yield_trend=None):\n",
        "    num_years = len(all_years)\n",
        "    test_years = []\n",
        "    if (test_fraction is None):\n",
        "      test_fraction = self.test_fraction\n",
        "\n",
        "    if (use_yield_trend is None):\n",
        "      use_yield_trend = self.use_yield_trend\n",
        "\n",
        "    if (use_yield_trend):\n",
        "      # If test_year_start 15, years with index >= 15 are added to the test set\n",
        "      test_year_start = num_years - np.floor(num_years * test_fraction).astype('int')\n",
        "      test_years = all_years[test_year_start:]\n",
        "    else:\n",
        "      # If test_year_pos = 5, every 5th year is added to test set.\n",
        "      # indices start with 0, so test_year_pos'th year has index (test_year_pos - 1)\n",
        "      test_year_pos = np.floor(1/test_fraction).astype('int')\n",
        "      test_years = all_years[test_year_pos - 1::test_year_pos]\n",
        "\n",
        "    return test_years\n",
        "\n",
        "  def trainTestSplit(self, yield_df, test_fraction=None, use_yield_trend=None):\n",
        "    all_years = sorted([yr[0] for yr in yield_df.select('FYEAR').distinct().collect()])\n",
        "    test_years = self.getTestYears(all_years, test_fraction, use_yield_trend)\n",
        "\n",
        "    return test_years\n",
        "\n",
        "  # Returns an array containings tuples (train_idxs, test_idxs) for each fold\n",
        "  # NOTE Y_train should include IDREGION, FYEAR as first two columns.\n",
        "  def customKFoldValidationSplit(self, Y_train_full, num_folds, log_fh=None):\n",
        "    \"\"\"\n",
        "    Custom K-fold Validation Splits:\n",
        "    When using yield trend, we cannot do k-fold cross-validation. The custom\n",
        "    K-Fold validation splits data in time-ordered fashion. The test data\n",
        "    always comes after the training data.\n",
        "    \"\"\"\n",
        "    all_years = sorted(np.unique(Y_train_full[:, 1]))\n",
        "    num_years = len(all_years)\n",
        "    num_test_years = 1\n",
        "    num_train_years = num_years - (num_test_years * num_folds)\n",
        "\n",
        "    custom_cv = []\n",
        "    custom_split_info = '\\nCustom sliding validation train, test splits'\n",
        "    custom_split_info += '\\n----------------------------------------------'\n",
        "\n",
        "    cv_test_years = []\n",
        "    for k in range(num_folds):\n",
        "      test_years_start = num_train_years + (k * num_test_years)\n",
        "      k_train_years = all_years[:test_years_start]\n",
        "      k_test_years = all_years[test_years_start:test_years_start + num_test_years]\n",
        "      cv_test_years += k_test_years\n",
        "      test_indexes = np.ravel(np.nonzero(np.isin(Y_train_full[:, 1], k_test_years)))\n",
        "      train_indexes = np.ravel(np.nonzero(np.isin(Y_train_full[:, 1], k_train_years)))\n",
        "      custom_cv.append(tuple((train_indexes, test_indexes)))\n",
        "\n",
        "      k_train_years = [str(y) for y in k_train_years]\n",
        "      k_test_years = [str(y) for y in k_test_years]\n",
        "      custom_split_info += '\\nValidation set ' + str(k + 1) + ' training years: ' + ', '.join(k_train_years)\n",
        "      custom_split_info += '\\nValidation set ' + str(k + 1) + ' test years: ' + ', '.join(k_test_years)\n",
        "\n",
        "    custom_split_info += '\\n'\n",
        "    if (log_fh is not None):\n",
        "      log_fh.write(custom_split_info)\n",
        "\n",
        "    if (self.verbose > 1):\n",
        "      print(custom_split_info)\n",
        "\n",
        "    return custom_cv, cv_test_years"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URFR5pOxO3XZ"
      },
      "source": [
        "#### Run Training and Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TO9qzRVO3Xb"
      },
      "outputs": [],
      "source": [
        "#%%writefile run_train_test_split.py\n",
        "def printTrainTestSplits(train_df, test_df, src, order_cols):\n",
        "  \"\"\"Print Training and Test Splits\"\"\"\n",
        "  print('\\n', src, 'training data')\n",
        "  train_df.orderBy(order_cols).show(5)\n",
        "  print('\\n', src, 'test data')\n",
        "  test_df.orderBy(order_cols).show(5)\n",
        "\n",
        "# Training, Test Split\n",
        "# --------------------\n",
        "def splitTrainingTest(df, year_col, test_years):\n",
        "  \"\"\"Splitting given df into training and test dataframes.\"\"\"\n",
        "  train_df = unionCountryDataSpark(df, year_col, test_years, True)\n",
        "  test_df = unionCountryDataSpark(df, year_col, test_years, False)\n",
        "\n",
        "  return [train_df, test_df]\n",
        "\n",
        "def splitDataIntoTrainingTestSets(cyp_config, preprocessed_dfs, log_fh):\n",
        "  \"\"\"\n",
        "  Split preprocessed data into training and test sets based on\n",
        "  availability of yield data.\n",
        "  \"\"\"\n",
        "  country_code = cyp_config.getCountryCode()\n",
        "  nuts_level = cyp_config.getNUTSLevel()\n",
        "  use_centroids = cyp_config.useCentroids()\n",
        "  use_remote_sensing = cyp_config.useRemoteSensing()\n",
        "  use_gaes = cyp_config.useGAES()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "\n",
        "  yield_df = preprocessed_dfs['YIELD']\n",
        "  train_test_splitter = CYPTrainTestSplitter(cyp_config)\n",
        "  countries = [country_code]\n",
        "  yield_df = yield_df.withColumn('COUNTRY', SparkF.substring(yield_df['IDREGION'], 0, 2))\n",
        "  if (country_code is None):\n",
        "    countries = sorted([cn[0] for cn in yield_df.select('COUNTRY').distinct().collect()])\n",
        "\n",
        "  test_years_info = '\\nTest years:'\n",
        "  test_years = {}\n",
        "  for cn in countries:\n",
        "    cn_test_years = train_test_splitter.trainTestSplit(yield_df.filter(yield_df['COUNTRY'] == cn))\n",
        "    test_years_info += '\\n' + cn + ': ' + ', '.join([str(y) for y in sorted(cn_test_years)])\n",
        "    test_years[cn] = cn_test_years\n",
        "\n",
        "  # log_fh.write(test_years_info + '\\n')\n",
        "  print(test_years_info)\n",
        "\n",
        "  # Times series data used for feature design.\n",
        "  ts_data_sources = {\n",
        "      'WOFOST' : preprocessed_dfs['WOFOST'],\n",
        "      'METEO' : preprocessed_dfs['METEO'],\n",
        "  }\n",
        "\n",
        "  if (use_remote_sensing):\n",
        "    ts_data_sources['REMOTE_SENSING'] = preprocessed_dfs['REMOTE_SENSING']\n",
        "\n",
        "  train_test_dfs = {}\n",
        "  for ts_src in ts_data_sources:\n",
        "    train_test_dfs[ts_src] = splitTrainingTest(ts_data_sources[ts_src], 'CAMPAIGN_YEAR', test_years)\n",
        "\n",
        "  # SOIL, GAES and CENTROIDS data are static.\n",
        "  train_test_dfs['SOIL'] = [preprocessed_dfs['SOIL'], preprocessed_dfs['SOIL']]\n",
        "  if (use_gaes):\n",
        "    train_test_dfs['GAES'] = [preprocessed_dfs['GAES'], preprocessed_dfs['GAES']]\n",
        "\n",
        "  if (use_centroids):\n",
        "    train_test_dfs['CENTROIDS'] = [preprocessed_dfs['CENTROIDS'],\n",
        "                                   preprocessed_dfs['CENTROIDS']]\n",
        "\n",
        "  # crop area\n",
        "  if (use_gaes):\n",
        "    crop_area_df = preprocessed_dfs['CROP_AREA']\n",
        "    train_test_dfs['CROP_AREA'] = splitTrainingTest(crop_area_df, 'FYEAR', test_years)\n",
        "\n",
        "  # yield data\n",
        "  yield_df = yield_df.drop('COUNTRY')\n",
        "  train_test_dfs['YIELD'] = splitTrainingTest(yield_df, 'FYEAR', test_years)\n",
        "\n",
        "  if (debug_level > 2):\n",
        "    for src in train_test_dfs:\n",
        "      if (src in ts_data_sources):\n",
        "        order_cols = ['IDREGION', 'CAMPAIGN_YEAR', 'CAMPAIGN_DEKAD']\n",
        "      elif ((src == 'YIELD') or (src == 'CROP_AREA')):\n",
        "        order_cols = ['IDREGION', 'FYEAR']\n",
        "      else:\n",
        "        order_cols = ['IDREGION']\n",
        "\n",
        "      train_df = train_test_dfs[src][0]\n",
        "      test_df = train_test_dfs[src][1]\n",
        "      printTrainTestSplits(train_df, test_df, src, order_cols)\n",
        "\n",
        "  return train_test_dfs, test_years"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCe_4_v--Ukb"
      },
      "source": [
        "### Data Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaSHuv0du9qf"
      },
      "source": [
        "#### Data Summarizer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TD_QGQJs-Ukl"
      },
      "outputs": [],
      "source": [
        "#%%writefile data_summary.py\n",
        "from pyspark.sql import Window\n",
        "import functools\n",
        "\n",
        "class CYPDataSummarizer:\n",
        "  def __init__(self, cyp_config):\n",
        "    self.verbose = cyp_config.getDebugLevel()\n",
        "\n",
        "  def wofostDVSSummary(self, wofost_df, early_season_end=None):\n",
        "    \"\"\"\n",
        "    Summary of crop calendar based on DVS.\n",
        "    Early season end is relative to end of the season, hence a negative number.\n",
        "    \"\"\"\n",
        "    join_cols = ['IDREGION', 'CAMPAIGN_YEAR']\n",
        "    dvs_summary = wofost_df.select(join_cols).distinct()\n",
        "\n",
        "    wofost_df = wofost_df.withColumn('SEASON_ALIGN', wofost_df['CAMPAIGN_DEKAD'] - wofost_df['DEKAD'])\n",
        "    dvs_summary = dvs_summary.join(wofost_df.filter(wofost_df['DVS'] > 0.0).groupBy(join_cols)\\\n",
        "                                   .agg(SparkF.min('CAMPAIGN_DEKAD').alias('START_DVS')), join_cols)\n",
        "    dvs_summary = dvs_summary.join(wofost_df.filter(wofost_df['DVS'] >= 100).groupBy(join_cols)\\\n",
        "                                   .agg(SparkF.min('CAMPAIGN_DEKAD').alias('START_DVS1')), join_cols)\n",
        "    dvs_summary = dvs_summary.join(wofost_df.filter(wofost_df['DVS'] >= 200).groupBy(join_cols)\\\n",
        "                                   .agg(SparkF.min('CAMPAIGN_DEKAD').alias('START_DVS2')), join_cols)\n",
        "    dvs_summary = dvs_summary.join(wofost_df.filter(wofost_df['DVS'] >= 200).groupBy(join_cols)\\\n",
        "                                   .agg(SparkF.min('DEKAD').alias('HARVEST')), join_cols)\n",
        "    dvs_summary = dvs_summary.join(wofost_df.filter(wofost_df['DVS'] >= 200).groupBy(join_cols)\\\n",
        "                                   .agg(SparkF.max('SEASON_ALIGN').alias('SEASON_ALIGN')), join_cols)\n",
        "\n",
        "    # Calendar year end season and early season dekads for comparing with MCYFS\n",
        "    # Campaign year early season dekad to filter data during feature design\n",
        "    dvs_summary = dvs_summary.withColumn('CALENDAR_END_SEASON', dvs_summary['HARVEST'] + 1)\n",
        "    dvs_summary = dvs_summary.withColumn('CAMPAIGN_EARLY_SEASON',\n",
        "                                         dvs_summary['CALENDAR_END_SEASON'] + dvs_summary['SEASON_ALIGN'])\n",
        "    if (early_season_end is not None):\n",
        "      dvs_summary = dvs_summary.withColumn('CALENDAR_EARLY_SEASON',\n",
        "                                         dvs_summary['CALENDAR_END_SEASON'] + early_season_end)\n",
        "      dvs_summary = dvs_summary.withColumn('CAMPAIGN_EARLY_SEASON',\n",
        "                                           dvs_summary['CAMPAIGN_EARLY_SEASON'] + early_season_end)\n",
        "\n",
        "    dvs_summary = dvs_summary.drop('HARVEST', 'SEASON_ALIGN')\n",
        "\n",
        "    return dvs_summary\n",
        "\n",
        "  def indicatorsSummary(self, df, min_cols, max_cols, avg_cols):\n",
        "    \"\"\"long term min, max and avg values of selected indicators by region\"\"\"\n",
        "    avgs = []\n",
        "    if (avg_cols[1:]):\n",
        "      avgs = [SparkF.bround(SparkF.avg(x), 2).alias('avg(' + x + ')') for x in avg_cols[1:]]\n",
        "    \n",
        "    if (min_cols[:1]):\n",
        "      summary = df.select(min_cols).groupBy('IDREGION').min()\n",
        "    else:\n",
        "      summary = df.select(min_cols).groupBy('IDREGION')\n",
        "\n",
        "    if (max_cols[1:]):\n",
        "      summary = summary.join(df.select(max_cols).groupBy('IDREGION').max(), 'IDREGION')\n",
        "\n",
        "    if (avgs):\n",
        "      summary = summary.join(df.select(avg_cols).groupBy('IDREGION').agg(*avgs), 'IDREGION')\n",
        "    return summary\n",
        "\n",
        "  def yieldSummary(self, yield_df):\n",
        "    \"\"\"long term min, max and avg values of yield by region\"\"\"\n",
        "    select_cols = ['IDREGION', 'YIELD']\n",
        "    yield_summary = yield_df.select(select_cols).groupBy('IDREGION').min('YIELD')\n",
        "    yield_summary = yield_summary.join(yield_df.select(select_cols).groupBy('IDREGION')\\\n",
        "                                       .agg(SparkF.max('YIELD')), 'IDREGION')\n",
        "    yield_summary = yield_summary.join(yield_df.select(select_cols).groupBy('IDREGION')\\\n",
        "                                       .agg(SparkF.bround(SparkF.avg('YIELD'), 2)\\\n",
        "                                            .alias('avg(YIELD)')), 'IDREGION')\n",
        "    return yield_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zym9pU3XsPrt"
      },
      "source": [
        "#### Run Data Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CNwR1WAsSLz"
      },
      "outputs": [],
      "source": [
        "#%%writefile run_data_summary.py\n",
        "def printDataSummary(df, data_source):\n",
        "  \"\"\"Print summary information\"\"\"\n",
        "  if (data_source == 'WOFOST_DVS'):\n",
        "    print('Crop calender information based on WOFOST data')\n",
        "    max_year = df.select('CAMPAIGN_YEAR').agg(SparkF.max('CAMPAIGN_YEAR')).collect()[0][0]\n",
        "    df.filter(df.CAMPAIGN_YEAR == max_year).orderBy('IDREGION').show(10)\n",
        "  else:\n",
        "    print(data_source, 'indicators summary')\n",
        "    df.orderBy('IDREGION').show()\n",
        "\n",
        "def getWOFOSTSummaryCols():\n",
        "  \"\"\"WOFOST columns used for data summary\"\"\"\n",
        "  # only RSM has non-zero min values\n",
        "  min_cols = ['IDREGION', 'RSM']\n",
        "  max_cols = ['IDREGION'] + ['WLIM_YB', 'WLIM_YS', 'DVS',\n",
        "                             'WLAI', 'RSM', 'TWC', 'TWR']\n",
        "  # biomass and DVS values grow over time\n",
        "  avg_cols = ['IDREGION', 'WLAI', 'RSM', 'TWC', 'TWR']\n",
        "\n",
        "  return [min_cols, max_cols, avg_cols]\n",
        "\n",
        "def getMeteoSummaryCols():\n",
        "  \"\"\"Meteo columns used for data summary\"\"\"\n",
        "  col_names = ['TMAX', 'TMIN', 'TAVG', 'PREC', 'ET0', 'CWB', 'RAD']\n",
        "  min_cols = ['IDREGION'] + col_names\n",
        "  max_cols = ['IDREGION'] + col_names\n",
        "  avg_cols = ['IDREGION'] + col_names\n",
        "\n",
        "  return [min_cols, max_cols, avg_cols]\n",
        "\n",
        "def getRemoteSensingSummaryCols():\n",
        "  \"\"\"Remote Sensing columns used for data summary\"\"\"\n",
        "  col_names = ['FAPAR']\n",
        "  min_cols = ['IDREGION'] + col_names\n",
        "  max_cols = ['IDREGION'] + col_names\n",
        "  avg_cols = ['IDREGION'] + col_names\n",
        "\n",
        "  return [min_cols, max_cols, avg_cols]\n",
        "\n",
        "def summarizeData(cyp_config, cyp_summarizer, train_test_dfs):\n",
        "  \"\"\"\n",
        "  Summarize data. Create DVS summary to infer crop calendar.\n",
        "  Summarize selected indicators for each data source.\n",
        "  \"\"\"\n",
        "  wofost_train_df = train_test_dfs['WOFOST'][0]\n",
        "  wofost_test_df = train_test_dfs['WOFOST'][1]\n",
        "  meteo_train_df = train_test_dfs['METEO'][0]\n",
        "  yield_train_df = train_test_dfs['YIELD'][0]\n",
        "\n",
        "  use_remote_sensing = cyp_config.useRemoteSensing()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  early_season = cyp_config.earlySeasonPrediction()\n",
        "  early_season_end = None\n",
        "  if (early_season):\n",
        "    early_season_end = cyp_config.getEarlySeasonEndDekad()\n",
        "\n",
        "  # DVS summary (crop calendar)\n",
        "  # NOTE this summary of crops based on wofost data should be used with caution\n",
        "  # 1. The summary is per region per year.\n",
        "  # 2. The summary is based on wofost simulations not real sowing and harvest dates\n",
        "  dvs_summary_train = cyp_summarizer.wofostDVSSummary(wofost_train_df, early_season_end)\n",
        "  dvs_summary_train = dvs_summary_train.drop('CALENDAR_END_SEASON', 'CALENDAR_EARLY_SEASON')\n",
        "  dvs_summary_test = cyp_summarizer.wofostDVSSummary(wofost_test_df, early_season_end)\n",
        "  dvs_summary_test = dvs_summary_test.drop('CALENDAR_END_SEASON', 'CALENDAR_EARLY_SEASON')\n",
        "  if (debug_level > 1):\n",
        "    printDataSummary(dvs_summary_train, 'WOFOST_DVS')\n",
        "\n",
        "  summary_cols = {\n",
        "      'WOFOST' : getWOFOSTSummaryCols(),\n",
        "      'METEO' : getMeteoSummaryCols(),\n",
        "  }\n",
        "\n",
        "  summary_sources_dfs = {\n",
        "      'WOFOST' : wofost_train_df,\n",
        "      'METEO' : meteo_train_df,\n",
        "  }\n",
        "\n",
        "  if (use_remote_sensing):\n",
        "    rs_train_df = train_test_dfs['REMOTE_SENSING'][0]\n",
        "    summary_cols['REMOTE_SENSING'] = getRemoteSensingSummaryCols()\n",
        "    summary_sources_dfs['REMOTE_SENSING'] = rs_train_df\n",
        "\n",
        "  summary_dfs = {}\n",
        "  for sum_src in summary_sources_dfs:\n",
        "    summary_dfs[sum_src] = cyp_summarizer.indicatorsSummary(summary_sources_dfs[sum_src],\n",
        "                                                            summary_cols[sum_src][0],\n",
        "                                                            summary_cols[sum_src][1],\n",
        "                                                            summary_cols[sum_src][2])\n",
        "\n",
        "  for src in summary_dfs:\n",
        "    if (debug_level > 2):\n",
        "      printDataSummary(summary_dfs[src], src)\n",
        "\n",
        "  yield_summary = cyp_summarizer.yieldSummary(yield_train_df)\n",
        "  if (debug_level > 2):\n",
        "    printDataSummary(yield_summary, 'YIELD')\n",
        "\n",
        "  summary_dfs['WOFOST_DVS'] = [dvs_summary_train, dvs_summary_test]\n",
        "  summary_dfs['YIELD'] = yield_summary\n",
        "\n",
        "  return summary_dfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWwazE4MJI4v"
      },
      "source": [
        "### Crop Calendar\n",
        "\n",
        "We infer crop calendar using WOFOST DVS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBVBgwxSJEIn"
      },
      "outputs": [],
      "source": [
        "#%%writefile crop_calendar.py\n",
        "import numpy as np\n",
        "\n",
        "def getCropCalendarPeriods(df):\n",
        "  \"\"\"Periods for per year crop calendar\"\"\"\n",
        "  # (maximum of 4 months = 12 dekads).\n",
        "  # Subtracting 11 because both ends of the period are included.\n",
        "  # p0 : if CAMPAIGN_EARLY_SEASON > df.START_DVS\n",
        "  #        START_DVS - 11 to START_DVS\n",
        "  #      else\n",
        "  #        START_DVS - 11 to CAMPAIGN_EARLY_SEASON\n",
        "  p0_filter = SparkF.when(df.CAMPAIGN_EARLY_SEASON > df.START_DVS,\n",
        "                          (df.CAMPAIGN_DEKAD >= (df.START_DVS - 11)) &\n",
        "                          (df.CAMPAIGN_DEKAD <= df.START_DVS))\\\n",
        "                          .otherwise((df.CAMPAIGN_DEKAD >= (df.START_DVS - 11)) &\n",
        "                                     (df.CAMPAIGN_DEKAD <= df.CAMPAIGN_EARLY_SEASON))\n",
        "  # p1 : if CAMPAIGN_EARLY_SEASON > (df.START_DVS + 1)\n",
        "  #        (START_DVS - 1) to (START_DVS + 1)\n",
        "  #      else\n",
        "  #        (START_DVS - 1) to CAMPAIGN_EARLY_SEASON\n",
        "  p1_filter = SparkF.when(df.CAMPAIGN_EARLY_SEASON > (df.START_DVS + 1),\n",
        "                          (df.CAMPAIGN_DEKAD >= (df.START_DVS - 1)) &\n",
        "                          (df.CAMPAIGN_DEKAD <= (df.START_DVS + 1)))\\\n",
        "                          .otherwise((df.CAMPAIGN_DEKAD >= (df.START_DVS - 1)) &\n",
        "                                     (df.CAMPAIGN_DEKAD <= df.CAMPAIGN_EARLY_SEASON))\n",
        "  # p2 : if CAMPAIGN_EARLY_SEASON > df.START_DVS1\n",
        "  #        START_DVS to START_DVS1\n",
        "  #      else\n",
        "  #        START_DVS to CAMPAIGN_EARLY_SEASON\n",
        "  p2_filter = SparkF.when(df.CAMPAIGN_EARLY_SEASON > df.START_DVS1,\n",
        "                          (df.CAMPAIGN_DEKAD >= df.START_DVS) &\n",
        "                          (df.CAMPAIGN_DEKAD <= df.START_DVS1))\\\n",
        "                          .otherwise((df.CAMPAIGN_DEKAD >= df.START_DVS) &\n",
        "                                     (df.CAMPAIGN_DEKAD <= df.CAMPAIGN_EARLY_SEASON))\n",
        "  # p3 : if CAMPAIGN_EARLY_SEASON > (df.START_DVS1 + 1)\n",
        "  #        (START_DVS1 - 1) to (START_DVS1 + 1)\n",
        "  #      else\n",
        "  #        (START_DVS1 - 1) to CAMPAIGN_EARLY_SEASON\n",
        "  p3_filter = SparkF.when(df.CAMPAIGN_EARLY_SEASON > (df.START_DVS1 + 1),\n",
        "                          (df.CAMPAIGN_DEKAD >= (df.START_DVS1 - 1)) &\n",
        "                          (df.CAMPAIGN_DEKAD <= (df.START_DVS1 + 1)))\\\n",
        "                          .otherwise((df.CAMPAIGN_DEKAD >= (df.START_DVS1 - 1)) &\n",
        "                                     (df.CAMPAIGN_DEKAD <= df.CAMPAIGN_EARLY_SEASON))\n",
        "  # p4 : if CAMPAIGN_EARLY_SEASON > df.START_DVS2\n",
        "  #        START_DVS1 to START_DVS2\n",
        "  #      else\n",
        "  #        START_DVS1 to CAMPAIGN_EARLY_SEASON\n",
        "  p4_filter = SparkF.when(df.CAMPAIGN_EARLY_SEASON > df.START_DVS2,\n",
        "                          (df.CAMPAIGN_DEKAD >= df.START_DVS1) &\n",
        "                          (df.CAMPAIGN_DEKAD <= df.START_DVS2))\\\n",
        "                          .otherwise((df.CAMPAIGN_DEKAD >= df.START_DVS1) &\n",
        "                                     (df.CAMPAIGN_DEKAD <= df.CAMPAIGN_EARLY_SEASON))\n",
        "  # p5 : if CAMPAIGN_EARLY_SEASON > (df.START_DVS2 + 1)\n",
        "  #        (START_DVS2 - 1) to (START_DVS2 + 1)\n",
        "  #      else\n",
        "  #        (START_DVS2 - 1) to CAMPAIGN_EARLY_SEASON\n",
        "  p5_filter = SparkF.when(df.CAMPAIGN_EARLY_SEASON > (df.START_DVS2 + 1),\n",
        "                          (df.CAMPAIGN_DEKAD >= (df.START_DVS2 - 1)) &\n",
        "                          (df.CAMPAIGN_DEKAD <= (df.START_DVS2 + 1)))\\\n",
        "                          .otherwise((df.CAMPAIGN_DEKAD >= (df.START_DVS2 - 1)) &\n",
        "                                     (df.CAMPAIGN_DEKAD <= df.CAMPAIGN_EARLY_SEASON))\n",
        "\n",
        "  cc_periods = {\n",
        "      'p0' : p0_filter,\n",
        "      'p1' : p1_filter,\n",
        "      'p2' : p2_filter,\n",
        "      'p3' : p3_filter,\n",
        "      'p4' : p4_filter,\n",
        "      'p5' : p5_filter,\n",
        "  }\n",
        "\n",
        "  return cc_periods\n",
        "\n",
        "def getSeasonStartFilter(df):\n",
        "  \"\"\"Filter for dekads after season start\"\"\"\n",
        "  return df.CAMPAIGN_DEKAD >= (df.START_DVS - 11)\n",
        "\n",
        "def getCountryCropCalendar(crop_cal):\n",
        "  \"\"\"Take averages to make the crop calendar per country\"\"\"\n",
        "  crop_cal = crop_cal.withColumn('COUNTRY', SparkF.substring('IDREGION', 1, 2))\n",
        "  aggrs = [ SparkF.bround(SparkF.avg(crop_cal['START_DVS'])).alias('START_DVS'),\n",
        "            SparkF.bround(SparkF.avg(crop_cal['START_DVS1'])).alias('START_DVS1'),\n",
        "            SparkF.bround(SparkF.avg(crop_cal['START_DVS2'])).alias('START_DVS2'),\n",
        "            SparkF.bround(SparkF.avg(crop_cal['CAMPAIGN_EARLY_SEASON'])).alias('CAMPAIGN_EARLY_SEASON') ]\n",
        "\n",
        "  crop_cal = crop_cal.groupBy('COUNTRY').agg(*aggrs)\n",
        "  return crop_cal\n",
        "\n",
        "def getCropCalendar(cyp_config, dvs_summary, log_fh):\n",
        "  \"\"\"Use DVS summary to infer the crop calendar\"\"\"\n",
        "  pd_dvs_summary = dvs_summary.toPandas()\n",
        "  early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "\n",
        "  avg_dvs_start = np.round(pd_dvs_summary['START_DVS'].mean(), 0)\n",
        "  avg_dvs1_start = np.round(pd_dvs_summary['START_DVS1'].mean(), 0)\n",
        "  avg_dvs2_start = np.round(pd_dvs_summary['START_DVS2'].mean(), 0)\n",
        "\n",
        "  # We look at 6 windows\n",
        "  # 0. Preplanting window (maximum of 4 months = 12 dekads).\n",
        "  # Subtracting 11 because both ends of the period are included.\n",
        "  p0_start = 1 if (avg_dvs_start - 11) < 1 else (avg_dvs_start - 11)\n",
        "  p0_end = avg_dvs_start\n",
        "\n",
        "  # 1. Planting window\n",
        "  p1_start = avg_dvs_start - 1\n",
        "  p1_end = avg_dvs_start + 1\n",
        "\n",
        "  # 2. Vegetative phase\n",
        "  p2_start = avg_dvs_start\n",
        "  p2_end = avg_dvs1_start\n",
        "\n",
        "  # 3. Flowering phase\n",
        "  p3_start = avg_dvs1_start - 1\n",
        "  p3_end = avg_dvs1_start + 1\n",
        "\n",
        "  # 4. Yield formation phase\n",
        "  p4_start = avg_dvs1_start\n",
        "  p4_end = avg_dvs2_start\n",
        "\n",
        "  # 5. Harvest window\n",
        "  p5_start = avg_dvs2_start - 1\n",
        "  p5_end = avg_dvs2_start + 1\n",
        "\n",
        "  early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "  early_season_end = 36\n",
        "  if (early_season_prediction):\n",
        "    early_season_end = np.round(pd_dvs_summary['CAMPAIGN_EARLY_SEASON'].mean(), 0)\n",
        "    p0_end = early_season_end if (p0_end > early_season_end) else p0_end\n",
        "    p1_end = early_season_end if (p1_end > early_season_end) else p1_end\n",
        "    p2_end = early_season_end if (p2_end > early_season_end) else p2_end\n",
        "    p3_end = early_season_end if (p3_end > early_season_end) else p3_end\n",
        "    p4_end = early_season_end if (p4_end > early_season_end) else p4_end\n",
        "    p5_end = early_season_end if (p5_end > early_season_end) else p5_end\n",
        "\n",
        "  crop_cal = {}\n",
        "  if (p0_end > p0_start):\n",
        "    crop_cal['p0'] = { 'desc' : 'pre-planting window', 'start' : p0_start, 'end' : p0_end }\n",
        "  if (p1_end > p1_start):\n",
        "    crop_cal['p1'] = { 'desc' : 'planting window', 'start' : p1_start, 'end' : p1_end }\n",
        "  if (p2_end > p2_start):\n",
        "    crop_cal['p2'] = { 'desc' : 'vegetative phase', 'start' : p2_start, 'end' : p2_end }\n",
        "  if (p3_end > p3_start):\n",
        "    crop_cal['p3'] = { 'desc' : 'flowering phase', 'start' : p3_start, 'end' : p3_end }\n",
        "  if (p4_end > p4_start):\n",
        "    crop_cal['p4'] = { 'desc' : 'yield formation phase', 'start' : p4_start, 'end' : p4_end }\n",
        "  if (p5_end > p5_start):\n",
        "    crop_cal['p5'] = { 'desc' : 'harvest window', 'start' : p5_start, 'end' : p5_end }\n",
        "\n",
        "  if (early_season_prediction):\n",
        "    early_season_rel_harvest = cyp_config.getEarlySeasonEndDekad()\n",
        "    early_season_info = '\\nEarly Season Prediction Dekad: ' + str(early_season_rel_harvest)\n",
        "    early_season_info += ', Campaign Dekad: ' + str(early_season_end)\n",
        "    log_fh.write(early_season_info + '\\n')\n",
        "    if (debug_level > 1):\n",
        "      print(early_season_info)\n",
        "\n",
        "  crop_cal_info = '\\nCrop Calendar'\n",
        "  crop_cal_info += '\\n-------------'\n",
        "  for p in crop_cal:\n",
        "    crop_cal_info += '\\nPeriod ' + p + ' (' + crop_cal[p]['desc'] + '): '\n",
        "    crop_cal_info += 'Campaign Dekads ' + str(crop_cal[p]['start']) + '-' + str(crop_cal[p]['end'])\n",
        "\n",
        "  log_fh.write(crop_cal_info + '\\n')\n",
        "  if (debug_level > 1):\n",
        "    print(crop_cal_info)\n",
        "\n",
        "  return crop_cal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QokglosfnQax"
      },
      "source": [
        "### Feature Design\n",
        "\n",
        "For WOFOST, Meteo and Remote Sensing features, we aggregate indicators or count days/dekads with indicators above or below some thresholds. We use 4 thresholds: +/- 1 STD and +/- 2STD above or below the average.\n",
        "\n",
        "We determine the start and end dekads using crop calendar inferred from WOFOST DVS summary. In the case of early season prediction, end dekad is set to the prediction dekad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHVsi9dzv4Pz"
      },
      "source": [
        "#### Featurizer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e94zGLjVnQa0"
      },
      "outputs": [],
      "source": [
        "#%%writefile feature_design.py\n",
        "from pyspark.sql import Window\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import functools\n",
        "\n",
        "class CYPFeaturizer:\n",
        "  def __init__(self, cyp_config):\n",
        "    self.use_per_year_cc = cyp_config.usePerYearCropCalendar()\n",
        "    self.use_features_v2 = cyp_config.useFeaturesV2()\n",
        "    self.use_gaes = cyp_config.useGAES()\n",
        "    self.verbose = cyp_config.getDebugLevel()\n",
        "    self.lt_stats = {}\n",
        "\n",
        "  def extractFeatures(self, df, data_source, crop_cal,\n",
        "                      max_cols, avg_cols, cum_avg_cols, extreme_cols,\n",
        "                      join_cols, fit=False):\n",
        "    \"\"\"\n",
        "    Extract aggregate and extreme features.\n",
        "    If fit=True, compute and save long-term stats.\n",
        "    \"\"\"\n",
        "    df = df.withColumn('COUNTRY', SparkF.substring('IDREGION', 1, 2))\n",
        "    if (not self.use_per_year_cc):\n",
        "      if (self.use_gaes):\n",
        "        df = df.join(crop_cal.select(['IDREGION', 'AEZ_ID']), 'IDREGION')\n",
        "\n",
        "      crop_cal = getCountryCropCalendar(crop_cal)\n",
        "      df = df.join(SparkF.broadcast(crop_cal), 'COUNTRY')\n",
        "    else:\n",
        "      df = df.join(crop_cal, join_cols)\n",
        "\n",
        "    # Calculate cumulative sums\n",
        "    if (self.use_features_v2 and cum_avg_cols):\n",
        "      w = Window.partitionBy(join_cols).orderBy('CAMPAIGN_DEKAD')\\\n",
        "                .rangeBetween(Window.unboundedPreceding, 0)\n",
        "      after_season_start = getSeasonStartFilter(df)\n",
        "      for c in cum_avg_cols:\n",
        "        df = df.withColumn(c, SparkF.sum(SparkF.when(after_season_start, df[c])\\\n",
        "                                         .otherwise(0.0)).over(w))\n",
        "\n",
        "    cc_periods = getCropCalendarPeriods(df)\n",
        "    aggrs = []\n",
        "    # max aggregation\n",
        "    for p in max_cols:\n",
        "      if (max_cols[p]):\n",
        "        aggrs += [SparkF.bround(SparkF.max(SparkF.when(cc_periods[p], df[x])), 2)\\\n",
        "                  .alias('max' + x + p) for x in max_cols[p] ]\n",
        "\n",
        "    # avg aggregation\n",
        "    for p in avg_cols:\n",
        "      if (avg_cols[p]):\n",
        "        aggrs += [SparkF.bround(SparkF.avg(SparkF.when(cc_periods[p], df[x])), 2)\\\n",
        "                  .alias('avg' + x + p) for x in avg_cols[p] ]\n",
        "\n",
        "    # if not computing extreme features, we can return\n",
        "    if (not extreme_cols):\n",
        "      ft_df = df.groupBy(join_cols).agg(*aggrs)\n",
        "      return ft_df\n",
        "\n",
        "    # compute long-term stats and save them\n",
        "    if (fit):\n",
        "      stat_aggrs = []\n",
        "      for p in extreme_cols:\n",
        "        if (extreme_cols[p]):\n",
        "          stat_aggrs += [ SparkF.bround(SparkF.avg(SparkF.when(cc_periods[p], df[x])), 2)\\\n",
        "                         .alias('avg' + x + p) for x in extreme_cols[p] ]\n",
        "          stat_aggrs += [ SparkF.bround(SparkF.stddev(SparkF.when(cc_periods[p], df[x])), 2)\\\n",
        "                         .alias('std' + x + p) for x in extreme_cols[p] ]\n",
        "\n",
        "      if (stat_aggrs):\n",
        "        if (self.use_gaes):\n",
        "          lt_stats = df.groupBy('AEZ_ID').agg(*stat_aggrs)\n",
        "        else:\n",
        "          lt_stats = df.groupBy('COUNTRY').agg(*stat_aggrs)\n",
        "\n",
        "        self.lt_stats[data_source] = lt_stats\n",
        "\n",
        "    if (self.use_gaes):\n",
        "      df = df.join(SparkF.broadcast(self.lt_stats[data_source]), 'AEZ_ID')\n",
        "    else:\n",
        "      df = df.join(SparkF.broadcast(self.lt_stats[data_source]), 'COUNTRY')\n",
        "\n",
        "    # features for extreme conditions\n",
        "    for p in extreme_cols:\n",
        "      if (extreme_cols[p]):\n",
        "        if (self.use_features_v2):\n",
        "          # sum zscore for values < long-term average\n",
        "          aggrs += [ SparkF.bround(SparkF.sum(SparkF.when(((df[x] - df['avg' + x + p]) < 0) & cc_periods[p],\n",
        "                                                          (df['avg' + x + p] - df[x]) / df['std' + x + p])), 2)\\\n",
        "                     .alias('Z-' + x + p) for x in extreme_cols[p] ]\n",
        "          # sum zscore for values > long-term average\n",
        "          aggrs += [ SparkF.bround(SparkF.sum(SparkF.when(((df[x] - df['avg' + x + p]) > 0) & cc_periods[p],\n",
        "                                                          (df[x] - df['avg' + x + p]) / df['std' + x + p])), 2)\\\n",
        "                     .alias('Z+' + x + p) for x in extreme_cols[p] ]\n",
        "\n",
        "        else:\n",
        "          # Count of days or dekads with values crossing threshold\n",
        "          for i in range(1, 3):\n",
        "            aggrs += [ SparkF.sum(SparkF.when((df[x] > (df['avg' + x + p] + i * df['std' + x + p])) &\n",
        "                                              cc_periods[p], 1))\\\n",
        "                      .alias(x + p + 'gt' + str(i) + 'STD') for x in extreme_cols[p] ]\n",
        "            aggrs += [ SparkF.sum(SparkF.when((df[x] < (df['avg' + x + p] - i * df['std' + x + p])) &\n",
        "                                              cc_periods[p], 1))\\\n",
        "                      .alias(x + p + 'lt' + str(i) + 'STD') for x in extreme_cols[p] ]\n",
        "\n",
        "    ft_df = df.groupBy(join_cols).agg(*aggrs)\n",
        "    ft_df = ft_df.na.fill(0.0)\n",
        "    return ft_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CZS6KDg8raw"
      },
      "source": [
        "#### Yield Trend Estimator Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2O70-hQ8rax"
      },
      "outputs": [],
      "source": [
        "#%%writefile yield_trend.py\n",
        "import numpy as np\n",
        "import functools\n",
        "from pyspark.sql import Window\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "class CYPYieldTrendEstimator:\n",
        "  def __init__(self, cyp_config):\n",
        "    self.verbose = cyp_config.getDebugLevel()\n",
        "    self.trend_windows = cyp_config.getTrendWindows()\n",
        "\n",
        "  def setTrendWindows(self, trend_windows):\n",
        "    \"\"\"Set trend window lengths\"\"\"\n",
        "    assert isinstance(trend_windows, list)\n",
        "    assert len(trend_windows) > 0\n",
        "    assert isinstance(trend_windows[0], int)\n",
        "    self.trend_windows = trend_windows\n",
        "\n",
        "  def getTrendWindowYields(self, df, trend_window, reg_id=None):\n",
        "    \"\"\"Extract previous years' yield values to separate columns\"\"\"\n",
        "    sel_cols = ['IDREGION', 'FYEAR', 'YIELD']\n",
        "    my_window = Window.partitionBy('IDREGION').orderBy('FYEAR')\n",
        "\n",
        "    yield_fts = df.select(sel_cols)\n",
        "    if (reg_id is not None):\n",
        "      yield_fts = yield_fts.filter(yield_fts.IDREGION == reg_id)\n",
        "\n",
        "    for i in range(trend_window):\n",
        "      yield_fts = yield_fts.withColumn('YIELD-' + str(i+1),\n",
        "                                       SparkF.lag(yield_fts.YIELD, i+1).over(my_window))\n",
        "      yield_fts = yield_fts.withColumn('YEAR-' + str(i+1),\n",
        "                                       SparkF.lag(yield_fts.FYEAR, i+1).over(my_window))\n",
        "\n",
        "    # drop columns withs null values\n",
        "    for i in range(trend_window):\n",
        "      yield_fts = yield_fts.filter(SparkF.col('YIELD-' + str(i+1)).isNotNull())\n",
        "\n",
        "    prev_yields = [ 'YIELD-' + str(i) for i in range(trend_window, 0, -1)]\n",
        "    prev_years = [ 'YEAR-' + str(i) for i in range(trend_window, 0, -1)]\n",
        "    sel_cols = ['IDREGION', 'FYEAR'] + prev_years + prev_yields\n",
        "    yield_fts = yield_fts.select(sel_cols)\n",
        "\n",
        "    return yield_fts\n",
        "\n",
        "  # Christos, Ante's suggestion\n",
        "  # - To avoid overfitting, trend estimation could use a window which skips a year in between\n",
        "  # So a window of 3 will use 6 years of data\n",
        "  def printYieldTrendRounds(self, df, reg_id, trend_windows=None):\n",
        "    \"\"\"Print the sequence of years used for yield trend estimation\"\"\"\n",
        "    reg_years = sorted([yr[0] for yr in df.filter(df['IDREGION'] == reg_id).select('FYEAR').distinct().collect()])\n",
        "    num_years = len(reg_years)\n",
        "    if (trend_windows is None):\n",
        "      trend_windows = self.trend_windows\n",
        "\n",
        "    for trend_window in trend_windows:\n",
        "      rounds = (num_years - trend_window)\n",
        "      if ((self.verbose > 2) and (trend_window == trend_windows[0])):\n",
        "        print('Trend window', trend_window)\n",
        "    \n",
        "      for rd in range(rounds):\n",
        "        test_year = reg_years[-(rd + 1)]\n",
        "        start_year = reg_years[-(rd + trend_window + 1)]\n",
        "        end_year = reg_years[-(rd + 2)]\n",
        "\n",
        "        if ((self.verbose > 2) and (trend_window == trend_windows[0])):\n",
        "          print('Round:', rd, 'Test year:', test_year,\n",
        "                'Trend Window:', start_year, '-', end_year)\n",
        "\n",
        "  def getLinearYieldTrend(self, window_years, window_yields, pred_year):\n",
        "    \"\"\"Linear yield trend prediction\"\"\"\n",
        "    coefs = np.polyfit(window_years, window_yields, 1)\n",
        "    return float(np.round(coefs[0] * pred_year + coefs[1], 2))\n",
        "\n",
        "  def getFixedWindowTrendFeatures(self, df, trend_window=None, pred_year=None):\n",
        "    \"\"\"Predict the yield trend for each IDREGION and FYEAR using a fixed window\"\"\"\n",
        "    join_cols = ['IDREGION', 'FYEAR']\n",
        "    if (trend_window is None):\n",
        "      trend_window = self.trend_windows[0]\n",
        "\n",
        "    yield_ft_df = self.getTrendWindowYields(df, trend_window)\n",
        "    if (pred_year is not None):\n",
        "      yield_ft_df = yield_ft_df.filter(yield_ft_df['FYEAR'] == pred_year)\n",
        "\n",
        "    pd_yield_ft_df = yield_ft_df.toPandas()\n",
        "    region_years = pd_yield_ft_df[join_cols].values\n",
        "    prev_year_cols = ['YEAR-' + str(i) for i in range(1, trend_window + 1)]\n",
        "    prev_yield_cols = ['YIELD-' + str(i) for i in range(1, trend_window + 1)]\n",
        "    window_years = pd_yield_ft_df[prev_year_cols].values\n",
        "    window_yields = pd_yield_ft_df[prev_yield_cols].values\n",
        "\n",
        "    yield_trend = []\n",
        "    for i in range(region_years.shape[0]):\n",
        "      yield_trend.append(self.getLinearYieldTrend(window_years[i, :],\n",
        "                                                  window_yields[i, :],\n",
        "                                                  region_years[i, 1]))\n",
        "\n",
        "    pd_yield_ft_df['YIELD_TREND'] = yield_trend\n",
        "    return pd_yield_ft_df\n",
        "\n",
        "  def getFixedWindowTrend(self, df, reg_id, pred_year, trend_window=None):\n",
        "    \"\"\"\n",
        "    Return linear trend prediction for given region and year\n",
        "    using fixed trend window.\n",
        "    \"\"\"\n",
        "    if (trend_window is None):\n",
        "      trend_window = self.trend_windows[0]\n",
        "\n",
        "    reg_df = df.filter((df['IDREGION'] == reg_id) & (df['FYEAR'] <= pred_year))\n",
        "    pd_yield_ft_df = self.getFixedWindowTrendFeatures(reg_df, trend_window, pred_year)\n",
        "    if (len(pd_yield_ft_df.index) == 0):\n",
        "      print('No data to estimate yield trend')\n",
        "      return None\n",
        "\n",
        "    if (len(pd_yield_ft_df.index) == 0):\n",
        "      return None\n",
        "\n",
        "    reg_year_filter = (df['IDREGION'] == reg_id) & (df['FYEAR'] == pred_year)\n",
        "    pd_yield_ft_df['ACTUAL'] = df.filter(reg_year_filter).select('YIELD').collect()[0][0]\n",
        "    pd_yield_ft_df = pd_yield_ft_df.rename(columns={'YIELD_TREND' : 'PREDICTED'})\n",
        "\n",
        "    return pd_yield_ft_df\n",
        "\n",
        "  def getL1OutCVPredictions(self, pd_yield_ft_df, trend_window, join_cols, iter):\n",
        "    \"\"\"1 iteration of leave-one-out cross-validation\"\"\"\n",
        "    iter_year_cols = ['YEAR-' + str(i) for i in range(1, trend_window + 1) if i != iter]\n",
        "    iter_yield_cols = ['YIELD-' + str(i) for i in range(1, trend_window + 1) if i != iter]\n",
        "    window_years = pd_yield_ft_df[iter_year_cols].values\n",
        "    window_yields = pd_yield_ft_df[iter_yield_cols].values\n",
        "\n",
        "    # We are going to predict yield value for YEAR-<iter>.\n",
        "    pred_years = pd_yield_ft_df['YEAR-' + str(iter)].values\n",
        "    predicted_trend = []\n",
        "    for i in range(pred_years.shape[0]):\n",
        "      predicted_trend.append(self.getLinearYieldTrend(window_years[i, :],\n",
        "                                                      window_yields[i, :],\n",
        "                                                      pred_years[i]))\n",
        "\n",
        "    pd_iter_preds = pd_yield_ft_df[join_cols]\n",
        "    pd_iter_preds['YTRUE' + str(iter)] = pd_yield_ft_df['YIELD-' + str(iter)]\n",
        "    pd_iter_preds['YPRED' + str(iter)] = predicted_trend\n",
        "\n",
        "    if (self.verbose > 2):\n",
        "      print('Leave-one-out cross-validation: iteration', iter)\n",
        "      print(pd_iter_preds.head(5))\n",
        "\n",
        "    return pd_iter_preds\n",
        "\n",
        "  def getL1outRMSE(self, cv_actual, cv_predicted):\n",
        "    \"\"\"Compute RMSE for leave-one-out predictions\"\"\"\n",
        "    return float(np.round(np.sqrt(mean_squared_error(cv_actual, cv_predicted)), 2))\n",
        "\n",
        "  def getMinRMSEIndex(self, cv_rmses):\n",
        "    \"\"\"Index of min rmse values\"\"\"\n",
        "    return np.nanargmin(cv_rmses)\n",
        "\n",
        "  def getL1OutCVRMSE(self, df, trend_window, join_cols, pred_year=None):\n",
        "    \"\"\"Run leave-one-out cross-validation and compute RMSE\"\"\"\n",
        "    join_cols = ['IDREGION', 'FYEAR']\n",
        "    pd_yield_ft_df = self.getFixedWindowTrendFeatures(df, trend_window, pred_year)\n",
        "    pd_l1out_preds = None\n",
        "    for i in range(1, trend_window + 1):\n",
        "      pd_iter_preds = self.getL1OutCVPredictions(pd_yield_ft_df, trend_window,\n",
        "                                                 join_cols, i)\n",
        "      if (pd_l1out_preds is None):\n",
        "        pd_l1out_preds = pd_iter_preds\n",
        "      else:\n",
        "        pd_l1out_preds = pd_l1out_preds.merge(pd_iter_preds, on=join_cols)\n",
        "\n",
        "    region_years = pd_l1out_preds[join_cols].values\n",
        "    ytrue_cols = ['YTRUE' + str(i) for i in range(1, trend_window + 1)]\n",
        "    ypred_cols = ['YPRED' + str(i) for i in range(1, trend_window + 1)]\n",
        "    l1out_ytrue = pd_l1out_preds[ytrue_cols].values\n",
        "    l1out_ypred = pd_l1out_preds[ypred_cols].values\n",
        "    cv_rmse = []\n",
        "    for i in range(region_years.shape[0]):\n",
        "      cv_rmse.append(self.getL1outRMSE(l1out_ytrue[i, :],\n",
        "                                       l1out_ypred[i, :]))\n",
        "\n",
        "    pd_l1out_rmse = pd_yield_ft_df[join_cols]\n",
        "    pd_l1out_rmse['YIELD_TREND' + str(trend_window)] = pd_yield_ft_df['YIELD_TREND']\n",
        "    pd_l1out_rmse['CV_RMSE' + str(trend_window)] = cv_rmse\n",
        "\n",
        "    return pd_l1out_rmse\n",
        "\n",
        "  def getOptimalTrendWindows(self, df, pred_year=None, trend_windows=None):\n",
        "    \"\"\"\n",
        "    Compute optimal yield trend values based on leave-one-out\n",
        "    cross validation errors for different trend windows.\n",
        "    \"\"\"\n",
        "    join_cols = ['IDREGION', 'FYEAR']\n",
        "    if (trend_windows is None):\n",
        "      trend_windows = self.trend_windows\n",
        "\n",
        "    pd_tw_rmses = None\n",
        "    for tw in trend_windows:\n",
        "      pd_l1out_rmse = self.getL1OutCVRMSE(df, tw, join_cols, pred_year)\n",
        "      if (pd_tw_rmses is None):\n",
        "        pd_tw_rmses = pd_l1out_rmse\n",
        "      else:\n",
        "        pd_tw_rmses = pd_tw_rmses.merge(pd_l1out_rmse, on=join_cols, how='left')\n",
        "\n",
        "    if (self.verbose > 2):\n",
        "      print('Leave-one-out cross-validation: RMSE')\n",
        "      print(pd_tw_rmses.sort_values(by=join_cols).head(5))\n",
        "\n",
        "    region_years = pd_tw_rmses[join_cols].values\n",
        "    tw_rmse_cols = ['CV_RMSE' + str(tw) for tw in trend_windows]\n",
        "    tw_trend_cols = ['YIELD_TREND' + str(tw) for tw in trend_windows]\n",
        "    tw_cv_rmses = pd_tw_rmses[tw_rmse_cols].values\n",
        "    tw_yield_trend = pd_tw_rmses[tw_trend_cols].values\n",
        "\n",
        "    opt_windows = []\n",
        "    yield_trend_preds = []\n",
        "    for i in range(region_years.shape[0]):\n",
        "      min_rmse_index = self.getMinRMSEIndex(tw_cv_rmses[i, :])\n",
        "      opt_windows.append(trend_windows[min_rmse_index])\n",
        "      yield_trend_preds.append(tw_yield_trend[i, min_rmse_index])\n",
        "\n",
        "    pd_opt_win_df = pd_tw_rmses[join_cols]\n",
        "    pd_opt_win_df['OPT_TW'] = opt_windows\n",
        "    pd_opt_win_df['YIELD_TREND'] = yield_trend_preds\n",
        "    if (self.verbose > 2):\n",
        "      print('Optimal trend windows')\n",
        "      print(pd_opt_win_df.sort_values(by=join_cols).head(5))\n",
        "\n",
        "    return pd_opt_win_df\n",
        "\n",
        "  def getOptimalWindowTrendFeatures(self, df, trend_windows=None):\n",
        "    \"\"\"\n",
        "    Get previous year yield values and predicted yield trend\n",
        "    by determining optimal trend window for each region and year.\n",
        "    NOTE: We have to select the same number of features, so we\n",
        "    select previous trend_windows[0] yield values.\n",
        "    \"\"\"\n",
        "    join_cols = ['IDREGION', 'FYEAR']\n",
        "    if (trend_windows is None):\n",
        "      trend_windows = self.trend_windows\n",
        "\n",
        "    pd_yield_ft_df = self.getTrendWindowYields(df, trend_windows[0]).toPandas()\n",
        "    pd_opt_win_df = self.getOptimalTrendWindows(df, trend_windows=trend_windows)\n",
        "    pd_opt_win_df = pd_opt_win_df.drop(columns=['OPT_TW'])\n",
        "    pd_yield_ft_df = pd_yield_ft_df.merge(pd_opt_win_df, on=join_cols)\n",
        "\n",
        "    return pd_yield_ft_df\n",
        "\n",
        "  def getOptimalWindowTrend(self, df, reg_id, pred_year, trend_windows=None):\n",
        "    \"\"\"\n",
        "    Compute the optimal trend window for given region and year based on\n",
        "    leave-one-out cross validation errors for different trend windows.\n",
        "    \"\"\"\n",
        "    df = df.filter(df['IDREGION'] == reg_id)\n",
        "    pd_opt_win_df = self.getOptimalTrendWindows(df, pred_year, trend_windows)\n",
        "    if (len(pd_opt_win_df.index) == 0):\n",
        "      return None\n",
        "\n",
        "    reg_year_filter = (df['IDREGION'] == reg_id) & (df['FYEAR'] == pred_year)\n",
        "    pd_opt_win_df['ACTUAL'] = df.filter(reg_year_filter).select('YIELD').collect()[0][0]\n",
        "    pd_opt_win_df = pd_opt_win_df.rename(columns={'YIELD_TREND' : 'PREDICTED'})\n",
        "\n",
        "    return pd_opt_win_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGNO_QZCsYf2"
      },
      "source": [
        "#### Create WOFOST, Meteo and Remote Sensing Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NpzcktZse_8"
      },
      "outputs": [],
      "source": [
        "#%%writefile run_feature_design.py\n",
        "def getTrendFeatureCols(ft_src):\n",
        "  \"\"\"Features from previous years to capture feature trend\"\"\"\n",
        "  return []\n",
        "\n",
        "def getCumulativeAvgCols(ft_src):\n",
        "  \"\"\"columns or indicators using avg of cumulative values\"\"\"\n",
        "  cum_cols = []\n",
        "  if (ft_src == 'METEO'):\n",
        "    cum_cols = ['CWB']\n",
        "\n",
        "  return cum_cols\n",
        "\n",
        "def wofostMaxFeatureCols():\n",
        "  \"\"\"columns or indicators using max aggregation\"\"\"\n",
        "  # must be in sync with crop calendar periods\n",
        "  max_cols = {\n",
        "      'p0' : [],\n",
        "      'p1' : [],\n",
        "      'p2' : ['WLIM_YB', 'WLAI'],\n",
        "      'p3' : [],\n",
        "      'p4' : ['WLIM_YB', 'WLIM_YS', 'WLAI'],\n",
        "      'p5' : [],\n",
        "  }\n",
        "\n",
        "  return max_cols\n",
        "\n",
        "def wofostAvgFeatureCols():\n",
        "  \"\"\"columns or indicators using avg aggregation\"\"\"\n",
        "  # must be in sync with crop calendar periods\n",
        "  avg_cols = {\n",
        "      'p0' : [],\n",
        "      'p1' : [],\n",
        "      'p2' : ['RSM'],\n",
        "      'p3' : [],\n",
        "      'p4' : ['RSM'],\n",
        "      'p5' : [],\n",
        "  }\n",
        "\n",
        "  return avg_cols\n",
        "\n",
        "def wofostCountFeatureCols():\n",
        "  \"\"\"columns or indicators using count aggregation\"\"\"\n",
        "  # must be in sync with crop calendar periods\n",
        "  count_cols = {\n",
        "      'p0' : [],\n",
        "      'p1' : [],\n",
        "      'p2' : ['RSM'],\n",
        "      'p3' : ['RSM'],\n",
        "      'p4' : ['RSM'],\n",
        "      'p5' : [],\n",
        "  }\n",
        "\n",
        "  return count_cols\n",
        "\n",
        "# Meteo Feature ideas:\n",
        "# Two dry summers caused drop in ground water level:\n",
        "#   rainfall sums going back to second of half of previous year\n",
        "# Previous year: high production, prices low, invest less in crop\n",
        "#   Focus on another crop\n",
        "def meteoMaxFeatureCols():\n",
        "  \"\"\"columns or indicators using max aggregation\"\"\"\n",
        "  # must be in sync with crop calendar periods\n",
        "  max_cols = { 'p0' : [], 'p1' : [], 'p2' : [], 'p3' : [], 'p4' : [], 'p5' : [] }\n",
        "\n",
        "  return max_cols\n",
        "\n",
        "def meteoAvgFeatureCols(features_v2=False):\n",
        "  \"\"\"columns or indicators using avg aggregation\"\"\"\n",
        "  # must be in sync with crop calendar periods\n",
        "  avg_cols = {\n",
        "      'p0' : ['CWB'],\n",
        "      'p1' : ['TAVG', 'CWB'],\n",
        "      'p2' : ['TAVG', 'CWB'],\n",
        "      'p3' : [],\n",
        "      'p4' : [],\n",
        "      'p5' : ['PREC'],\n",
        "  }\n",
        "\n",
        "  return avg_cols\n",
        "\n",
        "def meteoCountFeatureCols():\n",
        "  \"\"\"columns or indicators using count aggregation\"\"\"\n",
        "  # must be in sync with crop calendar periods\n",
        "  count_cols = {\n",
        "      'p0' : [],\n",
        "      'p1' : ['TMIN', 'PREC'],\n",
        "      'p2' : [],\n",
        "      'p3' : ['TMAX', 'PREC'],\n",
        "      'p4' : [],\n",
        "      'p5' : ['PREC'],\n",
        "  }\n",
        "\n",
        "  return count_cols\n",
        "\n",
        "def rsMaxFeatureCols():\n",
        "  \"\"\"columns or indicators using max aggregation\"\"\"\n",
        "  # must be in sync with crop calendar periods\n",
        "  max_cols = { 'p0' : [], 'p1' : [], 'p2' : [], 'p3' : [], 'p4' : [], 'p5' : [] }\n",
        "\n",
        "  return max_cols\n",
        "\n",
        "def rsAvgFeatureCols():\n",
        "  \"\"\"columns or indicators using avg aggregation\"\"\"\n",
        "  # must be in sync with crop calendar periods\n",
        "  avg_cols = {\n",
        "      'p0' : [],\n",
        "      'p1' : [],\n",
        "      'p2' : ['FAPAR'],\n",
        "      'p3' : [],\n",
        "      'p4' : ['FAPAR'],\n",
        "      'p5' : [],\n",
        "  }\n",
        "\n",
        "  return avg_cols\n",
        "\n",
        "def convertFeaturesToPandas(ft_dfs, join_cols):\n",
        "  \"\"\"Convert features to pandas and merge\"\"\"\n",
        "  train_ft_df = ft_dfs[0]\n",
        "  test_ft_df = ft_dfs[1]\n",
        "  train_ft_df = train_ft_df.withColumnRenamed('CAMPAIGN_YEAR', 'FYEAR')\n",
        "  test_ft_df = test_ft_df.withColumnRenamed('CAMPAIGN_YEAR', 'FYEAR')\n",
        "  pd_train_df = train_ft_df.toPandas()\n",
        "  pd_test_df = test_ft_df.toPandas()\n",
        "\n",
        "  return [pd_train_df, pd_test_df]\n",
        "\n",
        "def dropZeroColumns(pd_ft_dfs):\n",
        "  \"\"\"Drop columns which have all zeros in training data\"\"\"\n",
        "  pd_train_df = pd_ft_dfs[0]\n",
        "  pd_test_df = pd_ft_dfs[1]\n",
        "\n",
        "  pd_train_df = pd_train_df.loc[:, (pd_train_df != 0.0).any(axis=0)]\n",
        "  pd_train_df = pd_train_df.dropna(axis=1)\n",
        "  pd_test_df = pd_test_df[pd_train_df.columns]\n",
        "\n",
        "  return [pd_train_df, pd_test_df]\n",
        "\n",
        "def printFeatureData(pd_feature_dfs, join_cols):\n",
        "  for src in pd_feature_dfs:\n",
        "    pd_train_fts = pd_feature_dfs[src][0]\n",
        "    if (pd_train_fts is None):\n",
        "      continue\n",
        "\n",
        "    pd_test_fts = pd_feature_dfs[src][1]\n",
        "    all_cols = list(pd_train_fts.columns)\n",
        "    aggr_cols = [ c for c in all_cols if (('avg' in c) or ('max' in c))]\n",
        "    if (len(aggr_cols) > 0):\n",
        "      print('\\n', src, 'Aggregate Features: Training')\n",
        "      print(pd_train_fts[join_cols + aggr_cols].head(5))\n",
        "      print('\\n', src, 'Aggregate Features: Test')\n",
        "      print(pd_test_fts[join_cols + aggr_cols].head(5))\n",
        "\n",
        "    ext_cols = [ c for c in all_cols if (('Z+' in c) or ('Z-' in c) or\n",
        "                                         ('lt' in c) or ('gt' in c))]\n",
        "    if (len(ext_cols) > 0):\n",
        "      print('\\n', src, 'Features for Extreme Conditions: Training')\n",
        "      print(pd_train_fts[join_cols + ext_cols].head(5))\n",
        "      print('\\n', src, 'Features for Extreme Conditions: Test')\n",
        "      print(pd_test_fts[join_cols + ext_cols].head(5))\n",
        "\n",
        "def createFeatures(cyp_config, cyp_featurizer, train_test_dfs,\n",
        "                   summary_dfs, log_fh):\n",
        "  \"\"\"Create WOFOST, Meteo and Remote Sensing features\"\"\"\n",
        "  nuts_level = cyp_config.getNUTSLevel()\n",
        "  use_remote_sensing = cyp_config.useRemoteSensing()\n",
        "  use_gaes = cyp_config.useGAES()\n",
        "  use_per_year_cc = cyp_config.usePerYearCropCalendar()\n",
        "  use_features_v2 = cyp_config.useFeaturesV2()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "\n",
        "  wofost_train_df = train_test_dfs['WOFOST'][0]\n",
        "  wofost_test_df = train_test_dfs['WOFOST'][1]\n",
        "  meteo_train_df = train_test_dfs['METEO'][0]\n",
        "  meteo_test_df = train_test_dfs['METEO'][1]\n",
        "  yield_train_df = train_test_dfs['YIELD'][0]\n",
        "\n",
        "  dvs_train = summary_dfs['WOFOST_DVS'][0]\n",
        "  dvs_test = summary_dfs['WOFOST_DVS'][1]\n",
        "\n",
        "  if (debug_level > 2):\n",
        "    print('WOFOST training data size',\n",
        "          wofost_train_df.select(['IDREGION', 'CAMPAIGN_YEAR']).distinct().count())\n",
        "    print('WOFOST test data size',\n",
        "          wofost_test_df.select(['IDREGION', 'CAMPAIGN_YEAR']).distinct().count())\n",
        "    print('Meteo training data size',\n",
        "          meteo_train_df.select(['IDREGION', 'CAMPAIGN_YEAR']).distinct().count())\n",
        "    print('Meteo test data size',\n",
        "          meteo_test_df.select(['IDREGION', 'CAMPAIGN_YEAR']).distinct().count())\n",
        "    print('DVS Summary of training data',\n",
        "          dvs_summary_train.select(['IDREGION', 'CAMPAIGN_YEAR']).distinct().count())\n",
        "    print('DVS Summary of test data',\n",
        "          dvs_summary_test.select(['IDREGION', 'CAMPAIGN_YEAR']).distinct().count())\n",
        "\n",
        "  rs_train_df = None\n",
        "  rs_test_df = None\n",
        "  if (use_remote_sensing):\n",
        "    rs_train_df = train_test_dfs['REMOTE_SENSING'][0]\n",
        "    rs_test_df = train_test_dfs['REMOTE_SENSING'][1]\n",
        "    if (debug_level > 2):\n",
        "      print('Remote sensing training data size',\n",
        "            rs_train_df.select(['IDREGION', 'CAMPAIGN_YEAR']).distinct().count())\n",
        "      print('Remote sensing test data size',\n",
        "            rs_test_df.select(['IDREGION', 'CAMPAIGN_YEAR']).distinct().count())\n",
        "\n",
        "  join_cols = ['IDREGION', 'CAMPAIGN_YEAR']\n",
        "  aggr_ft_cols = {\n",
        "      'WOFOST' : [wofostMaxFeatureCols(), wofostAvgFeatureCols()],\n",
        "      'METEO' : [meteoMaxFeatureCols(), meteoAvgFeatureCols(use_features_v2)],\n",
        "  }\n",
        "\n",
        "  count_ft_cols = {\n",
        "      'WOFOST' : wofostCountFeatureCols(),\n",
        "      'METEO' : meteoCountFeatureCols(),\n",
        "  }\n",
        "\n",
        "  train_ft_src_dfs = {\n",
        "      'WOFOST' : wofost_train_df,\n",
        "      'METEO' : meteo_train_df,\n",
        "  }\n",
        "\n",
        "  test_ft_src_dfs = {\n",
        "      'WOFOST' : wofost_test_df,\n",
        "      'METEO' : meteo_test_df,\n",
        "  }\n",
        "\n",
        "  if (use_remote_sensing):\n",
        "    train_ft_src_dfs['REMOTE_SENSING'] = rs_train_df\n",
        "    test_ft_src_dfs['REMOTE_SENSING'] = rs_test_df\n",
        "    aggr_ft_cols['REMOTE_SENSING'] = [rsMaxFeatureCols(), rsAvgFeatureCols()]\n",
        "    count_ft_cols['REMOTE_SENSING'] = {}\n",
        "\n",
        "  if (use_gaes):\n",
        "    aez_df = train_test_dfs['GAES'][0]\n",
        "    dvs_train = dvs_train.join(aez_df.select(['IDREGION', 'AEZ_ID']), 'IDREGION')\n",
        "    dvs_test = dvs_test.join(aez_df.select(['IDREGION', 'AEZ_ID']), 'IDREGION')\n",
        "\n",
        "  crop_cal_train = dvs_train\n",
        "  crop_cal_test = dvs_test\n",
        "  if (not use_per_year_cc):\n",
        "    crop_cal_test = dvs_train\n",
        "\n",
        "  train_ft_dfs = {}\n",
        "  test_ft_dfs = {}\n",
        "  for ft_src in train_ft_src_dfs:\n",
        "    cum_avg_cols = []\n",
        "    if (use_features_v2):\n",
        "      cum_avg_cols = getCumulativeAvgCols(ft_src)\n",
        "\n",
        "    train_ft_dfs[ft_src] = cyp_featurizer.extractFeatures(train_ft_src_dfs[ft_src],\n",
        "                                                          ft_src,\n",
        "                                                          crop_cal_train,\n",
        "                                                          aggr_ft_cols[ft_src][0],\n",
        "                                                          aggr_ft_cols[ft_src][1],\n",
        "                                                          cum_avg_cols,\n",
        "                                                          count_ft_cols[ft_src],\n",
        "                                                          join_cols,\n",
        "                                                          True)\n",
        "    test_ft_dfs[ft_src] = cyp_featurizer.extractFeatures(test_ft_src_dfs[ft_src],\n",
        "                                                         ft_src,\n",
        "                                                         crop_cal_test,\n",
        "                                                         aggr_ft_cols[ft_src][0],\n",
        "                                                         aggr_ft_cols[ft_src][1],\n",
        "                                                         cum_avg_cols,\n",
        "                                                         count_ft_cols[ft_src],\n",
        "                                                         join_cols)\n",
        "\n",
        "  pd_conversion_dict = {\n",
        "      'WOFOST' : [ train_ft_dfs['WOFOST'], test_ft_dfs['WOFOST'] ],\n",
        "      'METEO' : [ train_ft_dfs['METEO'], test_ft_dfs['METEO'] ],\n",
        "  }\n",
        "\n",
        "  if (use_remote_sensing):\n",
        "      pd_conversion_dict['REMOTE_SENSING'] = [ train_ft_dfs['REMOTE_SENSING'], test_ft_dfs['REMOTE_SENSING'] ]\n",
        "\n",
        "  pd_feature_dfs = {}\n",
        "  for ft_src in pd_conversion_dict:\n",
        "    pd_feature_dfs[ft_src] = convertFeaturesToPandas(pd_conversion_dict[ft_src], join_cols)\n",
        "\n",
        "  # Check and drop features with all zeros (possible in early season prediction).\n",
        "  for ft_src in pd_feature_dfs:\n",
        "    pd_feature_dfs[ft_src] = dropZeroColumns(pd_feature_dfs[ft_src])\n",
        "\n",
        "  if (debug_level > 1):\n",
        "    join_cols = ['IDREGION', 'FYEAR']\n",
        "    printFeatureData(pd_feature_dfs, join_cols)\n",
        "\n",
        "  return pd_feature_dfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERZ4JYeyss7m"
      },
      "source": [
        "#### Create Trend Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nbqouti4swJ0"
      },
      "outputs": [],
      "source": [
        "#%%writefile run_trend_feature_design.py\n",
        "def createYieldTrendFeatures(cyp_config, cyp_trend_est,\n",
        "                             yield_train_df, yield_test_df, test_years):\n",
        "  \"\"\"Create yield trend features\"\"\"\n",
        "  join_cols = ['IDREGION', 'FYEAR']\n",
        "  find_optimal = cyp_config.findOptimalTrendWindow()\n",
        "  trend_window = cyp_config.getTrendWindows()[0]\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  yield_df = yield_train_df.union(yield_test_df.select(yield_train_df.columns))\n",
        "\n",
        "  if (find_optimal):\n",
        "    pd_train_features = cyp_trend_est.getOptimalWindowTrendFeatures(yield_train_df)\n",
        "    pd_test_features = cyp_trend_est.getOptimalWindowTrendFeatures(yield_df)\n",
        "  else:\n",
        "    pd_train_features = cyp_trend_est.getFixedWindowTrendFeatures(yield_train_df)\n",
        "    pd_test_features = cyp_trend_est.getFixedWindowTrendFeatures(yield_df)\n",
        "\n",
        "  pd_test_features = unionCountryDataPandas(pd_test_features, 'FYEAR',\n",
        "                                            test_years, False)\n",
        "  prev_year_cols = ['YEAR-' + str(i) for i in range(1, trend_window + 1)]\n",
        "  pd_train_features = pd_train_features.drop(columns=prev_year_cols)\n",
        "  pd_test_features = pd_test_features.drop(columns=prev_year_cols)\n",
        "\n",
        "  if (debug_level > 1):\n",
        "    print('\\nYield Trend Features: Train')\n",
        "    join_cols = ['IDREGION', 'FYEAR']\n",
        "    print(pd_train_features.sort_values(by=join_cols).head(5))\n",
        "    print('Total', len(pd_train_features.index), 'rows')\n",
        "    print('\\nYield Trend Features: Test')\n",
        "    print(pd_test_features.sort_values(by=join_cols).head(5))\n",
        "    print('Total', len(pd_test_features.index), 'rows')\n",
        "\n",
        "  return pd_train_features, pd_test_features\n",
        "\n",
        "def addFeaturesFromPreviousYears(cyp_config, pd_feature_dfs,\n",
        "                                 trend_window, test_years, join_cols):\n",
        "  \"\"\"Add features from previous years as trend features\"\"\"\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "\n",
        "  for ft_src in pd_feature_dfs:\n",
        "    trend_cols = getTrendFeatureCols(ft_src)\n",
        "    if (not trend_cols):\n",
        "      continue\n",
        "\n",
        "    pd_ft_train_df = pd_feature_dfs[ft_src][0]\n",
        "    pd_ft_test_df = pd_feature_dfs[ft_src][1]\n",
        "    pd_all_df = pd_ft_train_df.append(pd_ft_test_df).sort_values(by=join_cols)\n",
        "    all_cols = pd_ft_train_df.columns\n",
        "    common_cols = [c for c in trend_cols if c in all_cols]\n",
        "    if (not common_cols):\n",
        "      continue\n",
        "\n",
        "    for tc in trend_cols:\n",
        "      if (tc not in all_cols):\n",
        "        continue\n",
        "\n",
        "      for yr in range(1, trend_window + 1):\n",
        "        pd_all_df[tc + '-' + str(yr)] = pd_all_df.groupby(['IDREGION'])[tc].shift(yr)\n",
        "\n",
        "    pd_ft_train_df = unionCountryDataPandas(pd_all_df, 'FYEAR', test_years, True)\n",
        "    pd_ft_test_df = unionCountryDataPandas(pd_all_df, 'FYEAR', test_years, False)\n",
        "    pd_ft_train_df = pd_ft_train_df.dropna(axis=0)\n",
        "    pd_ft_test_df = pd_ft_test_df.dropna(axis=0)\n",
        "\n",
        "    sel_cols = ['IDREGION', 'FYEAR']\n",
        "    all_cols = pd_ft_train_df.columns\n",
        "    for tc in trend_cols:\n",
        "      tc_cols = [c for c in all_cols if tc in c]\n",
        "      sel_cols += tc_cols\n",
        "\n",
        "    if ((debug_level > 1) and (len(sel_cols) > 2)):\n",
        "      print('\\n' + ft_src + ' Trend Features: Train')\n",
        "      print(pd_ft_train_df[sel_cols].sort_values(by=join_cols).head(5).to_string(index=False))\n",
        "      print('\\n' + ft_src + ' Trend Features: Test')\n",
        "      print(pd_ft_test_df[sel_cols].sort_values(by=join_cols).head(5).to_string(index=False))\n",
        "\n",
        "    pd_feature_dfs[ft_src] = [pd_ft_train_df, pd_ft_test_df]\n",
        "\n",
        "  return pd_feature_dfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIDmOfx9KKuF"
      },
      "source": [
        "### Combine features and labels\n",
        "\n",
        "Combine wofost, meteo and soil with remote sensing. Combine with centroids or yield trend both if configured. Combine with yield data in the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsoMBrpIKKuH"
      },
      "outputs": [],
      "source": [
        "#%%writefile combine_features.py\n",
        "def combineFeaturesLabels(cyp_config, sqlCtx,\n",
        "                          prep_train_test_dfs, pd_feature_dfs,\n",
        "                          join_cols, log_fh):\n",
        "  \"\"\"\n",
        "  Combine wofost, meteo and soil with remote sensing. Combine centroids\n",
        "  and yield trend if configured. Combine with yield data in the end.\n",
        "  If configured, save features to a CSV file.\n",
        "  \"\"\"\n",
        "  pd_soil_df = prep_train_test_dfs['SOIL'][0].toPandas()\n",
        "  pd_yield_train_df = prep_train_test_dfs['YIELD'][0].toPandas()\n",
        "  pd_yield_test_df = prep_train_test_dfs['YIELD'][1].toPandas()\n",
        "\n",
        "  # Feature dataframes have already been converted to pandas\n",
        "  pd_wofost_train_ft = pd_feature_dfs['WOFOST'][0]\n",
        "  pd_wofost_test_ft = pd_feature_dfs['WOFOST'][1]\n",
        "  pd_meteo_train_ft = pd_feature_dfs['METEO'][0]\n",
        "  pd_meteo_test_ft = pd_feature_dfs['METEO'][1]\n",
        "\n",
        "  crop = cyp_config.getCropName()\n",
        "  country = cyp_config.getCountryCode()\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  use_centroids = cyp_config.useCentroids()\n",
        "  use_remote_sensing = cyp_config.useRemoteSensing()\n",
        "  use_gaes = cyp_config.useGAES()\n",
        "  save_features = cyp_config.saveFeatures()\n",
        "  use_sample_weights = cyp_config.useSampleWeights()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  \n",
        "  combine_info = '\\nCombine Features and Labels'\n",
        "  combine_info += '\\n---------------------------'\n",
        "  yield_min_year = pd_yield_train_df['FYEAR'].min()\n",
        "  combine_info += '\\nYield min year ' + str(yield_min_year) + '\\n'\n",
        "\n",
        "  # start with static SOIL data\n",
        "  pd_train_df = pd_soil_df.copy(deep=True)\n",
        "  pd_test_df = pd_soil_df.copy(deep=True)\n",
        "  combine_info += '\\nData size after including SOIL data: '\n",
        "  combine_info += '\\nTrain ' + str(len(pd_train_df.index)) + ' rows.'\n",
        "  combine_info += '\\nTest ' + str(len(pd_test_df.index)) + ' rows.\\n'\n",
        "\n",
        "  if (use_gaes):\n",
        "    pd_aez_df = prep_train_test_dfs['GAES'][0].toPandas()\n",
        "    pd_aez_df['COUNTRY'] = pd_aez_df['IDREGION'].str[:2]\n",
        "    if (len(pd_aez_df['COUNTRY'].unique()) > 1):\n",
        "      pd_sel_aez_df = pd_aez_df[['COUNTRY', 'AEZ_ID']].astype({ 'AEZ_ID' : 'str' })\n",
        "      pd_onehot_df = pd.get_dummies(pd_sel_aez_df, prefix=['CN', 'AEZ'])\n",
        "    else:\n",
        "      pd_sel_aez_df = pd_aez_df[['AEZ_ID']].astype({ 'AEZ_ID' : 'str' })\n",
        "      pd_onehot_df = pd.get_dummies(pd_sel_aez_df, prefix=['AEZ'])\n",
        "\n",
        "    pd_aez_df = pd.concat([pd_aez_df, pd_onehot_df], axis=1).drop(columns=['COUNTRY', 'AEZ_ID'])\n",
        "    pd_train_df = pd_train_df.merge(pd_aez_df, on=['IDREGION'])\n",
        "    pd_test_df = pd_test_df.merge(pd_aez_df, on=['IDREGION'])\n",
        "    combine_info += '\\nData size after including GAES data: '\n",
        "    combine_info += '\\nTrain ' + str(len(pd_train_df.index)) + ' rows.'\n",
        "    combine_info += '\\nTest ' + str(len(pd_test_df.index)) + ' rows.\\n'\n",
        "\n",
        "  if (use_centroids):\n",
        "    # combine with region centroids\n",
        "    pd_centroids_df = prep_train_test_dfs['CENTROIDS'][0].toPandas()\n",
        "    pd_train_df = pd_train_df.merge(pd_centroids_df, on=['IDREGION'])\n",
        "    pd_test_df = pd_test_df.merge(pd_centroids_df, on='IDREGION')\n",
        "    combine_info += '\\nData size after including CENTROIDS data: '\n",
        "    combine_info += '\\nTrain ' + str(len(pd_train_df.index)) + ' rows.'\n",
        "    combine_info += '\\nTest ' + str(len(pd_test_df.index)) + ' rows.\\n'\n",
        "\n",
        "  # combine with WOFOST features\n",
        "  static_cols = list(pd_train_df.columns)\n",
        "  pd_train_df = pd_train_df.merge(pd_wofost_train_ft, on=['IDREGION'])\n",
        "  pd_test_df = pd_test_df.merge(pd_wofost_test_ft, on=['IDREGION'])\n",
        "  wofost_cols = list(pd_wofost_train_ft.columns)\n",
        "  col_order = ['IDREGION', 'FYEAR'] + static_cols[1:] + wofost_cols[2:]\n",
        "  pd_train_df = pd_train_df[col_order]\n",
        "  pd_test_df = pd_test_df[col_order]\n",
        "  combine_info += '\\nData size after including WOFOST features: '\n",
        "  combine_info += '\\nTrain ' + str(len(pd_train_df.index)) + ' rows.'\n",
        "  combine_info += '\\nTest ' + str(len(pd_test_df.index)) + ' rows.\\n'\n",
        "\n",
        "  # combine with METEO features\n",
        "  pd_train_df = pd_train_df.merge(pd_meteo_train_ft, on=join_cols)\n",
        "  pd_test_df = pd_test_df.merge(pd_meteo_test_ft, on=join_cols)\n",
        "  combine_info += '\\nData size after including METEO features: '\n",
        "  combine_info += '\\nTrain ' + str(len(pd_train_df.index)) + ' rows.'\n",
        "  combine_info += '\\nTest ' + str(len(pd_test_df.index)) + ' rows.\\n'\n",
        "\n",
        "  # combine with remote sensing features\n",
        "  if (use_remote_sensing):\n",
        "    pd_rs_train_ft = pd_feature_dfs['REMOTE_SENSING'][0]\n",
        "    pd_rs_test_ft = pd_feature_dfs['REMOTE_SENSING'][1]\n",
        "\n",
        "    pd_train_df = pd_train_df.merge(pd_rs_train_ft, on=join_cols)\n",
        "    pd_test_df = pd_test_df.merge(pd_rs_test_ft, on=join_cols)\n",
        "    combine_info += '\\nData size after including REMOTE_SENSING features: '\n",
        "    combine_info += '\\nTrain ' + str(len(pd_train_df.index)) + ' rows.'\n",
        "    combine_info += '\\nTest ' + str(len(pd_test_df.index)) + ' rows.\\n'\n",
        "\n",
        "  if (use_gaes):\n",
        "    # combine with crop area\n",
        "    pd_area_train_df = prep_train_test_dfs['CROP_AREA'][0].toPandas()\n",
        "    pd_area_test_df = prep_train_test_dfs['CROP_AREA'][1].toPandas()\n",
        "    pd_train_df = pd_train_df.merge(pd_area_train_df, on=join_cols)\n",
        "    pd_test_df = pd_test_df.merge(pd_area_test_df, on=join_cols)\n",
        "\n",
        "    # We combine with crop area to keep the dataset the same as\n",
        "    # with NUTS2-NUTS3 disaggregation.\n",
        "    # But we don't use CROP_AREA as a feature.\n",
        "    pd_train_df = pd_train_df.drop(columns=['CROP_AREA'])\n",
        "    pd_test_df = pd_test_df.drop(columns=['CROP_AREA'])\n",
        "    combine_info += '\\nData size after combining with CROP_AREA: '\n",
        "    combine_info += '\\nTrain ' + str(len(pd_train_df.index)) + ' rows.'\n",
        "    combine_info += '\\nTest ' + str(len(pd_test_df.index)) + ' rows.\\n'\n",
        "\n",
        "  if (use_yield_trend):\n",
        "    # combine with yield trend features\n",
        "    pd_yield_trend_train_ft = pd_feature_dfs['YIELD_TREND'][0]\n",
        "    pd_yield_trend_test_ft = pd_feature_dfs['YIELD_TREND'][1]\n",
        "    pd_train_df = pd_train_df.merge(pd_yield_trend_train_ft, on=join_cols)\n",
        "    pd_test_df = pd_test_df.merge(pd_yield_trend_test_ft, on=join_cols)\n",
        "    combine_info += '\\nData size after including yield trend features: '\n",
        "    combine_info += '\\nTrain ' + str(len(pd_train_df.index)) + ' rows.'\n",
        "    combine_info += '\\nTest ' + str(len(pd_test_df.index)) + ' rows.\\n'\n",
        "\n",
        "  # combine with yield data\n",
        "  pd_train_df = pd_train_df.merge(pd_yield_train_df, on=join_cols)\n",
        "  pd_test_df = pd_test_df.merge(pd_yield_test_df, on=join_cols)\n",
        "  pd_train_df = pd_train_df.sort_values(by=join_cols)\n",
        "  pd_test_df = pd_test_df.sort_values(by=join_cols)\n",
        "  combine_info += '\\nData size after including yield (label) data: '\n",
        "  combine_info += '\\nTrain ' + str(len(pd_train_df.index)) + ' rows.'\n",
        "  combine_info += '\\nTest ' + str(len(pd_test_df.index)) + ' rows.\\n'\n",
        "\n",
        "  # sample weights\n",
        "  if (use_sample_weights):\n",
        "    assert use_gaes\n",
        "    pd_train_df['SAMPLE_WEIGHT'] = pd_train_df['CROP_AREA']\n",
        "    pd_test_df['SAMPLE_WEIGHT'] = pd_test_df['CROP_AREA']\n",
        "\n",
        "  log_fh.write(combine_info + '\\n')\n",
        "  if (debug_level > 1):\n",
        "    print(combine_info)\n",
        "    print('\\nAll Features and labels: Training')\n",
        "    print(pd_train_df.head(5))\n",
        "    print('\\nAll Features and labels: Test')\n",
        "    print(pd_test_df.head(5))\n",
        "\n",
        "  if (save_features):\n",
        "    early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "    early_season_end = cyp_config.getEarlySeasonEndDekad()\n",
        "    feature_file_path = cyp_config.getOutputPath()\n",
        "    features_file = getFeatureFilename(crop, use_yield_trend,\n",
        "                                       early_season_prediction, early_season_end,\n",
        "                                       country)\n",
        "    save_ft_path = feature_file_path + '/' + features_file\n",
        "    save_ft_info = '\\nSaving features to: ' + save_ft_path + '[train, test].csv'\n",
        "    log_fh.write(save_ft_info + '\\n')\n",
        "    if (debug_level > 1):\n",
        "      print(save_ft_info)\n",
        "\n",
        "    pd_train_df.to_csv(save_ft_path + '_train.csv', index=False, header=True)\n",
        "    pd_test_df.to_csv(save_ft_path + '_test.csv', index=False, header=True)\n",
        "\n",
        "    # NOTE: In some environments, Spark can write, but pandas cannot.\n",
        "    # In such cases, use the following code.\n",
        "    # spark_train_df = sqlCtx.createDataFrame(pd_train_df)\n",
        "    # spark_train_df.coalesce(1).write.option('header','true').mode('overwrite').csv(save_ft_path + '_train')\n",
        "    # spark_test_df = sqlCtx.createDataFrame(pd_test_df)\n",
        "    # spark_test_df.coalesce(1).write.option('header','true').mode('overwrite').csv(save_ft_path + '_test')\n",
        "\n",
        "  return pd_train_df, pd_test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQb85mf_wgvS"
      },
      "source": [
        "### Load Saved Features and Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5TyGb2iwnFr"
      },
      "outputs": [],
      "source": [
        "#%%writefile load_saved_features.py\n",
        "import pandas as pd\n",
        "\n",
        "def loadSavedFeaturesLabels(cyp_config, spark):\n",
        "  \"\"\"Load saved features from a CSV file\"\"\"\n",
        "  crop = cyp_config.getCropName()\n",
        "  country = cyp_config.getCountryCode()\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "  early_season_end = cyp_config.getEarlySeasonEndDekad()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "\n",
        "  feature_file_path = cyp_config.getOutputPath()\n",
        "  feature_file = getFeatureFilename(crop, use_yield_trend,\n",
        "                                    early_season_prediction, early_season_end,\n",
        "                                    country)\n",
        "\n",
        "  load_ft_path = feature_file_path + '/' + feature_file\n",
        "  pd_train_df = pd.read_csv(load_ft_path + '_train.csv', header=0)\n",
        "  pd_test_df = pd.read_csv(load_ft_path + '_test.csv', header=0)\n",
        "\n",
        "  # NOTE: In some environments, Spark can read, but pandas cannot.\n",
        "  # In such cases, use the following code.\n",
        "  # spark_train_df = spark.read.csv(load_ft_path + '_train.csv', header=True, inferSchema=True)\n",
        "  # spark_test_df = spark.read.csv(load_ft_path + '_test.csv', header=True, inferSchema=True)\n",
        "  # pd_train_df = spark_train_df.toPandas()\n",
        "  # pd_test_df = spark_test_df.toPandas()\n",
        "\n",
        "  if (debug_level > 1):\n",
        "    print('\\nAll Features and labels')\n",
        "    print(pd_train_df.head(5))\n",
        "    print(pd_test_df.head(5))\n",
        "\n",
        "  return pd_train_df, pd_test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PjL0Nv1jEpR"
      },
      "source": [
        "### Machine Learning using scikit learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drvMnHJBuBwK"
      },
      "source": [
        "#### Feature Selector Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBcbtxArL2Ld"
      },
      "outputs": [],
      "source": [
        "#%%writefile feature_selection.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.feature_selection import RFE\n",
        "from skopt import BayesSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.utils import parallel_backend\n",
        "\n",
        "class CYPFeatureSelector:\n",
        "  def __init__(self, cyp_config, X_train, Y_train, all_features,\n",
        "               custom_cv, train_weights=None):\n",
        "    self.X_train = X_train\n",
        "    self.Y_train = Y_train\n",
        "    self.train_weights = train_weights\n",
        "    self.custom_cv = custom_cv\n",
        "    self.all_features = all_features\n",
        "    self.feature_selectors = cyp_config.getFeatureSelectors(len(all_features))\n",
        "    self.scaler = cyp_config.getFeatureScaler()\n",
        "    self.cv_metric = cyp_config.getFeatureSelectionCVMetric()\n",
        "    self.use_sample_weights = cyp_config.useSampleWeights()\n",
        "    self.verbose = cyp_config.getDebugLevel()\n",
        "\n",
        "  def setCustomCV(self, custom_cv):\n",
        "    \"\"\"Set custom K-Fold validation splits\"\"\"\n",
        "    self.custom_cv = custom_cv\n",
        "\n",
        "  def setFeatures(self, features):\n",
        "    \"\"\"Set the list of all features\"\"\"\n",
        "    self.all_features = features\n",
        "\n",
        "  # K-fold validation to find the optimal number of features\n",
        "  # and optimal hyperparameters for estimator.\n",
        "  def featureSelectionParameterSearch(self, sel_name, selector, est, param_space, fit_params):\n",
        "    \"\"\"Use grid search with k-fold validation to optimize the number of features\"\"\"\n",
        "    # 3 values per parameter\n",
        "    # nparams_sampled = pow(3, len(param_space))\n",
        "    # if (nparams_sampled < 100):\n",
        "    #  nparams_sampled = 100\n",
        "\n",
        "    nparams_sampled = 40 + pow(2, len(param_space))\n",
        "    X_train_copy = np.copy(self.X_train)\n",
        "    pipeline = Pipeline([(\"scaler\", self.scaler),\n",
        "                         (\"selector\", selector),\n",
        "                         (\"estimator\", est)])\n",
        "\n",
        "    bayes_search = BayesSearchCV(estimator=pipeline,\n",
        "                                 search_spaces=param_space,\n",
        "                                 n_iter=nparams_sampled,\n",
        "                                 scoring=self.cv_metric,\n",
        "                                 cv=self.custom_cv,\n",
        "                                 n_points=4,\n",
        "                                 refit=modelRefitMeanVariance,\n",
        "                                 return_train_score=True,\n",
        "                                 n_jobs=-1)\n",
        "\n",
        "    # with parallel_backend('spark', n_jobs=-1):\n",
        "    bayes_search.fit(X_train_copy, np.ravel(self.Y_train), **fit_params)\n",
        "\n",
        "    best_params = bayes_search.best_params_\n",
        "    if (self.verbose > 2):\n",
        "      print('\\nFeature selection using', sel_name)\n",
        "      plotCVResults(bayes_search)\n",
        "\n",
        "    best_estimator = bayes_search.best_estimator_\n",
        "    with parallel_backend('spark', n_jobs=-1):\n",
        "      cv_scores = cross_val_score(best_estimator, X_train_copy, self.Y_train,\n",
        "                                  cv=self.custom_cv, scoring=self.cv_metric)\n",
        "    indices = []\n",
        "    # feature selectors should have 'get_support' function\n",
        "    selector = bayes_search.best_estimator_.named_steps['selector']\n",
        "    if ((isinstance(selector, SelectFromModel)) or (isinstance(selector, SelectKBest)) or\n",
        "        (isinstance(selector, RFE))):\n",
        "      indices = selector.get_support(indices=True)\n",
        "\n",
        "    if (self.verbose > 2):\n",
        "      print('\\nSelected Features:')\n",
        "      print('-------------------')\n",
        "      printInGroups(self.all_features, indices)\n",
        "\n",
        "    result = {\n",
        "        'indices' : indices,\n",
        "        'cv_scores' : cv_scores,\n",
        "        'estimator' : best_estimator,\n",
        "        'best_params' : best_params,\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "  # Compare different feature selectors using cross validation score.\n",
        "  # Also compare combined features with the best individual feature selector.\n",
        "  def compareFeatureSelectors(self, est_name, est, est_param_space):\n",
        "    \"\"\"Compare feature selectors based on K-fold validation scores\"\"\"\n",
        "    fs_results = {}\n",
        "    combined_indices = []\n",
        "    for sel_name in self.feature_selectors:\n",
        "      selector = self.feature_selectors[sel_name]['selector']\n",
        "      sel_param_space = self.feature_selectors[sel_name]['param_space']\n",
        "      param_space = sel_param_space.copy()\n",
        "      param_space.update(est_param_space)\n",
        "\n",
        "      fit_params = {}\n",
        "      if (self.use_sample_weights and (est_name != 'KNN')):\n",
        "        fit_params['estimator__sample_weight'] = self.train_weights\n",
        "\n",
        "      result = self.featureSelectionParameterSearch(sel_name, selector, est,\n",
        "                                                    param_space, fit_params)\n",
        "      param_space.clear()\n",
        "\n",
        "      fs_results[sel_name] = {\n",
        "          'indices' : result['indices'],\n",
        "          'mean_score' : np.mean(result['cv_scores']),\n",
        "          'std_score' : np.std(result['cv_scores']),\n",
        "          'estimator' : result['estimator'],\n",
        "          'best_params' : result['best_params']\n",
        "      }\n",
        "\n",
        "    # best selector has the highest score based on mean_score and std_score\n",
        "    # we subtract std_score because lower variance is better.\n",
        "    sel_names = list(fs_results.keys())\n",
        "    mean_scores = np.array([fs_results[s]['mean_score'] for s in sel_names])\n",
        "    std_scores = np.array([fs_results[s]['std_score'] for s in sel_names])\n",
        "    sel_scores = mean_scores - std_scores\n",
        "    best_score, best_index = max((val, idx) for (idx, val) in enumerate(sel_scores))\n",
        "    best_sel_name = sel_names[best_index]\n",
        "    for i in range(len(sel_names)):\n",
        "      s = sel_names[i]\n",
        "      fs_results[s]['sel_score'] = sel_scores[i]\n",
        "\n",
        "    return fs_results, best_sel_name\n",
        "\n",
        "  # Between the optimal sets for each estimator select the set with the higher score.\n",
        "  def selectOptimalFeatures(self, est, est_name, est_param_space, log_fh):\n",
        "    \"\"\"\n",
        "    Select optimal features by comparing individual feature selectors\n",
        "    and combined features\n",
        "    \"\"\"\n",
        "    X_train_selected = None\n",
        "    # set it to a large negative value\n",
        "    fs_summary = {}\n",
        "    row_count = 1\n",
        "\n",
        "    est_info = '\\nEstimator: ' + est_name\n",
        "    est_info += '\\n---------------------------'\n",
        "    log_fh.write(est_info)\n",
        "    print(est_info)\n",
        "\n",
        "    fs_results, best_sel = self.compareFeatureSelectors(est_name, est, est_param_space)\n",
        "\n",
        "    # result includes\n",
        "    # - 'best_selector' : name of the best selector\n",
        "    # - 'best_indices' : indices of features selected\n",
        "    # - 'fs_results' : dict with 'indices', 'mean_score' and 'std_score' for all selectors\n",
        "\n",
        "    for sel_name in fs_results:\n",
        "      mean_score = np.round(fs_results[sel_name]['mean_score'], 2)\n",
        "      std_score = np.round(fs_results[sel_name]['std_score'], 2)\n",
        "      sel_score = np.round(fs_results[sel_name]['sel_score'], 2)\n",
        "      est_sel_row = [est_name, sel_name, mean_score, std_score, sel_score]\n",
        "      fs_summary['row' + str(row_count)] = est_sel_row\n",
        "      row_count += 1\n",
        "\n",
        "    selected_indices = fs_results[best_sel]['indices']\n",
        "    best_estimator = fs_results[best_sel]['estimator']\n",
        "    fs_df_columns = ['estimator', 'selector', 'mean score', 'std score', 'selector score']\n",
        "    fs_df = pd.DataFrame.from_dict(fs_summary, orient='index', columns=fs_df_columns)\n",
        "    best_params = fs_results[best_sel]['best_params']\n",
        "    best_params_info = '\\nbest parameters:'\n",
        "    for c in best_params:\n",
        "      best_params_info += '\\n' + c + '=' + str(best_params[c])\n",
        "\n",
        "    log_fh.write(best_params_info)\n",
        "    print(best_params_info)\n",
        "\n",
        "    ftsel_summary_info = '\\nBest selector: ' + best_sel\n",
        "    ftsel_summary_info += '\\nFeature Selection Summary'\n",
        "    ftsel_summary_info += '\\n---------------------------'\n",
        "    ftsel_summary_info += '\\n' + fs_df.to_string(index=False) + '\\n'\n",
        "    log_fh.write(ftsel_summary_info)\n",
        "    print(ftsel_summary_info)\n",
        "\n",
        "    return selected_indices, best_estimator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWdWViKkuFM1"
      },
      "source": [
        "#### Algorithm Evaluator Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LutX8Ih9MsGx"
      },
      "outputs": [],
      "source": [
        "#%%writefile algorithm_evaluation.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "import multiprocessing as mp\n",
        "import shap\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.utils.fixes import loguniform\n",
        "from sklearn.utils import parallel_backend\n",
        "\n",
        "class CYPAlgorithmEvaluator:\n",
        "  def __init__(self, cyp_config, custom_cv=None,\n",
        "               train_weights=None, test_weights=None):\n",
        "    self.scaler = cyp_config.getFeatureScaler()\n",
        "    self.estimators = cyp_config.getEstimators()\n",
        "    self.custom_cv = custom_cv\n",
        "    self.cv_metric = cyp_config.getAlgorithmTrainingCVMetric()\n",
        "    self.train_weights = train_weights\n",
        "    self.test_weights = test_weights\n",
        "    self.metrics = cyp_config.getEvaluationMetrics()\n",
        "    self.verbose = cyp_config.getDebugLevel()\n",
        "    self.use_yield_trend = cyp_config.useYieldTrend()\n",
        "    self.trend_windows = cyp_config.getTrendWindows()\n",
        "    self.predict_residuals = cyp_config.predictYieldResiduals()\n",
        "    self.retrain_per_test_year = cyp_config.retrainPerTestYear()\n",
        "    self.use_sample_weights = cyp_config.useSampleWeights()\n",
        "\n",
        "  def setCustomCV(self, custom_cv):\n",
        "    \"\"\"Set custom K-Fold validation splits\"\"\"\n",
        "    self.custom_cv = custom_cv\n",
        "\n",
        "  # Nash-Sutcliffe Model Efficiency\n",
        "  def nse(self, Y_true, Y_pred):\n",
        "    \"\"\"\n",
        "        Nash Sutcliffe efficiency coefficient\n",
        "        input:\n",
        "          Y_pred: predicted\n",
        "          Y_true: observed\n",
        "        output:\n",
        "          nse: Nash Sutcliffe efficient coefficient\n",
        "        \"\"\"\n",
        "    return (1 - np.sum(np.square(Y_pred - Y_true))/np.sum(np.square(Y_true - np.mean(Y_true))))\n",
        "\n",
        "  def updateAlgorithmsSummary(self, alg_summary, alg_name, scores_list):\n",
        "    \"\"\"Update algorithms summary with scores for given algorithm\"\"\"\n",
        "    alg_row = [alg_name]\n",
        "    alg_index = len(alg_summary)\n",
        "    assert (len(scores_list) > 0)\n",
        "    for met in scores_list[0]:\n",
        "      for pred_scores in scores_list:\n",
        "        alg_row.append(pred_scores[met])\n",
        "\n",
        "    alg_summary['row' + str(alg_index)] = alg_row\n",
        "\n",
        "  def createPredictionDataFrames(self, Y_pred_arrays, data_cols):\n",
        "    \"\"\"\"Create pandas data frames from true and predicted values\"\"\"\n",
        "    pd_pred_dfs = []\n",
        "    for ar in Y_pred_arrays:\n",
        "      pd_df = pd.DataFrame(data=ar, columns=data_cols)\n",
        "      pd_pred_dfs.append(pd_df)\n",
        "\n",
        "    return pd_pred_dfs\n",
        "\n",
        "  def printPredictionDataFrames(self, pd_pred_dfs, pred_set_info, log_fh):\n",
        "    \"\"\"\"Print true and predicted values from pandas data frames\"\"\"\n",
        "    for i in range(len(pd_pred_dfs)):\n",
        "      pd_df = pd_pred_dfs[i]\n",
        "      set_info = pred_set_info[i]\n",
        "      df_info = '\\n Yield Predictions ' + set_info\n",
        "      df_info += '\\n--------------------------------'\n",
        "      df_info += '\\n' + pd_df.head(6).to_string(index=False)\n",
        "      log_fh.write(df_info + '\\n')\n",
        "      print(df_info)\n",
        "\n",
        "  def getNullMethodPredictions(self, Y_train_full, Y_test_full, cv_test_years, log_fh):\n",
        "    \"\"\"\n",
        "    The Null method or poor man's prediction. Y_*_full includes IDREGION, FYEAR.\n",
        "    If using yield trend, Y_*_full also include YIELD_TREND.\n",
        "    The null method predicts the YIELD_TREND or the average of the training set.\n",
        "    \"\"\"\n",
        "    Y_train = Y_train_full[:, 2]\n",
        "    if (self.use_yield_trend):\n",
        "      Y_train = Y_train_full[:, 3]\n",
        "\n",
        "    min_yield = np.round(np.min(Y_train), 2)\n",
        "    max_yield = np.round(np.max(Y_train), 2)\n",
        "    avg_yield = np.round(np.mean(Y_train), 2)\n",
        "    median_yield = np.round(np.median(np.ravel(Y_train)), 2)\n",
        "    cv_test_years = np.array(cv_test_years)\n",
        "\n",
        "    null_method_label = 'Null Method: '\n",
        "    if (self.use_yield_trend):\n",
        "      null_method_label += 'Predicting linear yield trend:'\n",
        "      data_cols = ['IDREGION', 'FYEAR', 'YIELD_TREND', 'YIELD']\n",
        "      Y_cv_full = Y_train_full[np.in1d(Y_train_full[:, 1], cv_test_years)]\n",
        "      Y_pred_arrays = [Y_train_full, Y_cv_full, Y_test_full]\n",
        "    else:\n",
        "      Y_train_full_n = np.insert(Y_train_full, 2, avg_yield, axis=1)\n",
        "      Y_test_full_n = np.insert(Y_test_full, 2, avg_yield, axis=1)\n",
        "      null_method_label += 'Predicting average of the training set:'\n",
        "      data_cols = ['IDREGION', 'FYEAR', 'YIELD_PRED', 'YIELD']\n",
        "      Y_cv_full = Y_train_full_n[np.in1d(Y_train_full_n[:, 1], cv_test_years)]\n",
        "      Y_pred_arrays = [Y_train_full_n, Y_cv_full, Y_test_full_n]\n",
        "\n",
        "    pd_pred_dfs = self.createPredictionDataFrames(Y_pred_arrays, data_cols)\n",
        "    null_method_info = '\\n' + null_method_label\n",
        "    null_method_info += '\\nMin Yield: ' + str(min_yield) + ', Max Yield: ' + str(max_yield)\n",
        "    null_method_info += '\\nMedian Yield: ' + str(median_yield) + ', Mean Yield: ' + str(avg_yield)\n",
        "    log_fh.write(null_method_info + '\\n')\n",
        "    print(null_method_info)\n",
        "    pred_set_info = ['Training Set', 'Validation Test Set', 'Test Set']\n",
        "    self.printPredictionDataFrames(pd_pred_dfs, pred_set_info, log_fh)\n",
        "\n",
        "    result = {\n",
        "        'train' : pd_pred_dfs[0],\n",
        "        'custom_cv' : pd_pred_dfs[1],\n",
        "        'test' : pd_pred_dfs[2],\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "  def evaluateNullMethodPredictions(self, pd_pred_dfs, alg_summary):\n",
        "    \"\"\"Evaluate predictions of the Null method\"\"\"\n",
        "    if (self.use_yield_trend):\n",
        "      alg_name = 'trend'\n",
        "      pred_col_name = 'YIELD_TREND'\n",
        "    else:\n",
        "      alg_name = 'average'\n",
        "      pred_col_name = 'YIELD_PRED'\n",
        "\n",
        "    scores_list = []\n",
        "    for pred_set in pd_pred_dfs:\n",
        "      pred_df = pd_pred_dfs[pred_set]\n",
        "      Y_true = pred_df['YIELD'].values\n",
        "      Y_pred = pred_df[pred_col_name].values\n",
        "      pred_scores = getPredictionScores(Y_true, Y_pred, self.metrics)\n",
        "      scores_list.append(pred_scores)\n",
        "\n",
        "    self.updateAlgorithmsSummary(alg_summary, alg_name, scores_list)\n",
        "\n",
        "  def getYieldTrendML(self, X_train, Y_train, X_test):\n",
        "    \"\"\"\n",
        "    Predict yield trend using a linear model.\n",
        "    No need to scale features. They are all yield values.\n",
        "    \"\"\"\n",
        "    est = Ridge(alpha=1, random_state=42, max_iter=1000,\n",
        "                copy_X=True, fit_intercept=True)\n",
        "    param_space = dict(estimator__alpha=loguniform(1e-1, 1e+2))\n",
        "    pipeline = Pipeline([(\"scaler\", self.scaler), (\"estimator\", est)])\n",
        "    nparams_sampled = pow(3, len(param_space))\n",
        "\n",
        "    X_train_copy = np.copy(X_train)\n",
        "    rand_search = RandomizedSearchCV(estimator=pipeline,\n",
        "                                     param_distributions=param_space,\n",
        "                                     n_iter=nparams_sampled,\n",
        "                                     scoring=self.cv_metric,\n",
        "                                     cv=self.custom_cv,\n",
        "                                     return_train_score=True,\n",
        "                                     refit=modelRefitMeanVariance)\n",
        "\n",
        "    fit_params = {}\n",
        "    if (self.use_sample_weights):\n",
        "      fit_params = { 'estimator__sample_weight' : self.train_weights }\n",
        "\n",
        "    with parallel_backend('spark', n_jobs=-1):\n",
        "      rand_search.fit(X_train_copy, np.ravel(Y_train), **fit_params)\n",
        "\n",
        "    best_params = rand_search.best_params_\n",
        "    best_estimator = rand_search.best_estimator_\n",
        "    if (self.verbose > 1):\n",
        "      print('\\nYield Trend: Ridge best parameters:')\n",
        "      for param in param_space:\n",
        "        print(param + '=', best_params[param])\n",
        "\n",
        "    Y_pred_train = np.reshape(best_estimator.predict(X_train), (X_train.shape[0], 1))\n",
        "    Y_pred_test = np.reshape(best_estimator.predict(X_test), (X_test.shape[0], 1))\n",
        "\n",
        "    return Y_pred_train, Y_pred_test\n",
        "\n",
        "  def estimateYieldTrendAndDetrend(self, X_train, Y_train, X_test, Y_test, features):\n",
        "    \"\"\"Estimate yield trend using machine learning and detrend\"\"\"\n",
        "    trend_window = self.trend_windows[0]\n",
        "    # NOTE assuming previous years' yield values are at the end\n",
        "    X_train_trend = X_train[:, -trend_window:]\n",
        "    X_test_trend = X_test[:, -trend_window:]\n",
        "\n",
        "    Y_train_trend, Y_test_trend = self.getYieldTrendML(X_train_trend, Y_train, X_test_trend)\n",
        "    # New features exclude previous years' yield and include YIELD_TREND\n",
        "    features_n = features[:-trend_window] + ['YIELD_TREND']\n",
        "    X_train_n = np.append(X_train[:, :-trend_window], Y_train_trend, axis=1)\n",
        "    X_test_n = np.append(X_test[:, :-trend_window], Y_test_trend, axis=1)\n",
        "    Y_train_res = np.reshape(Y_train, (X_train.shape[0], 1)) - Y_train_trend\n",
        "    Y_test_res = np.reshape(Y_test, (X_test.shape[0], 1)) - Y_test_trend\n",
        "\n",
        "    result =  {\n",
        "        'X_train' : X_train_n,\n",
        "        'Y_train' : Y_train_res,\n",
        "        'Y_train_trend' : Y_train_trend,\n",
        "        'X_test' : X_test_n,\n",
        "        'Y_test' : Y_test_res,\n",
        "        'Y_test_trend' : Y_test_trend,\n",
        "        'features' : features_n,\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "  def yieldPredictionsFromResiduals(self, pd_train_df, Y_train, pd_test_df, Y_test,\n",
        "                                    pd_cv_df, cv_test_years):\n",
        "    \"\"\"Predictions are residuals. Add trend back to get yield predictions.\"\"\"\n",
        "    pd_train_df['YIELD_RES'] = pd_train_df['YIELD']\n",
        "    pd_train_df['YIELD'] = Y_train\n",
        "    Y_custom_cv = pd_train_df[pd_train_df['FYEAR'].isin(cv_test_years)]['YIELD'].values\n",
        "    pd_cv_df['YIELD_RES'] = pd_cv_df['YIELD']\n",
        "    pd_cv_df['YIELD'] = Y_custom_cv\n",
        "    pd_test_df['YIELD_RES'] = pd_test_df['YIELD']\n",
        "    pd_test_df['YIELD'] = Y_test\n",
        "\n",
        "    for alg in self.estimators:\n",
        "      pd_train_df['YIELD_RES_PRED_' + alg] = pd_train_df['YIELD_PRED_' + alg]\n",
        "      pd_train_df['YIELD_PRED_' + alg] = pd_train_df['YIELD_RES_PRED_' + alg] + pd_train_df['YIELD_TREND']\n",
        "      pd_test_df['YIELD_RES_PRED_' + alg] = pd_test_df['YIELD_PRED_' + alg]\n",
        "      pd_test_df['YIELD_PRED_' + alg] = pd_test_df['YIELD_RES_PRED_' + alg] + pd_test_df['YIELD_TREND']\n",
        "      pd_cv_df['YIELD_RES_PRED_' + alg] = pd_cv_df['YIELD_PRED_' + alg]\n",
        "      pd_cv_df['YIELD_PRED_' + alg] = pd_cv_df['YIELD_RES_PRED_' + alg] + pd_cv_df['YIELD_TREND']\n",
        "\n",
        "    sel_cols = ['IDREGION', 'FYEAR', 'YIELD_TREND', 'YIELD_RES']\n",
        "    for alg in self.estimators:\n",
        "      sel_cols += ['YIELD_RES_PRED_' + alg, 'YIELD_PRED_' + alg]\n",
        "\n",
        "    sel_cols.append('YIELD')\n",
        "    result = {\n",
        "        'train' : pd_train_df[sel_cols],\n",
        "        'custom_cv' : pd_cv_df[sel_cols],\n",
        "        'test' : pd_test_df[sel_cols],\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "  def updateFeatureSelectionInfo(self, est_name, features, selected_indices,\n",
        "                                 ft_selection_counts, ft_importances, log_fh):\n",
        "    \"\"\"\n",
        "    Update feature selection counts.\n",
        "    Print selected features and importance.\n",
        "    \"\"\"\n",
        "    # update feature selection counts\n",
        "    for idx in selected_indices:\n",
        "      ft_count = 0\n",
        "      ft = features[idx]\n",
        "      ft_period = 'static'\n",
        "      for p in ft_selection_counts:\n",
        "        if p in ft:\n",
        "          ft_period = p\n",
        "\n",
        "      if (ft in ft_selection_counts[ft_period]):\n",
        "        ft_count = ft_selection_counts[ft_period][ft]\n",
        "\n",
        "      ft_selection_counts[ft_period][ft] = ft_count + 1\n",
        "\n",
        "    if (ft_importances is not None):\n",
        "      ft_importance_indices = []\n",
        "      ft_importance_values = [0.0 for i in range(len(features))]\n",
        "      for idx in reversed(np.argsort(ft_importances)):\n",
        "        ft_importance_indices.append(selected_indices[idx])\n",
        "        ft_importance_values[selected_indices[idx]] = str(np.round(ft_importances[idx], 2))\n",
        "\n",
        "      ft_importance_info = '\\nSelected features with importance:'\n",
        "      ft_importance_info += '\\n----------------------------------'\n",
        "      log_fh.write(ft_importance_info)\n",
        "      print(ft_importance_info)\n",
        "      printInGroups(features, ft_importance_indices, ft_importance_values, log_fh)\n",
        "    else:\n",
        "      sel_fts_info = '\\nSelected Features:'\n",
        "      sel_fts_info += '\\n-------------------'\n",
        "      log_fh.write(sel_fts_info)\n",
        "      print(sel_fts_info)\n",
        "      printInGroups(features, selected_indices, log_fh=log_fh)\n",
        "\n",
        "  def getCustomCVPredictions(self, est_name, best_est,\n",
        "                             X_train, Y_train, Y_cv_full):\n",
        "    \"\"\"Get predictions for custom cv test years\"\"\"\n",
        "    Y_pred_cv = np.zeros(Y_cv_full.shape[0])\n",
        "    fit_predict_args = []\n",
        "    for i in range(len(self.custom_cv)):\n",
        "      cv_train_idxs, cv_test_idxs = self.custom_cv[i]\n",
        "      sample_weights = None\n",
        "      if (self.use_sample_weights):\n",
        "        sample_weights = np.copy(self.train_weights[cv_train_idxs])\n",
        "\n",
        "      fit_params = {}\n",
        "      if (self.use_sample_weights and (est_name != 'KNN')):\n",
        "        fit_params = { 'estimator__sample_weight' : sample_weights }\n",
        "\n",
        "      fit_predict_args.append(\n",
        "          {\n",
        "              'X_train' : np.copy(X_train[cv_train_idxs, :]),\n",
        "              'Y_train' : np.copy(Y_train[cv_train_idxs]),\n",
        "              'X_test' : np.copy(X_train[cv_test_idxs, :]),\n",
        "              'fit_params' : fit_params,\n",
        "              'estimator' : deepcopy(best_est),\n",
        "          }\n",
        "      )\n",
        "\n",
        "    pool = mp.Pool(len(self.custom_cv))\n",
        "    Y_preds = pool.map(customFitPredict, fit_predict_args)\n",
        "    for i in range(len(self.custom_cv)):\n",
        "      cv_train_idxs, cv_test_idxs = self.custom_cv[i]\n",
        "      Y_pred_cv[cv_test_idxs] = Y_preds[i]\n",
        "\n",
        "    # clean up\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    return Y_pred_cv\n",
        "\n",
        "  def getPerTestYearPredictions(self, est_name, best_est,\n",
        "                                X_train, Y_train_full,\n",
        "                                X_test, Y_test_full, test_years):\n",
        "    \"\"\"For each test year, fit best_est on all previous years and predict\"\"\"\n",
        "    Y_train = Y_train_full[:, -1]\n",
        "    Y_test = Y_test_full[:, -1]\n",
        "    Y_pred_test = np.zeros(Y_test_full.shape[0])\n",
        "\n",
        "    # For the first test year, X_train and Y_train do not change. No need to refit.\n",
        "    test_indexes = np.where(Y_test_full[:, 1] == test_years[0])[0]\n",
        "    Y_pred_first_yr = best_est.predict(X_test[test_indexes, :])\n",
        "    Y_pred_test[test_indexes] = Y_pred_first_yr\n",
        "\n",
        "    if (est_name == 'GBDT'):\n",
        "      best_est.named_steps['estimator'].set_params(**{ 'warm_start' : True })\n",
        "\n",
        "    fit_predict_args = []\n",
        "    for i in range(1, len(test_years)):\n",
        "      extra_train_years = test_years[:i]\n",
        "      test_indexes = np.where(Y_test_full[:, 1] == test_years[i])[0]\n",
        "      sample_weights = None\n",
        "      if (self.use_sample_weights):\n",
        "        sample_weights = self.train_weights\n",
        "\n",
        "      train_indexes_n = np.ravel(np.nonzero(np.isin(Y_test_full[:, 1], extra_train_years)))\n",
        "      X_train_n = np.append(X_train, X_test[train_indexes_n, :], axis=0)\n",
        "      Y_train_n = np.append(Y_train, Y_test[train_indexes_n])\n",
        "      if (self.use_sample_weights):\n",
        "        sample_weights = np.append(sample_weights, self.test_weights[train_indexes_n], axis=0)\n",
        "\n",
        "      fit_params = {}\n",
        "      if (self.use_sample_weights and (est_name != 'KNN')):\n",
        "        fit_params['estimator__sample_weight'] = sample_weights\n",
        "\n",
        "      fit_predict_args.append(\n",
        "          {\n",
        "              'X_train' : np.copy(X_train_n),\n",
        "              'Y_train' : np.copy(Y_train_n),\n",
        "              'X_test' : np.copy(X_test[test_indexes, :]),\n",
        "              'fit_params' : fit_params,\n",
        "              'estimator' : deepcopy(best_est),\n",
        "          }\n",
        "      )\n",
        "\n",
        "    pool = mp.Pool(len(test_years) - 1)\n",
        "    Y_preds = pool.map(customFitPredict, fit_predict_args)\n",
        "    for i in range(1, len(test_years)):\n",
        "      test_indexes = np.where(Y_test_full[:, 1] == test_years[i])[0]\n",
        "      Y_pred_test[test_indexes] = Y_preds[i-1]\n",
        "\n",
        "    # clean up\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    return Y_pred_test\n",
        "\n",
        "  def combineAlgorithmPredictions(self, pd_ml_predictions, pd_alg_predictions, alg):\n",
        "    \"\"\"Combine predictions of ML algorithms.\"\"\"\n",
        "    join_cols = ['IDREGION', 'FYEAR']\n",
        "\n",
        "    if (pd_ml_predictions is None):\n",
        "      pd_ml_predictions = pd_alg_predictions\n",
        "      pd_ml_predictions = pd_ml_predictions.rename(columns={'YIELD_PRED': 'YIELD_PRED_' + alg })\n",
        "    else:\n",
        "      pd_alg_predictions = pd_alg_predictions[join_cols + ['YIELD_PRED']]\n",
        "      pd_ml_predictions = pd_ml_predictions.merge(pd_alg_predictions, on=join_cols)\n",
        "      pd_ml_predictions = pd_ml_predictions.rename(columns={'YIELD_PRED': 'YIELD_PRED_' + alg })\n",
        "      # Put YIELD at the end\n",
        "      all_cols = list(pd_ml_predictions.columns)\n",
        "      col_order = all_cols[:-2] + ['YIELD_PRED_' + alg, 'YIELD']\n",
        "      pd_ml_predictions = pd_ml_predictions[col_order]\n",
        "\n",
        "    return pd_ml_predictions\n",
        "\n",
        "  def getMLPredictions(self, X_train, Y_train_full, X_test, Y_test_full,\n",
        "                       cv_test_years, cyp_ftsel, features, log_fh):\n",
        "    \"\"\"Train and evaluate crop yield prediction algorithms\"\"\"\n",
        "    # Y_*_full\n",
        "    # IDREGION, FYEAR, YIELD_TREND, YIELD_PRED_Ridge, ..., YIELD_PRED_GBDT, YIELD\n",
        "    # NL11, NL12, NL13 (some regions can have missing values)\n",
        "    # 1999, ..., 2011 => training\n",
        "    # 2012, ..., 2018 => Test\n",
        "    # We need to aggregate to national level. Need predictions from all regions.\n",
        "    # cv_test_years\n",
        "    # 1999, ..., 2006 => 2007\n",
        "    # 1999, ..., 2007 => 2008\n",
        "    # ...\n",
        "    # cv_test_years : [2007, 2008, ..., 2011]\n",
        "\n",
        "    Y_train = Y_train_full[:, -1]\n",
        "    train_years = sorted(np.unique(Y_train_full[:, 1]))\n",
        "    Y_cv_full = np.copy(Y_train_full)\n",
        "    Y_test = Y_test_full[:, -1]\n",
        "    test_years = sorted(np.unique(Y_test_full[:, 1]))\n",
        "\n",
        "    pd_test_predictions = None\n",
        "    pd_cv_predictions = None\n",
        "    pd_train_predictions = None\n",
        "\n",
        "    # feature selection frequency\n",
        "    # NOTE must be in sync with crop calendar periods\n",
        "    ft_selection_counts = {\n",
        "        'static' : {}, 'p0' : {}, 'p1' : {}, 'p2' : {}, 'p3' : {}, 'p4' : {}, 'p5' : {},\n",
        "    }\n",
        "\n",
        "    for est_name in self.estimators:\n",
        "      # feature selection\n",
        "      est = self.estimators[est_name]['estimator']\n",
        "      param_space = self.estimators[est_name]['param_space']\n",
        "      sel_indices, best_est = cyp_ftsel.selectOptimalFeatures(est, est_name, param_space, log_fh)\n",
        "\n",
        "      # feature importance\n",
        "      ft_importances = None\n",
        "      if ((est_name == 'Ridge') or (est_name == 'Lasso')):\n",
        "        ft_importances = best_est.named_steps['estimator'].coef_\n",
        "      elif (est_name == 'SVR'):\n",
        "        try:\n",
        "          ft_importances = np.ravel(best_est.named_steps['estimator'].coef_)\n",
        "        except AttributeError as e:\n",
        "          ft_importances = None\n",
        "      elif ((est_name == 'GBDT') or (est_name == 'RF') or (est_name == 'ERT')):\n",
        "        ft_importances = best_est.named_steps['estimator'].feature_importances_\n",
        "\n",
        "      self.updateFeatureSelectionInfo(est_name, features, sel_indices, ft_selection_counts,\n",
        "                                      ft_importances, log_fh)\n",
        "\n",
        "      # Extract selected features\n",
        "      # X_train_sel = X_train[:, sel_indices]\n",
        "      # X_test_sel = X_test[:, sel_indices]\n",
        "      # fit_params = {}\n",
        "      # if (self.use_sample_weights and (est_name != 'KNN')):\n",
        "      #   fit_params['estimator__sample_weight'] = self.train_weights\n",
        "\n",
        "      # best_est.fit(X_train_sel, Y_train, **fit_params)\n",
        "\n",
        "      # Predictions\n",
        "      Y_pred_train = best_est.predict(X_train)\n",
        "      Y_pred_test = best_est.predict(X_test)\n",
        "\n",
        "      # custom cv predictions for cv metrics\n",
        "      Y_pred_cv = self.getCustomCVPredictions(est_name, best_est,\n",
        "                                              X_train, Y_train, Y_cv_full)\n",
        "\n",
        "      # per test year predictions\n",
        "      # 1999, ..., 2011 => 2012\n",
        "      # 1999, ..., 2012 => 2013\n",
        "      # ...\n",
        "      if (self.retrain_per_test_year):\n",
        "        Y_pred_test = self.getPerTestYearPredictions(est_name, best_est,\n",
        "                                                     X_train, Y_train_full,\n",
        "                                                     X_test, Y_test_full, test_years)\n",
        "\n",
        "      data_cols = ['IDREGION', 'FYEAR']\n",
        "      if (self.use_yield_trend):\n",
        "        data_cols.append('YIELD_TREND')\n",
        "        Y_train_full_n = np.insert(Y_train_full, 3, Y_pred_train, axis=1)\n",
        "        Y_cv_full_n = np.insert(Y_cv_full, 3, Y_pred_cv, axis=1)\n",
        "        Y_test_full_n = np.insert(Y_test_full, 3, Y_pred_test, axis=1)\n",
        "      else:\n",
        "        Y_train_full_n = np.insert(Y_train_full, 2, Y_pred_train, axis=1)\n",
        "        Y_cv_full_n = np.insert(Y_cv_full, 2, Y_pred_cv, axis=1)\n",
        "        Y_test_full_n = np.insert(Y_test_full, 2, Y_pred_test, axis=1)\n",
        "\n",
        "      data_cols += ['YIELD_PRED', 'YIELD']\n",
        "      Y_cv_full_n = Y_cv_full_n[np.in1d(Y_cv_full_n[:, 1], cv_test_years)]\n",
        "      Y_pred_arrays = [Y_train_full_n, Y_cv_full_n, Y_test_full_n]\n",
        "      pd_pred_dfs = self.createPredictionDataFrames(Y_pred_arrays, data_cols)\n",
        "      pd_train_predictions = self.combineAlgorithmPredictions(pd_train_predictions,\n",
        "                                                              pd_pred_dfs[0], est_name)\n",
        "      pd_cv_predictions = self.combineAlgorithmPredictions(pd_cv_predictions,\n",
        "                                                           pd_pred_dfs[1], est_name)\n",
        "      pd_test_predictions = self.combineAlgorithmPredictions(pd_test_predictions,\n",
        "                                                             pd_pred_dfs[2], est_name)\n",
        "\n",
        "    ft_counts_info = '\\nFeature Selection Frequencies'\n",
        "    ft_counts_info += '\\n-------------------------------'\n",
        "    for ft_period in ft_selection_counts:\n",
        "      ft_count_str = ft_period + ': '\n",
        "      for ft in sorted(ft_selection_counts[ft_period],\n",
        "                       key=ft_selection_counts[ft_period].get, reverse=True):\n",
        "        ft_count_str += ft + '(' + str(ft_selection_counts[ft_period][ft]) + '), '\n",
        "\n",
        "      if (len(ft_selection_counts[ft_period]) > 0):\n",
        "        # drop ', ' from the end\n",
        "        ft_count_str = ft_count_str[:-2]\n",
        "\n",
        "      ft_counts_info += '\\n' + ft_count_str\n",
        "\n",
        "    ft_counts_info += '\\n'\n",
        "    log_fh.write(ft_counts_info)\n",
        "    if (self.verbose > 1):\n",
        "      print(ft_counts_info)\n",
        "\n",
        "    result = {\n",
        "        'train' : pd_train_predictions,\n",
        "        'custom_cv' : pd_cv_predictions,\n",
        "        'test' : pd_test_predictions,\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "  def evaluateMLPredictions(self, pd_pred_dfs, alg_summary):\n",
        "    \"\"\"Evaluate predictions of ML algorithms and add entries to alg_summary.\"\"\"\n",
        "    for alg in self.estimators:\n",
        "      pred_col = 'YIELD_PRED_' + alg\n",
        "      scores_list = []\n",
        "      for pred_set in pd_pred_dfs:\n",
        "        pred_df = pd_pred_dfs[pred_set]\n",
        "        Y_true = pred_df['YIELD'].values\n",
        "        Y_pred = pred_df[pred_col].values\n",
        "        pred_scores = getPredictionScores(Y_true, Y_pred, self.metrics)\n",
        "        scores_list.append(pred_scores)\n",
        "\n",
        "      self.updateAlgorithmsSummary(alg_summary, alg, scores_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpxrWSjsxMdx"
      },
      "source": [
        "#### Run Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HbC8rSxjEpT"
      },
      "outputs": [],
      "source": [
        "#%%writefile run_machine_learning.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from joblibspark import register_spark\n",
        "\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "\n",
        "def dropHighlyCorrelatedFeatures(cyp_config, pd_train_df, pd_test_df,\n",
        "                                 log_fh, corr_method='pearson', corr_thresh=0.95):\n",
        "  \"\"\"Plot correlations. Drop columns that are highly correlated.\"\"\"\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  all_cols = list(pd_train_df.columns)[2:]\n",
        "  avg_cols = [c for c in all_cols if 'avg' in c] + ['YIELD']\n",
        "  max_cols = [c for c in all_cols if 'max' in c] + ['YIELD']\n",
        "  lt_th_cols = [c for c in all_cols if 'Z-' in c] + ['YIELD']\n",
        "  gt_th_cols = [c for c in all_cols if 'Z+' in c] + ['YIELD']\n",
        "  yt_cols = ['YIELD-' + str(i) for i in range(1, 6)]  + ['YIELD']\n",
        "\n",
        "  if (debug_level > 2):\n",
        "    plotCorrelation(pd_train_df, avg_cols)\n",
        "    plotCorrelation(pd_train_df, max_cols)\n",
        "    plotCorrelation(pd_train_df, lt_th_cols)\n",
        "    plotCorrelation(pd_train_df, gt_th_cols)\n",
        "    plotCorrelation(pd_train_df, yt_cols)\n",
        "\n",
        "  # drop highly correlated features\n",
        "  # Based on https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/\n",
        "\n",
        "  corr_columns = [c for c in all_cols if ((c != 'YIELD') and (c != 'YIELD_TREND'))]\n",
        "  corr_matrix = pd_train_df[corr_columns].corr(method=corr_method).abs()\n",
        "  ut_mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "  ut_matrix = corr_matrix.mask(ut_mask)\n",
        "  to_drop = [c for c in ut_matrix.columns if any(ut_matrix[c] > corr_thresh)]\n",
        "  drop_info = '\\nDropping highly correlated features'\n",
        "  drop_info += '\\n' + ', '.join(to_drop)\n",
        "\n",
        "  log_fh.write(drop_info + '\\n')\n",
        "  if ((debug_level > 1) and (to_drop)):\n",
        "    print(drop_info)\n",
        "\n",
        "  pd_train_df = pd_train_df.drop(columns=to_drop)\n",
        "  pd_test_df = pd_test_df.drop(columns=to_drop)\n",
        "\n",
        "  return pd_train_df, pd_test_df\n",
        "\n",
        "def getValidationSplits(cyp_config, pd_train_df, pd_test_df, log_fh):\n",
        "  \"\"\"Split features and label into training and test sets\"\"\"\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  use_sample_weights = cyp_config.useSampleWeights()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "\n",
        "  regions = [reg for reg in pd_train_df['IDREGION'].unique()]\n",
        "  num_regions = len(regions)\n",
        "\n",
        "  original_headers = list(pd_train_df.columns.values)\n",
        "  features = []\n",
        "  labels = []\n",
        "  if (use_yield_trend):\n",
        "    if (use_sample_weights):\n",
        "      features = original_headers[2:-3]\n",
        "      labels = original_headers[:2] + original_headers[-3:-1]\n",
        "    else:\n",
        "      features = original_headers[2:-2]\n",
        "      labels = original_headers[:2] + original_headers[-2:]\n",
        "  else:\n",
        "    if (use_sample_weights):\n",
        "      features = original_headers[2:-2]\n",
        "      labels = original_headers[:2] + original_headers[-2:-1]\n",
        "    else:\n",
        "      features = original_headers[2:-1]\n",
        "      labels = original_headers[:2] + original_headers[-1:]\n",
        "\n",
        "  X_train = pd_train_df[features].values\n",
        "  Y_train = pd_train_df[labels].values\n",
        "  train_weights = None\n",
        "  if (use_sample_weights):\n",
        "    train_weights = pd_train_df['SAMPLE_WEIGHT'].values\n",
        "\n",
        "  train_info = '\\nTraining Data Size: ' + str(len(pd_train_df.index)) + ' rows'\n",
        "  train_info += '\\nX cols: ' + str(X_train.shape[1]) + ', Y cols: ' + str(Y_train.shape[1])\n",
        "  train_info += '\\n' + pd_train_df.head(5).to_string(index=False)\n",
        "  log_fh.write(train_info + '\\n')\n",
        "  if (debug_level > 1):\n",
        "    print(train_info)\n",
        "\n",
        "  X_test = pd_test_df[features].values\n",
        "  Y_test = pd_test_df[labels].values\n",
        "  test_weights = None\n",
        "  if (use_sample_weights):\n",
        "    test_weights = pd_test_df['SAMPLE_WEIGHT'].values\n",
        "\n",
        "  test_info = '\\nTest Data Size: ' + str(len(pd_test_df.index)) + ' rows'\n",
        "  test_info += '\\nX cols: ' + str(X_test.shape[1]) + ', Y cols: ' + str(Y_test.shape[1])\n",
        "  test_info += '\\n' + pd_test_df.head(5).to_string(index=False)\n",
        "  log_fh.write(test_info + '\\n')\n",
        "  if (debug_level > 1):\n",
        "    print(test_info)\n",
        "\n",
        "  # print feature names\n",
        "  num_features = len(features)\n",
        "  indices = [idx for idx in range(num_features)]\n",
        "  feature_info = '\\nAll features'\n",
        "  feature_info += '\\n-------------'\n",
        "  log_fh.write(feature_info)\n",
        "  print(feature_info)\n",
        "  printInGroups(features, indices, log_fh=log_fh)\n",
        "\n",
        "  # num_folds for k-fold cv\n",
        "  num_folds = 5\n",
        "  custom_cv = num_folds\n",
        "  cv_test_years = []\n",
        "  if (use_yield_trend):\n",
        "    cyp_cv_splitter = CYPTrainTestSplitter(cyp_config)\n",
        "    custom_cv, cv_test_years = cyp_cv_splitter.customKFoldValidationSplit(Y_train, num_folds, log_fh)\n",
        "\n",
        "  result = {\n",
        "      'X_train' : X_train,\n",
        "      'Y_train_full' : Y_train,\n",
        "      'train_weights' : train_weights,\n",
        "      'X_test' : X_test,\n",
        "      'Y_test_full' : Y_test,\n",
        "      'test_weights' : test_weights,\n",
        "      'custom_cv' : custom_cv,\n",
        "      'cv_test_years' : cv_test_years,\n",
        "      'features' : features,\n",
        "  }\n",
        "\n",
        "  return result\n",
        "\n",
        "def printAlgorithmsEvaluationSummary(cyp_config, null_preds, ml_preds,\n",
        "                                     log_fh, country_code=None):\n",
        "  \"\"\"Print summary of algorithm evaluation\"\"\"\n",
        "  metrics = cyp_config.getEvaluationMetrics()\n",
        "  country = country_code\n",
        "  if (country_code is None):\n",
        "    country = cyp_config.getCountryCode()\n",
        "\n",
        "  alg_summary = {}\n",
        "  cyp_algeval = CYPAlgorithmEvaluator(cyp_config)\n",
        "  cyp_algeval.evaluateNullMethodPredictions(null_preds, alg_summary)\n",
        "  cyp_algeval.evaluateMLPredictions(ml_preds, alg_summary)\n",
        "  pd_pred_dfs = [ml_preds['train'], ml_preds['custom_cv'], ml_preds['test']]\n",
        "  pred_sets_info = ['Training Set', 'Validation Test Set', 'Test Set']\n",
        "  cyp_algeval.printPredictionDataFrames(pd_pred_dfs, pred_sets_info, log_fh)\n",
        "\n",
        "  alg_df_columns = ['algorithm']\n",
        "  for met in metrics:\n",
        "    alg_df_columns += ['train_' + met, 'cv_' + met, 'test_' + met]\n",
        "\n",
        "  alg_df = pd.DataFrame.from_dict(alg_summary, orient='index', columns=alg_df_columns)\n",
        "\n",
        "  eval_summary_info = '\\nAlgorithm Evaluation Summary for ' + country\n",
        "  eval_summary_info += '\\n-----------------------------------------'\n",
        "  eval_summary_info += '\\n' + alg_df.to_string(index=False) + '\\n'\n",
        "  log_fh.write(eval_summary_info)\n",
        "  print(eval_summary_info)\n",
        "\n",
        "def getMLPredictionsOneModel(cyp_config, pd_train_df, pd_test_df, log_fh):\n",
        "  \"\"\"Build one ML model and return predictions\"\"\"\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  predict_residuals = cyp_config.predictYieldResiduals()\n",
        "  use_sample_weights = cyp_config.useSampleWeights()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "\n",
        "  data_splits = getValidationSplits(cyp_config, pd_train_df, pd_test_df, log_fh)\n",
        "  X_train = data_splits['X_train']\n",
        "  Y_train_full = data_splits['Y_train_full']\n",
        "  X_test = data_splits['X_test']\n",
        "  Y_test_full = data_splits['Y_test_full']\n",
        "  features = data_splits['features']\n",
        "  custom_cv = data_splits['custom_cv']\n",
        "  cv_test_years = data_splits['cv_test_years']\n",
        "\n",
        "  train_weights = None\n",
        "  test_weights = None\n",
        "  if (use_sample_weights):\n",
        "    train_weights = data_splits['train_weights']\n",
        "    test_weights = data_splits['test_weights']\n",
        "\n",
        "  cyp_algeval = CYPAlgorithmEvaluator(cyp_config, custom_cv, train_weights, test_weights)\n",
        "  null_preds = cyp_algeval.getNullMethodPredictions(Y_train_full, Y_test_full,\n",
        "                                                    cv_test_years, log_fh)\n",
        "\n",
        "  Y_train_full_n = Y_train_full\n",
        "  Y_test_full_n = Y_test_full\n",
        "  if (use_yield_trend and predict_residuals):\n",
        "    result = cyp_algeval.estimateYieldTrendAndDetrend(X_train, Y_train_full[:, -1],\n",
        "                                                      X_test, Y_test_full[:, -1], features)\n",
        "    X_train = result['X_train']\n",
        "    Y_train_full_n = np.copy(Y_train_full)\n",
        "    Y_train_full_n[:, -1] = result['Y_train'][:, 0]\n",
        "    Y_train_full_n[:, -2] = result['Y_train_trend'][:, 0]\n",
        "    X_test = result['X_test']\n",
        "    Y_test_full_n = np.copy(Y_test_full)\n",
        "    Y_test_full_n[:, -1] = result['Y_test'][:,0]\n",
        "    Y_test_full_n[:, -2] = result['Y_test_trend'][:, 0]\n",
        "    features = result['features']\n",
        "\n",
        "  cyp_ftsel = CYPFeatureSelector(cyp_config, X_train, Y_train_full_n[:, -1], features,\n",
        "                                 custom_cv, train_weights)\n",
        "  ml_preds = cyp_algeval.getMLPredictions(X_train, Y_train_full_n, X_test, Y_test_full_n,\n",
        "                                          cv_test_years, cyp_ftsel, features, log_fh)\n",
        "\n",
        "  if (use_yield_trend and predict_residuals):\n",
        "    ml_preds = cyp_algeval.yieldPredictionsFromResiduals(ml_preds['train'],\n",
        "                                                         Y_train_full[:, -1],\n",
        "                                                         ml_preds['test'],\n",
        "                                                         Y_test_full[:, -1],\n",
        "                                                         ml_preds['custom_cv'],\n",
        "                                                         cv_test_years)\n",
        "\n",
        "  return null_preds, ml_preds\n",
        "\n",
        "def getMachineLearningPredictions(cyp_config, pd_train_df, pd_test_df, log_fh):\n",
        "  \"\"\"Train and evaluate algorithms\"\"\"\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  alg_names = list(cyp_config.getEstimators().keys())\n",
        "  country_code = cyp_config.getCountryCode()\n",
        "\n",
        "  # register spark parallel backend\n",
        "  register_spark()\n",
        "\n",
        "  eval_info = '\\nTraining and Evaluation'\n",
        "  eval_info += '\\n-------------------------'\n",
        "  log_fh.write(eval_info)\n",
        "  if (debug_level > 1):\n",
        "    print(eval_info)\n",
        "\n",
        "  null_preds, ml_preds = getMLPredictionsOneModel(cyp_config, pd_train_df, pd_test_df, log_fh)\n",
        "\n",
        "  if (debug_level > 1):\n",
        "    sel_cols = ['IDREGION', 'FYEAR', 'YIELD'] + ['YIELD_PRED_' + alg for alg in alg_names]\n",
        "    print('\\n', ml_preds['test'][sel_cols].head(5))\n",
        "\n",
        "  null_preds_train = null_preds['train']\n",
        "  null_preds_cv = null_preds['custom_cv']\n",
        "  null_preds_test = null_preds['test']\n",
        "  ml_preds_train = ml_preds['train']\n",
        "  ml_preds_cv = ml_preds['custom_cv']\n",
        "  ml_preds_test = ml_preds['test']\n",
        "\n",
        "  # print per country evaluation summary\n",
        "  if (country_code is not None):\n",
        "    printAlgorithmsEvaluationSummary(cyp_config, null_preds, ml_preds, log_fh)\n",
        "    ml_preds_test['COUNTRY'] = country_code\n",
        "  else:\n",
        "    ml_preds_test['COUNTRY'] = ml_preds_test['IDREGION'].str[:2]\n",
        "    countries = ml_preds_test['COUNTRY'].unique()\n",
        "    for c in countries:\n",
        "      null_preds_country = {\n",
        "          'train' : null_preds_train[null_preds_train['IDREGION'].str[:2] == c],\n",
        "          'custom_cv' : null_preds_cv[null_preds_cv['IDREGION'].str[:2] == c],\n",
        "          'test' : null_preds_test[null_preds_test['IDREGION'].str[:2] == c]\n",
        "      }\n",
        "\n",
        "      ml_preds_country = {\n",
        "          'train' : ml_preds_train[ml_preds_train['IDREGION'].str[:2] == c],\n",
        "          'custom_cv' : ml_preds_cv[ml_preds_cv['IDREGION'].str[:2] == c],\n",
        "          'test' : ml_preds_test[ml_preds_test['COUNTRY'] == c]\n",
        "      }\n",
        "\n",
        "      printAlgorithmsEvaluationSummary(cyp_config, null_preds_country, ml_preds_country,\n",
        "                                       log_fh, c)\n",
        "\n",
        "  return ml_preds_test\n",
        "\n",
        "def saveMLPredictions(cyp_config, sqlCtx, pd_ml_predictions, iter=None):\n",
        "  \"\"\"Save ML predictions to a CSV file\"\"\"\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  crop = cyp_config.getCropName()\n",
        "  country = cyp_config.getCountryCode()\n",
        "  nuts_level = cyp_config.getNUTSLevel()\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "  early_season_end = cyp_config.getEarlySeasonEndDekad()\n",
        "  ml_algs = cyp_config.getEstimators()\n",
        "\n",
        "  output_path = cyp_config.getOutputPath()\n",
        "  output_file = getPredictionFilename(crop, use_yield_trend,\n",
        "                                      early_season_prediction, early_season_end,\n",
        "                                      country, nuts_level)\n",
        "\n",
        "  save_pred_path = output_path + '/' + output_file\n",
        "  if (iter is not None):\n",
        "     save_pred_path += '-' + str(iter)\n",
        "\n",
        "  if (debug_level > 1):\n",
        "    print('\\nSaving predictions to', save_pred_path + '.csv')\n",
        "    print(pd_ml_predictions.head(5))\n",
        "\n",
        "  pd_ml_predictions.to_csv(save_pred_path + '.csv', index=False, header=True)\n",
        "\n",
        "  # NOTE: In some environments, Spark can write, but pandas cannot.\n",
        "  # In such cases, use the following code.\n",
        "  # spark_predictions_df = sqlCtx.createDataFrame(pd_ml_predictions)\n",
        "  # spark_predictions_df.coalesce(1)\\\n",
        "  #                     .write.option('header','true')\\\n",
        "  #                     .mode(\"overwrite\").csv(save_pred_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jDs9IO_xfPu"
      },
      "source": [
        "### Load Saved Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOFLzsa8xjT-"
      },
      "outputs": [],
      "source": [
        "#%%writefile load_saved_predictions.py\n",
        "import pandas as pd\n",
        "\n",
        "def loadSavedPredictions(cyp_config, spark):\n",
        "  \"\"\"Load machine learning predictions from saved CSV file\"\"\"\n",
        "  crop = cyp_config.getCropName()\n",
        "  country = cyp_config.getCountryCode()\n",
        "  nuts_level = cyp_config.getNUTSLevel()\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "  early_season_end = cyp_config.getEarlySeasonEndDekad()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "\n",
        "  pred_file_path = cyp_config.getOutputPath()\n",
        "  pred_file = getPredictionFilename(crop, use_yield_trend,\n",
        "                                    early_season_prediction, early_season_end,\n",
        "                                    country, nuts_level)\n",
        "  pred_file += '.csv'\n",
        "  pd_ml_predictions = pd.read_csv(pred_file_path + '/' + pred_file, header=0)\n",
        "\n",
        "  # NOTE: In some environments, Spark can read, but pandas cannot.\n",
        "  # In such cases, use the following code.\n",
        "  # all_pred_df = spark.read.csv(pred_file_path + '/' + pred_file, header=True, inferSchema=True)\n",
        "  # pd_ml_predictions = all_pred_df.toPandas()\n",
        "\n",
        "  if (debug_level > 1):\n",
        "    print(pd_ml_predictions.head(5))\n",
        "\n",
        "  return pd_ml_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uvKP54QxrUK"
      },
      "source": [
        "### Compare Predictions with MCYFS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj_ZCXfaxuxt"
      },
      "outputs": [],
      "source": [
        "#%%writefile compare_with_mcyfs.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def saveNUTS0Predictions(cyp_config, sqlCtx, nuts0_ml_predictions):\n",
        "  \"\"\"Save predictions aggregated to NUTS0\"\"\"\n",
        "  crop = cyp_config.getCropName()\n",
        "  country = cyp_config.getCountryCode()\n",
        "  nuts_level = 'NUTS0'\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "  early_season_end = cyp_config.getEarlySeasonEndDekad()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "\n",
        "  output_path = cyp_config.getOutputPath()\n",
        "  output_file = getPredictionFilename(crop, use_yield_trend,\n",
        "                                      early_season_prediction, early_season_end,\n",
        "                                      country, nuts_level)\n",
        "\n",
        "  save_pred_path = output_path + '/' + output_file\n",
        "  if (debug_level > 1):\n",
        "    print('\\nNUTS0 Predictions of ML algorithms')\n",
        "    print(nuts0_ml_predictions.head(5).to_string(index=False))\n",
        "    print('\\nSaving predictions to', save_pred_path + '.csv')\n",
        "\n",
        "  nuts0_ml_predictions.to_csv(save_pred_path + '.csv', index=False, header=True)\n",
        "\n",
        "  # NOTE: In some environments, Spark can write, but pandas cannot.\n",
        "  # In such cases, use the following code.\n",
        "  # spark_predictions_df = sqlCtx.createDataFrame(nuts0_ml_predictions)\n",
        "  # spark_predictions_df.coalesce(1)\\\n",
        "  #                     .write.option('header','true')\\\n",
        "  #                     .mode(\"overwrite\").csv(save_pred_path)\n",
        "\n",
        "def getDataForMCYFSComparison(spark, cyp_config, pd_ml_predictions, test_years):\n",
        "  \"\"\"Load and preprocess data for MCYFS comparison\"\"\"\n",
        "  data_path = cyp_config.getDataPath()\n",
        "  crop_id = cyp_config.getCropID()\n",
        "  nuts_level = cyp_config.getNUTSLevel()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  country_code = cyp_config.getCountryCode()\n",
        "  season_crosses_calyear = cyp_config.seasonCrossesCalendarYear()\n",
        "  early_season_end = cyp_config.getEarlySeasonEndDekad()\n",
        "\n",
        "  if (country_code is None):\n",
        "    countries = pd_ml_predictions['COUNTRY'].unique()\n",
        "    country_nuts = {}\n",
        "    for c in countries:\n",
        "      pd_ml_country_preds = pd_ml_predictions[pd_ml_predictions['IDREGION'].str[:2] == c]\n",
        "      first_reg = pd_ml_country_preds['IDREGION'].iloc[0]\n",
        "      nuts = 'NUTS' + str(len(first_reg[2:]))\n",
        "      country_nuts[c] = nuts\n",
        "  else:\n",
        "    country_nuts = { country_code : nuts_level }\n",
        "\n",
        "  if (run_tests):\n",
        "    test_util = TestUtil(spark)\n",
        "    test_util.runAllTests()\n",
        "\n",
        "  print('\\n##############')\n",
        "  print('# Load Data  #')\n",
        "  print('##############')\n",
        "\n",
        "  if (run_tests):\n",
        "    test_loader = TestDataLoader(spark)\n",
        "    test_loader.runAllTests()\n",
        "\n",
        "  data_sources = ['WOFOST', 'AREA_FRACTIONS', 'YIELD', 'YIELD_PRED_MCYFS']\n",
        "  cyp_config.setDataSources(data_sources)\n",
        "  cyp_loader = CYPDataLoader(spark, cyp_config)\n",
        "\n",
        "  lowest_nuts = 0\n",
        "  for c in country_nuts:\n",
        "    nuts_level_int = int(country_nuts[c][-1])\n",
        "    if (nuts_level_int > lowest_nuts):\n",
        "      lowest_nuts = nuts_level_int\n",
        "\n",
        "  area_nuts = ['NUTS' + str(i) for i in range(lowest_nuts, 0, -1)]\n",
        "  area_fraction_dfs = cyp_loader.loadData('AREA_FRACTIONS', area_nuts)\n",
        "  wofost_df = cyp_loader.loadData('WOFOST', nuts_level)\n",
        "  wofost_df = wofost_df.filter(wofost_df['CROP_ID'] == crop_id).drop('CROP_ID')\n",
        "  nuts0_yield_df = cyp_loader.loadData('YIELD', 'NUTS0')\n",
        "  mcyfs_yield_df = cyp_loader.loadData('YIELD_PRED_MCYFS', 'NUTS0')\n",
        "\n",
        "  print('\\n####################')\n",
        "  print('# Preprocess Data  #')\n",
        "  print('####################')\n",
        "\n",
        "  if (run_tests):\n",
        "    test_preprocessor = TestDataPreprocessor(spark)\n",
        "    test_preprocessor.runAllTests()\n",
        "\n",
        "  if (run_tests):\n",
        "    test_summarizer = TestDataSummarizer(spark)\n",
        "    test_summarizer.runAllTests()\n",
        "\n",
        "  cyp_preprocessor = CYPDataPreprocessor(spark, cyp_config)\n",
        "  cyp_summarizer = CYPDataSummarizer(cyp_config)\n",
        "\n",
        "  country_area_dfs = {}\n",
        "  country_dvs_summaries = {}\n",
        "  country_nuts0_yields = {}\n",
        "  country_mcyfs_yields = {}\n",
        "  for c in country_nuts:\n",
        "    nuts_level_int = int(country_nuts[c][-1])\n",
        "    sel_af_dfs = area_fraction_dfs[-nuts_level_int:]\n",
        "    for i in range(len(sel_af_dfs)):\n",
        "      nuts_af_df = cyp_preprocessor.preprocessAreaFractions(sel_af_dfs[i], crop_id)\n",
        "      nuts_af_df = nuts_af_df.filter(SparkF.substring(nuts_af_df['IDREGION'], 1, 2) == c)\n",
        "      nuts_af_df = nuts_af_df.filter(nuts_af_df['FYEAR'].isin(test_years[c]))\n",
        "      sel_af_dfs[i] = nuts_af_df\n",
        "\n",
        "    country_area_dfs[c] = sel_af_dfs\n",
        "\n",
        "    # DVS summary with crop season information\n",
        "    sel_wofost_df = wofost_df.filter(SparkF.substring(wofost_df['IDREGION'], 1, 2) == c)\n",
        "    crop_season = cyp_preprocessor.getCropSeasonInformation(sel_wofost_df, season_crosses_calyear)\n",
        "    sel_wofost_df = cyp_preprocessor.preprocessWofost(sel_wofost_df, crop_season, season_crosses_calyear)\n",
        "    dvs_summary = cyp_summarizer.wofostDVSSummary(sel_wofost_df, early_season_end)\n",
        "    dvs_summary = dvs_summary.filter(dvs_summary['CAMPAIGN_YEAR'].isin(test_years[c]))\n",
        "    country_dvs_summaries[c] = dvs_summary\n",
        "\n",
        "    # NUTS0 Yield and MCYFS predictions\n",
        "    sel_nuts0_yield_df = nuts0_yield_df.filter(SparkF.substring(nuts0_yield_df['IDREGION'], 1, 2) == c)\n",
        "    if (debug_level > 1):\n",
        "      print('NUTS0 Yield for', c, 'before preprocessing')\n",
        "      sel_nuts0_yield_df.show(10)\n",
        "\n",
        "    sel_nuts0_yield_df = cyp_preprocessor.preprocessYield(sel_nuts0_yield_df, crop_id)\n",
        "    sel_nuts0_yield_df = sel_nuts0_yield_df.filter(sel_nuts0_yield_df['FYEAR'].isin(test_years[c]))\n",
        "    if (debug_level > 1):\n",
        "      print('NUTS0 Yield for', c, 'after preprocessing')\n",
        "      sel_nuts0_yield_df.show(10)\n",
        "\n",
        "    sel_mcyfs_yield_df = mcyfs_yield_df.filter(SparkF.substring(mcyfs_yield_df['IDREGION'], 1, 2) == c)\n",
        "    if (debug_level > 1):\n",
        "      print('MCYFS yield predictions for', c, 'before preprocessing')\n",
        "      sel_mcyfs_yield_df.show(10)\n",
        "\n",
        "    sel_mcyfs_yield_df = cyp_preprocessor.preprocessYieldMCYFS(sel_mcyfs_yield_df, crop_id)\n",
        "    sel_mcyfs_yield_df = sel_mcyfs_yield_df.filter(sel_mcyfs_yield_df['FYEAR'].isin(test_years[c]))\n",
        "    if (debug_level > 1):\n",
        "      print('MCYFS yield predictions for', c, 'after preprocessing')\n",
        "      sel_mcyfs_yield_df.show(10)\n",
        "\n",
        "    # Check we have yield data for crop\n",
        "    assert (sel_nuts0_yield_df is not None)\n",
        "    assert (sel_mcyfs_yield_df is not None)\n",
        "\n",
        "    country_nuts0_yields[c] = sel_nuts0_yield_df\n",
        "    country_mcyfs_yields[c] = sel_mcyfs_yield_df\n",
        "\n",
        "  data_dfs = {\n",
        "      'WOFOST_DVS' : country_dvs_summaries,\n",
        "      'AREA_FRACTIONS' : country_area_dfs,\n",
        "      'YIELD_NUTS0' : country_nuts0_yields,\n",
        "      'YIELD_PRED_MCYFS' : country_mcyfs_yields,\n",
        "  }\n",
        "\n",
        "  return data_dfs\n",
        "\n",
        "def fillMissingDataWithAverage(pd_pred_df, print_debug):\n",
        "  \"\"\"Fill missing data with regional average or zero\"\"\"\n",
        "  regions = pd_pred_df['IDREGION'].unique()\n",
        "\n",
        "  for reg_id in regions:\n",
        "    reg_filter = (pd_pred_df['IDREGION'] == reg_id)\n",
        "    pd_reg_pred_df = pd_pred_df[reg_filter]\n",
        "\n",
        "    if (len(pd_reg_pred_df[pd_reg_pred_df['YIELD_PRED'].notnull()].index) == 0):\n",
        "      if (print_debug):\n",
        "        print('No data for', reg_id)\n",
        "\n",
        "      pd_pred_df.loc[reg_filter, 'FRACTION'] = 0.0\n",
        "      pd_pred_df.loc[reg_filter, 'YIELD_PRED'] = 0.0\n",
        "    else:\n",
        "      reg_avg_yield_pred = pd_pred_df.loc[reg_filter, 'YIELD_PRED'].mean()\n",
        "      pd_pred_df.loc[reg_filter, 'YIELD_PRED'] = pd_pred_df.loc[reg_filter, 'YIELD_PRED']\\\n",
        "                                                           .fillna(reg_avg_yield_pred)\n",
        "\n",
        "  return pd_pred_df\n",
        "\n",
        "def recalculateAreaFractions(pd_pred_df, print_debug):\n",
        "  \"\"\"Recalculate area fractions by excluding regions with missing data\"\"\"\n",
        "  join_cols = ['IDREG_PARENT', 'FYEAR']\n",
        "  pd_af_sum = pd_pred_df.groupby(join_cols).agg(FRACTION_SUM=('FRACTION', 'sum')).reset_index()\n",
        "  pd_pred_df = pd_pred_df.merge(pd_af_sum, on=join_cols, how='left')\n",
        "  pd_pred_df['FRACTION'] = pd_pred_df['FRACTION'] / pd_pred_df['FRACTION_SUM']\n",
        "  pd_pred_df = pd_pred_df.drop(columns=['FRACTION_SUM'])\n",
        "\n",
        "  return pd_pred_df\n",
        "\n",
        "def aggregatePredictionsToNUTS0(cyp_config, pd_ml_predictions,\n",
        "                                area_dfs, test_years, join_cols):\n",
        "  \"\"\"Aggregate regional predictions to national level\"\"\"\n",
        "  alg_names = list(cyp_config.getEstimators().keys())\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "\n",
        "  pd_area_dfs = []\n",
        "  for af_df in area_dfs:\n",
        "    pd_af_df = af_df.toPandas()\n",
        "    pd_af_df = pd_af_df[pd_af_df['FYEAR'].isin(test_years)]\n",
        "    pd_area_dfs.append(pd_af_df)\n",
        "\n",
        "  nuts0_pred_df = None\n",
        "  for alg in alg_names:\n",
        "    sel_cols = ['IDREGION', 'FYEAR', 'YIELD_PRED_' + alg]\n",
        "    pd_alg_pred_df = pd_ml_predictions[sel_cols]\n",
        "    pd_alg_pred_df = pd_alg_pred_df.rename(columns={'YIELD_PRED_' + alg : 'YIELD_PRED'})\n",
        "\n",
        "    for idx in range(len(pd_area_dfs)):\n",
        "      pd_af_df = pd_area_dfs[idx]\n",
        "      # merge with area fractions to get all regions and years\n",
        "      pd_alg_pred_df = pd_af_df.merge(pd_alg_pred_df, on=join_cols)\n",
        "      print_debug = (debug_level > 2) and (alg == alg_names[0])\n",
        "      pd_alg_pred_df = fillMissingDataWithAverage(pd_alg_pred_df, print_debug)\n",
        "      pd_alg_pred_df['IDREG_PARENT'] = pd_alg_pred_df['IDREGION'].str[:-1]\n",
        "      pd_alg_pred_df = recalculateAreaFractions(pd_alg_pred_df, print_debug)\n",
        "      if (print_debug):\n",
        "        print('\\nAggregation to NUTS' + str(len(pd_area_dfs) - (idx + 1)))\n",
        "        print(pd_alg_pred_df[pd_alg_pred_df['FYEAR'] == test_years[0]].head(10))\n",
        "\n",
        "      pd_alg_pred_df['YPRED_WEIGHTED'] = pd_alg_pred_df['YIELD_PRED'] * pd_alg_pred_df['FRACTION']\n",
        "      pd_alg_pred_df = pd_alg_pred_df.groupby(by=['IDREG_PARENT', 'FYEAR'])\\\n",
        "                                     .agg(YPRED_WEIGHTED=('YPRED_WEIGHTED', 'sum')).reset_index()\n",
        "      pd_alg_pred_df = pd_alg_pred_df.rename(columns={'IDREG_PARENT': 'IDREGION',\n",
        "                                                      'YPRED_WEIGHTED': 'YIELD_PRED' })\n",
        "\n",
        "    pd_alg_pred_df = pd_alg_pred_df.rename(columns={ 'YIELD_PRED': 'YIELD_PRED_' + alg })\n",
        "    if (nuts0_pred_df is None):\n",
        "      nuts0_pred_df = pd_alg_pred_df\n",
        "    else:\n",
        "      nuts0_pred_df = nuts0_pred_df.merge(pd_alg_pred_df, on=join_cols)\n",
        "\n",
        "  return nuts0_pred_df\n",
        "\n",
        "def getMCYFSPrediction(pd_mcyfs_pred_df, pred_year, pred_dekad, print_debug):\n",
        "  \"\"\"Get MCYFS prediction for given year with prediction date close to pred_dekad\"\"\"\n",
        "  pd_pred_year = pd_mcyfs_pred_df[pd_mcyfs_pred_df['FYEAR'] == pred_year]\n",
        "  mcyfs_pred_dekads = pd_pred_year['PRED_DEKAD'].unique()\n",
        "  if (len(mcyfs_pred_dekads) == 0):\n",
        "    return 0.0\n",
        "\n",
        "  mcyfs_pred_dekads = sorted(mcyfs_pred_dekads)\n",
        "  mcyfs_pred_dekad = mcyfs_pred_dekads[-1]\n",
        "  if (pred_dekad < mcyfs_pred_dekad):\n",
        "    for dek in mcyfs_pred_dekads:\n",
        "      if dek >= pred_dekad:\n",
        "        mcyfs_pred_dekad = dek\n",
        "        break\n",
        "\n",
        "  pd_pred_dek = pd_pred_year[pd_pred_year['PRED_DEKAD'] == mcyfs_pred_dekad]\n",
        "  yield_pred_list = pd_pred_dek['YIELD_PRED'].values\n",
        "\n",
        "  if (print_debug):\n",
        "    print('\\nAll MCYFS dekads for', pred_year, ':', mcyfs_pred_dekads)\n",
        "    print('MCYFS prediction dekad', mcyfs_pred_dekad)\n",
        "    print('ML Baseline prediction dekad', pred_dekad)\n",
        "    print('MCYFS prediction:', yield_pred_list[0], '\\n')\n",
        "\n",
        "  return yield_pred_list[0]\n",
        "\n",
        "def getNUTS0Yield(pd_nuts0_yield_df, pred_year, print_debug):\n",
        "  \"\"\"Get the true (reported) Eurostat yield value\"\"\"\n",
        "  nuts0_yield_year = pd_nuts0_yield_df[pd_nuts0_yield_df['FYEAR'] == pred_year]\n",
        "  pred_year_yield = nuts0_yield_year['YIELD'].values\n",
        "  if (len(pred_year_yield) == 0):\n",
        "    return 0.0\n",
        "\n",
        "  if (print_debug):\n",
        "    print(pred_year, 'Eurostat yield', pred_year_yield[0])\n",
        "\n",
        "  return pred_year_yield[0]\n",
        "\n",
        "def comparePredictionsWithMCYFS(sqlCtx, cyp_config, pd_ml_predictions, log_fh):\n",
        "  \"\"\"Compare ML Baseline predictions with MCYFS predictions\"\"\"\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  alg_names = list(cyp_config.getEstimators().keys())\n",
        "  metrics = cyp_config.getEvaluationMetrics()\n",
        "  # sometimes spark complains about int64 data types\n",
        "  test_years = {}\n",
        "  countries = sorted(pd_ml_predictions['COUNTRY'].unique())\n",
        "  for cn in countries:\n",
        "    pd_cn_predictions = pd_ml_predictions[pd_ml_predictions['COUNTRY'] == cn]\n",
        "    test_years[cn] = [int(yr) for yr in pd_cn_predictions['FYEAR'].unique()]\n",
        "  early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "\n",
        "  spark = sqlCtx.sparkSession\n",
        "  # We need WOFOST summary, AREA_FRACTIONS, MCYFS yield predictions\n",
        "  # and NUTS0 Eurostat YIELD for comparison with MCYFS\n",
        "  data_dfs = getDataForMCYFSComparison(spark, cyp_config, pd_ml_predictions, test_years)\n",
        "  dvs_summaries = data_dfs['WOFOST_DVS']\n",
        "  area_fraction_dfs = data_dfs['AREA_FRACTIONS']\n",
        "  nuts0_yield_dfs = data_dfs['YIELD_NUTS0']\n",
        "  mcyfs_yield_dfs = data_dfs['YIELD_PRED_MCYFS']\n",
        "\n",
        "  nuts0_preds_combined_df = None\n",
        "  for country in countries:\n",
        "    pd_dvs_summary = dvs_summaries[country].toPandas()\n",
        "    pd_nuts0_yield_df = nuts0_yield_dfs[country].toPandas()\n",
        "    pd_mcyfs_pred_df = mcyfs_yield_dfs[country].toPandas()\n",
        "    area_dfs = area_fraction_dfs[country]\n",
        "\n",
        "    join_cols = ['IDREGION', 'FYEAR']\n",
        "    pd_country_ml_preds = pd_ml_predictions[pd_ml_predictions['COUNTRY'] == country]\n",
        "    pd_country_ml_preds = pd_country_ml_preds.drop(columns=['COUNTRY'])\n",
        "    nuts0_pred_df = aggregatePredictionsToNUTS0(cyp_config, pd_country_ml_preds,\n",
        "                                                area_dfs, test_years[country], join_cols)\n",
        "\n",
        "    crop_season_cols = ['IDREGION', 'CAMPAIGN_YEAR', 'CALENDAR_END_SEASON', 'CALENDAR_EARLY_SEASON']\n",
        "    pd_dvs_summary = pd_dvs_summary[crop_season_cols].rename(columns={ 'CAMPAIGN_YEAR' : 'FYEAR' })\n",
        "    pd_dvs_summary = pd_dvs_summary.groupby('FYEAR').agg(END_SEASON=('CALENDAR_END_SEASON', 'mean'),\n",
        "                                                         EARLY_SEASON=('CALENDAR_EARLY_SEASON', 'mean'))\\\n",
        "                                                         .round(0).reset_index()\n",
        "    if (debug_level > 1):\n",
        "      print(pd_dvs_summary.head(5).to_string(index=False))\n",
        "\n",
        "    alg_summary = {}\n",
        "    Y_pred_mcyfs = []\n",
        "    Y_true = []\n",
        "    nuts0_pred_df['YIELD_PRED_MCYFS'] = 0.0\n",
        "    nuts0_pred_df['YIELD'] = 0.0\n",
        "    nuts0_pred_df = nuts0_pred_df.sort_values(by=join_cols)\n",
        "    ml_pred_years = nuts0_pred_df['FYEAR'].unique()\n",
        "    mcyfs_pred_years = []\n",
        "    print_debug = (debug_level > 2)\n",
        "    if (print_debug):\n",
        "      print('\\nPredictions and true values for', country)\n",
        "\n",
        "    for yr in ml_pred_years:\n",
        "      pred_dekad = pd_dvs_summary[pd_dvs_summary['FYEAR'] == yr]['END_SEASON'].values[0]\n",
        "      if (early_season_prediction):\n",
        "        pred_dekad = pd_dvs_summary[pd_dvs_summary['FYEAR'] == yr]['EARLY_SEASON'].values[0]\n",
        "\n",
        "      mcyfs_pred = getMCYFSPrediction(pd_mcyfs_pred_df, yr, pred_dekad, print_debug)\n",
        "      nuts0_yield = getNUTS0Yield(pd_nuts0_yield_df, yr, print_debug)\n",
        "      if ((mcyfs_pred > 0.0) and (nuts0_yield > 0.0)):\n",
        "        nuts0_pred_df.loc[nuts0_pred_df['FYEAR'] == yr, 'YIELD'] = nuts0_yield\n",
        "        nuts0_pred_df.loc[nuts0_pred_df['FYEAR'] == yr, 'YIELD_PRED_MCYFS'] = mcyfs_pred\n",
        "        mcyfs_pred_years.append(yr)\n",
        "\n",
        "    nuts0_pred_df = nuts0_pred_df[nuts0_pred_df['FYEAR'].isin(mcyfs_pred_years)]\n",
        "    Y_true = nuts0_pred_df['YIELD'].values\n",
        "    if (print_debug):\n",
        "      print(nuts0_pred_df.head(5))\n",
        "\n",
        "    if (nuts0_preds_combined_df is None):\n",
        "      nuts0_preds_combined_df = nuts0_pred_df\n",
        "    else:\n",
        "      nuts0_preds_combined_df = nuts0_preds_combined_df.append(nuts0_pred_df)\n",
        "\n",
        "    if (len(mcyfs_pred_years) > 0):\n",
        "      for alg in alg_names:\n",
        "        Y_pred_alg = nuts0_pred_df['YIELD_PRED_' + alg].values\n",
        "        alg_nuts0_scores = getPredictionScores(Y_true, Y_pred_alg, metrics)\n",
        "\n",
        "        alg_row = [alg]\n",
        "        for met in alg_nuts0_scores:\n",
        "          alg_row.append(alg_nuts0_scores[met])\n",
        "\n",
        "        alg_index = len(alg_summary)\n",
        "        alg_summary['row' + str(alg_index)] = alg_row\n",
        "\n",
        "      Y_pred_mcyfs = nuts0_pred_df['YIELD_PRED_MCYFS'].values\n",
        "      mcyfs_nuts0_scores = getPredictionScores(Y_true, Y_pred_mcyfs, metrics)\n",
        "      alg_row = ['MCYFS_Predictions']\n",
        "      for met in mcyfs_nuts0_scores:\n",
        "        alg_row.append(mcyfs_nuts0_scores[met])\n",
        "\n",
        "      alg_index = len(alg_summary)\n",
        "      alg_summary['row' + str(alg_index)] = alg_row\n",
        "\n",
        "    alg_df_columns = ['algorithm']\n",
        "    for met in metrics:\n",
        "      alg_df_columns += ['test_' + met]\n",
        "\n",
        "    alg_df = pd.DataFrame.from_dict(alg_summary, orient='index',\n",
        "                                    columns=alg_df_columns)\n",
        "    eval_summary_info = '\\nAlgorithm Evaluation Summary (NUTS0) for ' + country\n",
        "    eval_summary_info += '\\n-------------------------------------------'\n",
        "    eval_summary_info += '\\n' + alg_df.to_string(index=False) + '\\n'\n",
        "    log_fh.write(eval_summary_info)\n",
        "    print(eval_summary_info)\n",
        "\n",
        "  save_predictions = cyp_config.savePredictions()\n",
        "  if (save_predictions):\n",
        "    saveNUTS0Predictions(cyp_config, sqlCtx, nuts0_preds_combined_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7FqdmXOjYUm"
      },
      "source": [
        "## Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPwEyntUYHsS"
      },
      "source": [
        "### Test Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bx6fS731YHsV"
      },
      "outputs": [],
      "source": [
        "#%%writefile test_util.py\n",
        "import numpy as np\n",
        "\n",
        "class TestUtil():\n",
        "  def __init__(self, spark):\n",
        "    self.good_date = spark.createDataFrame([(1, '19940102'),\n",
        "                                          (2, '15831224')],\n",
        "                                         ['ID', 'DATE'])\n",
        "    self.bad_date = spark.createDataFrame([(1, '14341224'),\n",
        "                                           (2, '12345678'),\n",
        "                                          (3, '123-12-24')],\n",
        "                                         ['ID', 'DATE'])\n",
        "\n",
        "  def testDateFormat(self):\n",
        "    print('\\n Test Date Format')\n",
        "    self.good_date = self.good_date.withColumn('FYEAR', getYear('DATE'))\n",
        "    self.good_date.show()\n",
        "    self.bad_date = self.bad_date.withColumn('FYEAR', getYear('DATE'))\n",
        "    self.bad_date.show()\n",
        "    assert (self.bad_date.filter(self.bad_date.FYEAR.isNull()).count() == 2)\n",
        "    self.bad_date = self.bad_date.withColumn('MONTH', getMonth('DATE'))\n",
        "    self.bad_date.show()\n",
        "    assert (self.bad_date.filter(self.bad_date.MONTH.isNull()).count() == 2)\n",
        "    self.bad_date = self.bad_date.withColumn('DAY', getDay('DATE'))\n",
        "    # check the day here for first date, it's incorrect\n",
        "    # seems to be a Spark issue\n",
        "    self.bad_date.show()\n",
        "    assert (self.bad_date.filter(self.bad_date.DAY.isNull()).count() == 2)\n",
        "    self.bad_date = self.bad_date.withColumn('DEKAD', getDekad('DATE'))\n",
        "    self.bad_date.show()\n",
        "    assert (self.bad_date.filter(self.bad_date.DEKAD.isNull()).count() == 2)\n",
        "\n",
        "  def testGetYear(self):\n",
        "    print('\\n Test getYear')\n",
        "    self.good_date = self.good_date.withColumn('FYEAR', getYear('DATE'))\n",
        "    self.good_date.show()\n",
        "    year1 = self.good_date.filter(self.good_date.ID == 1).select('FYEAR').collect()[0][0]\n",
        "    assert year1 == 1994\n",
        "    year2 = self.good_date.filter(self.good_date.ID == 2).select('FYEAR').collect()[0][0]\n",
        "    assert year2 == 1583\n",
        "\n",
        "  def testGetMonth(self):\n",
        "    print('\\n Test getMonth')\n",
        "    self.good_date = self.good_date.withColumn('MONTH', getMonth('DATE'))\n",
        "    self.good_date.show()\n",
        "    month1 = self.good_date.filter(self.good_date.ID == 1).select('MONTH').collect()[0][0]\n",
        "    assert month1 == 1\n",
        "    month2 = self.good_date.filter(self.good_date.ID == 2).select('MONTH').collect()[0][0]\n",
        "    assert month2 == 12\n",
        "\n",
        "  def testGetDay(self):\n",
        "    print('\\n Test getDay')\n",
        "    self.good_date = self.good_date.withColumn('DAY', getDay('DATE'))\n",
        "    self.good_date.show()\n",
        "    day1 = self.good_date.filter(self.good_date.ID == 1).select('DAY').collect()[0][0]\n",
        "    assert day1 == 2\n",
        "    day2 = self.good_date.filter(self.good_date.ID == 2).select('DAY').collect()[0][0]\n",
        "    assert day2 == 24\n",
        "\n",
        "  def testGetDekad(self):\n",
        "    print('\\n Test getDekad')\n",
        "    self.good_date = self.good_date.withColumn('DEKAD', getDekad('DATE'))\n",
        "    self.good_date.show()\n",
        "    dekad1 = self.good_date.filter(self.good_date.ID == 1).select('DEKAD').collect()[0][0]\n",
        "    assert dekad1 == 1\n",
        "    dekad2 = self.good_date.filter(self.good_date.ID == 2).select('DEKAD').collect()[0][0]\n",
        "    assert dekad2 == 36\n",
        "\n",
        "  def testCropIDToName(self):\n",
        "    print('\\n Test cropIDToName')\n",
        "    crop_name = cropIDToName(crop_name_dict, 6)\n",
        "    print(6, ':' + crop_name)\n",
        "    assert crop_name == 'sugarbeet'\n",
        "    crop_name = cropIDToName(crop_name_dict, 8)\n",
        "    print(8, ':' + crop_name)\n",
        "    assert crop_name == 'NA'\n",
        "\n",
        "  def testCropNameToID(self):\n",
        "    print('\\n Test cropNameToID')\n",
        "    crop_id = cropNameToID(crop_id_dict, 'Potatoes')\n",
        "    print('Potatoes:', crop_id)\n",
        "    assert crop_id == 7\n",
        "    crop_id = cropNameToID(crop_id_dict, 'Soybean')\n",
        "    print('Soybean:', crop_id)\n",
        "    assert crop_id == 0\n",
        "\n",
        "  def testPrintInGroups(self):\n",
        "    print('\\n Test printInGroups')\n",
        "    features = ['feat' + str(i+1) for i in range(15)]\n",
        "    num_features = len(features)\n",
        "    num_half = np.cast['int64'](np.floor(num_features/2))\n",
        "    indices1 = [ i for i in range(num_features)]\n",
        "    indices2 = [ 2*i for i in range(num_half)]\n",
        "    indices3 = [ (2*i + 1) for i in range(num_half)]\n",
        "\n",
        "    printInGroups(features, indices1)\n",
        "    printInGroups(features, indices2)\n",
        "    printInGroups(features, indices3)\n",
        "\n",
        "  def testPlotTrend(self):\n",
        "    print('\\n Test plotTrend')\n",
        "    years = [yr for yr in range(2000, 2010)]\n",
        "    trend_values = [ (i + 1) for i in range(50, 60)]\n",
        "    actual_values = []\n",
        "    for tval in trend_values:\n",
        "      if (tval % 2) == 0:\n",
        "        actual_values.append(tval + 0.5)\n",
        "      else:\n",
        "        actual_values.append(tval - 0.5)\n",
        "\n",
        "    plotTrend(years, actual_values, trend_values, 'YIELD')\n",
        "\n",
        "  def testPlotTrueVSPredicted(self):\n",
        "    print('\\n Test plotTrueVSPredicted')\n",
        "    Y_true = [ (i + 1) for i in range(50, 60)]\n",
        "    Y_predicted = []\n",
        "    for tval in Y_true:\n",
        "      if (tval % 2) == 0:\n",
        "        Y_predicted.append(tval + 0.5)\n",
        "      else:\n",
        "        Y_predicted.append(tval - 0.5)\n",
        "\n",
        "    Y_true = np.asarray(Y_true)\n",
        "    Y_predicted = np.asarray(Y_predicted)\n",
        "\n",
        "    plotTrueVSPredicted(Y_true, Y_predicted)\n",
        "\n",
        "  def runAllTests(self):\n",
        "    print('\\nTest Utility Functions BEGIN\\n')\n",
        "    self.testDateFormat()\n",
        "    self.testGetYear()\n",
        "    self.testGetMonth()\n",
        "    self.testGetDay()\n",
        "    self.testGetDekad()\n",
        "    self.testCropIDToName()\n",
        "    self.testCropNameToID()\n",
        "    self.testPrintInGroups()\n",
        "    self.testPlotTrend()\n",
        "    self.testPlotTrueVSPredicted()\n",
        "    print('\\nTest Utility Functions END\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iW23SQzYN09"
      },
      "source": [
        "### Test Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWKTClTNYN0_"
      },
      "outputs": [],
      "source": [
        "#%%writefile test_data_loading.py\n",
        "class TestDataLoader():\n",
        "  def __init__(self, spark):\n",
        "    cyp_config = CYPConfiguration()\n",
        "    self.nuts_level = cyp_config.getNUTSLevel()\n",
        "    data_sources = { 'SOIL' : self.nuts_level }\n",
        "    cyp_config.setDataSources(data_sources)\n",
        "    cyp_config.setDebugLevel(2)\n",
        "\n",
        "    self.data_loader = CYPDataLoader(spark, cyp_config)\n",
        "\n",
        "  def testDataLoad(self):\n",
        "    print('\\nTest loadData, loadAllData')\n",
        "    soil_df = self.data_loader.loadData('SOIL', self.nuts_level)\n",
        "    assert soil_df is not None\n",
        "    soil_df.show(5)\n",
        "\n",
        "    all_dfs = self.data_loader.loadAllData()\n",
        "    soil_df = all_dfs['SOIL']\n",
        "    assert soil_df is not None\n",
        "    soil_df.show(5)\n",
        "\n",
        "  def runAllTests(self):\n",
        "    print('\\nTest Data Loader BEGIN\\n')\n",
        "    self.testDataLoad()\n",
        "    print('\\nTest Data Loader END\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vil46Q08YQ9v"
      },
      "source": [
        "### Test Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNZQd9kEYQ9w"
      },
      "outputs": [],
      "source": [
        "#%%writefile test_data_preprocessing.py\n",
        "class TestDataPreprocessor():\n",
        "  def __init__(self, spark):\n",
        "    cyp_config = CYPConfiguration()\n",
        "    cyp_config.setDebugLevel(2)\n",
        "    self.preprocessor = CYPDataPreprocessor(spark, cyp_config)\n",
        "\n",
        "    # create a small wofost data set\n",
        "    # preprocessing currently extracts the year and dekad only\n",
        "    self.wofost_df = spark.createDataFrame([(6, 'NL11', '19790110', 0.0, 0),\n",
        "                                            (6, 'NL11', '19790121', 0.0, 0),\n",
        "                                            (6, 'NL11', '19790331', 0.0, 50),\n",
        "                                            (6, 'NL11', '19790510', 0.0, 100),\n",
        "                                            (6, 'NL11', '19790821', 0.0, 150),\n",
        "                                            (6, 'NL11', '19790831', 0.0, 201),\n",
        "                                            (6, 'NL11', '19790910', 0.0, 201),\n",
        "                                            (6, 'NL11', '19800110', 0.0, 0),\n",
        "                                            (6, 'NL11', '19800121', 0.0, 0),\n",
        "                                            (6, 'NL11', '19800331', 0.0, 50),\n",
        "                                            (6, 'NL11', '19800610', 0.0, 100),\n",
        "                                            (6, 'NL11', '19800721', 0.0, 150),\n",
        "                                            (6, 'NL11', '19800831', 0.0, 200),\n",
        "                                            (6, 'NL11', '19800910', 0.0, 201),\n",
        "                                            (6, 'NL11', '19800921', 0.0, 201)],\n",
        "                                           ['CROP_ID', 'IDREGION', 'DATE', 'POT_YB', 'DVS'])\n",
        "\n",
        "    self.crop_season = None\n",
        "\n",
        "    # Create a small meteo dekadal data set\n",
        "    # Preprocessing currently extracts the year and dekad, and computes climate\n",
        "    # water balance.\n",
        "    self.meteo_dekdf = spark.createDataFrame([('NL11', '19790110', 1.2, 0.2),\n",
        "                                              ('NL11', '19790121', 2.1, 0.2),\n",
        "                                              ('NL11', '19790131', 0.2, 1.2),\n",
        "                                              ('NL11', '19790210', 0.1, 2.0),\n",
        "                                              ('NL11', '19790221', 0.0, 2.1),\n",
        "                                              ('NL11', '19790228', 1.0, 0.8),\n",
        "                                              ('NL11', '19790310', 1.1, 1.0),\n",
        "                                              ('NL11', '19800110', 1.2, 0.2),\n",
        "                                              ('NL11', '19800121', 2.1, 0.2),\n",
        "                                              ('NL11', '19800131', 0.2, 1.2),\n",
        "                                              ('NL11', '19800210', 0.1, 2.0),\n",
        "                                              ('NL11', '19800221', 0.0, 2.1),\n",
        "                                              ('NL11', '19800228', 1.0, 0.8),\n",
        "                                              ('NL11', '19800310', 1.1, 1.0)],\n",
        "                                             ['IDREGION', 'DATE', 'PREC', 'ET0'])\n",
        "\n",
        "    # Create a small meteo daily data set\n",
        "    # Preprocessing currently converts daily data to dekadal data by taking AVG\n",
        "    # for all indicators except TMAX (MAX is used instead) and TMIN (MIN is used instead).\n",
        "    query = 'select IDREGION, FYEAR, DEKAD, max(TMAX) as TMAX, min(TMIN) as TMIN, '\n",
        "    query = query + ' bround(avg(TAVG), 2) as TAVG, bround(sum(PREC), 2) as PREC, '\n",
        "    query = query + ' bround(sum(ET0), 2) as ET0, bround(avg(RAD), 2) as RAD, '\n",
        "    query = query + ' bround(sum(CWB), 2) as CWB '\n",
        "    self.meteo_daydf = spark.createDataFrame([('NL11', '19790101', 1.2, 0.2, 8.5, -1.2, 5.5, 10000.0),\n",
        "                                              ('NL11', '19790102', 2.1, 0.2, 9.1, 0.3, 6.1, 12000.0),\n",
        "                                              ('NL11', '19790103', 0.2, 1.2, 10.4, 1.2, 7.2, 14000.0),\n",
        "                                              ('NL11', '19790104', 0.1, 2.0, 8.1, -1.5, 5.2, 10000.0),\n",
        "                                              ('NL11', '19790105', 0.0, 2.1, 10.2, 1.0, 7.5, 13000.0),\n",
        "                                              ('NL11', '19790106', 1.2, 0.2, 11.2, 2.5, 8.2, 16000.0),\n",
        "                                              ('NL11', '19790112', 2.1, 0.5, 9.2, 0.5, 5.5, 12000.0),\n",
        "                                              ('NL11', '19790113', 0.2, 1.1, 10.2, 1.4, 7.1, 14000.0),\n",
        "                                              ('NL11', '19790114', 0.1, 2.0, 12.0, 3.2, 8.3, 15000.0),\n",
        "                                              ('NL11', '19790115', 0.0, 1.5, 13.1, 4.5, 9.2, 17000.0),\n",
        "                                              ('NL11', '19790122', 2.1, 0.5, 9.2, 0.5, 5.5, 12000.0),\n",
        "                                              ('NL11', '19790123', 0.2, 1.1, 10.2, 1.4, 7.1, 14000.0),\n",
        "                                              ('NL11', '19790124', 0.1, 2.0, 12.0, 3.2, 8.3, 15000.0),\n",
        "                                              ('NL11', '19790125', 0.0, 1.5, 13.1, 4.5, 9.2, 17000.0),\n",
        "                                              ('NL11', '19800101', 1.2, 0.2, 8.5, -1.2, 5.5, 10000.0),\n",
        "                                              ('NL11', '19800102', 2.1, 0.2, 9.1, 0.3, 6.1, 12000.0),\n",
        "                                              ('NL11', '19800103', 0.2, 1.2, 10.4, 1.2, 7.2, 14000.0),\n",
        "                                              ('NL11', '19800104', 0.1, 2.0, 8.1, -1.5, 5.2, 10000.0),\n",
        "                                              ('NL11', '19800105', 0.0, 2.1, 10.2, 1.0, 7.5, 13000.0),\n",
        "                                              ('NL11', '19800106', 1.2, 0.2, 11.2, 2.5, 8.2, 16000.0),\n",
        "                                              ('NL11', '19800112', 2.1, 0.5, 9.2, 0.5, 5.5, 12000.0),\n",
        "                                              ('NL11', '19800113', 0.2, 1.1, 10.2, 1.4, 7.1, 14000.0),\n",
        "                                              ('NL11', '19800114', 0.1, 2.0, 12.0, 3.2, 8.3, 15000.0),\n",
        "                                              ('NL11', '19800115', 0.0, 1.5, 13.1, 4.5, 9.2, 17000.0),\n",
        "                                              ('NL11', '19800122', 2.1, 0.5, 9.2, 0.5, 5.5, 12000.0),\n",
        "                                              ('NL11', '19800123', 0.2, 1.1, 10.2, 1.4, 7.1, 14000.0),\n",
        "                                              ('NL11', '19800124', 0.1, 2.0, 12.0, 3.2, 8.3, 15000.0),\n",
        "                                              ('NL11', '19800125', 0.0, 1.5, 13.1, 4.5, 9.2, 17000.0)],\n",
        "                                             ['IDREGION', 'DATE', 'PREC', 'ET0', 'TMAX', 'TMIN', 'TAVG', 'RAD'])\n",
        "\n",
        "    # Create a small remote sensing data set\n",
        "    # Preprocessing currently extracts the year and dekad\n",
        "    self.rs_df1 = spark.createDataFrame([('NL11', '19790321', 0.47),\n",
        "                                         ('NL11', '19790331', 0.49),\n",
        "                                         ('NL11', '19790410', 0.55),\n",
        "                                         ('NL11', '19790421', 0.49),\n",
        "                                         ('NL11', '19790430', 0.64),\n",
        "                                         ('NL11', '19800110', 0.42),\n",
        "                                         ('NL11', '19800121', 2.43),\n",
        "                                         ('NL11', '19800131', 0.41),\n",
        "                                         ('NL11', '19800210', 0.42),\n",
        "                                         ('NL11', '19800221', 0.44),\n",
        "                                         ('NL11', '19800228', 0.45),\n",
        "                                         ('NL11', '19800310', 2.43)],\n",
        "                                        ['IDREGION', 'DATE', 'FAPAR'])\n",
        "\n",
        "    self.rs_df2 = spark.createDataFrame([('FR10', '19790110', 0.42),\n",
        "                                         ('FR10', '19790121', 0.43),\n",
        "                                         ('FR10', '19790131', 0.41),\n",
        "                                         ('FR10', '19790210', 0.42),\n",
        "                                         ('FR10', '19790221', 0.44),\n",
        "                                         ('FR10', '19790228', 0.45),\n",
        "                                         ('FR10', '19790310', 0.47),\n",
        "                                         ('FR10', '19790321', 0.49),\n",
        "                                         ('FR10', '19790331', 0.55),\n",
        "                                         ('FR10', '19790410', 0.62),\n",
        "                                         ('FR10', '19790421', 0.66),\n",
        "                                         ('FR10', '19800110', 0.42),\n",
        "                                         ('FR10', '19800121', 2.43),\n",
        "                                         ('FR10', '19800131', 0.41),\n",
        "                                         ('FR10', '19800210', 0.42),\n",
        "                                         ('FR10', '19800221', 0.44),\n",
        "                                         ('FR10', '19800228', 0.45),\n",
        "                                         ('FR10', '19800310', 2.43)],\n",
        "                                        ['IDREGION', 'DATE', 'FAPAR'])\n",
        "  \n",
        "    self.crop_season_nuts3 = spark.createDataFrame([('FR101', '1979', 0, 27),\n",
        "                                                    ('FR101', '1980', 27, 28),\n",
        "                                                    ('FR102', '1979', 0, 27),\n",
        "                                                    ('FR102', '1980', 27, 29)],\n",
        "                                                   ['IDREGION', 'FYEAR', 'PREV_SEASON_END', 'SEASON_END'])\n",
        "\n",
        "    # Create small yield data sets\n",
        "    # Two formats are preprocessed: (1) year and yield are columns,\n",
        "    # (2) years are columns with yield values in rows\n",
        "    # Preprocessing currently converts (2) into 1\n",
        "    self.yield_df1 = spark.createDataFrame([('potatoes', 'FR102', '1989', 29.75),\n",
        "                                            ('potatoes', 'FR102', '1990', 25.44),\n",
        "                                            ('potatoes', 'FR103', '1989', 30.2),\n",
        "                                            ('potatoes', 'FR103', '1990', 29.9),\n",
        "                                            ('sugarbeet', 'FR102', '1989', 66.0),\n",
        "                                            ('sugarbeet', 'FR102', '1990', 55.0),\n",
        "                                            ('sugarbeet', 'FR103', '1989', 69.3),\n",
        "                                            ('sugarbeet', 'FR103', '1990', 59.1)],\n",
        "                                           ['Crop', 'IDREGION', 'FYEAR', 'YIELD'])\n",
        "\n",
        "    self.yield_df2 = spark.createDataFrame([('Total potatoes', 'NL11', 38.0, 40.5, 40.0),\n",
        "                                            ('Total potatoes', 'NL12', 49.0, 44.0, 46.8),\n",
        "                                            ('Spring barley', 'NL13', 4.6, 5.5, 6.6),\n",
        "                                            ('Spring barley', 'NL12', 5.6, 6.1, 7.0)],\n",
        "                                           ['Crop', 'IDREGION', '1994', '1995', '1996'])\n",
        "\n",
        "  def testExtractYearDekad(self):\n",
        "    print('WOFOST data after extracting year and dekad')\n",
        "    print('-------------------------------------------')\n",
        "    self.preprocessor.extractYearDekad(self.wofost_df).show(10)\n",
        "\n",
        "  def testPreprocessWofost(self):\n",
        "    print('WOFOST data after preprocessing')\n",
        "    print('--------------------------------')\n",
        "    self.wofost_df = self.wofost_df.filter(self.wofost_df['CROP_ID'] == 6).drop('CROP_ID')\n",
        "    self.crop_season = self.preprocessor.getCropSeasonInformation(self.wofost_df,\n",
        "                                                                  False)\n",
        "    self.wofost_df = self.preprocessor.preprocessWofost(self.wofost_df,\n",
        "                                                        self.crop_season,\n",
        "                                                        False)\n",
        "    self.wofost_df.show(5)\n",
        "    self.crop_season.show(5)\n",
        "\n",
        "  def testPreprocessMeteo(self):\n",
        "    print('Meteo dekadal data after preprocessing')\n",
        "    print('--------------------------------------')\n",
        "    self.meteo_dekdf = self.preprocessor.preprocessMeteo(self.meteo_dekdf,\n",
        "                                                         self.crop_season,\n",
        "                                                         False)\n",
        "    self.meteo_dekdf.show(5)\n",
        "\n",
        "  def testPreprocessMeteoDaily(self):\n",
        "    self.meteo_daydf = self.preprocessor.preprocessMeteo(self.meteo_daydf,\n",
        "                                                         self.crop_season,\n",
        "                                                         False)\n",
        "    self.meteo_daydf = self.preprocessor.preprocessMeteoDaily(self.meteo_daydf)\n",
        "    print('Meteo daily data after preprocessing')\n",
        "    print('------------------------------------')\n",
        "    self.meteo_daydf.show(5)\n",
        "\n",
        "  def testPreprocessRemoteSensing(self):\n",
        "    self.rs_df1 = self.preprocessor.preprocessRemoteSensing(self.rs_df1,\n",
        "                                                            self.crop_season,\n",
        "                                                            False)\n",
        "    print('Remote sensing data after preprocessing')\n",
        "    print('---------------------------------------')\n",
        "    self.rs_df1.show(5)\n",
        "\n",
        "  def testRemoteSensingNUTS2ToNUTS3(self):\n",
        "    print('Remote sensing data before preprocessing')\n",
        "    print('---------------------------------------')\n",
        "    self.rs_df2.show()\n",
        "    nuts3_regions = [reg[0] for reg in self.yield_df1.select('IDREGION').distinct().collect()]\n",
        "    self.rs_df2 = self.preprocessor.remoteSensingNUTS2ToNUTS3(self.rs_df2, nuts3_regions)\n",
        "    print('Remote sensing data at NUTS3')\n",
        "    print('-----------------------------')\n",
        "    self.rs_df2.show(5)\n",
        "\n",
        "    self.rs_df2 = self.preprocessor.preprocessRemoteSensing(self.rs_df2,\n",
        "                                                            self.crop_season_nuts3,\n",
        "                                                            False)\n",
        "    print('Remote sensing data after preprocessing')\n",
        "    print('---------------------------------------')\n",
        "    self.rs_df2.show(5)\n",
        "\n",
        "  def testPreprocessYield(self):\n",
        "    self.yield_df1 = self.preprocessor.preprocessYield(self.yield_df1, 7)\n",
        "    print('Yield data format 1 after preprocessing')\n",
        "    print('--------------------------------------')\n",
        "    self.yield_df1.show(5)\n",
        "\n",
        "    print('Yield data format 2 before preprocessing')\n",
        "    print('----------------------------------------')\n",
        "    self.yield_df2.show(5)\n",
        "\n",
        "    self.yield_df2 = self.preprocessor.preprocessYield(self.yield_df2, 7)\n",
        "    print('Yield data format 2 after preprocessing')\n",
        "    print('----------------------------------------')\n",
        "    self.yield_df2.show(5)\n",
        "\n",
        "  def runAllTests(self):\n",
        "    print('\\nTest Data Preprocessor BEGIN\\n')\n",
        "    self.testExtractYearDekad()\n",
        "    self.testPreprocessWofost()\n",
        "    self.testPreprocessMeteo()\n",
        "    self.testPreprocessMeteoDaily()\n",
        "    self.testPreprocessRemoteSensing()\n",
        "    self.testRemoteSensingNUTS2ToNUTS3()\n",
        "    self.testPreprocessYield()\n",
        "    print('\\nTest Data Preprocessor END\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylzks219YUwm"
      },
      "source": [
        "### Test Data Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD-xHJSiYUwn"
      },
      "outputs": [],
      "source": [
        "#%%writefile test_data_summary.py\n",
        "class TestDataSummarizer():\n",
        "  def __init__(self, spark):\n",
        "    cyp_config = CYPConfiguration()\n",
        "    cyp_config.setDebugLevel(2)\n",
        "    self.data_summarizer = CYPDataSummarizer(cyp_config)\n",
        "\n",
        "    # create a small wofost data set\n",
        "    self.wofost_df = spark.createDataFrame([('NL11', '1979', 14, 0.0, 0.0, 5.0),\n",
        "                                            ('NL11', '1979', 15, 2.0, 1.0, 10.0),\n",
        "                                            ('NL11', '1979', 16, 5.0, 4.0, 7.0),\n",
        "                                            ('NL11', '1979', 17, 15.0, 12.0, 4.0),\n",
        "                                            ('NL11', '1979', 18, 40.0, 35.0, 6.0),\n",
        "                                            ('NL11', '1979', 19, 100.0, 80.0, 5.0),\n",
        "                                            ('NL11', '1979', 20, 150.0, 120.0, 3.0),\n",
        "                                            ('NL11', '1980', 13, 0.0, 0.0, 15.0),\n",
        "                                            ('NL11', '1980', 14, 5.0, 2.0, 12.0),\n",
        "                                            ('NL11', '1980', 15, 15.0, 12.0, 10.0),\n",
        "                                            ('NL11', '1980', 16, 50.0, 40.0, 8.0),\n",
        "                                            ('NL11', '1980', 17, 100.0, 80.0, 4.0),\n",
        "                                            ('NL11', '1980', 18, 200.0, 140.0, 5.0),\n",
        "                                            ('NL11', '1980', 19, 200.0, 150.0, 12.0),\n",
        "                                            ('NL11', '1980', 20, 200.0, 150.0, 12.0)],\n",
        "                                           ['IDREGION', 'FYEAR', 'DEKAD', 'POT_YB', 'WLIM_YB', 'RSM'])\n",
        "\n",
        "    self.wofost_df2 = spark.createDataFrame([('NL11', '1979', 12, '1979', 22, 0),\n",
        "                                             ('NL11', '1979', 13, '1979', 23, 1),\n",
        "                                             ('NL11', '1979', 14, '1979', 24, 4),\n",
        "                                             ('NL11', '1979', 15, '1979', 25, 70),\n",
        "                                             ('NL11', '1979', 16, '1979', 26, 101),\n",
        "                                             ('NL11', '1979', 19, '1979', 29, 150),\n",
        "                                             ('NL11', '1979', 21, '1979', 31, 180),\n",
        "                                             ('NL11', '1979', 23, '1979', 33, 200),\n",
        "                                             ('NL11', '1979', 24, '1979', 34, 201),\n",
        "                                             ('NL11', '1979', 25, '1979', 35, 201),\n",
        "                                             ('NL11', '1980', 12, '1980', 22, 0),\n",
        "                                             ('NL11', '1980', 13, '1980', 23, 2),\n",
        "                                             ('NL11', '1980', 14, '1980', 24, 15),\n",
        "                                             ('NL11', '1980', 15, '1980', 25, 80),\n",
        "                                             ('NL11', '1980', 16, '1980', 26, 99),\n",
        "                                             ('NL11', '1980', 19, '1980', 29, 140),\n",
        "                                             ('NL11', '1980', 21, '1980', 31, 170),\n",
        "                                             ('NL11', '1980', 23, '1980', 33, 195),\n",
        "                                             ('NL11', '1980', 24, '1980', 34, 201),\n",
        "                                             ('NL11', '1980', 25, '1980', 35, 201)],\n",
        "                                           ['IDREGION', 'FYEAR', 'DEKAD', 'CAMPAIGN_YEAR', 'CAMPAIGN_DEKAD', 'DVS'])\n",
        "\n",
        "    # Create a small meteo dekadal data set\n",
        "    self.meteo_df = spark.createDataFrame([('NL11', '1979', 1, '1979', 11, 1.2, 8.5, -1.2, 5.5, 10000.0),\n",
        "                                           ('NL11', '1979', 2, '1979', 12, 2.1, 9.1, 0.3, 6.1, 12000.0),\n",
        "                                           ('NL11', '1979', 3, '1979', 13, 0.2, 10.4, 1.2, 7.2, 14000.0),\n",
        "                                           ('NL11', '1979', 4, '1979', 14, 0.1, 8.1, -1.5, 5.2, 10000.0),\n",
        "                                           ('NL11', '1979', 5, '1979', 15, 0.0, 10.2, 1.0, 7.5, 13000.0),\n",
        "                                           ('NL12', '1979', 1, '1979', 12, 1.2, 11.2, 2.5, 8.2, 16000.0),\n",
        "                                           ('NL12', '1979', 2, '1979', 13, 2.1, 9.2, 0.5, 5.5, 12000.0),\n",
        "                                           ('NL12', '1979', 3, '1979', 14, 0.2, 10.2, 1.4, 7.1, 14000.0),\n",
        "                                           ('NL12', '1979', 4, '1979', 15, 0.1, 12.0, 3.2, 8.3, 15000.0),\n",
        "                                           ('NL12', '1979', 5, '1979', 16, 0.0, 13.1, 4.5, 9.2, 17000.0)],\n",
        "                                          ['IDREGION', 'FYEAR', 'DEKAD', 'CAMPAIGN_YEAR', 'CAMPAIGN_DEKAD', 'PREC', 'TMAX', 'TMIN', 'TAVG', 'RAD'])\n",
        "\n",
        "    # Create a small remote sensing data set\n",
        "    # Preprocessing currently extracts the year and dekad\n",
        "    self.rs_df = spark.createDataFrame([('NL11', '1979', 1, 0.42),\n",
        "                                         ('NL11', '1979', 2, 0.41),\n",
        "                                         ('NL11', '1979', 3, 0.42),\n",
        "                                         ('NL11', '1980', 1, 0.44),\n",
        "                                         ('NL11', '1980', 2, 0.45),\n",
        "                                        ('NL11', '1980', 3, 0.43)],\n",
        "                                        ['IDREGION', 'FYEAR', 'DEKAD', 'FAPAR'])\n",
        "\n",
        "    # Create a small yield data set\n",
        "    self.yield_df = spark.createDataFrame([(7, 'FR102', '1989', 29.75),\n",
        "                                           (7, 'FR102', '1990', 25.44),\n",
        "                                           (7, 'FR103', '1989', 30.2),\n",
        "                                           (7, 'FR103', '1990', 29.9),\n",
        "                                           (6, 'FR102', '1989', 66.0),\n",
        "                                           (6, 'FR102', '1990', 55.0),\n",
        "                                           (6, 'FR103', '1989', 69.3),\n",
        "                                           (6, 'FR103', '1990', 59.1)],\n",
        "                                          ['CROP_ID', 'IDREGION', 'FYEAR', 'YIELD'])\n",
        "\n",
        "  def testWofostDVSSummary(self):\n",
        "    print('WOFOST Crop Calendar Summary using DVS')\n",
        "    print('-----------------------------')\n",
        "    self.data_summarizer.wofostDVSSummary(self.wofost_df2).show()\n",
        "\n",
        "  def testWofostIndicatorsSummary(self):\n",
        "    print('WOFOST indicators summary')\n",
        "    print('--------------------------')\n",
        "    min_cols = ['IDREGION']\n",
        "    max_cols = ['IDREGION', 'POT_YB', 'WLIM_YB']\n",
        "    avg_cols = ['IDREGION', 'RSM']\n",
        "    self.data_summarizer.indicatorsSummary(self.wofost_df, min_cols, max_cols, avg_cols).show()\n",
        "\n",
        "  def testMeteoIndicatorsSummary(self):\n",
        "    print('Meteo indicators summary')\n",
        "    print('-------------------------')\n",
        "    meteo_cols = self.meteo_df.columns[3:]\n",
        "    min_cols = ['IDREGION'] + meteo_cols\n",
        "    max_cols = ['IDREGION'] + meteo_cols\n",
        "    avg_cols = ['IDREGION'] + meteo_cols\n",
        "    self.data_summarizer.indicatorsSummary(self.meteo_df, min_cols, max_cols, avg_cols).show()\n",
        "\n",
        "  def testRemoteSensingSummary(self):\n",
        "    print('Remote sensing indicators summary')\n",
        "    print('----------------------------------')\n",
        "    rs_cols = ['FAPAR']\n",
        "    min_cols = ['IDREGION'] + rs_cols\n",
        "    max_cols = ['IDREGION'] + rs_cols\n",
        "    avg_cols = ['IDREGION'] + rs_cols\n",
        "    self.data_summarizer.indicatorsSummary(self.rs_df, min_cols, max_cols, avg_cols).show()\n",
        "\n",
        "  def testYieldSummary(self):\n",
        "    crop = 'potatoes'\n",
        "    print('Yield summary for', crop)\n",
        "    print('-----------------------------')\n",
        "    crop_id = cropNameToID(crop_id_dict, crop)\n",
        "    self.yield_df = self.yield_df.filter(self.yield_df.CROP_ID == crop_id)\n",
        "    self.data_summarizer.yieldSummary(self.yield_df).show()\n",
        "\n",
        "  def runAllTests(self):\n",
        "    print('\\nTest Data Summarizer BEGIN\\n')\n",
        "    self.testWofostDVSSummary()\n",
        "    self.testWofostIndicatorsSummary()\n",
        "    self.testMeteoIndicatorsSummary()\n",
        "    self.testRemoteSensingSummary()\n",
        "    self.testYieldSummary()\n",
        "    print('\\nTest Data Summarizer END\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjepG1gAYXrc"
      },
      "source": [
        "### Test Yield Trend Estimation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRoEhIT6YXre"
      },
      "outputs": [],
      "source": [
        "#%%writefile test_yield_trend.py\n",
        "class TestYieldTrendEstimator():\n",
        "  def __init__(self, yield_df):\n",
        "    # TODO: Create a small yield data set\n",
        "    self.yield_df = yield_df\n",
        "    cyp_config = CYPConfiguration()\n",
        "    self.verbose = 2\n",
        "    cyp_config.setDebugLevel(self.verbose)\n",
        "    self.trend_est = CYPYieldTrendEstimator(cyp_config)\n",
        "\n",
        "  def testYieldTrendTwoRegions(self):\n",
        "    print('\\nFind the optimal trend window and estimate trend for first 2 regions')\n",
        "    pd_yield_df = self.yield_df.toPandas()\n",
        "    regions = sorted(pd_yield_df['IDREGION'].unique())\n",
        "    reg1 = regions[0]\n",
        "    pd_reg1_df = pd_yield_df[pd_yield_df['IDREGION'] == reg1]\n",
        "    reg1_num_years = len(pd_reg1_df.index)\n",
        "    reg1_max_year = pd_reg1_df['FYEAR'].max()\n",
        "    reg1_min_year = pd_reg1_df['FYEAR'].min()\n",
        "\n",
        "    if (self.verbose > 2):\n",
        "      print('\\nPrint Yield Trend Rounds')\n",
        "      print('------------------------')\n",
        "\n",
        "    trend_windows = [5]\n",
        "    self.trend_est.printYieldTrendRounds(self.yield_df, reg1, trend_windows)\n",
        "\n",
        "    if (self.verbose > 1):\n",
        "      print('\\n Fixed Trend Window prediction for region 1')\n",
        "      print('---------------------------------------------------')\n",
        "    trend_window = 5\n",
        "    pd_fixed_win_df = self.trend_est.getFixedWindowTrend(self.yield_df, reg1, reg1_max_year,\n",
        "                                                         trend_window)\n",
        "    if (self.verbose > 1):\n",
        "      print(pd_fixed_win_df.head(1))\n",
        "\n",
        "    if (self.verbose > 1):\n",
        "      print('\\n Optimal Trend Window and prediction for region 1')\n",
        "      print('---------------------------------------------------')\n",
        "  \n",
        "    trend_windows = [5, 7]\n",
        "    pd_opt_win_df = self.trend_est.getOptimalWindowTrend(self.yield_df, reg1, reg1_max_year,\n",
        "                                                         trend_windows)\n",
        "    if (self.verbose > 1):\n",
        "      print(pd_opt_win_df.head(1))\n",
        "\n",
        "    reg2 = regions[1]\n",
        "    pd_reg2_df = pd_yield_df[pd_yield_df['IDREGION'] == reg2]\n",
        "    reg2_num_years = len(pd_reg2_df.index)\n",
        "    reg2_max_year = pd_reg2_df['FYEAR'].max()\n",
        "    reg2_min_year = pd_reg2_df['FYEAR'].min()\n",
        "\n",
        "    if (self.verbose > 1):\n",
        "      print('\\n Fixed Trend Window prediction for region 2')\n",
        "      print('---------------------------------------------------')\n",
        "    trend_window = 5\n",
        "    pd_fixed_win_df = self.trend_est.getFixedWindowTrend(self.yield_df, reg2, reg2_max_year,\n",
        "                                                         trend_window)\n",
        "    if (self.verbose > 1):\n",
        "      print(pd_fixed_win_df.head(1))\n",
        "\n",
        "    if (self.verbose > 1):\n",
        "      print('\\n Optimal Trend Window and prediction for region 2')\n",
        "      print('---------------------------------------------------')\n",
        "\n",
        "    pd_opt_win_df = self.trend_est.getOptimalWindowTrend(self.yield_df, reg2, reg2_max_year,\n",
        "                                                         trend_windows)\n",
        "    if (self.verbose > 1):\n",
        "      print(pd_opt_win_df.head(1))\n",
        "\n",
        "  def testYieldTrendAllRegions(self):\n",
        "    print('\\nYield trend estimation for all regions')\n",
        "\n",
        "    print('\\nOptimal Trend Windows')\n",
        "    pd_trend_df = self.trend_est.getOptimalWindowTrendFeatures(self.yield_df)\n",
        "    print(pd_trend_df.head(5))\n",
        "\n",
        "    print('\\nFixed Trend Window')\n",
        "    pd_trend_df = self.trend_est.getFixedWindowTrendFeatures(self.yield_df)\n",
        "    print(pd_trend_df.head(5))\n",
        "\n",
        "  def runAllTests(self):\n",
        "    print('\\nTest Yield Trend Estimator BEGIN\\n')\n",
        "    self.testYieldTrendTwoRegions()\n",
        "    self.testYieldTrendAllRegions()\n",
        "    print('\\nTest Yield Trend Estimator END\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih1GgFhZe13K"
      },
      "source": [
        "### Test custom train, test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQNoWy51e13M"
      },
      "outputs": [],
      "source": [
        "#%%writefile test_train_test_split.py\n",
        "class TestCustomTrainTestSplit:\n",
        "  def __init__(self, yield_df):\n",
        "    cyp_config = CYPConfiguration()\n",
        "    self.verbose = 2\n",
        "    cyp_config.setDebugLevel(self.verbose)\n",
        "    self.yield_df = yield_df\n",
        "    self.trTsSplitter = CYPTrainTestSplitter(cyp_config)\n",
        "\n",
        "  def testCustomTrainTestSplit(self):\n",
        "    print('\\nTest customTrainTestSplit')\n",
        "    test_fraction = 0.2\n",
        "    regions = [reg[0] for reg in self.yield_df.select('IDREGION').distinct().collect()]\n",
        "    num_regions = len(regions)\n",
        "    test_years = self.trTsSplitter.trainTestSplit(self.yield_df, test_fraction, True)\n",
        "    all_years = [yr[0] for yr in self.yield_df.select('FYEAR').distinct().collect()]\n",
        "    yield_train_df = self.yield_df.filter(~self.yield_df['FYEAR'].isin(test_years))\n",
        "    yield_test_df = self.yield_df.filter(self.yield_df['FYEAR'].isin(test_years))\n",
        "\n",
        "    if(self.verbose > 1):\n",
        "      print('\\nCustom training, test split using yield trend')\n",
        "      print('---------------------------------------------')\n",
        "      print('Estimated size of test data', num_regions * np.floor(len(all_years) * test_fraction))\n",
        "      print('Data Size:', yield_train_df.count(), yield_test_df.count())\n",
        "      print('Test years:', test_years)\n",
        "\n",
        "    test_years = self.trTsSplitter.trainTestSplit(self.yield_df, test_fraction, False)\n",
        "\n",
        "    if(self.verbose > 1):\n",
        "      print('\\ncustom training, test split without yield trend')\n",
        "      print('------------------------------------------------')\n",
        "      print('Estimated size of test data', num_regions * np.floor(len(all_years) * test_fraction))\n",
        "      print('Data Size:', yield_train_df.count(), yield_test_df.count())\n",
        "      print('Test years:', test_years)\n",
        "\n",
        "  def testCustomKFoldValidationSplit(self):\n",
        "    print('\\nTest customKFoldValidationSplit')\n",
        "    test_fraction = 0.2\n",
        "    num_folds = 5\n",
        "    test_years = self.trTsSplitter.trainTestSplit(self.yield_df, test_fraction, 'Y')\n",
        "    yield_train_df = self.yield_df.filter(~self.yield_df['FYEAR'].isin(test_years))\n",
        "    yield_test_df = self.yield_df.filter(self.yield_df['FYEAR'].isin(test_years))\n",
        "    yield_cols = yield_train_df.columns\n",
        "    pd_yield_train_df = yield_train_df.toPandas()\n",
        "    Y_train_full = pd_yield_train_df[yield_cols].values\n",
        "\n",
        "    custom_cv, _ = self.trTsSplitter.customKFoldValidationSplit(Y_train_full, num_folds)\n",
        "\n",
        "  def runAllTests(self):\n",
        "    print('\\nTest Custom Train, Test Splitter BEGIN\\n')\n",
        "    self.testCustomTrainTestSplit()\n",
        "    self.testCustomKFoldValidationSplit()\n",
        "    print('\\nTest Custom Train, Test Splitter END\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NrPZSmEViO5"
      },
      "source": [
        "## Run Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3_hCaxR0GQ"
      },
      "source": [
        "### Unzip the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhXPNZgmR2Rc"
      },
      "outputs": [],
      "source": [
        "# Copy data over and unzip\n",
        "# ! unzip drive/MyDrive/Wageningen\\ PhD/Dilli\\ -\\ Deep\\ Learning/data/NUTS3-all.zip\n",
        "! unzip drive/MyDrive/Wageningen\\ PhD/Dilli\\ -\\ Deep\\ Learning/data/NUTS3-PC.zip >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqkSBh7CfBOK",
        "outputId": "d559ffcd-1120-46f4-fb0c-dd3e4271df86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWnqPIU4cM_z"
      },
      "source": [
        "### Set Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2r5-pmuVgS1",
        "outputId": "d4996271-7a3d-4a11-ea2d-5ecc44092040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##################\n",
            "# Configuration  #\n",
            "##################\n",
            "\n",
            "Current ML Baseline Configuration\n",
            "--------------------------------\n",
            "Crop name: soft wheat\n",
            "Crop ID: 90\n",
            "Crop growing season crosses calendar year boundary: Y\n",
            "Country code (e.g. NL): IT\n",
            "NUTS level for yield prediction: NUTS3\n",
            "Input data sources: WOFOST, METEO_DAILY, SOIL, YIELD, REMOTE_SENSING, GAES, CROP_AREA\n",
            "Remove data or regions with duplicate or missing values: N\n",
            "Estimate and use yield trend: Y\n",
            "Predict yield residuals instead of full yield: N\n",
            "Find optimal trend window: N\n",
            "List of trend window lengths (number of years): 5, 7, 10\n",
            "Use centroid coordinates and distance to coast: N\n",
            "Use remote sensing data (FAPAR): Y\n",
            "Use agro-environmental zones data: Y\n",
            "Use per region per year crop calendar: Y\n",
            "Predict yield early in the season: Y\n",
            "Early season end dekad relative to harvest: -6\n",
            "Path to all input data. Default is current directory.: NUTS3-IT\n",
            "Path to all output files. Default is current directory.: .\n",
            "Use feature design v2: Y\n",
            "Save features to a CSV file: Y\n",
            "Use features from a CSV file: N\n",
            "Use data sample weights based on crop area: N\n",
            "Retrain a model for every test year: N\n",
            "Save predictions to a CSV file: Y\n",
            "Use predictions from a CSV file: N\n",
            "Compare predictions with MARS Crop Yield Forecasting System: N\n",
            "Debug level to control amount of debug information: 2\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if (test_env == 'notebook'):\n",
        "  cyp_config = CYPConfiguration()\n",
        "  if (run_tests):\n",
        "    test_util = TestUtil(spark)\n",
        "    test_util.runAllTests()\n",
        "\n",
        "  my_config = {\n",
        "      'crop_name' : 'soft wheat',\n",
        "      'season_crosses_calendar_year' : 'Y',\n",
        "      'country_code' : 'IT',\n",
        "      'data_sources' : [ 'WOFOST', 'METEO_DAILY', 'SOIL', 'YIELD'],\n",
        "      'clean_data' : 'N',\n",
        "      'data_path' : 'NUTS3-IT',\n",
        "      'output_path' : '.',\n",
        "      'nuts_level' : 'NUTS3',\n",
        "      'use_yield_trend' : 'Y',\n",
        "      'predict_yield_residuals' : 'N',\n",
        "      'trend_windows' : [5, 7, 10],\n",
        "      'find_optimal_trend_window' : 'N',\n",
        "      'use_centroids' : 'N',\n",
        "      'use_remote_sensing' : 'Y',\n",
        "      'use_gaes' : 'Y',\n",
        "      'use_per_year_crop_calendar' : 'Y',\n",
        "      'early_season_prediction' : 'Y',\n",
        "      'early_season_end_dekad' : -6,\n",
        "      'use_features_v2' : 'Y',\n",
        "      'save_features' : 'Y',\n",
        "      'use_saved_features' : 'N',\n",
        "      'use_sample_weights' : 'N',\n",
        "      'retrain_per_test_year' : 'N',\n",
        "      'save_predictions' : 'Y',\n",
        "      'use_saved_predictions' : 'N',\n",
        "      'compare_with_mcyfs' : 'N',\n",
        "      'debug_level' : 2,\n",
        "  }\n",
        "\n",
        "  cyp_config.updateConfiguration(my_config)\n",
        "  crop = cyp_config.getCropName()\n",
        "  country = cyp_config.getCountryCode()\n",
        "  nuts_level = cyp_config.getNUTSLevel()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  use_saved_predictions = cyp_config.useSavedPredictions()\n",
        "  use_saved_features = cyp_config.useSavedFeatures()\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "  early_season_end = cyp_config.getEarlySeasonEndDekad()\n",
        "\n",
        "  print('##################')\n",
        "  print('# Configuration  #')\n",
        "  print('##################')\n",
        "  output_path = cyp_config.getOutputPath()\n",
        "  log_file = getLogFilename(crop, use_yield_trend,\n",
        "                            early_season_prediction, early_season_end,\n",
        "                            country)\n",
        "  log_fh = open(output_path + '/' + log_file, 'w+')\n",
        "  cyp_config.printConfig(log_fh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cytBAKTO2SfR"
      },
      "source": [
        "### Load and Preprocess Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpkFFtVi2SfU",
        "outputId": "992d1358-694f-4c22-81be-0d0e952f47c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#################\n",
            "# Data Loading  #\n",
            "#################\n",
            "Data file name \"NUTS3-IT/WOFOST_NUTS3_IT.csv\"\n",
            "Data file name \"NUTS3-IT/METEO_DAILY_NUTS3_IT.csv\"\n",
            "Data file name \"NUTS3-IT/SOIL_NUTS3_IT.csv\"\n",
            "Data file name \"NUTS3-IT/YIELD_NUTS3_IT.csv\"\n",
            "Data file name \"NUTS3-IT/REMOTE_SENSING_NUTS3_IT.csv\"\n",
            "Data file name \"NUTS3-IT/GAES_NUTS3_IT.csv\"\n",
            "Data file name \"NUTS3-IT/CROP_AREA_NUTS3_IT.csv\"\n",
            "Loaded data: WOFOST, METEO, SOIL, YIELD, REMOTE_SENSING, GAES, CROP_AREA\n",
            "\n",
            "\n",
            "#######################\n",
            "# Data Preprocessing  #\n",
            "#######################\n",
            "WOFOST data available for 108 region(s)\n",
            "Season end information\n",
            "+--------+-----+---------------+----------+\n",
            "|IDREGION|FYEAR|PREV_SEASON_END|SEASON_END|\n",
            "+--------+-----+---------------+----------+\n",
            "|   ITC11| 1979|              0|        21|\n",
            "|   ITC11| 1980|             21|        21|\n",
            "|   ITC11| 1981|             21|        20|\n",
            "|   ITC11| 1982|             20|        20|\n",
            "|   ITC11| 1983|             20|        21|\n",
            "|   ITC11| 1984|             21|        21|\n",
            "|   ITC11| 1985|             21|        20|\n",
            "|   ITC11| 1986|             20|        20|\n",
            "|   ITC11| 1987|             20|        20|\n",
            "|   ITC11| 1988|             20|        20|\n",
            "+--------+-----+---------------+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "WOFOST data\n",
            "+--------+-------------+-----+-----+--------------+------+------+-------+-------+-----+-----+---+-------+------+------+\n",
            "|IDREGION|CAMPAIGN_YEAR|FYEAR|DEKAD|CAMPAIGN_DEKAD|POT_YB|POT_YS|WLIM_YB|WLIM_YS| PLAI| WLAI|DVS|    RSM|   TWC|   TWR|\n",
            "+--------+-------------+-----+-----+--------------+------+------+-------+-------+-----+-----+---+-------+------+------+\n",
            "|   ITC11|         1980| 1979|   22|             1|   0.0|   0.0|    0.0|    0.0|  0.0|  0.0|  0|    0.0|   0.0|   0.0|\n",
            "|   ITC11|         1980| 1979|   23|             2|   0.0|   0.0|    0.0|    0.0|  0.0|  0.0|  0|    0.0|   0.0|   0.0|\n",
            "|   ITC11|         1980| 1979|   24|             3|   0.0|   0.0|    0.0|    0.0|  0.0|  0.0|  0|    0.0|   0.0|   0.0|\n",
            "|   ITC11|         1980| 1979|   25|             4|   0.0|   0.0|    0.0|    0.0|  0.0|  0.0|  0|    0.0|   0.0|   0.0|\n",
            "|   ITC11|         1980| 1979|   26|             5|   0.0|   0.0|    0.0|    0.0|  0.0|  0.0|  0|    0.0|   0.0|   0.0|\n",
            "|   ITC11|         1980| 1979|   27|             6|   0.0|   0.0|    0.0|    0.0|  0.0|  0.0|  0|    0.0|   0.0|   0.0|\n",
            "|   ITC11|         1980| 1979|   28|             7|   0.0|   0.0|    0.0|    0.0|  0.0|  0.0|  0|    0.0|   0.0|   0.0|\n",
            "|   ITC11|         1980| 1979|   29|             8|   0.0|   0.0|    0.0|    0.0|  0.0|  0.0|  0|    0.0|   0.0|   0.0|\n",
            "|   ITC11|         1980| 1979|   30|             9|   0.0|   0.0|    0.0|    0.0|  0.0|  0.0|  0|    0.0|   0.0|   0.0|\n",
            "|   ITC11|         1980| 1979|   31|            10|27.487|   0.0|  27.01|    0.0|0.038|0.037|  0|43.8455|9.0E-4|0.0018|\n",
            "+--------+-------------+-----+-----+--------------+------+------+-------+-------+-----+-----+---+-------+------+------+\n",
            "only showing top 10 rows\n",
            "\n",
            "METEO data available for 108 region(s)\n",
            "METEO data\n",
            "+--------+-----+-----+-------------+--------------+----+----+----+-----+----+----+----+-----+-----+-----+\n",
            "|IDREGION|FYEAR|DEKAD|CAMPAIGN_YEAR|CAMPAIGN_DEKAD|TMAX|TMIN|TAVG|VPRES|WSPD|PREC| ET0|  RAD| RELH|  CWB|\n",
            "+--------+-----+-----+-------------+--------------+----+----+----+-----+----+----+----+-----+-----+-----+\n",
            "|   ITC11| 1979|   22|         1980|             1|26.9|16.5|21.7|17.06| 0.2| 0.0|4.17|25179|65.76|-4.17|\n",
            "|   ITC11| 1979|   22|         1980|             1|27.3|16.1|21.7|19.36| 0.0| 0.3|4.14|24453| 74.6|-3.84|\n",
            "|   ITC11| 1979|   22|         1980|             1|29.0|19.2|24.1|21.51| 0.1| 1.2|3.75|20124|71.74|-2.55|\n",
            "|   ITC11| 1979|   22|         1980|             1|29.3|19.2|24.2|22.37| 0.4| 1.2|3.85|20338| 73.9|-2.65|\n",
            "|   ITC11| 1979|   22|         1980|             1|30.2|19.0|24.6|21.53| 0.0| 1.8|4.31|23876|69.61|-2.51|\n",
            "|   ITC11| 1979|   22|         1980|             1|27.7|16.9|22.3|19.89| 0.6| 5.0|3.86|21345|73.87| 1.14|\n",
            "|   ITC11| 1979|   22|         1980|             1|29.5|18.6|24.0|21.67| 0.2| 2.0|4.05|22071|72.52|-2.05|\n",
            "|   ITC11| 1979|   22|         1980|             1|29.0|19.9|24.5|23.66| 0.1| 1.1|3.35|17133|77.09|-2.25|\n",
            "|   ITC11| 1979|   22|         1980|             1|30.3|18.2|24.2|21.97| 0.0| 0.0|4.48|24972|72.64|-4.48|\n",
            "|   ITC11| 1979|   22|         1980|             1|28.9|17.4|23.1|19.69| 0.4| 0.0|4.85|27695|69.48|-4.85|\n",
            "+--------+-----+-----+-------------+--------------+----+----+----+-----+----+----+----+-----+-----+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "REMOTE_SENSING data available for 100 region(s)\n",
            "REMOTE_SENSING data\n",
            "+--------+-----+-----+-------------+--------------+-----+\n",
            "|IDREGION|FYEAR|DEKAD|CAMPAIGN_YEAR|CAMPAIGN_DEKAD|FAPAR|\n",
            "+--------+-----+-----+-------------+--------------+-----+\n",
            "|   ITC11| 1999|    1|         1999|            17|0.259|\n",
            "|   ITC11| 1999|    3|         1999|            19|0.223|\n",
            "|   ITC11| 1999|    4|         1999|            20|0.189|\n",
            "|   ITC11| 1999|    5|         1999|            21|0.152|\n",
            "|   ITC11| 1999|    6|         1999|            22|0.184|\n",
            "|   ITC11| 1999|    7|         1999|            23|0.254|\n",
            "|   ITC11| 1999|    8|         1999|            24|0.316|\n",
            "|   ITC11| 1999|    9|         1999|            25|0.448|\n",
            "|   ITC11| 1999|   10|         1999|            26|0.497|\n",
            "|   ITC11| 1999|   11|         1999|            27|0.528|\n",
            "+--------+-----+-----+-------------+--------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "SOIL data available for 108 region(s)\n",
            "SOIL data\n",
            "+--------+------+\n",
            "|IDREGION|SM_WHC|\n",
            "+--------+------+\n",
            "|   ITC11|  0.14|\n",
            "|   ITC12|  0.16|\n",
            "|   ITC13|  0.15|\n",
            "|   ITC14|  0.16|\n",
            "|   ITC15|  0.16|\n",
            "|   ITC16|  0.15|\n",
            "|   ITC17|  0.14|\n",
            "|   ITC18|  0.13|\n",
            "|   ITC31|  0.16|\n",
            "|   ITC32|  0.17|\n",
            "+--------+------+\n",
            "only showing top 10 rows\n",
            "\n",
            "GAES data available for 75 region(s)\n",
            "GAES data\n",
            "+--------+------+-----------+-----------+-----------+-----------+--------------+--------------+--------------+------------+\n",
            "|IDREGION|AEZ_ID|   AVG_ELEV|   STD_ELEV|  AVG_SLOPE|  STD_SLOPE|AVG_FIELD_SIZE|STD_FIELD_SIZE|IRRIG_AREA_ALL|IRRIG_AREA90|\n",
            "+--------+------+-----------+-----------+-----------+-----------+--------------+--------------+--------------+------------+\n",
            "|   ITC11|  1984|238.5133438|33.21763169| 0.43820554|0.538568974|        3.3125|   4.011961283|      363780.0|    15349.87|\n",
            "|   ITC12|  2030|178.5450237|27.21312028|0.430334598|0.328627825|   1.136363636|   0.504524979|      363780.0|    15349.87|\n",
            "|   ITC15|  2030|190.3461538|50.05798623|0.515962362|0.426442683|   8.642857143|   3.590662494|      363780.0|    15349.87|\n",
            "|   ITC16|  2030|330.3179039| 98.4167333| 0.59047085|0.647366822|   6.533333333|   4.401568984|      363780.0|    15349.87|\n",
            "|   ITC17|  2030|186.9065041|63.46732566| 0.88977921|0.771891177|   4.178571429|   4.521772724|      363780.0|    15349.87|\n",
            "|   ITC18|  2030| 117.330036|51.30765183|0.646143854|0.760196328|   9.038461538|   15.31810141|      363780.0|    15349.87|\n",
            "|   ITC46|  1984|160.6366843| 88.5943744|0.609255135|1.323380947|           0.9|   0.547722558|      576859.0|    24357.34|\n",
            "|   ITC47|  1984|100.6519375|77.72721265|0.557265401|1.521515846|           6.4|   4.659518335|      576859.0|    24357.34|\n",
            "|   ITC48|  2030|85.99411072| 60.3988802|0.289115816|0.576367378|   5.346153846|   4.492515143|      576859.0|    24357.34|\n",
            "|   ITC4B|  2030|22.27952553|22.50199506|0.098320901|0.218069345|         7.625|    12.6114766|      576859.0|    24357.34|\n",
            "+--------+------+-----------+-----------+-----------+-----------+--------------+--------------+--------------+------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "CROP_AREA data available for 103 region(s)\n",
            "CROP_AREA data\n",
            "+--------+-----+---------+\n",
            "|IDREGION|FYEAR|CROP_AREA|\n",
            "+--------+-----+---------+\n",
            "|   ITC11| 1995|  21000.0|\n",
            "|   ITC11| 1996|  22700.0|\n",
            "|   ITC11| 1997|  23610.0|\n",
            "|   ITC11| 1998|  23600.0|\n",
            "|   ITC11| 1999|  21440.0|\n",
            "|   ITC11| 2000|  21450.0|\n",
            "|   ITC11| 2001|  18500.0|\n",
            "|   ITC11| 2002|  21793.0|\n",
            "|   ITC11| 2003|  13500.0|\n",
            "|   ITC11| 2004|  13500.0|\n",
            "+--------+-----+---------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Yield before preprocessing\n",
            "+----------+--------+-----+-----+\n",
            "|      CROP|IDREGION|FYEAR|YIELD|\n",
            "+----------+--------+-----+-----+\n",
            "|Soft wheat|   ITC11| 1995|  4.5|\n",
            "|Soft wheat|   ITC11| 1996| 4.74|\n",
            "|Soft wheat|   ITC11| 1997| 4.45|\n",
            "|Soft wheat|   ITC11| 1998| 5.56|\n",
            "|Soft wheat|   ITC11| 1999| 5.84|\n",
            "|Soft wheat|   ITC11| 2000| 5.83|\n",
            "|Soft wheat|   ITC11| 2001| 5.38|\n",
            "|Soft wheat|   ITC11| 2002| 4.79|\n",
            "|Soft wheat|   ITC11| 2003|  4.8|\n",
            "|Soft wheat|   ITC11| 2004| 5.54|\n",
            "+----------+--------+-----+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "Yield after preprocessing\n",
            "+--------+-----+-----+\n",
            "|IDREGION|FYEAR|YIELD|\n",
            "+--------+-----+-----+\n",
            "|   ITC11| 1995|  4.5|\n",
            "|   ITC11| 1996| 4.74|\n",
            "|   ITC11| 1997| 4.45|\n",
            "|   ITC11| 1998| 5.56|\n",
            "|   ITC11| 1999| 5.84|\n",
            "|   ITC11| 2000| 5.83|\n",
            "|   ITC11| 2001| 5.38|\n",
            "|   ITC11| 2002| 4.79|\n",
            "|   ITC11| 2003|  4.8|\n",
            "|   ITC11| 2004| 5.54|\n",
            "+--------+-----+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if ((test_env == 'notebook') and\n",
        "    (not use_saved_predictions) and\n",
        "    (not use_saved_features)):\n",
        "\n",
        "  print('#################')\n",
        "  print('# Data Loading  #')\n",
        "  print('#################')\n",
        "\n",
        "  if (run_tests):\n",
        "    test_loader = TestDataLoader(spark)\n",
        "    test_loader.runAllTests()\n",
        "\n",
        "  cyp_loader = CYPDataLoader(spark, cyp_config)\n",
        "  data_dfs = cyp_loader.loadAllData()\n",
        "\n",
        "  print('#######################')\n",
        "  print('# Data Preprocessing  #')\n",
        "  print('#######################')\n",
        "\n",
        "  if (run_tests):\n",
        "    test_preprocessor = TestDataPreprocessor(spark)\n",
        "    test_preprocessor.runAllTests()\n",
        "\n",
        "  cyp_preprocessor = CYPDataPreprocessor(spark, cyp_config)\n",
        "  data_dfs = preprocessData(cyp_config, cyp_preprocessor, data_dfs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePa1-zrp-Vay"
      },
      "source": [
        "### Split Data into Training and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtLALA3gtbiQ",
        "outputId": "cc8ec1c1-296b-4b5b-f7f9-63624258be03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###########################\n",
            "# Training and Test Split #\n",
            "###########################\n",
            "\n",
            "Test years:\n",
            "IT: 2012, 2013, 2014, 2015, 2016, 2017, 2018\n"
          ]
        }
      ],
      "source": [
        "if ((test_env == 'notebook') and\n",
        "    (not use_saved_predictions) and\n",
        "    (not use_saved_features)):\n",
        "\n",
        "  print('###########################')\n",
        "  print('# Training and Test Split #')\n",
        "  print('###########################')\n",
        "\n",
        "  if (run_tests):\n",
        "    yield_df = data_dfs['YIELD']\n",
        "    test_custom = TestCustomTrainTestSplit(yield_df)\n",
        "    test_custom.runAllTests()\n",
        "\n",
        "  prep_train_test_dfs, test_years = splitDataIntoTrainingTestSets(cyp_config, data_dfs, log_fh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7w13GLYExv1"
      },
      "source": [
        "### Summarize Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxYQ_BnPExv3",
        "outputId": "45d32578-08b6-43fb-9f75-1f0950a318fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#################\n",
            "# Data Summary  #\n",
            "#################\n",
            "Crop calender information based on WOFOST data\n",
            "+--------+-------------+---------+----------+----------+---------------------+\n",
            "|IDREGION|CAMPAIGN_YEAR|START_DVS|START_DVS1|START_DVS2|CAMPAIGN_EARLY_SEASON|\n",
            "+--------+-------------+---------+----------+----------+---------------------+\n",
            "|   ITC11|         2011|       14|        29|        34|                   29|\n",
            "|   ITC12|         2011|       14|        30|        34|                   29|\n",
            "|   ITC13|         2011|       13|        29|        34|                   29|\n",
            "|   ITC14|         2011|       15|        30|        35|                   30|\n",
            "|   ITC15|         2011|       14|        30|        34|                   29|\n",
            "|   ITC16|         2011|       14|        29|        34|                   29|\n",
            "|   ITC17|         2011|       15|        30|        34|                   29|\n",
            "|   ITC18|         2011|       15|        30|        35|                   30|\n",
            "|   ITC31|         2011|       16|        29|        34|                   29|\n",
            "|   ITC32|         2011|        1|         1|         1|                   29|\n",
            "+--------+-------------+---------+----------+----------+---------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if ((test_env == 'notebook') and\n",
        "    (not use_saved_predictions) and\n",
        "    (not use_saved_features)):\n",
        "\n",
        "  print('#################')\n",
        "  print('# Data Summary  #')\n",
        "  print('#################')\n",
        "\n",
        "  if (run_tests):\n",
        "    test_summarizer = TestDataSummarizer(spark)\n",
        "    test_summarizer.runAllTests()\n",
        "\n",
        "  cyp_summarizer = CYPDataSummarizer(cyp_config)\n",
        "  summary_dfs = summarizeData(cyp_config, cyp_summarizer, prep_train_test_dfs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvVoVMRgcejF"
      },
      "source": [
        "### Create Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucxoQlMXwKEg",
        "outputId": "e53b20bd-bde0-4d64-f3fd-acce71c63c88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###################\n",
            "# Feature Design  #\n",
            "###################\n",
            "\n",
            " WOFOST Aggregate Features: Training\n",
            "  IDREGION  FYEAR  maxWLIM_YBp2  maxWLAIp2  maxWLIM_YBp4  maxWLIM_YSp4  \\\n",
            "0    ITC17   1990      10549.60       6.41      10549.60        378.44   \n",
            "1    ITC46   1993       8559.64       4.30      11460.64       4667.39   \n",
            "2    ITF11   2009      11377.06       6.51      11377.06       1066.94   \n",
            "3    ITF43   2006      11265.96       7.13      11265.96        491.79   \n",
            "4    ITF65   2008      13352.55       0.00      13352.55       2666.66   \n",
            "\n",
            "   maxWLAIp4  avgRSMp2  avgRSMp4  \n",
            "0       6.38     74.79     51.12  \n",
            "1       4.30     95.99     66.47  \n",
            "2       6.21     96.54     47.92  \n",
            "3       6.71     81.38     37.22  \n",
            "4       0.00     -8.27     -8.27  \n",
            "\n",
            " WOFOST Aggregate Features: Test\n",
            "  IDREGION  FYEAR  maxWLIM_YBp2  maxWLAIp2  maxWLIM_YBp4  maxWLIM_YSp4  \\\n",
            "0    ITF35   2012      18370.26       0.00      18370.26       6495.34   \n",
            "1    ITG14   2013      11581.79       5.88      13387.59       2642.56   \n",
            "2    ITG11   2014      12210.87       5.49      12210.87       1992.31   \n",
            "3    ITF47   2018      10091.66       7.90          0.00          0.00   \n",
            "4    ITH35   2017       9504.26       6.19          0.00          0.00   \n",
            "\n",
            "   maxWLAIp4  avgRSMp2  avgRSMp4  \n",
            "0       0.00      1.53      1.53  \n",
            "1       5.87     82.29     51.87  \n",
            "2       5.35     90.22     65.32  \n",
            "3       0.00     63.34      0.00  \n",
            "4       0.00     91.66      0.00  \n",
            "\n",
            " WOFOST Features for Extreme Conditions: Training\n",
            "  IDREGION  FYEAR  Z-RSMp2  Z+RSMp2  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4\n",
            "0    ITC17   1990    12.68     0.65     0.36     0.65     0.07     0.00\n",
            "1    ITC46   1993     2.81     5.91     1.13     0.09     0.84     0.16\n",
            "2    ITF11   2009     2.19    11.47     0.00     1.25     0.00     0.29\n",
            "3    ITF43   2006     8.42     2.21     1.23     0.00     0.63     0.00\n",
            "4    ITF65   2008     4.25     0.00     3.85     0.00     1.76     0.00\n",
            "\n",
            " WOFOST Features for Extreme Conditions: Test\n",
            "  IDREGION  FYEAR  Z-RSMp2  Z+RSMp2  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4\n",
            "0    ITF35   2012     3.81     0.00     3.14     0.00      1.4     0.00\n",
            "1    ITG14   2013     0.65     8.84     0.00     4.92      0.0     3.34\n",
            "2    ITG11   2014     0.00    12.92     0.00     4.50      0.0     2.43\n",
            "3    ITF47   2018    19.92     0.00     1.54     0.00      0.0     0.00\n",
            "4    ITH35   2017     1.68     3.40     0.00     1.44      0.0     0.00\n",
            "\n",
            " METEO Aggregate Features: Training\n",
            "  IDREGION  FYEAR  avgCWBp0  avgTAVGp1  avgCWBp1  avgTAVGp2  avgCWBp2  \\\n",
            "0    ITC11   1980     31.70       5.96    139.29       6.75    231.05   \n",
            "1    ITC11   1981    -16.17       2.12      5.66       7.35     23.05   \n",
            "2    ITC11   1982     18.56       4.60     50.77       6.40     94.19   \n",
            "3    ITC11   1983     65.45       5.82    251.25       7.16    288.48   \n",
            "4    ITC11   1984    -84.39       3.64   -104.13       6.11    -27.86   \n",
            "\n",
            "   avgPRECp5  \n",
            "0        0.0  \n",
            "1        0.0  \n",
            "2        0.0  \n",
            "3        0.0  \n",
            "4        0.0  \n",
            "\n",
            " METEO Aggregate Features: Test\n",
            "  IDREGION  FYEAR  avgCWBp0  avgTAVGp1  avgCWBp1  avgTAVGp2  avgCWBp2  \\\n",
            "0    ITC11   2013    -86.75       6.92      1.83       7.08     71.62   \n",
            "1    ITC11   2017   -102.47       7.48    -10.15       7.77     46.66   \n",
            "2    ITC11   2018   -151.03       5.57   -195.96       7.45    -93.77   \n",
            "3    ITC12   2013    -75.83       6.84     39.84       6.82    139.76   \n",
            "4    ITC12   2016     -7.14       5.33      6.26       8.07     39.64   \n",
            "\n",
            "   avgPRECp5  \n",
            "0        0.0  \n",
            "1        0.0  \n",
            "2        0.0  \n",
            "3        0.0  \n",
            "4        0.0  \n",
            "\n",
            " METEO Features for Extreme Conditions: Training\n",
            "  IDREGION  FYEAR  Z-TMINp1  Z-PRECp1  Z+TMINp1  Z+PRECp1  Z-TMAXp3  Z-PRECp3  \\\n",
            "0    ITC11   1980      5.62      9.35      4.45      4.05     11.15      2.78   \n",
            "1    ITC11   1981     31.44     10.51      0.24      0.00      9.31      6.17   \n",
            "2    ITC11   1982     20.48      7.53      2.65      4.13      0.00      4.14   \n",
            "3    ITC11   1983      4.58      5.38     17.17     19.20     10.85      5.47   \n",
            "4    ITC11   1984     22.50      9.95      2.97      0.80     20.59      0.48   \n",
            "\n",
            "   Z+TMAXp3  Z+PRECp3  Z-PRECp5  Z+PRECp5  \n",
            "0      1.39      2.14       0.0       0.0  \n",
            "1      4.34      8.80       0.0       0.0  \n",
            "2      9.04      0.00       0.0       0.0  \n",
            "3      1.38      7.63       0.0       0.0  \n",
            "4      0.00     12.97       0.0       0.0  \n",
            "\n",
            " METEO Features for Extreme Conditions: Test\n",
            "  IDREGION  FYEAR  Z-TMINp1  Z-PRECp1  Z+TMINp1  Z+PRECp1  Z-TMAXp3  Z-PRECp3  \\\n",
            "0    ITC11   2013      8.40      7.83     20.31     16.42      9.32      4.16   \n",
            "1    ITC11   2017      4.17      7.52     26.65     28.68      0.00      0.00   \n",
            "2    ITC11   2018     10.25      9.13      4.85      1.67      4.05      0.56   \n",
            "3    ITC12   2013     13.33      9.03     15.00     18.77      7.35      3.09   \n",
            "4    ITC12   2016     18.05     11.59      3.20      0.00      5.87      2.53   \n",
            "\n",
            "   Z+TMAXp3  Z+PRECp3  Z-PRECp5  Z+PRECp5  \n",
            "0      4.45     17.97       0.0       0.0  \n",
            "1      0.00      0.00       0.0       0.0  \n",
            "2      4.90      4.41       0.0       0.0  \n",
            "3      6.84     27.04       0.0       0.0  \n",
            "4      3.21      4.95       0.0       0.0  \n",
            "\n",
            "Yield Trend Features: Train\n",
            "  IDREGION  FYEAR  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND\n",
            "0    ITC11   2000     4.50     4.74     4.45     5.56     5.84         6.07\n",
            "1    ITC11   2001     4.74     4.45     5.56     5.84     5.83         6.36\n",
            "2    ITC11   2002     4.45     5.56     5.84     5.83     5.38         6.05\n",
            "3    ITC11   2003     5.56     5.84     5.83     5.38     4.79         4.88\n",
            "4    ITC11   2004     5.84     5.83     5.38     4.79     4.80         4.39\n",
            "Total 1014 rows\n",
            "\n",
            "Yield Trend Features: Test\n",
            "   IDREGION  FYEAR  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND\n",
            "12    ITC11   2012     6.20     5.20     4.96     5.23     5.30         4.85\n",
            "13    ITC11   2013     5.20     4.96     5.23     5.30     7.27         6.94\n",
            "14    ITC11   2014     4.96     5.23     5.30     7.27     5.47         6.56\n",
            "15    ITC11   2015     5.23     5.30     7.27     5.47     5.90         6.29\n",
            "16    ITC11   2016     5.30     7.27     5.47     5.90     5.60         5.68\n",
            "Total 630 rows\n"
          ]
        }
      ],
      "source": [
        "if ((test_env == 'notebook') and\n",
        "    (not use_saved_predictions) and\n",
        "    (not use_saved_features)):\n",
        "\n",
        "  print('###################')\n",
        "  print('# Feature Design  #')\n",
        "  print('###################')\n",
        "\n",
        "  # WOFOST, Meteo and Remote Sensing Features\n",
        "  cyp_featurizer = CYPFeaturizer(cyp_config)\n",
        "  pd_feature_dfs = createFeatures(cyp_config, cyp_featurizer,\n",
        "                                  prep_train_test_dfs, summary_dfs, log_fh)\n",
        "\n",
        "  # trend features\n",
        "  join_cols = ['IDREGION', 'FYEAR']\n",
        "  if (use_yield_trend):\n",
        "    yield_train_df = prep_train_test_dfs['YIELD'][0]\n",
        "    yield_test_df = prep_train_test_dfs['YIELD'][1]\n",
        "\n",
        "    # Trend features from feature data\n",
        "    # use_features_v2 = cyp_config.useFeaturesV2()\n",
        "    # if (use_features_v2):\n",
        "    #   pd_feature_dfs = addFeaturesFromPreviousYears(cyp_config, pd_feature_dfs,\n",
        "    #                                                 1, test_years, join_cols)\n",
        "\n",
        "    if (run_tests):\n",
        "      test_yield_trend = TestYieldTrendEstimator(yield_train_df)\n",
        "      test_yield_trend.runAllTests()\n",
        "\n",
        "    # Trend features from label data\n",
        "    cyp_trend_est = CYPYieldTrendEstimator(cyp_config)\n",
        "    pd_yield_train_ft, pd_yield_test_ft = createYieldTrendFeatures(cyp_config, cyp_trend_est,\n",
        "                                                                   yield_train_df, yield_test_df,\n",
        "                                                                   test_years)\n",
        "    pd_feature_dfs['YIELD_TREND'] = [pd_yield_train_ft, pd_yield_test_ft]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOy8NcAtfnhu"
      },
      "source": [
        "### Combine Features and Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1brZTniBfpaO",
        "outputId": "48c2ec9d-9ec3-4b14-f1c0-62aaf4c471dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Combine Features and Labels\n",
            "---------------------------\n",
            "Yield min year 1995\n",
            "\n",
            "Data size after including SOIL data: \n",
            "Train 108 rows.\n",
            "Test 108 rows.\n",
            "\n",
            "Data size after including GAES data: \n",
            "Train 75 rows.\n",
            "Test 75 rows.\n",
            "\n",
            "Data size after including WOFOST features: \n",
            "Train 2349 rows.\n",
            "Test 495 rows.\n",
            "\n",
            "Data size after including METEO features: \n",
            "Train 2349 rows.\n",
            "Test 495 rows.\n",
            "\n",
            "Data size after including REMOTE_SENSING features: \n",
            "Train 968 rows.\n",
            "Test 495 rows.\n",
            "\n",
            "Data size after combining with CROP_AREA: \n",
            "Train 780 rows.\n",
            "Test 429 rows.\n",
            "\n",
            "Data size after including yield trend features: \n",
            "Train 711 rows.\n",
            "Test 424 rows.\n",
            "\n",
            "Data size after including yield (label) data: \n",
            "Train 711 rows.\n",
            "Test 424 rows.\n",
            "\n",
            "\n",
            "All Features and labels: Training\n",
            "    IDREGION  FYEAR  SM_WHC    AVG_ELEV   STD_ELEV  AVG_SLOPE  STD_SLOPE  \\\n",
            "485    ITC11   2000    0.14  238.513344  33.217632   0.438206   0.538569   \n",
            "486    ITC11   2001    0.14  238.513344  33.217632   0.438206   0.538569   \n",
            "480    ITC11   2002    0.14  238.513344  33.217632   0.438206   0.538569   \n",
            "488    ITC11   2003    0.14  238.513344  33.217632   0.438206   0.538569   \n",
            "489    ITC11   2004    0.14  238.513344  33.217632   0.438206   0.538569   \n",
            "\n",
            "     AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  ...  Z+PRECp3  Z-PRECp5  \\\n",
            "485          3.3125        4.011961        363780.0  ...      9.94       0.0   \n",
            "486          3.3125        4.011961        363780.0  ...      8.97       0.0   \n",
            "480          3.3125        4.011961        363780.0  ...     18.05       0.0   \n",
            "488          3.3125        4.011961        363780.0  ...      1.32       0.0   \n",
            "489          3.3125        4.011961        363780.0  ...      0.00       0.0   \n",
            "\n",
            "     Z+PRECp5  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  YIELD  \n",
            "485       0.0     4.50     4.74     4.45     5.56     5.84         6.07   5.83  \n",
            "486       0.0     4.74     4.45     5.56     5.84     5.83         6.36   5.38  \n",
            "480       0.0     4.45     5.56     5.84     5.83     5.38         6.05   4.79  \n",
            "488       0.0     5.56     5.84     5.83     5.38     4.79         4.88   4.80  \n",
            "489       0.0     5.84     5.83     5.38     4.79     4.80         4.39   5.54  \n",
            "\n",
            "[5 rows x 52 columns]\n",
            "\n",
            "All Features and labels: Test\n",
            "    IDREGION  FYEAR  SM_WHC    AVG_ELEV   STD_ELEV  AVG_SLOPE  STD_SLOPE  \\\n",
            "289    ITC11   2012    0.14  238.513344  33.217632   0.438206   0.538569   \n",
            "285    ITC11   2013    0.14  238.513344  33.217632   0.438206   0.538569   \n",
            "287    ITC11   2014    0.14  238.513344  33.217632   0.438206   0.538569   \n",
            "288    ITC11   2015    0.14  238.513344  33.217632   0.438206   0.538569   \n",
            "290    ITC11   2016    0.14  238.513344  33.217632   0.438206   0.538569   \n",
            "\n",
            "     AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  ...  Z+PRECp3  Z-PRECp5  \\\n",
            "289          3.3125        4.011961        363780.0  ...      3.90       0.0   \n",
            "285          3.3125        4.011961        363780.0  ...     17.97       0.0   \n",
            "287          3.3125        4.011961        363780.0  ...      7.39       0.0   \n",
            "288          3.3125        4.011961        363780.0  ...      4.61       0.0   \n",
            "290          3.3125        4.011961        363780.0  ...      4.75       0.0   \n",
            "\n",
            "     Z+PRECp5  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  YIELD  \n",
            "289       0.0     6.20     5.20     4.96     5.23     5.30         4.85   7.27  \n",
            "285       0.0     5.20     4.96     5.23     5.30     7.27         6.94   5.47  \n",
            "287       0.0     4.96     5.23     5.30     7.27     5.47         6.56   5.90  \n",
            "288       0.0     5.23     5.30     7.27     5.47     5.90         6.29   5.60  \n",
            "290       0.0     5.30     7.27     5.47     5.90     5.60         5.68   6.00  \n",
            "\n",
            "[5 rows x 52 columns]\n",
            "\n",
            "Saving features to: ./ft_soft_wheat_IT_trend_early-6[train, test].csv\n"
          ]
        }
      ],
      "source": [
        "if ((test_env == 'notebook') and\n",
        "    (not use_saved_predictions) and\n",
        "    (not use_saved_features)):\n",
        "\n",
        "  pd_train_df, pd_test_df = combineFeaturesLabels(cyp_config, sqlContext,\n",
        "                                                  prep_train_test_dfs, pd_feature_dfs,\n",
        "                                                  join_cols, log_fh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lzdenOWYxYr"
      },
      "source": [
        "### Apply Machine Learning using scikit learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1AKH4dbtR1d",
        "outputId": "7e1ca599-6878-484c-82a2-de68b6535b8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###################################\n",
            "# Machine Learning using sklearn  #\n",
            "###################################\n",
            "\n",
            "Training and Evaluation\n",
            "-------------------------\n",
            "\n",
            "Training Data Size: 711 rows\n",
            "X cols: 48, Y cols: 4\n",
            "IDREGION  FYEAR  SM_WHC   AVG_ELEV  STD_ELEV  AVG_SLOPE  STD_SLOPE  AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  IRRIG_AREA90  AEZ_1960  AEZ_1984  AEZ_2030  AEZ_2107  AEZ_2150  maxWLIM_YBp2  maxWLAIp2  maxWLIM_YBp4  maxWLIM_YSp4  maxWLAIp4  avgRSMp2  avgRSMp4  Z-RSMp2  Z+RSMp2  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4  avgCWBp0  avgTAVGp1  avgCWBp1  avgTAVGp2  avgCWBp2  avgPRECp5  Z-TMINp1  Z-PRECp1  Z+TMINp1  Z+PRECp1  Z-TMAXp3  Z-PRECp3  Z+TMAXp3  Z+PRECp3  Z-PRECp5  Z+PRECp5  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  YIELD\n",
            "   ITC11   2000    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       7718.48       5.46           0.0           0.0        0.0     95.43       0.0     1.02     3.17     0.00     1.69      0.0      0.0     41.16       3.52    134.39       7.41    139.75        0.0     19.19      7.97      0.90      2.62      2.85      1.15      2.71      9.94       0.0       0.0     4.50     4.74     4.45     5.56     5.84         6.07   5.83\n",
            "   ITC11   2001    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       9854.76       6.68           0.0           0.0        0.0     95.10       0.0     4.70     6.50     0.00     0.83      0.0      0.0     89.30       7.62    321.07       8.05    368.68        0.0      0.34      5.45     27.14     18.12      6.86      1.13      1.32      8.97       0.0       0.0     4.74     4.45     5.56     5.84     5.83         6.36   5.38\n",
            "   ITC11   2002    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       6712.30       5.20           0.0           0.0        0.0     85.65       0.0    13.91     2.68     0.00     1.55      0.0      0.0   -101.08       2.72   -110.58       6.52    -76.81        0.0     31.91     10.30      2.37      0.02     16.28      1.17      0.00     18.05       0.0       0.0     4.45     5.56     5.84     5.83     5.38         6.05   4.79\n",
            "   ITC11   2003    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       5945.80       4.56           0.0           0.0        0.0     94.40       0.0     4.05     4.84     0.56     0.00      0.0      0.0     58.65       8.08    200.01       6.97    221.74        0.0      1.17      4.28     34.97     32.32      1.73      3.79      8.20      1.32       0.0       0.0     5.56     5.84     5.83     5.38     4.79         4.88   4.80\n",
            "   ITC11   2004    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       8238.97       4.96           0.0           0.0        0.0     98.49       0.0     2.14     8.88     0.42     0.00      0.0      0.0    -69.47       5.95     35.28       6.80     94.43        0.0      8.71      6.70     20.15     21.59      1.45      3.80      4.59      0.00       0.0       0.0     5.84     5.83     5.38     4.79     4.80         4.39   5.54\n",
            "\n",
            "Test Data Size: 424 rows\n",
            "X cols: 48, Y cols: 4\n",
            "IDREGION  FYEAR  SM_WHC   AVG_ELEV  STD_ELEV  AVG_SLOPE  STD_SLOPE  AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  IRRIG_AREA90  AEZ_1960  AEZ_1984  AEZ_2030  AEZ_2107  AEZ_2150  maxWLIM_YBp2  maxWLAIp2  maxWLIM_YBp4  maxWLIM_YSp4  maxWLAIp4  avgRSMp2  avgRSMp4  Z-RSMp2  Z+RSMp2  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4  avgCWBp0  avgTAVGp1  avgCWBp1  avgTAVGp2  avgCWBp2  avgPRECp5  Z-TMINp1  Z-PRECp1  Z+TMINp1  Z+PRECp1  Z-TMAXp3  Z-PRECp3  Z+TMAXp3  Z+PRECp3  Z-PRECp5  Z+PRECp5  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  YIELD\n",
            "   ITC11   2012    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       9105.92       5.96          0.00          0.00       0.00     77.46      0.00    24.76     0.91     0.00     0.46     0.00     0.00   -118.01       6.81    -48.95       7.15    -62.35        0.0      4.01     10.06      7.96      0.00      4.10      3.30      2.29      3.90       0.0       0.0     6.20     5.20     4.96     5.23     5.30         4.85   7.27\n",
            "   ITC11   2013    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       7817.13       4.24       7817.13        969.13       4.24    100.37     98.61     0.00    10.02     0.00     1.90     0.00     1.11    -86.75       6.92      1.83       7.08     71.62        0.0      8.40      7.83     20.31     16.42      9.32      4.16      4.45     17.97       0.0       0.0     5.20     4.96     5.23     5.30     7.27         6.94   5.47\n",
            "   ITC11   2014    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0      10795.92       6.59      10795.92        695.09       6.34     92.90     75.92     7.79     6.47     0.11     0.07     0.00     0.08    -45.90       4.01      9.27       8.54    148.12        0.0     22.12      8.71      1.45      5.24      9.62      5.19      7.34      7.39       0.0       0.0     4.96     5.23     5.30     7.27     5.47         6.56   5.90\n",
            "   ITC11   2015    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       8590.39       5.61       8590.39        370.42       5.61     97.56     68.11     5.50    10.87     0.49     0.24     0.27     0.00    -37.00       9.34    158.93       8.31    226.06        0.0      0.40      4.69     42.06     34.69      9.36      6.69      6.20      4.61       0.0       0.0     5.23     5.30     7.27     5.47     5.90         6.29   5.60\n",
            "   ITC11   2016    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0      10098.18       6.21          0.00          0.00       0.00     83.63      0.00    15.73     1.73     0.00     0.14     0.00     0.00      4.93       5.66     14.92       8.21     32.67        0.0      8.90     10.41      5.26      0.00      5.92      2.99      1.16      4.75       0.0       0.0     5.30     7.27     5.47     5.90     5.60         5.68   6.00\n",
            "\n",
            "All features\n",
            "-------------\n",
            "\n",
            "1: SM_WHC, 2: AVG_ELEV, 3: STD_ELEV, 4: AVG_SLOPE, 5: STD_SLOPE\n",
            "6: AVG_FIELD_SIZE, 7: STD_FIELD_SIZE, 8: IRRIG_AREA_ALL, 9: IRRIG_AREA90, 10: AEZ_1960\n",
            "11: AEZ_1984, 12: AEZ_2030, 13: AEZ_2107, 14: AEZ_2150, 15: maxWLIM_YBp2\n",
            "16: maxWLAIp2, 17: maxWLIM_YBp4, 18: maxWLIM_YSp4, 19: maxWLAIp4, 20: avgRSMp2\n",
            "21: avgRSMp4, 22: Z-RSMp2, 23: Z+RSMp2, 24: Z-RSMp3, 25: Z+RSMp3\n",
            "26: Z-RSMp4, 27: Z+RSMp4, 28: avgCWBp0, 29: avgTAVGp1, 30: avgCWBp1\n",
            "31: avgTAVGp2, 32: avgCWBp2, 33: avgPRECp5, 34: Z-TMINp1, 35: Z-PRECp1\n",
            "36: Z+TMINp1, 37: Z+PRECp1, 38: Z-TMAXp3, 39: Z-PRECp3, 40: Z+TMAXp3\n",
            "41: Z+PRECp3, 42: Z-PRECp5, 43: Z+PRECp5, 44: YIELD-5, 45: YIELD-4\n",
            "46: YIELD-3, 47: YIELD-2, 48: YIELD-1\n",
            "\n",
            "\n",
            "Custom sliding validation train, test splits\n",
            "----------------------------------------------\n",
            "Validation set 1 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006\n",
            "Validation set 1 test years: 2007\n",
            "Validation set 2 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007\n",
            "Validation set 2 test years: 2008\n",
            "Validation set 3 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008\n",
            "Validation set 3 test years: 2009\n",
            "Validation set 4 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009\n",
            "Validation set 4 test years: 2010\n",
            "Validation set 5 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010\n",
            "Validation set 5 test years: 2011\n",
            "\n",
            "\n",
            "Null Method: Predicting linear yield trend:\n",
            "Min Yield: 0.3, Max Yield: 7.6\n",
            "Median Yield: 4.11, Mean Yield: 4.23\n",
            "\n",
            " Yield Predictions Training Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2000        6.07  5.83\n",
            "   ITC11  2001        6.36  5.38\n",
            "   ITC11  2002        6.05  4.79\n",
            "   ITC11  2003        4.88   4.8\n",
            "   ITC11  2004        4.39  5.54\n",
            "   ITC11  2005        4.92   5.5\n",
            "\n",
            " Yield Predictions Validation Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2007        7.06   6.2\n",
            "   ITC11  2008        7.09   5.2\n",
            "   ITC11  2009        5.89  4.96\n",
            "   ITC11  2010        4.91  5.23\n",
            "   ITC11  2011        4.28   5.3\n",
            "   ITC12  2007        5.41   5.1\n",
            "\n",
            " Yield Predictions Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2012        4.85  7.27\n",
            "   ITC11  2013        6.94  5.47\n",
            "   ITC11  2014        6.56   5.9\n",
            "   ITC11  2015        6.29   5.6\n",
            "   ITC11  2016        5.68   6.0\n",
            "   ITC11  2017        5.32   5.4\n",
            "\n",
            "Estimator: GBDT\n",
            "---------------------------\n",
            "\n",
            "best parameters:\n",
            "estimator__learning_rate=0.1\n",
            "estimator__loss=huber\n",
            "estimator__min_samples_leaf=20\n",
            "selector__estimator__min_samples_leaf=20\n",
            "selector__max_features=10\n",
            "\n",
            "Best selector: random_forest\n",
            "Feature Selection Summary\n",
            "---------------------------\n",
            "estimator      selector  mean score  std score  selector score\n",
            "     GBDT random_forest       -0.54       0.15           -0.69\n",
            "     GBDT     RFE_Lasso       -0.59       0.13           -0.73\n",
            "\n",
            "\n",
            "Selected features with importance:\n",
            "----------------------------------\n",
            "\n",
            "48: YIELD-1=0.31, 44: YIELD-5=0.22, 45: YIELD-4=0.17, 47: YIELD-2=0.1, 46: YIELD-3=0.07\n",
            "8: IRRIG_AREA_ALL=0.06, 31: avgTAVGp2=0.05, 4: AVG_SLOPE=0.01, 9: IRRIG_AREA90=0.0, 15: maxWLIM_YBp2=0.0\n",
            "\n",
            "\n",
            "\n",
            "Feature Selection Frequencies\n",
            "-------------------------------\n",
            "static: AVG_SLOPE(1), IRRIG_AREA_ALL(1), IRRIG_AREA90(1), YIELD-5(1), YIELD-4(1), YIELD-3(1), YIELD-2(1), YIELD-1(1)\n",
            "p0: \n",
            "p1: \n",
            "p2: maxWLIM_YBp2(1), avgTAVGp2(1)\n",
            "p3: \n",
            "p4: \n",
            "p5: \n",
            "\n",
            "\n",
            "   IDREGION FYEAR YIELD YIELD_PRED_GBDT\n",
            "0    ITC11  2012  7.27        5.266012\n",
            "1    ITC11  2013  5.47         5.73782\n",
            "2    ITC11  2014   5.9        5.582148\n",
            "3    ITC11  2015   5.6        5.655333\n",
            "4    ITC11  2016   6.0        5.579332\n",
            "\n",
            " Yield Predictions Training Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2000        6.07        5.651173  5.83\n",
            "   ITC11  2001        6.36        5.555893  5.38\n",
            "   ITC11  2002        6.05        5.655745  4.79\n",
            "   ITC11  2003        4.88        5.367714   4.8\n",
            "   ITC11  2004        4.39        5.319222  5.54\n",
            "   ITC11  2005        4.92        5.459774   5.5\n",
            "\n",
            " Yield Predictions Validation Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2007        7.06        5.507099   6.2\n",
            "   ITC11  2008        7.09        6.034171   5.2\n",
            "   ITC11  2009        5.89        5.605071  4.96\n",
            "   ITC11  2010        4.91        5.303309  5.23\n",
            "   ITC11  2011        4.28        5.154714   5.3\n",
            "   ITC12  2007        5.41        4.788372   5.1\n",
            "\n",
            " Yield Predictions Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2012        4.85        5.266012  7.27\n",
            "   ITC11  2013        6.94         5.73782  5.47\n",
            "   ITC11  2014        6.56        5.582148   5.9\n",
            "   ITC11  2015        6.29        5.655333   5.6\n",
            "   ITC11  2016        5.68        5.579332   6.0\n",
            "   ITC11  2017        5.32         5.66956   5.4\n",
            "\n",
            "Algorithm Evaluation Summary for IT\n",
            "-----------------------------------------\n",
            "algorithm  train_MAPE  cv_MAPE  test_MAPE  train_RMSE  cv_RMSE  test_RMSE  train_R2  cv_R2  test_R2\n",
            "    trend       18.79    23.61      20.93       20.27    22.64      21.68      0.65   0.55     0.63\n",
            "     GBDT       15.45    19.73      20.54       14.91    17.68      17.48      0.81   0.73     0.76\n",
            "\n",
            "\n",
            "Training and Evaluation\n",
            "-------------------------\n",
            "\n",
            "Training Data Size: 711 rows\n",
            "X cols: 48, Y cols: 4\n",
            "IDREGION  FYEAR  SM_WHC   AVG_ELEV  STD_ELEV  AVG_SLOPE  STD_SLOPE  AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  IRRIG_AREA90  AEZ_1960  AEZ_1984  AEZ_2030  AEZ_2107  AEZ_2150  maxWLIM_YBp2  maxWLAIp2  maxWLIM_YBp4  maxWLIM_YSp4  maxWLAIp4  avgRSMp2  avgRSMp4  Z-RSMp2  Z+RSMp2  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4  avgCWBp0  avgTAVGp1  avgCWBp1  avgTAVGp2  avgCWBp2  avgPRECp5  Z-TMINp1  Z-PRECp1  Z+TMINp1  Z+PRECp1  Z-TMAXp3  Z-PRECp3  Z+TMAXp3  Z+PRECp3  Z-PRECp5  Z+PRECp5  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  YIELD\n",
            "   ITC11   2000    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       7718.48       5.46           0.0           0.0        0.0     95.43       0.0     1.02     3.17     0.00     1.69      0.0      0.0     41.16       3.52    134.39       7.41    139.75        0.0     19.19      7.97      0.90      2.62      2.85      1.15      2.71      9.94       0.0       0.0     4.50     4.74     4.45     5.56     5.84         6.07   5.83\n",
            "   ITC11   2001    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       9854.76       6.68           0.0           0.0        0.0     95.10       0.0     4.70     6.50     0.00     0.83      0.0      0.0     89.30       7.62    321.07       8.05    368.68        0.0      0.34      5.45     27.14     18.12      6.86      1.13      1.32      8.97       0.0       0.0     4.74     4.45     5.56     5.84     5.83         6.36   5.38\n",
            "   ITC11   2002    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       6712.30       5.20           0.0           0.0        0.0     85.65       0.0    13.91     2.68     0.00     1.55      0.0      0.0   -101.08       2.72   -110.58       6.52    -76.81        0.0     31.91     10.30      2.37      0.02     16.28      1.17      0.00     18.05       0.0       0.0     4.45     5.56     5.84     5.83     5.38         6.05   4.79\n",
            "   ITC11   2003    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       5945.80       4.56           0.0           0.0        0.0     94.40       0.0     4.05     4.84     0.56     0.00      0.0      0.0     58.65       8.08    200.01       6.97    221.74        0.0      1.17      4.28     34.97     32.32      1.73      3.79      8.20      1.32       0.0       0.0     5.56     5.84     5.83     5.38     4.79         4.88   4.80\n",
            "   ITC11   2004    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       8238.97       4.96           0.0           0.0        0.0     98.49       0.0     2.14     8.88     0.42     0.00      0.0      0.0    -69.47       5.95     35.28       6.80     94.43        0.0      8.71      6.70     20.15     21.59      1.45      3.80      4.59      0.00       0.0       0.0     5.84     5.83     5.38     4.79     4.80         4.39   5.54\n",
            "\n",
            "Test Data Size: 424 rows\n",
            "X cols: 48, Y cols: 4\n",
            "IDREGION  FYEAR  SM_WHC   AVG_ELEV  STD_ELEV  AVG_SLOPE  STD_SLOPE  AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  IRRIG_AREA90  AEZ_1960  AEZ_1984  AEZ_2030  AEZ_2107  AEZ_2150  maxWLIM_YBp2  maxWLAIp2  maxWLIM_YBp4  maxWLIM_YSp4  maxWLAIp4  avgRSMp2  avgRSMp4  Z-RSMp2  Z+RSMp2  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4  avgCWBp0  avgTAVGp1  avgCWBp1  avgTAVGp2  avgCWBp2  avgPRECp5  Z-TMINp1  Z-PRECp1  Z+TMINp1  Z+PRECp1  Z-TMAXp3  Z-PRECp3  Z+TMAXp3  Z+PRECp3  Z-PRECp5  Z+PRECp5  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  YIELD\n",
            "   ITC11   2012    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       9105.92       5.96          0.00          0.00       0.00     77.46      0.00    24.76     0.91     0.00     0.46     0.00     0.00   -118.01       6.81    -48.95       7.15    -62.35        0.0      4.01     10.06      7.96      0.00      4.10      3.30      2.29      3.90       0.0       0.0     6.20     5.20     4.96     5.23     5.30         4.85   7.27\n",
            "   ITC11   2013    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       7817.13       4.24       7817.13        969.13       4.24    100.37     98.61     0.00    10.02     0.00     1.90     0.00     1.11    -86.75       6.92      1.83       7.08     71.62        0.0      8.40      7.83     20.31     16.42      9.32      4.16      4.45     17.97       0.0       0.0     5.20     4.96     5.23     5.30     7.27         6.94   5.47\n",
            "   ITC11   2014    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0      10795.92       6.59      10795.92        695.09       6.34     92.90     75.92     7.79     6.47     0.11     0.07     0.00     0.08    -45.90       4.01      9.27       8.54    148.12        0.0     22.12      8.71      1.45      5.24      9.62      5.19      7.34      7.39       0.0       0.0     4.96     5.23     5.30     7.27     5.47         6.56   5.90\n",
            "   ITC11   2015    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       8590.39       5.61       8590.39        370.42       5.61     97.56     68.11     5.50    10.87     0.49     0.24     0.27     0.00    -37.00       9.34    158.93       8.31    226.06        0.0      0.40      4.69     42.06     34.69      9.36      6.69      6.20      4.61       0.0       0.0     5.23     5.30     7.27     5.47     5.90         6.29   5.60\n",
            "   ITC11   2016    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0      10098.18       6.21          0.00          0.00       0.00     83.63      0.00    15.73     1.73     0.00     0.14     0.00     0.00      4.93       5.66     14.92       8.21     32.67        0.0      8.90     10.41      5.26      0.00      5.92      2.99      1.16      4.75       0.0       0.0     5.30     7.27     5.47     5.90     5.60         5.68   6.00\n",
            "\n",
            "All features\n",
            "-------------\n",
            "\n",
            "1: SM_WHC, 2: AVG_ELEV, 3: STD_ELEV, 4: AVG_SLOPE, 5: STD_SLOPE\n",
            "6: AVG_FIELD_SIZE, 7: STD_FIELD_SIZE, 8: IRRIG_AREA_ALL, 9: IRRIG_AREA90, 10: AEZ_1960\n",
            "11: AEZ_1984, 12: AEZ_2030, 13: AEZ_2107, 14: AEZ_2150, 15: maxWLIM_YBp2\n",
            "16: maxWLAIp2, 17: maxWLIM_YBp4, 18: maxWLIM_YSp4, 19: maxWLAIp4, 20: avgRSMp2\n",
            "21: avgRSMp4, 22: Z-RSMp2, 23: Z+RSMp2, 24: Z-RSMp3, 25: Z+RSMp3\n",
            "26: Z-RSMp4, 27: Z+RSMp4, 28: avgCWBp0, 29: avgTAVGp1, 30: avgCWBp1\n",
            "31: avgTAVGp2, 32: avgCWBp2, 33: avgPRECp5, 34: Z-TMINp1, 35: Z-PRECp1\n",
            "36: Z+TMINp1, 37: Z+PRECp1, 38: Z-TMAXp3, 39: Z-PRECp3, 40: Z+TMAXp3\n",
            "41: Z+PRECp3, 42: Z-PRECp5, 43: Z+PRECp5, 44: YIELD-5, 45: YIELD-4\n",
            "46: YIELD-3, 47: YIELD-2, 48: YIELD-1\n",
            "\n",
            "\n",
            "Custom sliding validation train, test splits\n",
            "----------------------------------------------\n",
            "Validation set 1 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006\n",
            "Validation set 1 test years: 2007\n",
            "Validation set 2 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007\n",
            "Validation set 2 test years: 2008\n",
            "Validation set 3 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008\n",
            "Validation set 3 test years: 2009\n",
            "Validation set 4 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009\n",
            "Validation set 4 test years: 2010\n",
            "Validation set 5 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010\n",
            "Validation set 5 test years: 2011\n",
            "\n",
            "\n",
            "Null Method: Predicting linear yield trend:\n",
            "Min Yield: 0.3, Max Yield: 7.6\n",
            "Median Yield: 4.11, Mean Yield: 4.23\n",
            "\n",
            " Yield Predictions Training Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2000        6.07  5.83\n",
            "   ITC11  2001        6.36  5.38\n",
            "   ITC11  2002        6.05  4.79\n",
            "   ITC11  2003        4.88   4.8\n",
            "   ITC11  2004        4.39  5.54\n",
            "   ITC11  2005        4.92   5.5\n",
            "\n",
            " Yield Predictions Validation Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2007        7.06   6.2\n",
            "   ITC11  2008        7.09   5.2\n",
            "   ITC11  2009        5.89  4.96\n",
            "   ITC11  2010        4.91  5.23\n",
            "   ITC11  2011        4.28   5.3\n",
            "   ITC12  2007        5.41   5.1\n",
            "\n",
            " Yield Predictions Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2012        4.85  7.27\n",
            "   ITC11  2013        6.94  5.47\n",
            "   ITC11  2014        6.56   5.9\n",
            "   ITC11  2015        6.29   5.6\n",
            "   ITC11  2016        5.68   6.0\n",
            "   ITC11  2017        5.32   5.4\n",
            "\n",
            "Estimator: GBDT\n",
            "---------------------------\n",
            "\n",
            "best parameters:\n",
            "estimator__learning_rate=0.018578447569970295\n",
            "estimator__loss=lad\n",
            "estimator__min_samples_leaf=19\n",
            "selector__estimator__min_samples_leaf=7\n",
            "selector__max_features=10\n",
            "\n",
            "Best selector: random_forest\n",
            "Feature Selection Summary\n",
            "---------------------------\n",
            "estimator      selector  mean score  std score  selector score\n",
            "     GBDT random_forest       -0.55       0.16           -0.72\n",
            "     GBDT     RFE_Lasso       -0.56       0.16           -0.72\n",
            "\n",
            "\n",
            "Selected features with importance:\n",
            "----------------------------------\n",
            "\n",
            "48: YIELD-1=0.27, 47: YIELD-2=0.23, 45: YIELD-4=0.16, 44: YIELD-5=0.09, 46: YIELD-3=0.06\n",
            "31: avgTAVGp2=0.06, 15: maxWLIM_YBp2=0.05, 8: IRRIG_AREA_ALL=0.03, 4: AVG_SLOPE=0.03, 9: IRRIG_AREA90=0.01\n",
            "\n",
            "\n",
            "\n",
            "Feature Selection Frequencies\n",
            "-------------------------------\n",
            "static: AVG_SLOPE(1), IRRIG_AREA_ALL(1), IRRIG_AREA90(1), YIELD-5(1), YIELD-4(1), YIELD-3(1), YIELD-2(1), YIELD-1(1)\n",
            "p0: \n",
            "p1: \n",
            "p2: maxWLIM_YBp2(1), avgTAVGp2(1)\n",
            "p3: \n",
            "p4: \n",
            "p5: \n",
            "\n",
            "\n",
            "   IDREGION FYEAR YIELD YIELD_PRED_GBDT\n",
            "0    ITC11  2012  7.27        5.177654\n",
            "1    ITC11  2013  5.47        5.604778\n",
            "2    ITC11  2014   5.9        5.246287\n",
            "3    ITC11  2015   5.6        5.542973\n",
            "4    ITC11  2016   6.0        5.541525\n",
            "\n",
            " Yield Predictions Training Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2000        6.07        5.619566  5.83\n",
            "   ITC11  2001        6.36        5.523688  5.38\n",
            "   ITC11  2002        6.05        5.572783  4.79\n",
            "   ITC11  2003        4.88         5.23918   4.8\n",
            "   ITC11  2004        4.39        5.120363  5.54\n",
            "   ITC11  2005        4.92        5.187387   5.5\n",
            "\n",
            " Yield Predictions Validation Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2007        7.06        5.398954   6.2\n",
            "   ITC11  2008        7.09        5.778968   5.2\n",
            "   ITC11  2009        5.89        5.632201  4.96\n",
            "   ITC11  2010        4.91        5.352779  5.23\n",
            "   ITC11  2011        4.28        5.196185   5.3\n",
            "   ITC12  2007        5.41        4.739197   5.1\n",
            "\n",
            " Yield Predictions Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2012        4.85        5.177654  7.27\n",
            "   ITC11  2013        6.94        5.604778  5.47\n",
            "   ITC11  2014        6.56        5.246287   5.9\n",
            "   ITC11  2015        6.29        5.542973   5.6\n",
            "   ITC11  2016        5.68        5.541525   6.0\n",
            "   ITC11  2017        5.32        5.713105   5.4\n",
            "\n",
            "Algorithm Evaluation Summary for IT\n",
            "-----------------------------------------\n",
            "algorithm  train_MAPE  cv_MAPE  test_MAPE  train_RMSE  cv_RMSE  test_RMSE  train_R2  cv_R2  test_R2\n",
            "    trend       18.79    23.61      20.93       20.27    22.64      21.68      0.65   0.55     0.63\n",
            "     GBDT       14.65    20.15      20.77       14.63    18.18      18.06      0.82   0.71     0.74\n",
            "\n",
            "\n",
            "Training and Evaluation\n",
            "-------------------------\n",
            "\n",
            "Training Data Size: 711 rows\n",
            "X cols: 48, Y cols: 4\n",
            "IDREGION  FYEAR  SM_WHC   AVG_ELEV  STD_ELEV  AVG_SLOPE  STD_SLOPE  AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  IRRIG_AREA90  AEZ_1960  AEZ_1984  AEZ_2030  AEZ_2107  AEZ_2150  maxWLIM_YBp2  maxWLAIp2  maxWLIM_YBp4  maxWLIM_YSp4  maxWLAIp4  avgRSMp2  avgRSMp4  Z-RSMp2  Z+RSMp2  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4  avgCWBp0  avgTAVGp1  avgCWBp1  avgTAVGp2  avgCWBp2  avgPRECp5  Z-TMINp1  Z-PRECp1  Z+TMINp1  Z+PRECp1  Z-TMAXp3  Z-PRECp3  Z+TMAXp3  Z+PRECp3  Z-PRECp5  Z+PRECp5  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  YIELD\n",
            "   ITC11   2000    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       7718.48       5.46           0.0           0.0        0.0     95.43       0.0     1.02     3.17     0.00     1.69      0.0      0.0     41.16       3.52    134.39       7.41    139.75        0.0     19.19      7.97      0.90      2.62      2.85      1.15      2.71      9.94       0.0       0.0     4.50     4.74     4.45     5.56     5.84         6.07   5.83\n",
            "   ITC11   2001    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       9854.76       6.68           0.0           0.0        0.0     95.10       0.0     4.70     6.50     0.00     0.83      0.0      0.0     89.30       7.62    321.07       8.05    368.68        0.0      0.34      5.45     27.14     18.12      6.86      1.13      1.32      8.97       0.0       0.0     4.74     4.45     5.56     5.84     5.83         6.36   5.38\n",
            "   ITC11   2002    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       6712.30       5.20           0.0           0.0        0.0     85.65       0.0    13.91     2.68     0.00     1.55      0.0      0.0   -101.08       2.72   -110.58       6.52    -76.81        0.0     31.91     10.30      2.37      0.02     16.28      1.17      0.00     18.05       0.0       0.0     4.45     5.56     5.84     5.83     5.38         6.05   4.79\n",
            "   ITC11   2003    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       5945.80       4.56           0.0           0.0        0.0     94.40       0.0     4.05     4.84     0.56     0.00      0.0      0.0     58.65       8.08    200.01       6.97    221.74        0.0      1.17      4.28     34.97     32.32      1.73      3.79      8.20      1.32       0.0       0.0     5.56     5.84     5.83     5.38     4.79         4.88   4.80\n",
            "   ITC11   2004    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       8238.97       4.96           0.0           0.0        0.0     98.49       0.0     2.14     8.88     0.42     0.00      0.0      0.0    -69.47       5.95     35.28       6.80     94.43        0.0      8.71      6.70     20.15     21.59      1.45      3.80      4.59      0.00       0.0       0.0     5.84     5.83     5.38     4.79     4.80         4.39   5.54\n",
            "\n",
            "Test Data Size: 424 rows\n",
            "X cols: 48, Y cols: 4\n",
            "IDREGION  FYEAR  SM_WHC   AVG_ELEV  STD_ELEV  AVG_SLOPE  STD_SLOPE  AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  IRRIG_AREA90  AEZ_1960  AEZ_1984  AEZ_2030  AEZ_2107  AEZ_2150  maxWLIM_YBp2  maxWLAIp2  maxWLIM_YBp4  maxWLIM_YSp4  maxWLAIp4  avgRSMp2  avgRSMp4  Z-RSMp2  Z+RSMp2  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4  avgCWBp0  avgTAVGp1  avgCWBp1  avgTAVGp2  avgCWBp2  avgPRECp5  Z-TMINp1  Z-PRECp1  Z+TMINp1  Z+PRECp1  Z-TMAXp3  Z-PRECp3  Z+TMAXp3  Z+PRECp3  Z-PRECp5  Z+PRECp5  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  YIELD\n",
            "   ITC11   2012    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       9105.92       5.96          0.00          0.00       0.00     77.46      0.00    24.76     0.91     0.00     0.46     0.00     0.00   -118.01       6.81    -48.95       7.15    -62.35        0.0      4.01     10.06      7.96      0.00      4.10      3.30      2.29      3.90       0.0       0.0     6.20     5.20     4.96     5.23     5.30         4.85   7.27\n",
            "   ITC11   2013    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       7817.13       4.24       7817.13        969.13       4.24    100.37     98.61     0.00    10.02     0.00     1.90     0.00     1.11    -86.75       6.92      1.83       7.08     71.62        0.0      8.40      7.83     20.31     16.42      9.32      4.16      4.45     17.97       0.0       0.0     5.20     4.96     5.23     5.30     7.27         6.94   5.47\n",
            "   ITC11   2014    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0      10795.92       6.59      10795.92        695.09       6.34     92.90     75.92     7.79     6.47     0.11     0.07     0.00     0.08    -45.90       4.01      9.27       8.54    148.12        0.0     22.12      8.71      1.45      5.24      9.62      5.19      7.34      7.39       0.0       0.0     4.96     5.23     5.30     7.27     5.47         6.56   5.90\n",
            "   ITC11   2015    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       8590.39       5.61       8590.39        370.42       5.61     97.56     68.11     5.50    10.87     0.49     0.24     0.27     0.00    -37.00       9.34    158.93       8.31    226.06        0.0      0.40      4.69     42.06     34.69      9.36      6.69      6.20      4.61       0.0       0.0     5.23     5.30     7.27     5.47     5.90         6.29   5.60\n",
            "   ITC11   2016    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0      10098.18       6.21          0.00          0.00       0.00     83.63      0.00    15.73     1.73     0.00     0.14     0.00     0.00      4.93       5.66     14.92       8.21     32.67        0.0      8.90     10.41      5.26      0.00      5.92      2.99      1.16      4.75       0.0       0.0     5.30     7.27     5.47     5.90     5.60         5.68   6.00\n",
            "\n",
            "All features\n",
            "-------------\n",
            "\n",
            "1: SM_WHC, 2: AVG_ELEV, 3: STD_ELEV, 4: AVG_SLOPE, 5: STD_SLOPE\n",
            "6: AVG_FIELD_SIZE, 7: STD_FIELD_SIZE, 8: IRRIG_AREA_ALL, 9: IRRIG_AREA90, 10: AEZ_1960\n",
            "11: AEZ_1984, 12: AEZ_2030, 13: AEZ_2107, 14: AEZ_2150, 15: maxWLIM_YBp2\n",
            "16: maxWLAIp2, 17: maxWLIM_YBp4, 18: maxWLIM_YSp4, 19: maxWLAIp4, 20: avgRSMp2\n",
            "21: avgRSMp4, 22: Z-RSMp2, 23: Z+RSMp2, 24: Z-RSMp3, 25: Z+RSMp3\n",
            "26: Z-RSMp4, 27: Z+RSMp4, 28: avgCWBp0, 29: avgTAVGp1, 30: avgCWBp1\n",
            "31: avgTAVGp2, 32: avgCWBp2, 33: avgPRECp5, 34: Z-TMINp1, 35: Z-PRECp1\n",
            "36: Z+TMINp1, 37: Z+PRECp1, 38: Z-TMAXp3, 39: Z-PRECp3, 40: Z+TMAXp3\n",
            "41: Z+PRECp3, 42: Z-PRECp5, 43: Z+PRECp5, 44: YIELD-5, 45: YIELD-4\n",
            "46: YIELD-3, 47: YIELD-2, 48: YIELD-1\n",
            "\n",
            "\n",
            "Custom sliding validation train, test splits\n",
            "----------------------------------------------\n",
            "Validation set 1 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006\n",
            "Validation set 1 test years: 2007\n",
            "Validation set 2 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007\n",
            "Validation set 2 test years: 2008\n",
            "Validation set 3 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008\n",
            "Validation set 3 test years: 2009\n",
            "Validation set 4 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009\n",
            "Validation set 4 test years: 2010\n",
            "Validation set 5 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010\n",
            "Validation set 5 test years: 2011\n",
            "\n",
            "\n",
            "Null Method: Predicting linear yield trend:\n",
            "Min Yield: 0.3, Max Yield: 7.6\n",
            "Median Yield: 4.11, Mean Yield: 4.23\n",
            "\n",
            " Yield Predictions Training Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2000        6.07  5.83\n",
            "   ITC11  2001        6.36  5.38\n",
            "   ITC11  2002        6.05  4.79\n",
            "   ITC11  2003        4.88   4.8\n",
            "   ITC11  2004        4.39  5.54\n",
            "   ITC11  2005        4.92   5.5\n",
            "\n",
            " Yield Predictions Validation Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2007        7.06   6.2\n",
            "   ITC11  2008        7.09   5.2\n",
            "   ITC11  2009        5.89  4.96\n",
            "   ITC11  2010        4.91  5.23\n",
            "   ITC11  2011        4.28   5.3\n",
            "   ITC12  2007        5.41   5.1\n",
            "\n",
            " Yield Predictions Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2012        4.85  7.27\n",
            "   ITC11  2013        6.94  5.47\n",
            "   ITC11  2014        6.56   5.9\n",
            "   ITC11  2015        6.29   5.6\n",
            "   ITC11  2016        5.68   6.0\n",
            "   ITC11  2017        5.32   5.4\n",
            "\n",
            "Estimator: GBDT\n",
            "---------------------------\n",
            "\n",
            "best parameters:\n",
            "estimator__learning_rate=0.03682432130207943\n",
            "estimator__loss=huber\n",
            "estimator__min_samples_leaf=20\n",
            "selector__estimator__min_samples_leaf=5\n",
            "selector__max_features=10\n",
            "\n",
            "Best selector: random_forest\n",
            "Feature Selection Summary\n",
            "---------------------------\n",
            "estimator      selector  mean score  std score  selector score\n",
            "     GBDT random_forest       -0.55       0.16           -0.71\n",
            "     GBDT     RFE_Lasso       -0.56       0.18           -0.74\n",
            "\n",
            "\n",
            "Selected features with importance:\n",
            "----------------------------------\n",
            "\n",
            "48: YIELD-1=0.32, 45: YIELD-4=0.26, 47: YIELD-2=0.15, 8: IRRIG_AREA_ALL=0.12, 46: YIELD-3=0.06\n",
            "44: YIELD-5=0.05, 31: avgTAVGp2=0.02, 4: AVG_SLOPE=0.01, 9: IRRIG_AREA90=0.01, 15: maxWLIM_YBp2=0.0\n",
            "\n",
            "\n",
            "\n",
            "Feature Selection Frequencies\n",
            "-------------------------------\n",
            "static: AVG_SLOPE(1), IRRIG_AREA_ALL(1), IRRIG_AREA90(1), YIELD-5(1), YIELD-4(1), YIELD-3(1), YIELD-2(1), YIELD-1(1)\n",
            "p0: \n",
            "p1: \n",
            "p2: maxWLIM_YBp2(1), avgTAVGp2(1)\n",
            "p3: \n",
            "p4: \n",
            "p5: \n",
            "\n",
            "\n",
            "   IDREGION FYEAR YIELD YIELD_PRED_GBDT\n",
            "0    ITC11  2012  7.27        5.317467\n",
            "1    ITC11  2013  5.47        5.598096\n",
            "2    ITC11  2014   5.9        5.495131\n",
            "3    ITC11  2015   5.6        5.629616\n",
            "4    ITC11  2016   6.0        5.522656\n",
            "\n",
            " Yield Predictions Training Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2000        6.07        5.630741  5.83\n",
            "   ITC11  2001        6.36        5.540141  5.38\n",
            "   ITC11  2002        6.05        5.565247  4.79\n",
            "   ITC11  2003        4.88        5.271878   4.8\n",
            "   ITC11  2004        4.39        5.125029  5.54\n",
            "   ITC11  2005        4.92        5.253099   5.5\n",
            "\n",
            " Yield Predictions Validation Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2007        7.06        5.478761   6.2\n",
            "   ITC11  2008        7.09        6.036557   5.2\n",
            "   ITC11  2009        5.89        5.756747  4.96\n",
            "   ITC11  2010        4.91        5.347116  5.23\n",
            "   ITC11  2011        4.28        5.318857   5.3\n",
            "   ITC12  2007        5.41        4.742428   5.1\n",
            "\n",
            " Yield Predictions Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2012        4.85        5.317467  7.27\n",
            "   ITC11  2013        6.94        5.598096  5.47\n",
            "   ITC11  2014        6.56        5.495131   5.9\n",
            "   ITC11  2015        6.29        5.629616   5.6\n",
            "   ITC11  2016        5.68        5.522656   6.0\n",
            "   ITC11  2017        5.32        5.669156   5.4\n",
            "\n",
            "Algorithm Evaluation Summary for IT\n",
            "-----------------------------------------\n",
            "algorithm  train_MAPE  cv_MAPE  test_MAPE  train_RMSE  cv_RMSE  test_RMSE  train_R2  cv_R2  test_R2\n",
            "    trend       18.79    23.61      20.93       20.27    22.64      21.68      0.65   0.55     0.63\n",
            "     GBDT       15.65    20.21      20.89       14.95    17.94      17.57      0.81   0.72     0.76\n",
            "\n",
            "\n",
            "Training and Evaluation\n",
            "-------------------------\n",
            "\n",
            "Training Data Size: 711 rows\n",
            "X cols: 48, Y cols: 4\n",
            "IDREGION  FYEAR  SM_WHC   AVG_ELEV  STD_ELEV  AVG_SLOPE  STD_SLOPE  AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  IRRIG_AREA90  AEZ_1960  AEZ_1984  AEZ_2030  AEZ_2107  AEZ_2150  maxWLIM_YBp2  maxWLAIp2  maxWLIM_YBp4  maxWLIM_YSp4  maxWLAIp4  avgRSMp2  avgRSMp4  Z-RSMp2  Z+RSMp2  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4  avgCWBp0  avgTAVGp1  avgCWBp1  avgTAVGp2  avgCWBp2  avgPRECp5  Z-TMINp1  Z-PRECp1  Z+TMINp1  Z+PRECp1  Z-TMAXp3  Z-PRECp3  Z+TMAXp3  Z+PRECp3  Z-PRECp5  Z+PRECp5  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  YIELD\n",
            "   ITC11   2000    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       7718.48       5.46           0.0           0.0        0.0     95.43       0.0     1.02     3.17     0.00     1.69      0.0      0.0     41.16       3.52    134.39       7.41    139.75        0.0     19.19      7.97      0.90      2.62      2.85      1.15      2.71      9.94       0.0       0.0     4.50     4.74     4.45     5.56     5.84         6.07   5.83\n",
            "   ITC11   2001    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       9854.76       6.68           0.0           0.0        0.0     95.10       0.0     4.70     6.50     0.00     0.83      0.0      0.0     89.30       7.62    321.07       8.05    368.68        0.0      0.34      5.45     27.14     18.12      6.86      1.13      1.32      8.97       0.0       0.0     4.74     4.45     5.56     5.84     5.83         6.36   5.38\n",
            "   ITC11   2002    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       6712.30       5.20           0.0           0.0        0.0     85.65       0.0    13.91     2.68     0.00     1.55      0.0      0.0   -101.08       2.72   -110.58       6.52    -76.81        0.0     31.91     10.30      2.37      0.02     16.28      1.17      0.00     18.05       0.0       0.0     4.45     5.56     5.84     5.83     5.38         6.05   4.79\n",
            "   ITC11   2003    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       5945.80       4.56           0.0           0.0        0.0     94.40       0.0     4.05     4.84     0.56     0.00      0.0      0.0     58.65       8.08    200.01       6.97    221.74        0.0      1.17      4.28     34.97     32.32      1.73      3.79      8.20      1.32       0.0       0.0     5.56     5.84     5.83     5.38     4.79         4.88   4.80\n",
            "   ITC11   2004    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       8238.97       4.96           0.0           0.0        0.0     98.49       0.0     2.14     8.88     0.42     0.00      0.0      0.0    -69.47       5.95     35.28       6.80     94.43        0.0      8.71      6.70     20.15     21.59      1.45      3.80      4.59      0.00       0.0       0.0     5.84     5.83     5.38     4.79     4.80         4.39   5.54\n",
            "\n",
            "Test Data Size: 424 rows\n",
            "X cols: 48, Y cols: 4\n",
            "IDREGION  FYEAR  SM_WHC   AVG_ELEV  STD_ELEV  AVG_SLOPE  STD_SLOPE  AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  IRRIG_AREA90  AEZ_1960  AEZ_1984  AEZ_2030  AEZ_2107  AEZ_2150  maxWLIM_YBp2  maxWLAIp2  maxWLIM_YBp4  maxWLIM_YSp4  maxWLAIp4  avgRSMp2  avgRSMp4  Z-RSMp2  Z+RSMp2  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4  avgCWBp0  avgTAVGp1  avgCWBp1  avgTAVGp2  avgCWBp2  avgPRECp5  Z-TMINp1  Z-PRECp1  Z+TMINp1  Z+PRECp1  Z-TMAXp3  Z-PRECp3  Z+TMAXp3  Z+PRECp3  Z-PRECp5  Z+PRECp5  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  YIELD\n",
            "   ITC11   2012    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       9105.92       5.96          0.00          0.00       0.00     77.46      0.00    24.76     0.91     0.00     0.46     0.00     0.00   -118.01       6.81    -48.95       7.15    -62.35        0.0      4.01     10.06      7.96      0.00      4.10      3.30      2.29      3.90       0.0       0.0     6.20     5.20     4.96     5.23     5.30         4.85   7.27\n",
            "   ITC11   2013    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       7817.13       4.24       7817.13        969.13       4.24    100.37     98.61     0.00    10.02     0.00     1.90     0.00     1.11    -86.75       6.92      1.83       7.08     71.62        0.0      8.40      7.83     20.31     16.42      9.32      4.16      4.45     17.97       0.0       0.0     5.20     4.96     5.23     5.30     7.27         6.94   5.47\n",
            "   ITC11   2014    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0      10795.92       6.59      10795.92        695.09       6.34     92.90     75.92     7.79     6.47     0.11     0.07     0.00     0.08    -45.90       4.01      9.27       8.54    148.12        0.0     22.12      8.71      1.45      5.24      9.62      5.19      7.34      7.39       0.0       0.0     4.96     5.23     5.30     7.27     5.47         6.56   5.90\n",
            "   ITC11   2015    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       8590.39       5.61       8590.39        370.42       5.61     97.56     68.11     5.50    10.87     0.49     0.24     0.27     0.00    -37.00       9.34    158.93       8.31    226.06        0.0      0.40      4.69     42.06     34.69      9.36      6.69      6.20      4.61       0.0       0.0     5.23     5.30     7.27     5.47     5.90         6.29   5.60\n",
            "   ITC11   2016    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0      10098.18       6.21          0.00          0.00       0.00     83.63      0.00    15.73     1.73     0.00     0.14     0.00     0.00      4.93       5.66     14.92       8.21     32.67        0.0      8.90     10.41      5.26      0.00      5.92      2.99      1.16      4.75       0.0       0.0     5.30     7.27     5.47     5.90     5.60         5.68   6.00\n",
            "\n",
            "All features\n",
            "-------------\n",
            "\n",
            "1: SM_WHC, 2: AVG_ELEV, 3: STD_ELEV, 4: AVG_SLOPE, 5: STD_SLOPE\n",
            "6: AVG_FIELD_SIZE, 7: STD_FIELD_SIZE, 8: IRRIG_AREA_ALL, 9: IRRIG_AREA90, 10: AEZ_1960\n",
            "11: AEZ_1984, 12: AEZ_2030, 13: AEZ_2107, 14: AEZ_2150, 15: maxWLIM_YBp2\n",
            "16: maxWLAIp2, 17: maxWLIM_YBp4, 18: maxWLIM_YSp4, 19: maxWLAIp4, 20: avgRSMp2\n",
            "21: avgRSMp4, 22: Z-RSMp2, 23: Z+RSMp2, 24: Z-RSMp3, 25: Z+RSMp3\n",
            "26: Z-RSMp4, 27: Z+RSMp4, 28: avgCWBp0, 29: avgTAVGp1, 30: avgCWBp1\n",
            "31: avgTAVGp2, 32: avgCWBp2, 33: avgPRECp5, 34: Z-TMINp1, 35: Z-PRECp1\n",
            "36: Z+TMINp1, 37: Z+PRECp1, 38: Z-TMAXp3, 39: Z-PRECp3, 40: Z+TMAXp3\n",
            "41: Z+PRECp3, 42: Z-PRECp5, 43: Z+PRECp5, 44: YIELD-5, 45: YIELD-4\n",
            "46: YIELD-3, 47: YIELD-2, 48: YIELD-1\n",
            "\n",
            "\n",
            "Custom sliding validation train, test splits\n",
            "----------------------------------------------\n",
            "Validation set 1 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006\n",
            "Validation set 1 test years: 2007\n",
            "Validation set 2 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007\n",
            "Validation set 2 test years: 2008\n",
            "Validation set 3 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008\n",
            "Validation set 3 test years: 2009\n",
            "Validation set 4 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009\n",
            "Validation set 4 test years: 2010\n",
            "Validation set 5 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010\n",
            "Validation set 5 test years: 2011\n",
            "\n",
            "\n",
            "Null Method: Predicting linear yield trend:\n",
            "Min Yield: 0.3, Max Yield: 7.6\n",
            "Median Yield: 4.11, Mean Yield: 4.23\n",
            "\n",
            " Yield Predictions Training Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2000        6.07  5.83\n",
            "   ITC11  2001        6.36  5.38\n",
            "   ITC11  2002        6.05  4.79\n",
            "   ITC11  2003        4.88   4.8\n",
            "   ITC11  2004        4.39  5.54\n",
            "   ITC11  2005        4.92   5.5\n",
            "\n",
            " Yield Predictions Validation Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2007        7.06   6.2\n",
            "   ITC11  2008        7.09   5.2\n",
            "   ITC11  2009        5.89  4.96\n",
            "   ITC11  2010        4.91  5.23\n",
            "   ITC11  2011        4.28   5.3\n",
            "   ITC12  2007        5.41   5.1\n",
            "\n",
            " Yield Predictions Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2012        4.85  7.27\n",
            "   ITC11  2013        6.94  5.47\n",
            "   ITC11  2014        6.56   5.9\n",
            "   ITC11  2015        6.29   5.6\n",
            "   ITC11  2016        5.68   6.0\n",
            "   ITC11  2017        5.32   5.4\n",
            "\n",
            "Estimator: GBDT\n",
            "---------------------------\n",
            "\n",
            "best parameters:\n",
            "estimator__learning_rate=0.05359185282737429\n",
            "estimator__loss=huber\n",
            "estimator__min_samples_leaf=20\n",
            "selector__estimator__min_samples_leaf=5\n",
            "selector__max_features=10\n",
            "\n",
            "Best selector: random_forest\n",
            "Feature Selection Summary\n",
            "---------------------------\n",
            "estimator      selector  mean score  std score  selector score\n",
            "     GBDT random_forest       -0.56       0.16           -0.72\n",
            "     GBDT     RFE_Lasso       -0.55       0.19           -0.75\n",
            "\n",
            "\n",
            "Selected features with importance:\n",
            "----------------------------------\n",
            "\n",
            "48: YIELD-1=0.34, 47: YIELD-2=0.23, 45: YIELD-4=0.21, 44: YIELD-5=0.12, 8: IRRIG_AREA_ALL=0.04\n",
            "31: avgTAVGp2=0.04, 46: YIELD-3=0.02, 4: AVG_SLOPE=0.0, 15: maxWLIM_YBp2=0.0, 9: IRRIG_AREA90=0.0\n",
            "\n",
            "\n",
            "\n",
            "Feature Selection Frequencies\n",
            "-------------------------------\n",
            "static: AVG_SLOPE(1), IRRIG_AREA_ALL(1), IRRIG_AREA90(1), YIELD-5(1), YIELD-4(1), YIELD-3(1), YIELD-2(1), YIELD-1(1)\n",
            "p0: \n",
            "p1: \n",
            "p2: maxWLIM_YBp2(1), avgTAVGp2(1)\n",
            "p3: \n",
            "p4: \n",
            "p5: \n",
            "\n",
            "\n",
            "   IDREGION FYEAR YIELD YIELD_PRED_GBDT\n",
            "0    ITC11  2012  7.27        5.318463\n",
            "1    ITC11  2013  5.47        5.595588\n",
            "2    ITC11  2014   5.9        5.495468\n",
            "3    ITC11  2015   5.6        5.602078\n",
            "4    ITC11  2016   6.0        5.605179\n",
            "\n",
            " Yield Predictions Training Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2000        6.07        5.550141  5.83\n",
            "   ITC11  2001        6.36        5.539795  5.38\n",
            "   ITC11  2002        6.05        5.553078  4.79\n",
            "   ITC11  2003        4.88        5.143584   4.8\n",
            "   ITC11  2004        4.39        5.085547  5.54\n",
            "   ITC11  2005        4.92         5.31908   5.5\n",
            "\n",
            " Yield Predictions Validation Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2007        7.06        5.539142   6.2\n",
            "   ITC11  2008        7.09        6.060916   5.2\n",
            "   ITC11  2009        5.89        5.582064  4.96\n",
            "   ITC11  2010        4.91        5.194411  5.23\n",
            "   ITC11  2011        4.28         5.17805   5.3\n",
            "   ITC12  2007        5.41        4.976394   5.1\n",
            "\n",
            " Yield Predictions Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2012        4.85        5.318463  7.27\n",
            "   ITC11  2013        6.94        5.595588  5.47\n",
            "   ITC11  2014        6.56        5.495468   5.9\n",
            "   ITC11  2015        6.29        5.602078   5.6\n",
            "   ITC11  2016        5.68        5.605179   6.0\n",
            "   ITC11  2017        5.32         5.64657   5.4\n",
            "\n",
            "Algorithm Evaluation Summary for IT\n",
            "-----------------------------------------\n",
            "algorithm  train_MAPE  cv_MAPE  test_MAPE  train_RMSE  cv_RMSE  test_RMSE  train_R2  cv_R2  test_R2\n",
            "    trend       18.79    23.61      20.93       20.27    22.64      21.68      0.65   0.55     0.63\n",
            "     GBDT       15.78    20.29      21.14       15.05    17.81      17.80      0.81   0.72     0.75\n",
            "\n",
            "\n",
            "Training and Evaluation\n",
            "-------------------------\n",
            "\n",
            "Training Data Size: 711 rows\n",
            "X cols: 48, Y cols: 4\n",
            "IDREGION  FYEAR  SM_WHC   AVG_ELEV  STD_ELEV  AVG_SLOPE  STD_SLOPE  AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  IRRIG_AREA90  AEZ_1960  AEZ_1984  AEZ_2030  AEZ_2107  AEZ_2150  maxWLIM_YBp2  maxWLAIp2  maxWLIM_YBp4  maxWLIM_YSp4  maxWLAIp4  avgRSMp2  avgRSMp4  Z-RSMp2  Z+RSMp2  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4  avgCWBp0  avgTAVGp1  avgCWBp1  avgTAVGp2  avgCWBp2  avgPRECp5  Z-TMINp1  Z-PRECp1  Z+TMINp1  Z+PRECp1  Z-TMAXp3  Z-PRECp3  Z+TMAXp3  Z+PRECp3  Z-PRECp5  Z+PRECp5  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  YIELD\n",
            "   ITC11   2000    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       7718.48       5.46           0.0           0.0        0.0     95.43       0.0     1.02     3.17     0.00     1.69      0.0      0.0     41.16       3.52    134.39       7.41    139.75        0.0     19.19      7.97      0.90      2.62      2.85      1.15      2.71      9.94       0.0       0.0     4.50     4.74     4.45     5.56     5.84         6.07   5.83\n",
            "   ITC11   2001    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       9854.76       6.68           0.0           0.0        0.0     95.10       0.0     4.70     6.50     0.00     0.83      0.0      0.0     89.30       7.62    321.07       8.05    368.68        0.0      0.34      5.45     27.14     18.12      6.86      1.13      1.32      8.97       0.0       0.0     4.74     4.45     5.56     5.84     5.83         6.36   5.38\n",
            "   ITC11   2002    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       6712.30       5.20           0.0           0.0        0.0     85.65       0.0    13.91     2.68     0.00     1.55      0.0      0.0   -101.08       2.72   -110.58       6.52    -76.81        0.0     31.91     10.30      2.37      0.02     16.28      1.17      0.00     18.05       0.0       0.0     4.45     5.56     5.84     5.83     5.38         6.05   4.79\n",
            "   ITC11   2003    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       5945.80       4.56           0.0           0.0        0.0     94.40       0.0     4.05     4.84     0.56     0.00      0.0      0.0     58.65       8.08    200.01       6.97    221.74        0.0      1.17      4.28     34.97     32.32      1.73      3.79      8.20      1.32       0.0       0.0     5.56     5.84     5.83     5.38     4.79         4.88   4.80\n",
            "   ITC11   2004    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       8238.97       4.96           0.0           0.0        0.0     98.49       0.0     2.14     8.88     0.42     0.00      0.0      0.0    -69.47       5.95     35.28       6.80     94.43        0.0      8.71      6.70     20.15     21.59      1.45      3.80      4.59      0.00       0.0       0.0     5.84     5.83     5.38     4.79     4.80         4.39   5.54\n",
            "\n",
            "Test Data Size: 424 rows\n",
            "X cols: 48, Y cols: 4\n",
            "IDREGION  FYEAR  SM_WHC   AVG_ELEV  STD_ELEV  AVG_SLOPE  STD_SLOPE  AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  IRRIG_AREA90  AEZ_1960  AEZ_1984  AEZ_2030  AEZ_2107  AEZ_2150  maxWLIM_YBp2  maxWLAIp2  maxWLIM_YBp4  maxWLIM_YSp4  maxWLAIp4  avgRSMp2  avgRSMp4  Z-RSMp2  Z+RSMp2  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4  avgCWBp0  avgTAVGp1  avgCWBp1  avgTAVGp2  avgCWBp2  avgPRECp5  Z-TMINp1  Z-PRECp1  Z+TMINp1  Z+PRECp1  Z-TMAXp3  Z-PRECp3  Z+TMAXp3  Z+PRECp3  Z-PRECp5  Z+PRECp5  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  YIELD\n",
            "   ITC11   2012    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       9105.92       5.96          0.00          0.00       0.00     77.46      0.00    24.76     0.91     0.00     0.46     0.00     0.00   -118.01       6.81    -48.95       7.15    -62.35        0.0      4.01     10.06      7.96      0.00      4.10      3.30      2.29      3.90       0.0       0.0     6.20     5.20     4.96     5.23     5.30         4.85   7.27\n",
            "   ITC11   2013    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       7817.13       4.24       7817.13        969.13       4.24    100.37     98.61     0.00    10.02     0.00     1.90     0.00     1.11    -86.75       6.92      1.83       7.08     71.62        0.0      8.40      7.83     20.31     16.42      9.32      4.16      4.45     17.97       0.0       0.0     5.20     4.96     5.23     5.30     7.27         6.94   5.47\n",
            "   ITC11   2014    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0      10795.92       6.59      10795.92        695.09       6.34     92.90     75.92     7.79     6.47     0.11     0.07     0.00     0.08    -45.90       4.01      9.27       8.54    148.12        0.0     22.12      8.71      1.45      5.24      9.62      5.19      7.34      7.39       0.0       0.0     4.96     5.23     5.30     7.27     5.47         6.56   5.90\n",
            "   ITC11   2015    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       8590.39       5.61       8590.39        370.42       5.61     97.56     68.11     5.50    10.87     0.49     0.24     0.27     0.00    -37.00       9.34    158.93       8.31    226.06        0.0      0.40      4.69     42.06     34.69      9.36      6.69      6.20      4.61       0.0       0.0     5.23     5.30     7.27     5.47     5.90         6.29   5.60\n",
            "   ITC11   2016    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0      10098.18       6.21          0.00          0.00       0.00     83.63      0.00    15.73     1.73     0.00     0.14     0.00     0.00      4.93       5.66     14.92       8.21     32.67        0.0      8.90     10.41      5.26      0.00      5.92      2.99      1.16      4.75       0.0       0.0     5.30     7.27     5.47     5.90     5.60         5.68   6.00\n",
            "\n",
            "All features\n",
            "-------------\n",
            "\n",
            "1: SM_WHC, 2: AVG_ELEV, 3: STD_ELEV, 4: AVG_SLOPE, 5: STD_SLOPE\n",
            "6: AVG_FIELD_SIZE, 7: STD_FIELD_SIZE, 8: IRRIG_AREA_ALL, 9: IRRIG_AREA90, 10: AEZ_1960\n",
            "11: AEZ_1984, 12: AEZ_2030, 13: AEZ_2107, 14: AEZ_2150, 15: maxWLIM_YBp2\n",
            "16: maxWLAIp2, 17: maxWLIM_YBp4, 18: maxWLIM_YSp4, 19: maxWLAIp4, 20: avgRSMp2\n",
            "21: avgRSMp4, 22: Z-RSMp2, 23: Z+RSMp2, 24: Z-RSMp3, 25: Z+RSMp3\n",
            "26: Z-RSMp4, 27: Z+RSMp4, 28: avgCWBp0, 29: avgTAVGp1, 30: avgCWBp1\n",
            "31: avgTAVGp2, 32: avgCWBp2, 33: avgPRECp5, 34: Z-TMINp1, 35: Z-PRECp1\n",
            "36: Z+TMINp1, 37: Z+PRECp1, 38: Z-TMAXp3, 39: Z-PRECp3, 40: Z+TMAXp3\n",
            "41: Z+PRECp3, 42: Z-PRECp5, 43: Z+PRECp5, 44: YIELD-5, 45: YIELD-4\n",
            "46: YIELD-3, 47: YIELD-2, 48: YIELD-1\n",
            "\n",
            "\n",
            "Custom sliding validation train, test splits\n",
            "----------------------------------------------\n",
            "Validation set 1 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006\n",
            "Validation set 1 test years: 2007\n",
            "Validation set 2 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007\n",
            "Validation set 2 test years: 2008\n",
            "Validation set 3 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008\n",
            "Validation set 3 test years: 2009\n",
            "Validation set 4 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009\n",
            "Validation set 4 test years: 2010\n",
            "Validation set 5 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010\n",
            "Validation set 5 test years: 2011\n",
            "\n",
            "\n",
            "Null Method: Predicting linear yield trend:\n",
            "Min Yield: 0.3, Max Yield: 7.6\n",
            "Median Yield: 4.11, Mean Yield: 4.23\n",
            "\n",
            " Yield Predictions Training Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2000        6.07  5.83\n",
            "   ITC11  2001        6.36  5.38\n",
            "   ITC11  2002        6.05  4.79\n",
            "   ITC11  2003        4.88   4.8\n",
            "   ITC11  2004        4.39  5.54\n",
            "   ITC11  2005        4.92   5.5\n",
            "\n",
            " Yield Predictions Validation Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2007        7.06   6.2\n",
            "   ITC11  2008        7.09   5.2\n",
            "   ITC11  2009        5.89  4.96\n",
            "   ITC11  2010        4.91  5.23\n",
            "   ITC11  2011        4.28   5.3\n",
            "   ITC12  2007        5.41   5.1\n",
            "\n",
            " Yield Predictions Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2012        4.85  7.27\n",
            "   ITC11  2013        6.94  5.47\n",
            "   ITC11  2014        6.56   5.9\n",
            "   ITC11  2015        6.29   5.6\n",
            "   ITC11  2016        5.68   6.0\n",
            "   ITC11  2017        5.32   5.4\n",
            "\n",
            "Estimator: GBDT\n",
            "---------------------------\n",
            "\n",
            "best parameters:\n",
            "estimator__learning_rate=0.1\n",
            "estimator__loss=huber\n",
            "estimator__min_samples_leaf=5\n",
            "selector__estimator__min_samples_leaf=20\n",
            "selector__max_features=10\n",
            "\n",
            "Best selector: random_forest\n",
            "Feature Selection Summary\n",
            "---------------------------\n",
            "estimator      selector  mean score  std score  selector score\n",
            "     GBDT random_forest       -0.55       0.16           -0.72\n",
            "     GBDT     RFE_Lasso       -0.56       0.17           -0.73\n",
            "\n",
            "\n",
            "Selected features with importance:\n",
            "----------------------------------\n",
            "\n",
            "48: YIELD-1=0.34, 45: YIELD-4=0.32, 47: YIELD-2=0.24, 46: YIELD-3=0.03, 44: YIELD-5=0.03\n",
            "31: avgTAVGp2=0.02, 9: IRRIG_AREA90=0.01, 4: AVG_SLOPE=0.01, 8: IRRIG_AREA_ALL=0.01, 15: maxWLIM_YBp2=0.0\n",
            "\n",
            "\n",
            "\n",
            "Feature Selection Frequencies\n",
            "-------------------------------\n",
            "static: AVG_SLOPE(1), IRRIG_AREA_ALL(1), IRRIG_AREA90(1), YIELD-5(1), YIELD-4(1), YIELD-3(1), YIELD-2(1), YIELD-1(1)\n",
            "p0: \n",
            "p1: \n",
            "p2: maxWLIM_YBp2(1), avgTAVGp2(1)\n",
            "p3: \n",
            "p4: \n",
            "p5: \n",
            "\n",
            "\n",
            "   IDREGION FYEAR YIELD YIELD_PRED_GBDT\n",
            "0    ITC11  2012  7.27        5.393187\n",
            "1    ITC11  2013  5.47        5.633086\n",
            "2    ITC11  2014   5.9        5.562102\n",
            "3    ITC11  2015   5.6         5.65656\n",
            "4    ITC11  2016   6.0         5.69168\n",
            "\n",
            " Yield Predictions Training Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2000        6.07        5.572637  5.83\n",
            "   ITC11  2001        6.36        5.748569  5.38\n",
            "   ITC11  2002        6.05        5.611477  4.79\n",
            "   ITC11  2003        4.88        5.322038   4.8\n",
            "   ITC11  2004        4.39        5.095224  5.54\n",
            "   ITC11  2005        4.92        5.306487   5.5\n",
            "\n",
            " Yield Predictions Validation Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2007        7.06        5.453355   6.2\n",
            "   ITC11  2008        7.09        6.033929   5.2\n",
            "   ITC11  2009        5.89        5.708761  4.96\n",
            "   ITC11  2010        4.91        5.398687  5.23\n",
            "   ITC11  2011        4.28        5.158678   5.3\n",
            "   ITC12  2007        5.41        4.788559   5.1\n",
            "\n",
            " Yield Predictions Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2012        4.85        5.393187  7.27\n",
            "   ITC11  2013        6.94        5.633086  5.47\n",
            "   ITC11  2014        6.56        5.562102   5.9\n",
            "   ITC11  2015        6.29         5.65656   5.6\n",
            "   ITC11  2016        5.68         5.69168   6.0\n",
            "   ITC11  2017        5.32        5.710965   5.4\n",
            "\n",
            "Algorithm Evaluation Summary for IT\n",
            "-----------------------------------------\n",
            "algorithm  train_MAPE  cv_MAPE  test_MAPE  train_RMSE  cv_RMSE  test_RMSE  train_R2  cv_R2  test_R2\n",
            "    trend       18.79    23.61      20.93       20.27    22.64      21.68      0.65   0.55     0.63\n",
            "     GBDT       15.45    19.78      20.98       14.94    17.82      17.56      0.81   0.72     0.76\n",
            "\n",
            "\n",
            "Training and Evaluation\n",
            "-------------------------\n",
            "\n",
            "Training Data Size: 711 rows\n",
            "X cols: 48, Y cols: 4\n",
            "IDREGION  FYEAR  SM_WHC   AVG_ELEV  STD_ELEV  AVG_SLOPE  STD_SLOPE  AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  IRRIG_AREA90  AEZ_1960  AEZ_1984  AEZ_2030  AEZ_2107  AEZ_2150  maxWLIM_YBp2  maxWLAIp2  maxWLIM_YBp4  maxWLIM_YSp4  maxWLAIp4  avgRSMp2  avgRSMp4  Z-RSMp2  Z+RSMp2  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4  avgCWBp0  avgTAVGp1  avgCWBp1  avgTAVGp2  avgCWBp2  avgPRECp5  Z-TMINp1  Z-PRECp1  Z+TMINp1  Z+PRECp1  Z-TMAXp3  Z-PRECp3  Z+TMAXp3  Z+PRECp3  Z-PRECp5  Z+PRECp5  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  YIELD\n",
            "   ITC11   2000    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       7718.48       5.46           0.0           0.0        0.0     95.43       0.0     1.02     3.17     0.00     1.69      0.0      0.0     41.16       3.52    134.39       7.41    139.75        0.0     19.19      7.97      0.90      2.62      2.85      1.15      2.71      9.94       0.0       0.0     4.50     4.74     4.45     5.56     5.84         6.07   5.83\n",
            "   ITC11   2001    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       9854.76       6.68           0.0           0.0        0.0     95.10       0.0     4.70     6.50     0.00     0.83      0.0      0.0     89.30       7.62    321.07       8.05    368.68        0.0      0.34      5.45     27.14     18.12      6.86      1.13      1.32      8.97       0.0       0.0     4.74     4.45     5.56     5.84     5.83         6.36   5.38\n",
            "   ITC11   2002    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       6712.30       5.20           0.0           0.0        0.0     85.65       0.0    13.91     2.68     0.00     1.55      0.0      0.0   -101.08       2.72   -110.58       6.52    -76.81        0.0     31.91     10.30      2.37      0.02     16.28      1.17      0.00     18.05       0.0       0.0     4.45     5.56     5.84     5.83     5.38         6.05   4.79\n",
            "   ITC11   2003    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       5945.80       4.56           0.0           0.0        0.0     94.40       0.0     4.05     4.84     0.56     0.00      0.0      0.0     58.65       8.08    200.01       6.97    221.74        0.0      1.17      4.28     34.97     32.32      1.73      3.79      8.20      1.32       0.0       0.0     5.56     5.84     5.83     5.38     4.79         4.88   4.80\n",
            "   ITC11   2004    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       8238.97       4.96           0.0           0.0        0.0     98.49       0.0     2.14     8.88     0.42     0.00      0.0      0.0    -69.47       5.95     35.28       6.80     94.43        0.0      8.71      6.70     20.15     21.59      1.45      3.80      4.59      0.00       0.0       0.0     5.84     5.83     5.38     4.79     4.80         4.39   5.54\n",
            "\n",
            "Test Data Size: 424 rows\n",
            "X cols: 48, Y cols: 4\n",
            "IDREGION  FYEAR  SM_WHC   AVG_ELEV  STD_ELEV  AVG_SLOPE  STD_SLOPE  AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  IRRIG_AREA90  AEZ_1960  AEZ_1984  AEZ_2030  AEZ_2107  AEZ_2150  maxWLIM_YBp2  maxWLAIp2  maxWLIM_YBp4  maxWLIM_YSp4  maxWLAIp4  avgRSMp2  avgRSMp4  Z-RSMp2  Z+RSMp2  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4  avgCWBp0  avgTAVGp1  avgCWBp1  avgTAVGp2  avgCWBp2  avgPRECp5  Z-TMINp1  Z-PRECp1  Z+TMINp1  Z+PRECp1  Z-TMAXp3  Z-PRECp3  Z+TMAXp3  Z+PRECp3  Z-PRECp5  Z+PRECp5  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  YIELD\n",
            "   ITC11   2012    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       9105.92       5.96          0.00          0.00       0.00     77.46      0.00    24.76     0.91     0.00     0.46     0.00     0.00   -118.01       6.81    -48.95       7.15    -62.35        0.0      4.01     10.06      7.96      0.00      4.10      3.30      2.29      3.90       0.0       0.0     6.20     5.20     4.96     5.23     5.30         4.85   7.27\n",
            "   ITC11   2013    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       7817.13       4.24       7817.13        969.13       4.24    100.37     98.61     0.00    10.02     0.00     1.90     0.00     1.11    -86.75       6.92      1.83       7.08     71.62        0.0      8.40      7.83     20.31     16.42      9.32      4.16      4.45     17.97       0.0       0.0     5.20     4.96     5.23     5.30     7.27         6.94   5.47\n",
            "   ITC11   2014    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0      10795.92       6.59      10795.92        695.09       6.34     92.90     75.92     7.79     6.47     0.11     0.07     0.00     0.08    -45.90       4.01      9.27       8.54    148.12        0.0     22.12      8.71      1.45      5.24      9.62      5.19      7.34      7.39       0.0       0.0     4.96     5.23     5.30     7.27     5.47         6.56   5.90\n",
            "   ITC11   2015    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       8590.39       5.61       8590.39        370.42       5.61     97.56     68.11     5.50    10.87     0.49     0.24     0.27     0.00    -37.00       9.34    158.93       8.31    226.06        0.0      0.40      4.69     42.06     34.69      9.36      6.69      6.20      4.61       0.0       0.0     5.23     5.30     7.27     5.47     5.90         6.29   5.60\n",
            "   ITC11   2016    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0      10098.18       6.21          0.00          0.00       0.00     83.63      0.00    15.73     1.73     0.00     0.14     0.00     0.00      4.93       5.66     14.92       8.21     32.67        0.0      8.90     10.41      5.26      0.00      5.92      2.99      1.16      4.75       0.0       0.0     5.30     7.27     5.47     5.90     5.60         5.68   6.00\n",
            "\n",
            "All features\n",
            "-------------\n",
            "\n",
            "1: SM_WHC, 2: AVG_ELEV, 3: STD_ELEV, 4: AVG_SLOPE, 5: STD_SLOPE\n",
            "6: AVG_FIELD_SIZE, 7: STD_FIELD_SIZE, 8: IRRIG_AREA_ALL, 9: IRRIG_AREA90, 10: AEZ_1960\n",
            "11: AEZ_1984, 12: AEZ_2030, 13: AEZ_2107, 14: AEZ_2150, 15: maxWLIM_YBp2\n",
            "16: maxWLAIp2, 17: maxWLIM_YBp4, 18: maxWLIM_YSp4, 19: maxWLAIp4, 20: avgRSMp2\n",
            "21: avgRSMp4, 22: Z-RSMp2, 23: Z+RSMp2, 24: Z-RSMp3, 25: Z+RSMp3\n",
            "26: Z-RSMp4, 27: Z+RSMp4, 28: avgCWBp0, 29: avgTAVGp1, 30: avgCWBp1\n",
            "31: avgTAVGp2, 32: avgCWBp2, 33: avgPRECp5, 34: Z-TMINp1, 35: Z-PRECp1\n",
            "36: Z+TMINp1, 37: Z+PRECp1, 38: Z-TMAXp3, 39: Z-PRECp3, 40: Z+TMAXp3\n",
            "41: Z+PRECp3, 42: Z-PRECp5, 43: Z+PRECp5, 44: YIELD-5, 45: YIELD-4\n",
            "46: YIELD-3, 47: YIELD-2, 48: YIELD-1\n",
            "\n",
            "\n",
            "Custom sliding validation train, test splits\n",
            "----------------------------------------------\n",
            "Validation set 1 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006\n",
            "Validation set 1 test years: 2007\n",
            "Validation set 2 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007\n",
            "Validation set 2 test years: 2008\n",
            "Validation set 3 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008\n",
            "Validation set 3 test years: 2009\n",
            "Validation set 4 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009\n",
            "Validation set 4 test years: 2010\n",
            "Validation set 5 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010\n",
            "Validation set 5 test years: 2011\n",
            "\n",
            "\n",
            "Null Method: Predicting linear yield trend:\n",
            "Min Yield: 0.3, Max Yield: 7.6\n",
            "Median Yield: 4.11, Mean Yield: 4.23\n",
            "\n",
            " Yield Predictions Training Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2000        6.07  5.83\n",
            "   ITC11  2001        6.36  5.38\n",
            "   ITC11  2002        6.05  4.79\n",
            "   ITC11  2003        4.88   4.8\n",
            "   ITC11  2004        4.39  5.54\n",
            "   ITC11  2005        4.92   5.5\n",
            "\n",
            " Yield Predictions Validation Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2007        7.06   6.2\n",
            "   ITC11  2008        7.09   5.2\n",
            "   ITC11  2009        5.89  4.96\n",
            "   ITC11  2010        4.91  5.23\n",
            "   ITC11  2011        4.28   5.3\n",
            "   ITC12  2007        5.41   5.1\n",
            "\n",
            " Yield Predictions Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2012        4.85  7.27\n",
            "   ITC11  2013        6.94  5.47\n",
            "   ITC11  2014        6.56   5.9\n",
            "   ITC11  2015        6.29   5.6\n",
            "   ITC11  2016        5.68   6.0\n",
            "   ITC11  2017        5.32   5.4\n",
            "\n",
            "Estimator: GBDT\n",
            "---------------------------\n",
            "\n",
            "best parameters:\n",
            "estimator__learning_rate=0.06019498962669458\n",
            "estimator__loss=huber\n",
            "estimator__min_samples_leaf=7\n",
            "selector__estimator__min_samples_leaf=9\n",
            "selector__max_features=12\n",
            "\n",
            "Best selector: random_forest\n",
            "Feature Selection Summary\n",
            "---------------------------\n",
            "estimator      selector  mean score  std score  selector score\n",
            "     GBDT random_forest       -0.57       0.15           -0.72\n",
            "     GBDT     RFE_Lasso       -0.59       0.14           -0.74\n",
            "\n",
            "\n",
            "Selected features with importance:\n",
            "----------------------------------\n",
            "\n",
            "48: YIELD-1=0.29, 47: YIELD-2=0.17, 45: YIELD-4=0.16, 44: YIELD-5=0.16, 8: IRRIG_AREA_ALL=0.13\n",
            "31: avgTAVGp2=0.03, 46: YIELD-3=0.02, 2: AVG_ELEV=0.02, 29: avgTAVGp1=0.01, 4: AVG_SLOPE=0.01\n",
            "9: IRRIG_AREA90=0.0, 15: maxWLIM_YBp2=0.0\n",
            "\n",
            "\n",
            "Feature Selection Frequencies\n",
            "-------------------------------\n",
            "static: AVG_ELEV(1), AVG_SLOPE(1), IRRIG_AREA_ALL(1), IRRIG_AREA90(1), YIELD-5(1), YIELD-4(1), YIELD-3(1), YIELD-2(1), YIELD-1(1)\n",
            "p0: \n",
            "p1: avgTAVGp1(1)\n",
            "p2: maxWLIM_YBp2(1), avgTAVGp2(1)\n",
            "p3: \n",
            "p4: \n",
            "p5: \n",
            "\n",
            "\n",
            "   IDREGION FYEAR YIELD YIELD_PRED_GBDT\n",
            "0    ITC11  2012  7.27        5.199842\n",
            "1    ITC11  2013  5.47        5.613325\n",
            "2    ITC11  2014   5.9        5.399423\n",
            "3    ITC11  2015   5.6        5.468969\n",
            "4    ITC11  2016   6.0        5.412155\n",
            "\n",
            " Yield Predictions Training Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2000        6.07        5.555436  5.83\n",
            "   ITC11  2001        6.36        5.428611  5.38\n",
            "   ITC11  2002        6.05        5.402747  4.79\n",
            "   ITC11  2003        4.88        5.208108   4.8\n",
            "   ITC11  2004        4.39        5.075192  5.54\n",
            "   ITC11  2005        4.92        5.191597   5.5\n",
            "\n",
            " Yield Predictions Validation Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2007        7.06        5.565116   6.2\n",
            "   ITC11  2008        7.09        5.729801   5.2\n",
            "   ITC11  2009        5.89         5.53626  4.96\n",
            "   ITC11  2010        4.91        5.083434  5.23\n",
            "   ITC11  2011        4.28        5.209185   5.3\n",
            "   ITC12  2007        5.41        4.644101   5.1\n",
            "\n",
            " Yield Predictions Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2012        4.85        5.199842  7.27\n",
            "   ITC11  2013        6.94        5.613325  5.47\n",
            "   ITC11  2014        6.56        5.399423   5.9\n",
            "   ITC11  2015        6.29        5.468969   5.6\n",
            "   ITC11  2016        5.68        5.412155   6.0\n",
            "   ITC11  2017        5.32        5.582787   5.4\n",
            "\n",
            "Algorithm Evaluation Summary for IT\n",
            "-----------------------------------------\n",
            "algorithm  train_MAPE  cv_MAPE  test_MAPE  train_RMSE  cv_RMSE  test_RMSE  train_R2  cv_R2  test_R2\n",
            "    trend       18.79    23.61      20.93       20.27    22.64      21.68      0.65   0.55     0.63\n",
            "     GBDT       15.39    19.95      20.98       14.64    17.78      17.76      0.82   0.72     0.75\n",
            "\n",
            "\n",
            "Training and Evaluation\n",
            "-------------------------\n",
            "\n",
            "Training Data Size: 711 rows\n",
            "X cols: 48, Y cols: 4\n",
            "IDREGION  FYEAR  SM_WHC   AVG_ELEV  STD_ELEV  AVG_SLOPE  STD_SLOPE  AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  IRRIG_AREA90  AEZ_1960  AEZ_1984  AEZ_2030  AEZ_2107  AEZ_2150  maxWLIM_YBp2  maxWLAIp2  maxWLIM_YBp4  maxWLIM_YSp4  maxWLAIp4  avgRSMp2  avgRSMp4  Z-RSMp2  Z+RSMp2  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4  avgCWBp0  avgTAVGp1  avgCWBp1  avgTAVGp2  avgCWBp2  avgPRECp5  Z-TMINp1  Z-PRECp1  Z+TMINp1  Z+PRECp1  Z-TMAXp3  Z-PRECp3  Z+TMAXp3  Z+PRECp3  Z-PRECp5  Z+PRECp5  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  YIELD\n",
            "   ITC11   2000    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       7718.48       5.46           0.0           0.0        0.0     95.43       0.0     1.02     3.17     0.00     1.69      0.0      0.0     41.16       3.52    134.39       7.41    139.75        0.0     19.19      7.97      0.90      2.62      2.85      1.15      2.71      9.94       0.0       0.0     4.50     4.74     4.45     5.56     5.84         6.07   5.83\n",
            "   ITC11   2001    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       9854.76       6.68           0.0           0.0        0.0     95.10       0.0     4.70     6.50     0.00     0.83      0.0      0.0     89.30       7.62    321.07       8.05    368.68        0.0      0.34      5.45     27.14     18.12      6.86      1.13      1.32      8.97       0.0       0.0     4.74     4.45     5.56     5.84     5.83         6.36   5.38\n",
            "   ITC11   2002    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       6712.30       5.20           0.0           0.0        0.0     85.65       0.0    13.91     2.68     0.00     1.55      0.0      0.0   -101.08       2.72   -110.58       6.52    -76.81        0.0     31.91     10.30      2.37      0.02     16.28      1.17      0.00     18.05       0.0       0.0     4.45     5.56     5.84     5.83     5.38         6.05   4.79\n",
            "   ITC11   2003    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       5945.80       4.56           0.0           0.0        0.0     94.40       0.0     4.05     4.84     0.56     0.00      0.0      0.0     58.65       8.08    200.01       6.97    221.74        0.0      1.17      4.28     34.97     32.32      1.73      3.79      8.20      1.32       0.0       0.0     5.56     5.84     5.83     5.38     4.79         4.88   4.80\n",
            "   ITC11   2004    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       8238.97       4.96           0.0           0.0        0.0     98.49       0.0     2.14     8.88     0.42     0.00      0.0      0.0    -69.47       5.95     35.28       6.80     94.43        0.0      8.71      6.70     20.15     21.59      1.45      3.80      4.59      0.00       0.0       0.0     5.84     5.83     5.38     4.79     4.80         4.39   5.54\n",
            "\n",
            "Test Data Size: 424 rows\n",
            "X cols: 48, Y cols: 4\n",
            "IDREGION  FYEAR  SM_WHC   AVG_ELEV  STD_ELEV  AVG_SLOPE  STD_SLOPE  AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  IRRIG_AREA90  AEZ_1960  AEZ_1984  AEZ_2030  AEZ_2107  AEZ_2150  maxWLIM_YBp2  maxWLAIp2  maxWLIM_YBp4  maxWLIM_YSp4  maxWLAIp4  avgRSMp2  avgRSMp4  Z-RSMp2  Z+RSMp2  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4  avgCWBp0  avgTAVGp1  avgCWBp1  avgTAVGp2  avgCWBp2  avgPRECp5  Z-TMINp1  Z-PRECp1  Z+TMINp1  Z+PRECp1  Z-TMAXp3  Z-PRECp3  Z+TMAXp3  Z+PRECp3  Z-PRECp5  Z+PRECp5  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  YIELD\n",
            "   ITC11   2012    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       9105.92       5.96          0.00          0.00       0.00     77.46      0.00    24.76     0.91     0.00     0.46     0.00     0.00   -118.01       6.81    -48.95       7.15    -62.35        0.0      4.01     10.06      7.96      0.00      4.10      3.30      2.29      3.90       0.0       0.0     6.20     5.20     4.96     5.23     5.30         4.85   7.27\n",
            "   ITC11   2013    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       7817.13       4.24       7817.13        969.13       4.24    100.37     98.61     0.00    10.02     0.00     1.90     0.00     1.11    -86.75       6.92      1.83       7.08     71.62        0.0      8.40      7.83     20.31     16.42      9.32      4.16      4.45     17.97       0.0       0.0     5.20     4.96     5.23     5.30     7.27         6.94   5.47\n",
            "   ITC11   2014    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0      10795.92       6.59      10795.92        695.09       6.34     92.90     75.92     7.79     6.47     0.11     0.07     0.00     0.08    -45.90       4.01      9.27       8.54    148.12        0.0     22.12      8.71      1.45      5.24      9.62      5.19      7.34      7.39       0.0       0.0     4.96     5.23     5.30     7.27     5.47         6.56   5.90\n",
            "   ITC11   2015    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       8590.39       5.61       8590.39        370.42       5.61     97.56     68.11     5.50    10.87     0.49     0.24     0.27     0.00    -37.00       9.34    158.93       8.31    226.06        0.0      0.40      4.69     42.06     34.69      9.36      6.69      6.20      4.61       0.0       0.0     5.23     5.30     7.27     5.47     5.90         6.29   5.60\n",
            "   ITC11   2016    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0      10098.18       6.21          0.00          0.00       0.00     83.63      0.00    15.73     1.73     0.00     0.14     0.00     0.00      4.93       5.66     14.92       8.21     32.67        0.0      8.90     10.41      5.26      0.00      5.92      2.99      1.16      4.75       0.0       0.0     5.30     7.27     5.47     5.90     5.60         5.68   6.00\n",
            "\n",
            "All features\n",
            "-------------\n",
            "\n",
            "1: SM_WHC, 2: AVG_ELEV, 3: STD_ELEV, 4: AVG_SLOPE, 5: STD_SLOPE\n",
            "6: AVG_FIELD_SIZE, 7: STD_FIELD_SIZE, 8: IRRIG_AREA_ALL, 9: IRRIG_AREA90, 10: AEZ_1960\n",
            "11: AEZ_1984, 12: AEZ_2030, 13: AEZ_2107, 14: AEZ_2150, 15: maxWLIM_YBp2\n",
            "16: maxWLAIp2, 17: maxWLIM_YBp4, 18: maxWLIM_YSp4, 19: maxWLAIp4, 20: avgRSMp2\n",
            "21: avgRSMp4, 22: Z-RSMp2, 23: Z+RSMp2, 24: Z-RSMp3, 25: Z+RSMp3\n",
            "26: Z-RSMp4, 27: Z+RSMp4, 28: avgCWBp0, 29: avgTAVGp1, 30: avgCWBp1\n",
            "31: avgTAVGp2, 32: avgCWBp2, 33: avgPRECp5, 34: Z-TMINp1, 35: Z-PRECp1\n",
            "36: Z+TMINp1, 37: Z+PRECp1, 38: Z-TMAXp3, 39: Z-PRECp3, 40: Z+TMAXp3\n",
            "41: Z+PRECp3, 42: Z-PRECp5, 43: Z+PRECp5, 44: YIELD-5, 45: YIELD-4\n",
            "46: YIELD-3, 47: YIELD-2, 48: YIELD-1\n",
            "\n",
            "\n",
            "Custom sliding validation train, test splits\n",
            "----------------------------------------------\n",
            "Validation set 1 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006\n",
            "Validation set 1 test years: 2007\n",
            "Validation set 2 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007\n",
            "Validation set 2 test years: 2008\n",
            "Validation set 3 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008\n",
            "Validation set 3 test years: 2009\n",
            "Validation set 4 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009\n",
            "Validation set 4 test years: 2010\n",
            "Validation set 5 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010\n",
            "Validation set 5 test years: 2011\n",
            "\n",
            "\n",
            "Null Method: Predicting linear yield trend:\n",
            "Min Yield: 0.3, Max Yield: 7.6\n",
            "Median Yield: 4.11, Mean Yield: 4.23\n",
            "\n",
            " Yield Predictions Training Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2000        6.07  5.83\n",
            "   ITC11  2001        6.36  5.38\n",
            "   ITC11  2002        6.05  4.79\n",
            "   ITC11  2003        4.88   4.8\n",
            "   ITC11  2004        4.39  5.54\n",
            "   ITC11  2005        4.92   5.5\n",
            "\n",
            " Yield Predictions Validation Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2007        7.06   6.2\n",
            "   ITC11  2008        7.09   5.2\n",
            "   ITC11  2009        5.89  4.96\n",
            "   ITC11  2010        4.91  5.23\n",
            "   ITC11  2011        4.28   5.3\n",
            "   ITC12  2007        5.41   5.1\n",
            "\n",
            " Yield Predictions Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2012        4.85  7.27\n",
            "   ITC11  2013        6.94  5.47\n",
            "   ITC11  2014        6.56   5.9\n",
            "   ITC11  2015        6.29   5.6\n",
            "   ITC11  2016        5.68   6.0\n",
            "   ITC11  2017        5.32   5.4\n",
            "\n",
            "Estimator: GBDT\n",
            "---------------------------\n",
            "\n",
            "best parameters:\n",
            "estimator__learning_rate=0.04466429340197229\n",
            "estimator__loss=huber\n",
            "estimator__min_samples_leaf=6\n",
            "selector__estimator__min_samples_leaf=6\n",
            "selector__max_features=11\n",
            "\n",
            "Best selector: random_forest\n",
            "Feature Selection Summary\n",
            "---------------------------\n",
            "estimator      selector  mean score  std score  selector score\n",
            "     GBDT random_forest       -0.56       0.17           -0.72\n",
            "     GBDT     RFE_Lasso       -0.56       0.20           -0.75\n",
            "\n",
            "\n",
            "Selected features with importance:\n",
            "----------------------------------\n",
            "\n",
            "48: YIELD-1=0.23, 46: YIELD-3=0.19, 47: YIELD-2=0.16, 8: IRRIG_AREA_ALL=0.15, 45: YIELD-4=0.14\n",
            "44: YIELD-5=0.05, 31: avgTAVGp2=0.04, 4: AVG_SLOPE=0.02, 2: AVG_ELEV=0.01, 9: IRRIG_AREA90=0.01\n",
            "15: maxWLIM_YBp2=0.0\n",
            "\n",
            "\n",
            "Feature Selection Frequencies\n",
            "-------------------------------\n",
            "static: AVG_ELEV(1), AVG_SLOPE(1), IRRIG_AREA_ALL(1), IRRIG_AREA90(1), YIELD-5(1), YIELD-4(1), YIELD-3(1), YIELD-2(1), YIELD-1(1)\n",
            "p0: \n",
            "p1: \n",
            "p2: maxWLIM_YBp2(1), avgTAVGp2(1)\n",
            "p3: \n",
            "p4: \n",
            "p5: \n",
            "\n",
            "\n",
            "   IDREGION FYEAR YIELD YIELD_PRED_GBDT\n",
            "0    ITC11  2012  7.27        5.232116\n",
            "1    ITC11  2013  5.47         5.63776\n",
            "2    ITC11  2014   5.9         5.47938\n",
            "3    ITC11  2015   5.6        5.527979\n",
            "4    ITC11  2016   6.0        5.555874\n",
            "\n",
            " Yield Predictions Training Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2000        6.07        5.629129  5.83\n",
            "   ITC11  2001        6.36        5.501068  5.38\n",
            "   ITC11  2002        6.05        5.481001  4.79\n",
            "   ITC11  2003        4.88        5.297025   4.8\n",
            "   ITC11  2004        4.39        5.078416  5.54\n",
            "   ITC11  2005        4.92        5.248755   5.5\n",
            "\n",
            " Yield Predictions Validation Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2007        7.06        5.634899   6.2\n",
            "   ITC11  2008        7.09        6.053756   5.2\n",
            "   ITC11  2009        5.89         5.75374  4.96\n",
            "   ITC11  2010        4.91        5.306024  5.23\n",
            "   ITC11  2011        4.28        5.104619   5.3\n",
            "   ITC12  2007        5.41        4.586269   5.1\n",
            "\n",
            " Yield Predictions Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2012        4.85        5.232116  7.27\n",
            "   ITC11  2013        6.94         5.63776  5.47\n",
            "   ITC11  2014        6.56         5.47938   5.9\n",
            "   ITC11  2015        6.29        5.527979   5.6\n",
            "   ITC11  2016        5.68        5.555874   6.0\n",
            "   ITC11  2017        5.32        5.600251   5.4\n",
            "\n",
            "Algorithm Evaluation Summary for IT\n",
            "-----------------------------------------\n",
            "algorithm  train_MAPE  cv_MAPE  test_MAPE  train_RMSE  cv_RMSE  test_RMSE  train_R2  cv_R2  test_R2\n",
            "    trend       18.79    23.61      20.93       20.27    22.64      21.68      0.65   0.55     0.63\n",
            "     GBDT       16.06    19.99      21.41       15.00    17.88      17.86      0.81   0.72     0.75\n",
            "\n",
            "\n",
            "Training and Evaluation\n",
            "-------------------------\n",
            "\n",
            "Training Data Size: 711 rows\n",
            "X cols: 48, Y cols: 4\n",
            "IDREGION  FYEAR  SM_WHC   AVG_ELEV  STD_ELEV  AVG_SLOPE  STD_SLOPE  AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  IRRIG_AREA90  AEZ_1960  AEZ_1984  AEZ_2030  AEZ_2107  AEZ_2150  maxWLIM_YBp2  maxWLAIp2  maxWLIM_YBp4  maxWLIM_YSp4  maxWLAIp4  avgRSMp2  avgRSMp4  Z-RSMp2  Z+RSMp2  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4  avgCWBp0  avgTAVGp1  avgCWBp1  avgTAVGp2  avgCWBp2  avgPRECp5  Z-TMINp1  Z-PRECp1  Z+TMINp1  Z+PRECp1  Z-TMAXp3  Z-PRECp3  Z+TMAXp3  Z+PRECp3  Z-PRECp5  Z+PRECp5  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  YIELD\n",
            "   ITC11   2000    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       7718.48       5.46           0.0           0.0        0.0     95.43       0.0     1.02     3.17     0.00     1.69      0.0      0.0     41.16       3.52    134.39       7.41    139.75        0.0     19.19      7.97      0.90      2.62      2.85      1.15      2.71      9.94       0.0       0.0     4.50     4.74     4.45     5.56     5.84         6.07   5.83\n",
            "   ITC11   2001    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       9854.76       6.68           0.0           0.0        0.0     95.10       0.0     4.70     6.50     0.00     0.83      0.0      0.0     89.30       7.62    321.07       8.05    368.68        0.0      0.34      5.45     27.14     18.12      6.86      1.13      1.32      8.97       0.0       0.0     4.74     4.45     5.56     5.84     5.83         6.36   5.38\n",
            "   ITC11   2002    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       6712.30       5.20           0.0           0.0        0.0     85.65       0.0    13.91     2.68     0.00     1.55      0.0      0.0   -101.08       2.72   -110.58       6.52    -76.81        0.0     31.91     10.30      2.37      0.02     16.28      1.17      0.00     18.05       0.0       0.0     4.45     5.56     5.84     5.83     5.38         6.05   4.79\n",
            "   ITC11   2003    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       5945.80       4.56           0.0           0.0        0.0     94.40       0.0     4.05     4.84     0.56     0.00      0.0      0.0     58.65       8.08    200.01       6.97    221.74        0.0      1.17      4.28     34.97     32.32      1.73      3.79      8.20      1.32       0.0       0.0     5.56     5.84     5.83     5.38     4.79         4.88   4.80\n",
            "   ITC11   2004    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       8238.97       4.96           0.0           0.0        0.0     98.49       0.0     2.14     8.88     0.42     0.00      0.0      0.0    -69.47       5.95     35.28       6.80     94.43        0.0      8.71      6.70     20.15     21.59      1.45      3.80      4.59      0.00       0.0       0.0     5.84     5.83     5.38     4.79     4.80         4.39   5.54\n",
            "\n",
            "Test Data Size: 424 rows\n",
            "X cols: 48, Y cols: 4\n",
            "IDREGION  FYEAR  SM_WHC   AVG_ELEV  STD_ELEV  AVG_SLOPE  STD_SLOPE  AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  IRRIG_AREA90  AEZ_1960  AEZ_1984  AEZ_2030  AEZ_2107  AEZ_2150  maxWLIM_YBp2  maxWLAIp2  maxWLIM_YBp4  maxWLIM_YSp4  maxWLAIp4  avgRSMp2  avgRSMp4  Z-RSMp2  Z+RSMp2  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4  avgCWBp0  avgTAVGp1  avgCWBp1  avgTAVGp2  avgCWBp2  avgPRECp5  Z-TMINp1  Z-PRECp1  Z+TMINp1  Z+PRECp1  Z-TMAXp3  Z-PRECp3  Z+TMAXp3  Z+PRECp3  Z-PRECp5  Z+PRECp5  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  YIELD\n",
            "   ITC11   2012    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       9105.92       5.96          0.00          0.00       0.00     77.46      0.00    24.76     0.91     0.00     0.46     0.00     0.00   -118.01       6.81    -48.95       7.15    -62.35        0.0      4.01     10.06      7.96      0.00      4.10      3.30      2.29      3.90       0.0       0.0     6.20     5.20     4.96     5.23     5.30         4.85   7.27\n",
            "   ITC11   2013    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       7817.13       4.24       7817.13        969.13       4.24    100.37     98.61     0.00    10.02     0.00     1.90     0.00     1.11    -86.75       6.92      1.83       7.08     71.62        0.0      8.40      7.83     20.31     16.42      9.32      4.16      4.45     17.97       0.0       0.0     5.20     4.96     5.23     5.30     7.27         6.94   5.47\n",
            "   ITC11   2014    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0      10795.92       6.59      10795.92        695.09       6.34     92.90     75.92     7.79     6.47     0.11     0.07     0.00     0.08    -45.90       4.01      9.27       8.54    148.12        0.0     22.12      8.71      1.45      5.24      9.62      5.19      7.34      7.39       0.0       0.0     4.96     5.23     5.30     7.27     5.47         6.56   5.90\n",
            "   ITC11   2015    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       8590.39       5.61       8590.39        370.42       5.61     97.56     68.11     5.50    10.87     0.49     0.24     0.27     0.00    -37.00       9.34    158.93       8.31    226.06        0.0      0.40      4.69     42.06     34.69      9.36      6.69      6.20      4.61       0.0       0.0     5.23     5.30     7.27     5.47     5.90         6.29   5.60\n",
            "   ITC11   2016    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0      10098.18       6.21          0.00          0.00       0.00     83.63      0.00    15.73     1.73     0.00     0.14     0.00     0.00      4.93       5.66     14.92       8.21     32.67        0.0      8.90     10.41      5.26      0.00      5.92      2.99      1.16      4.75       0.0       0.0     5.30     7.27     5.47     5.90     5.60         5.68   6.00\n",
            "\n",
            "All features\n",
            "-------------\n",
            "\n",
            "1: SM_WHC, 2: AVG_ELEV, 3: STD_ELEV, 4: AVG_SLOPE, 5: STD_SLOPE\n",
            "6: AVG_FIELD_SIZE, 7: STD_FIELD_SIZE, 8: IRRIG_AREA_ALL, 9: IRRIG_AREA90, 10: AEZ_1960\n",
            "11: AEZ_1984, 12: AEZ_2030, 13: AEZ_2107, 14: AEZ_2150, 15: maxWLIM_YBp2\n",
            "16: maxWLAIp2, 17: maxWLIM_YBp4, 18: maxWLIM_YSp4, 19: maxWLAIp4, 20: avgRSMp2\n",
            "21: avgRSMp4, 22: Z-RSMp2, 23: Z+RSMp2, 24: Z-RSMp3, 25: Z+RSMp3\n",
            "26: Z-RSMp4, 27: Z+RSMp4, 28: avgCWBp0, 29: avgTAVGp1, 30: avgCWBp1\n",
            "31: avgTAVGp2, 32: avgCWBp2, 33: avgPRECp5, 34: Z-TMINp1, 35: Z-PRECp1\n",
            "36: Z+TMINp1, 37: Z+PRECp1, 38: Z-TMAXp3, 39: Z-PRECp3, 40: Z+TMAXp3\n",
            "41: Z+PRECp3, 42: Z-PRECp5, 43: Z+PRECp5, 44: YIELD-5, 45: YIELD-4\n",
            "46: YIELD-3, 47: YIELD-2, 48: YIELD-1\n",
            "\n",
            "\n",
            "Custom sliding validation train, test splits\n",
            "----------------------------------------------\n",
            "Validation set 1 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006\n",
            "Validation set 1 test years: 2007\n",
            "Validation set 2 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007\n",
            "Validation set 2 test years: 2008\n",
            "Validation set 3 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008\n",
            "Validation set 3 test years: 2009\n",
            "Validation set 4 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009\n",
            "Validation set 4 test years: 2010\n",
            "Validation set 5 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010\n",
            "Validation set 5 test years: 2011\n",
            "\n",
            "\n",
            "Null Method: Predicting linear yield trend:\n",
            "Min Yield: 0.3, Max Yield: 7.6\n",
            "Median Yield: 4.11, Mean Yield: 4.23\n",
            "\n",
            " Yield Predictions Training Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2000        6.07  5.83\n",
            "   ITC11  2001        6.36  5.38\n",
            "   ITC11  2002        6.05  4.79\n",
            "   ITC11  2003        4.88   4.8\n",
            "   ITC11  2004        4.39  5.54\n",
            "   ITC11  2005        4.92   5.5\n",
            "\n",
            " Yield Predictions Validation Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2007        7.06   6.2\n",
            "   ITC11  2008        7.09   5.2\n",
            "   ITC11  2009        5.89  4.96\n",
            "   ITC11  2010        4.91  5.23\n",
            "   ITC11  2011        4.28   5.3\n",
            "   ITC12  2007        5.41   5.1\n",
            "\n",
            " Yield Predictions Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2012        4.85  7.27\n",
            "   ITC11  2013        6.94  5.47\n",
            "   ITC11  2014        6.56   5.9\n",
            "   ITC11  2015        6.29   5.6\n",
            "   ITC11  2016        5.68   6.0\n",
            "   ITC11  2017        5.32   5.4\n",
            "\n",
            "Estimator: GBDT\n",
            "---------------------------\n",
            "\n",
            "best parameters:\n",
            "estimator__learning_rate=0.05034945928430793\n",
            "estimator__loss=huber\n",
            "estimator__min_samples_leaf=20\n",
            "selector__estimator__alpha=10.0\n",
            "selector__n_features_to_select=10\n",
            "\n",
            "Best selector: RFE_Lasso\n",
            "Feature Selection Summary\n",
            "---------------------------\n",
            "estimator      selector  mean score  std score  selector score\n",
            "     GBDT random_forest       -0.60       0.16           -0.77\n",
            "     GBDT     RFE_Lasso       -0.56       0.18           -0.74\n",
            "\n",
            "\n",
            "Selected features with importance:\n",
            "----------------------------------\n",
            "\n",
            "47: YIELD-2=0.34, 48: YIELD-1=0.24, 44: YIELD-5=0.21, 45: YIELD-4=0.17, 46: YIELD-3=0.02\n",
            "39: Z-PRECp3=0.01, 41: Z+PRECp3=0.01, 40: Z+TMAXp3=0.01, 43: Z+PRECp5=0.0, 42: Z-PRECp5=0.0\n",
            "\n",
            "\n",
            "\n",
            "Feature Selection Frequencies\n",
            "-------------------------------\n",
            "static: YIELD-5(1), YIELD-4(1), YIELD-3(1), YIELD-2(1), YIELD-1(1)\n",
            "p0: \n",
            "p1: \n",
            "p2: \n",
            "p3: Z-PRECp3(1), Z+TMAXp3(1), Z+PRECp3(1)\n",
            "p4: \n",
            "p5: Z-PRECp5(1), Z+PRECp5(1)\n",
            "\n",
            "\n",
            "   IDREGION FYEAR YIELD YIELD_PRED_GBDT\n",
            "0    ITC11  2012  7.27        5.313721\n",
            "1    ITC11  2013  5.47        5.588679\n",
            "2    ITC11  2014   5.9        5.673709\n",
            "3    ITC11  2015   5.6        5.699837\n",
            "4    ITC11  2016   6.0        5.761251\n",
            "\n",
            " Yield Predictions Training Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2000        6.07        5.595763  5.83\n",
            "   ITC11  2001        6.36        5.617318  5.38\n",
            "   ITC11  2002        6.05        5.496598  4.79\n",
            "   ITC11  2003        4.88        5.188377   4.8\n",
            "   ITC11  2004        4.39        5.046613  5.54\n",
            "   ITC11  2005        4.92        5.280586   5.5\n",
            "\n",
            " Yield Predictions Validation Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2007        7.06        5.743751   6.2\n",
            "   ITC11  2008        7.09        5.617341   5.2\n",
            "   ITC11  2009        5.89        5.665166  4.96\n",
            "   ITC11  2010        4.91        5.227504  5.23\n",
            "   ITC11  2011        4.28        5.255626   5.3\n",
            "   ITC12  2007        5.41        5.027866   5.1\n",
            "\n",
            " Yield Predictions Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2012        4.85        5.313721  7.27\n",
            "   ITC11  2013        6.94        5.588679  5.47\n",
            "   ITC11  2014        6.56        5.673709   5.9\n",
            "   ITC11  2015        6.29        5.699837   5.6\n",
            "   ITC11  2016        5.68        5.761251   6.0\n",
            "   ITC11  2017        5.32        5.828319   5.4\n",
            "\n",
            "Algorithm Evaluation Summary for IT\n",
            "-----------------------------------------\n",
            "algorithm  train_MAPE  cv_MAPE  test_MAPE  train_RMSE  cv_RMSE  test_RMSE  train_R2  cv_R2  test_R2\n",
            "    trend       18.79    23.61      20.93       20.27    22.64      21.68      0.65   0.55     0.63\n",
            "     GBDT       15.90    20.54      20.61       15.18    17.85      17.30      0.80   0.72     0.76\n",
            "\n",
            "\n",
            "Training and Evaluation\n",
            "-------------------------\n",
            "\n",
            "Training Data Size: 711 rows\n",
            "X cols: 48, Y cols: 4\n",
            "IDREGION  FYEAR  SM_WHC   AVG_ELEV  STD_ELEV  AVG_SLOPE  STD_SLOPE  AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  IRRIG_AREA90  AEZ_1960  AEZ_1984  AEZ_2030  AEZ_2107  AEZ_2150  maxWLIM_YBp2  maxWLAIp2  maxWLIM_YBp4  maxWLIM_YSp4  maxWLAIp4  avgRSMp2  avgRSMp4  Z-RSMp2  Z+RSMp2  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4  avgCWBp0  avgTAVGp1  avgCWBp1  avgTAVGp2  avgCWBp2  avgPRECp5  Z-TMINp1  Z-PRECp1  Z+TMINp1  Z+PRECp1  Z-TMAXp3  Z-PRECp3  Z+TMAXp3  Z+PRECp3  Z-PRECp5  Z+PRECp5  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  YIELD\n",
            "   ITC11   2000    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       7718.48       5.46           0.0           0.0        0.0     95.43       0.0     1.02     3.17     0.00     1.69      0.0      0.0     41.16       3.52    134.39       7.41    139.75        0.0     19.19      7.97      0.90      2.62      2.85      1.15      2.71      9.94       0.0       0.0     4.50     4.74     4.45     5.56     5.84         6.07   5.83\n",
            "   ITC11   2001    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       9854.76       6.68           0.0           0.0        0.0     95.10       0.0     4.70     6.50     0.00     0.83      0.0      0.0     89.30       7.62    321.07       8.05    368.68        0.0      0.34      5.45     27.14     18.12      6.86      1.13      1.32      8.97       0.0       0.0     4.74     4.45     5.56     5.84     5.83         6.36   5.38\n",
            "   ITC11   2002    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       6712.30       5.20           0.0           0.0        0.0     85.65       0.0    13.91     2.68     0.00     1.55      0.0      0.0   -101.08       2.72   -110.58       6.52    -76.81        0.0     31.91     10.30      2.37      0.02     16.28      1.17      0.00     18.05       0.0       0.0     4.45     5.56     5.84     5.83     5.38         6.05   4.79\n",
            "   ITC11   2003    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       5945.80       4.56           0.0           0.0        0.0     94.40       0.0     4.05     4.84     0.56     0.00      0.0      0.0     58.65       8.08    200.01       6.97    221.74        0.0      1.17      4.28     34.97     32.32      1.73      3.79      8.20      1.32       0.0       0.0     5.56     5.84     5.83     5.38     4.79         4.88   4.80\n",
            "   ITC11   2004    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       8238.97       4.96           0.0           0.0        0.0     98.49       0.0     2.14     8.88     0.42     0.00      0.0      0.0    -69.47       5.95     35.28       6.80     94.43        0.0      8.71      6.70     20.15     21.59      1.45      3.80      4.59      0.00       0.0       0.0     5.84     5.83     5.38     4.79     4.80         4.39   5.54\n",
            "\n",
            "Test Data Size: 424 rows\n",
            "X cols: 48, Y cols: 4\n",
            "IDREGION  FYEAR  SM_WHC   AVG_ELEV  STD_ELEV  AVG_SLOPE  STD_SLOPE  AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  IRRIG_AREA90  AEZ_1960  AEZ_1984  AEZ_2030  AEZ_2107  AEZ_2150  maxWLIM_YBp2  maxWLAIp2  maxWLIM_YBp4  maxWLIM_YSp4  maxWLAIp4  avgRSMp2  avgRSMp4  Z-RSMp2  Z+RSMp2  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4  avgCWBp0  avgTAVGp1  avgCWBp1  avgTAVGp2  avgCWBp2  avgPRECp5  Z-TMINp1  Z-PRECp1  Z+TMINp1  Z+PRECp1  Z-TMAXp3  Z-PRECp3  Z+TMAXp3  Z+PRECp3  Z-PRECp5  Z+PRECp5  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  YIELD\n",
            "   ITC11   2012    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       9105.92       5.96          0.00          0.00       0.00     77.46      0.00    24.76     0.91     0.00     0.46     0.00     0.00   -118.01       6.81    -48.95       7.15    -62.35        0.0      4.01     10.06      7.96      0.00      4.10      3.30      2.29      3.90       0.0       0.0     6.20     5.20     4.96     5.23     5.30         4.85   7.27\n",
            "   ITC11   2013    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       7817.13       4.24       7817.13        969.13       4.24    100.37     98.61     0.00    10.02     0.00     1.90     0.00     1.11    -86.75       6.92      1.83       7.08     71.62        0.0      8.40      7.83     20.31     16.42      9.32      4.16      4.45     17.97       0.0       0.0     5.20     4.96     5.23     5.30     7.27         6.94   5.47\n",
            "   ITC11   2014    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0      10795.92       6.59      10795.92        695.09       6.34     92.90     75.92     7.79     6.47     0.11     0.07     0.00     0.08    -45.90       4.01      9.27       8.54    148.12        0.0     22.12      8.71      1.45      5.24      9.62      5.19      7.34      7.39       0.0       0.0     4.96     5.23     5.30     7.27     5.47         6.56   5.90\n",
            "   ITC11   2015    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       8590.39       5.61       8590.39        370.42       5.61     97.56     68.11     5.50    10.87     0.49     0.24     0.27     0.00    -37.00       9.34    158.93       8.31    226.06        0.0      0.40      4.69     42.06     34.69      9.36      6.69      6.20      4.61       0.0       0.0     5.23     5.30     7.27     5.47     5.90         6.29   5.60\n",
            "   ITC11   2016    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0      10098.18       6.21          0.00          0.00       0.00     83.63      0.00    15.73     1.73     0.00     0.14     0.00     0.00      4.93       5.66     14.92       8.21     32.67        0.0      8.90     10.41      5.26      0.00      5.92      2.99      1.16      4.75       0.0       0.0     5.30     7.27     5.47     5.90     5.60         5.68   6.00\n",
            "\n",
            "All features\n",
            "-------------\n",
            "\n",
            "1: SM_WHC, 2: AVG_ELEV, 3: STD_ELEV, 4: AVG_SLOPE, 5: STD_SLOPE\n",
            "6: AVG_FIELD_SIZE, 7: STD_FIELD_SIZE, 8: IRRIG_AREA_ALL, 9: IRRIG_AREA90, 10: AEZ_1960\n",
            "11: AEZ_1984, 12: AEZ_2030, 13: AEZ_2107, 14: AEZ_2150, 15: maxWLIM_YBp2\n",
            "16: maxWLAIp2, 17: maxWLIM_YBp4, 18: maxWLIM_YSp4, 19: maxWLAIp4, 20: avgRSMp2\n",
            "21: avgRSMp4, 22: Z-RSMp2, 23: Z+RSMp2, 24: Z-RSMp3, 25: Z+RSMp3\n",
            "26: Z-RSMp4, 27: Z+RSMp4, 28: avgCWBp0, 29: avgTAVGp1, 30: avgCWBp1\n",
            "31: avgTAVGp2, 32: avgCWBp2, 33: avgPRECp5, 34: Z-TMINp1, 35: Z-PRECp1\n",
            "36: Z+TMINp1, 37: Z+PRECp1, 38: Z-TMAXp3, 39: Z-PRECp3, 40: Z+TMAXp3\n",
            "41: Z+PRECp3, 42: Z-PRECp5, 43: Z+PRECp5, 44: YIELD-5, 45: YIELD-4\n",
            "46: YIELD-3, 47: YIELD-2, 48: YIELD-1\n",
            "\n",
            "\n",
            "Custom sliding validation train, test splits\n",
            "----------------------------------------------\n",
            "Validation set 1 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006\n",
            "Validation set 1 test years: 2007\n",
            "Validation set 2 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007\n",
            "Validation set 2 test years: 2008\n",
            "Validation set 3 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008\n",
            "Validation set 3 test years: 2009\n",
            "Validation set 4 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009\n",
            "Validation set 4 test years: 2010\n",
            "Validation set 5 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010\n",
            "Validation set 5 test years: 2011\n",
            "\n",
            "\n",
            "Null Method: Predicting linear yield trend:\n",
            "Min Yield: 0.3, Max Yield: 7.6\n",
            "Median Yield: 4.11, Mean Yield: 4.23\n",
            "\n",
            " Yield Predictions Training Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2000        6.07  5.83\n",
            "   ITC11  2001        6.36  5.38\n",
            "   ITC11  2002        6.05  4.79\n",
            "   ITC11  2003        4.88   4.8\n",
            "   ITC11  2004        4.39  5.54\n",
            "   ITC11  2005        4.92   5.5\n",
            "\n",
            " Yield Predictions Validation Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2007        7.06   6.2\n",
            "   ITC11  2008        7.09   5.2\n",
            "   ITC11  2009        5.89  4.96\n",
            "   ITC11  2010        4.91  5.23\n",
            "   ITC11  2011        4.28   5.3\n",
            "   ITC12  2007        5.41   5.1\n",
            "\n",
            " Yield Predictions Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2012        4.85  7.27\n",
            "   ITC11  2013        6.94  5.47\n",
            "   ITC11  2014        6.56   5.9\n",
            "   ITC11  2015        6.29   5.6\n",
            "   ITC11  2016        5.68   6.0\n",
            "   ITC11  2017        5.32   5.4\n",
            "\n",
            "Estimator: GBDT\n",
            "---------------------------\n",
            "\n",
            "best parameters:\n",
            "estimator__learning_rate=0.1\n",
            "estimator__loss=huber\n",
            "estimator__min_samples_leaf=20\n",
            "selector__estimator__min_samples_leaf=5\n",
            "selector__max_features=10\n",
            "\n",
            "Best selector: random_forest\n",
            "Feature Selection Summary\n",
            "---------------------------\n",
            "estimator      selector  mean score  std score  selector score\n",
            "     GBDT random_forest       -0.53       0.16           -0.69\n",
            "     GBDT     RFE_Lasso       -0.53       0.18           -0.71\n",
            "\n",
            "\n",
            "Selected features with importance:\n",
            "----------------------------------\n",
            "\n",
            "44: YIELD-5=0.26, 47: YIELD-2=0.17, 45: YIELD-4=0.16, 8: IRRIG_AREA_ALL=0.15, 48: YIELD-1=0.13\n",
            "46: YIELD-3=0.11, 31: avgTAVGp2=0.02, 4: AVG_SLOPE=0.01, 15: maxWLIM_YBp2=0.0, 9: IRRIG_AREA90=0.0\n",
            "\n",
            "\n",
            "\n",
            "Feature Selection Frequencies\n",
            "-------------------------------\n",
            "static: AVG_SLOPE(1), IRRIG_AREA_ALL(1), IRRIG_AREA90(1), YIELD-5(1), YIELD-4(1), YIELD-3(1), YIELD-2(1), YIELD-1(1)\n",
            "p0: \n",
            "p1: \n",
            "p2: maxWLIM_YBp2(1), avgTAVGp2(1)\n",
            "p3: \n",
            "p4: \n",
            "p5: \n",
            "\n",
            "\n",
            "   IDREGION FYEAR YIELD YIELD_PRED_GBDT\n",
            "0    ITC11  2012  7.27        5.373873\n",
            "1    ITC11  2013  5.47        5.804872\n",
            "2    ITC11  2014   5.9        5.605964\n",
            "3    ITC11  2015   5.6        5.733958\n",
            "4    ITC11  2016   6.0        5.750231\n",
            "\n",
            " Yield Predictions Training Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2000        6.07          5.6238  5.83\n",
            "   ITC11  2001        6.36        5.545649  5.38\n",
            "   ITC11  2002        6.05        5.686153  4.79\n",
            "   ITC11  2003        4.88         5.37946   4.8\n",
            "   ITC11  2004        4.39         5.07402  5.54\n",
            "   ITC11  2005        4.92        5.465611   5.5\n",
            "\n",
            " Yield Predictions Validation Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2007        7.06        5.255333   6.2\n",
            "   ITC11  2008        7.09        6.154829   5.2\n",
            "   ITC11  2009        5.89        5.857298  4.96\n",
            "   ITC11  2010        4.91        5.202556  5.23\n",
            "   ITC11  2011        4.28        5.225299   5.3\n",
            "   ITC12  2007        5.41         4.61734   5.1\n",
            "\n",
            " Yield Predictions Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2012        4.85        5.373873  7.27\n",
            "   ITC11  2013        6.94        5.804872  5.47\n",
            "   ITC11  2014        6.56        5.605964   5.9\n",
            "   ITC11  2015        6.29        5.733958   5.6\n",
            "   ITC11  2016        5.68        5.750231   6.0\n",
            "   ITC11  2017        5.32        5.733958   5.4\n",
            "\n",
            "Algorithm Evaluation Summary for IT\n",
            "-----------------------------------------\n",
            "algorithm  train_MAPE  cv_MAPE  test_MAPE  train_RMSE  cv_RMSE  test_RMSE  train_R2  cv_R2  test_R2\n",
            "    trend       18.79    23.61      20.93       20.27    22.64      21.68      0.65   0.55     0.63\n",
            "     GBDT       15.68    20.66      21.12       15.01    18.10      17.92      0.81   0.71     0.75\n",
            "\n",
            "\n",
            "Training and Evaluation\n",
            "-------------------------\n",
            "\n",
            "Training Data Size: 711 rows\n",
            "X cols: 48, Y cols: 4\n",
            "IDREGION  FYEAR  SM_WHC   AVG_ELEV  STD_ELEV  AVG_SLOPE  STD_SLOPE  AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  IRRIG_AREA90  AEZ_1960  AEZ_1984  AEZ_2030  AEZ_2107  AEZ_2150  maxWLIM_YBp2  maxWLAIp2  maxWLIM_YBp4  maxWLIM_YSp4  maxWLAIp4  avgRSMp2  avgRSMp4  Z-RSMp2  Z+RSMp2  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4  avgCWBp0  avgTAVGp1  avgCWBp1  avgTAVGp2  avgCWBp2  avgPRECp5  Z-TMINp1  Z-PRECp1  Z+TMINp1  Z+PRECp1  Z-TMAXp3  Z-PRECp3  Z+TMAXp3  Z+PRECp3  Z-PRECp5  Z+PRECp5  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  YIELD\n",
            "   ITC11   2000    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       7718.48       5.46           0.0           0.0        0.0     95.43       0.0     1.02     3.17     0.00     1.69      0.0      0.0     41.16       3.52    134.39       7.41    139.75        0.0     19.19      7.97      0.90      2.62      2.85      1.15      2.71      9.94       0.0       0.0     4.50     4.74     4.45     5.56     5.84         6.07   5.83\n",
            "   ITC11   2001    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       9854.76       6.68           0.0           0.0        0.0     95.10       0.0     4.70     6.50     0.00     0.83      0.0      0.0     89.30       7.62    321.07       8.05    368.68        0.0      0.34      5.45     27.14     18.12      6.86      1.13      1.32      8.97       0.0       0.0     4.74     4.45     5.56     5.84     5.83         6.36   5.38\n",
            "   ITC11   2002    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       6712.30       5.20           0.0           0.0        0.0     85.65       0.0    13.91     2.68     0.00     1.55      0.0      0.0   -101.08       2.72   -110.58       6.52    -76.81        0.0     31.91     10.30      2.37      0.02     16.28      1.17      0.00     18.05       0.0       0.0     4.45     5.56     5.84     5.83     5.38         6.05   4.79\n",
            "   ITC11   2003    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       5945.80       4.56           0.0           0.0        0.0     94.40       0.0     4.05     4.84     0.56     0.00      0.0      0.0     58.65       8.08    200.01       6.97    221.74        0.0      1.17      4.28     34.97     32.32      1.73      3.79      8.20      1.32       0.0       0.0     5.56     5.84     5.83     5.38     4.79         4.88   4.80\n",
            "   ITC11   2004    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       8238.97       4.96           0.0           0.0        0.0     98.49       0.0     2.14     8.88     0.42     0.00      0.0      0.0    -69.47       5.95     35.28       6.80     94.43        0.0      8.71      6.70     20.15     21.59      1.45      3.80      4.59      0.00       0.0       0.0     5.84     5.83     5.38     4.79     4.80         4.39   5.54\n",
            "\n",
            "Test Data Size: 424 rows\n",
            "X cols: 48, Y cols: 4\n",
            "IDREGION  FYEAR  SM_WHC   AVG_ELEV  STD_ELEV  AVG_SLOPE  STD_SLOPE  AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  IRRIG_AREA90  AEZ_1960  AEZ_1984  AEZ_2030  AEZ_2107  AEZ_2150  maxWLIM_YBp2  maxWLAIp2  maxWLIM_YBp4  maxWLIM_YSp4  maxWLAIp4  avgRSMp2  avgRSMp4  Z-RSMp2  Z+RSMp2  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4  avgCWBp0  avgTAVGp1  avgCWBp1  avgTAVGp2  avgCWBp2  avgPRECp5  Z-TMINp1  Z-PRECp1  Z+TMINp1  Z+PRECp1  Z-TMAXp3  Z-PRECp3  Z+TMAXp3  Z+PRECp3  Z-PRECp5  Z+PRECp5  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  YIELD\n",
            "   ITC11   2012    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       9105.92       5.96          0.00          0.00       0.00     77.46      0.00    24.76     0.91     0.00     0.46     0.00     0.00   -118.01       6.81    -48.95       7.15    -62.35        0.0      4.01     10.06      7.96      0.00      4.10      3.30      2.29      3.90       0.0       0.0     6.20     5.20     4.96     5.23     5.30         4.85   7.27\n",
            "   ITC11   2013    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       7817.13       4.24       7817.13        969.13       4.24    100.37     98.61     0.00    10.02     0.00     1.90     0.00     1.11    -86.75       6.92      1.83       7.08     71.62        0.0      8.40      7.83     20.31     16.42      9.32      4.16      4.45     17.97       0.0       0.0     5.20     4.96     5.23     5.30     7.27         6.94   5.47\n",
            "   ITC11   2014    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0      10795.92       6.59      10795.92        695.09       6.34     92.90     75.92     7.79     6.47     0.11     0.07     0.00     0.08    -45.90       4.01      9.27       8.54    148.12        0.0     22.12      8.71      1.45      5.24      9.62      5.19      7.34      7.39       0.0       0.0     4.96     5.23     5.30     7.27     5.47         6.56   5.90\n",
            "   ITC11   2015    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0       8590.39       5.61       8590.39        370.42       5.61     97.56     68.11     5.50    10.87     0.49     0.24     0.27     0.00    -37.00       9.34    158.93       8.31    226.06        0.0      0.40      4.69     42.06     34.69      9.36      6.69      6.20      4.61       0.0       0.0     5.23     5.30     7.27     5.47     5.90         6.29   5.60\n",
            "   ITC11   2016    0.14 238.513344 33.217632   0.438206   0.538569          3.3125        4.011961        363780.0      15349.87         0         1         0         0         0      10098.18       6.21          0.00          0.00       0.00     83.63      0.00    15.73     1.73     0.00     0.14     0.00     0.00      4.93       5.66     14.92       8.21     32.67        0.0      8.90     10.41      5.26      0.00      5.92      2.99      1.16      4.75       0.0       0.0     5.30     7.27     5.47     5.90     5.60         5.68   6.00\n",
            "\n",
            "All features\n",
            "-------------\n",
            "\n",
            "1: SM_WHC, 2: AVG_ELEV, 3: STD_ELEV, 4: AVG_SLOPE, 5: STD_SLOPE\n",
            "6: AVG_FIELD_SIZE, 7: STD_FIELD_SIZE, 8: IRRIG_AREA_ALL, 9: IRRIG_AREA90, 10: AEZ_1960\n",
            "11: AEZ_1984, 12: AEZ_2030, 13: AEZ_2107, 14: AEZ_2150, 15: maxWLIM_YBp2\n",
            "16: maxWLAIp2, 17: maxWLIM_YBp4, 18: maxWLIM_YSp4, 19: maxWLAIp4, 20: avgRSMp2\n",
            "21: avgRSMp4, 22: Z-RSMp2, 23: Z+RSMp2, 24: Z-RSMp3, 25: Z+RSMp3\n",
            "26: Z-RSMp4, 27: Z+RSMp4, 28: avgCWBp0, 29: avgTAVGp1, 30: avgCWBp1\n",
            "31: avgTAVGp2, 32: avgCWBp2, 33: avgPRECp5, 34: Z-TMINp1, 35: Z-PRECp1\n",
            "36: Z+TMINp1, 37: Z+PRECp1, 38: Z-TMAXp3, 39: Z-PRECp3, 40: Z+TMAXp3\n",
            "41: Z+PRECp3, 42: Z-PRECp5, 43: Z+PRECp5, 44: YIELD-5, 45: YIELD-4\n",
            "46: YIELD-3, 47: YIELD-2, 48: YIELD-1\n",
            "\n",
            "\n",
            "Custom sliding validation train, test splits\n",
            "----------------------------------------------\n",
            "Validation set 1 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006\n",
            "Validation set 1 test years: 2007\n",
            "Validation set 2 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007\n",
            "Validation set 2 test years: 2008\n",
            "Validation set 3 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008\n",
            "Validation set 3 test years: 2009\n",
            "Validation set 4 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009\n",
            "Validation set 4 test years: 2010\n",
            "Validation set 5 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010\n",
            "Validation set 5 test years: 2011\n",
            "\n",
            "\n",
            "Null Method: Predicting linear yield trend:\n",
            "Min Yield: 0.3, Max Yield: 7.6\n",
            "Median Yield: 4.11, Mean Yield: 4.23\n",
            "\n",
            " Yield Predictions Training Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2000        6.07  5.83\n",
            "   ITC11  2001        6.36  5.38\n",
            "   ITC11  2002        6.05  4.79\n",
            "   ITC11  2003        4.88   4.8\n",
            "   ITC11  2004        4.39  5.54\n",
            "   ITC11  2005        4.92   5.5\n",
            "\n",
            " Yield Predictions Validation Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2007        7.06   6.2\n",
            "   ITC11  2008        7.09   5.2\n",
            "   ITC11  2009        5.89  4.96\n",
            "   ITC11  2010        4.91  5.23\n",
            "   ITC11  2011        4.28   5.3\n",
            "   ITC12  2007        5.41   5.1\n",
            "\n",
            " Yield Predictions Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "   ITC11  2012        4.85  7.27\n",
            "   ITC11  2013        6.94  5.47\n",
            "   ITC11  2014        6.56   5.9\n",
            "   ITC11  2015        6.29   5.6\n",
            "   ITC11  2016        5.68   6.0\n",
            "   ITC11  2017        5.32   5.4\n",
            "\n",
            "Estimator: GBDT\n",
            "---------------------------\n",
            "\n",
            "best parameters:\n",
            "estimator__learning_rate=0.03927336719776336\n",
            "estimator__loss=huber\n",
            "estimator__min_samples_leaf=5\n",
            "selector__estimator__alpha=10.0\n",
            "selector__n_features_to_select=18\n",
            "\n",
            "Best selector: RFE_Lasso\n",
            "Feature Selection Summary\n",
            "---------------------------\n",
            "estimator      selector  mean score  std score  selector score\n",
            "     GBDT random_forest       -0.57       0.15           -0.71\n",
            "     GBDT     RFE_Lasso       -0.52       0.17           -0.70\n",
            "\n",
            "\n",
            "Selected features with importance:\n",
            "----------------------------------\n",
            "\n",
            "48: YIELD-1=0.22, 47: YIELD-2=0.21, 45: YIELD-4=0.21, 44: YIELD-5=0.14, 46: YIELD-3=0.07\n",
            "31: avgTAVGp2=0.06, 32: avgCWBp2=0.02, 34: Z-TMINp1=0.01, 40: Z+TMAXp3=0.01, 39: Z-PRECp3=0.01\n",
            "37: Z+PRECp1=0.01, 41: Z+PRECp3=0.01, 35: Z-PRECp1=0.01, 36: Z+TMINp1=0.01, 38: Z-TMAXp3=0.0\n",
            "42: Z-PRECp5=0.0, 43: Z+PRECp5=0.0, 33: avgPRECp5=0.0\n",
            "\n",
            "\n",
            "Feature Selection Frequencies\n",
            "-------------------------------\n",
            "static: YIELD-5(1), YIELD-4(1), YIELD-3(1), YIELD-2(1), YIELD-1(1)\n",
            "p0: \n",
            "p1: Z-TMINp1(1), Z-PRECp1(1), Z+TMINp1(1), Z+PRECp1(1)\n",
            "p2: avgTAVGp2(1), avgCWBp2(1)\n",
            "p3: Z-TMAXp3(1), Z-PRECp3(1), Z+TMAXp3(1), Z+PRECp3(1)\n",
            "p4: \n",
            "p5: avgPRECp5(1), Z-PRECp5(1), Z+PRECp5(1)\n",
            "\n",
            "\n",
            "   IDREGION FYEAR YIELD YIELD_PRED_GBDT\n",
            "0    ITC11  2012  7.27        5.308176\n",
            "1    ITC11  2013  5.47         5.86605\n",
            "2    ITC11  2014   5.9        5.643502\n",
            "3    ITC11  2015   5.6        5.353036\n",
            "4    ITC11  2016   6.0        5.822819\n",
            "\n",
            " Yield Predictions Training Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2000        6.07        5.681447  5.83\n",
            "   ITC11  2001        6.36        5.358407  5.38\n",
            "   ITC11  2002        6.05        5.591266  4.79\n",
            "   ITC11  2003        4.88        5.094664   4.8\n",
            "   ITC11  2004        4.39         5.07393  5.54\n",
            "   ITC11  2005        4.92        5.335014   5.5\n",
            "\n",
            " Yield Predictions Validation Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2007        7.06        5.377686   6.2\n",
            "   ITC11  2008        7.09         5.73566   5.2\n",
            "   ITC11  2009        5.89        5.577485  4.96\n",
            "   ITC11  2010        4.91        5.575707  5.23\n",
            "   ITC11  2011        4.28        5.142422   5.3\n",
            "   ITC12  2007        5.41        4.731514   5.1\n",
            "\n",
            " Yield Predictions Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD\n",
            "   ITC11  2012        4.85        5.308176  7.27\n",
            "   ITC11  2013        6.94         5.86605  5.47\n",
            "   ITC11  2014        6.56        5.643502   5.9\n",
            "   ITC11  2015        6.29        5.353036   5.6\n",
            "   ITC11  2016        5.68        5.822819   6.0\n",
            "   ITC11  2017        5.32        5.574397   5.4\n",
            "\n",
            "Algorithm Evaluation Summary for IT\n",
            "-----------------------------------------\n",
            "algorithm  train_MAPE  cv_MAPE  test_MAPE  train_RMSE  cv_RMSE  test_RMSE  train_R2  cv_R2  test_R2\n",
            "    trend       18.79    23.61      20.93       20.27    22.64      21.68      0.65   0.55     0.63\n",
            "     GBDT       15.09    20.32      21.21       14.21    17.88      17.49      0.83   0.72     0.76\n",
            "\n",
            "\n",
            "Saving predictions to ./pred_soft_wheat_IT_NUTS3_trend_early-6-mul.csv\n",
            "  IDREGION FYEAR YIELD_TREND YIELD_PRED_GBDT YIELD COUNTRY\n",
            "0    ITC11  2012        4.85        5.266012  7.27      IT\n",
            "1    ITC11  2013        6.94         5.73782  5.47      IT\n",
            "2    ITC11  2014        6.56        5.582148   5.9      IT\n",
            "3    ITC11  2015        6.29        5.655333   5.6      IT\n",
            "4    ITC11  2016        5.68        5.579332   6.0      IT\n"
          ]
        }
      ],
      "source": [
        "from pickle import NONE\n",
        "if ((test_env == 'notebook') and\n",
        "    (not use_saved_predictions)):\n",
        "\n",
        "  if (use_saved_features):\n",
        "    pd_train_df, pd_test_df = loadSavedFeaturesLabels(cyp_config, spark)\n",
        "\n",
        "  print('###################################')\n",
        "  print('# Machine Learning using sklearn  #')\n",
        "  print('###################################')\n",
        "\n",
        "  # drop mutually correlated features\n",
        "  # corr_threshold = cyp_config.getFeatureCorrelationThreshold()\n",
        "  # pd_train_df, pd_test_df = dropHighlyCorrelatedFeatures(cyp_config, pd_train_df, pd_test_df,\n",
        "  #                                                        log_fh, corr_thresh=corr_threshold)\n",
        "\n",
        "  num_iters = 10\n",
        "  pd_test_preds_all = None\n",
        "  for i in range(num_iters):\n",
        "    pd_test_preds = getMachineLearningPredictions(cyp_config, pd_train_df, pd_test_df, log_fh)\n",
        "\n",
        "    # save machine learning predictions\n",
        "    save_predictions = cyp_config.savePredictions()\n",
        "    if (save_predictions):\n",
        "      saveMLPredictions(cyp_config, sqlContext, pd_test_preds, i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYTG01XmFgYw"
      },
      "source": [
        "### SHAP interpretability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqxh4tLXFox3"
      },
      "outputs": [],
      "source": [
        "# import shap\n",
        "\n",
        "# if (use_saved_features):\n",
        "#     pd_train_df, pd_test_df = loadSavedFeaturesLabels(cyp_config, spark)\n",
        "\n",
        "# if (country == 'FR'):\n",
        "#   all_cols = list(pd_train_df.columns)\n",
        "#   ft_cols = all_cols[2:-2]\n",
        "#   label_cols = all_cols[:2] + all_cols[-2:]\n",
        "\n",
        "#   X_train = pd_train_df[ft_cols].values\n",
        "#   Y_train_full = pd_train_df[label_cols].values\n",
        "#   X_test = pd_test_df[ft_cols].values\n",
        "#   Y_test_full = pd_test_df[label_cols].values\n",
        "\n",
        "#   Y_train = Y_train_full[:, -1]\n",
        "#   scaler = StandardScaler()\n",
        "#   scaler.fit(X_train)\n",
        "#   X_train = scaler.transform(X_train)\n",
        "#   X_test = scaler.transform(X_test)\n",
        "\n",
        "#   # Soft Wheat, FR\n",
        "#   # estimator__learning_rate=0.02530420354612464\n",
        "#   # estimator__loss=lad\n",
        "#   # estimator__min_samples_leaf=5\n",
        "#   # selector__estimator__alpha=0.01\n",
        "#   # selector__n_features_to_select=49\n",
        "\n",
        "#   # Grain Maize, FR\n",
        "#   # estimator__learning_rate=0.1\n",
        "#   # estimator__loss=lad\n",
        "#   # estimator__min_samples_leaf=20\n",
        "#   # selector__estimator__alpha=0.01\n",
        "#   # selector__n_features_to_select=50\n",
        "\n",
        "#   if (crop == 'soft wheat'):\n",
        "#     lr = 0.025\n",
        "#   else:\n",
        "#     lr = 0.1\n",
        "\n",
        "#   model = GradientBoostingRegressor(loss='lad', max_features='log2',\n",
        "#                                     max_depth=10, min_samples_leaf=5,\n",
        "#                                     tol=1e-3, n_iter_no_change=5,\n",
        "#                                     subsample=0.6, ccp_alpha=1e-2,\n",
        "#                                     learning_rate=lr,\n",
        "#                                     n_estimators=500, random_state=42)\n",
        "#   model.fit(X_train, Y_train)\n",
        "#   # model = xgboost.XGBRegressor().fit(X_train, Y_train)\n",
        "\n",
        "#   # explain the model's predictions using SHAP\n",
        "#   explainer = shap.Explainer(model, feature_names=ft_cols)\n",
        "#   shap_values = explainer(X_test)\n",
        "\n",
        "#   shap.plots.bar(shap_values, max_display=40)\n",
        "#   shap.plots.beeswarm(shap_values, max_display=41)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivn9xbXlPs4d"
      },
      "source": [
        "### Compare Predictions with JRC Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyzPXCxEtpC1"
      },
      "outputs": [],
      "source": [
        "if (test_env == 'notebook'):\n",
        "\n",
        "  if (use_saved_predictions):\n",
        "    pd_ml_predictions = loadSavedPredictions(cyp_config, spark)\n",
        "\n",
        "  compareWithMCYFS = cyp_config.compareWithMCYFS()\n",
        "  if (compareWithMCYFS):\n",
        "    comparePredictionsWithMCYFS(sqlContext, cyp_config, pd_ml_predictions, log_fh)\n",
        "\n",
        "  log_fh.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqDd-vReQ4s_"
      },
      "outputs": [],
      "source": [
        "! mv *.log *.csv drive/MyDrive/Wageningen\\ PhD/Dilli\\ -\\ Deep\\ Learning/saved-output/GBDT-outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tpv4BJtL-lT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "RX-t0S8wUr5X",
        "o7FqdmXOjYUm",
        "kPwEyntUYHsS"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}