{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RX-t0S8wUr5X"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ksndBerUYL_"
      },
      "source": [
        "# Crop Yield Prediction - ML Baseline\n",
        "\n",
        "We use WOFOST crop growth indicators, weather variables, geographic information, soil data and remote sensing indicators to predict the yield."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX-t0S8wUr5X"
      },
      "source": [
        "## Google Colab Notes\n",
        "\n",
        "**To run the script in Google Colab environment**\n",
        "1. Download the data directory and save it somewhere convenient.\n",
        "2. Open the notebook using Google Colaboratory.\n",
        "3. Create a copy of the notebook for yourself.\n",
        "4. Click connect on the right hand side of the bar below menu items. When you are connected to a machine, you will see a green tick mark and bars showing RAM and disk.\n",
        "5. Click the folder icon on the left sidebar and click upload. Upload the data files you downloaded. Click *Ok* when you see a warning saying the files will be deleted after the session is disconnected.\n",
        "6. Use *Runtime* -> *Run before* option to run all cells before **Set Configuration**.\n",
        "7. Run the remaining cells except **Python Script Main**. The configuration subsection allows you to change configuration and rerun experiments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP2t4BbJGJKZ"
      },
      "source": [
        "## Global Variables and Spark Installation/Initialization\n",
        "\n",
        "Initialize Spark session and global variables. Package installation is required only in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcalFxFgYk6s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fadbb4e9-8d03-44f3-d93b-66fdc954bc7e"
      },
      "source": [
        "#%%writefile globals.py\n",
        "test_env = 'notebook'\n",
        "# test_env = 'cluster'\n",
        "# test_env = 'pkg'\n",
        "\n",
        "# change to False to skip tests\n",
        "run_tests = False\n",
        "\n",
        "# NUTS levels\n",
        "nuts_levels = ['NUTS' + str(i) for i in range(4)]\n",
        "\n",
        "# country codes\n",
        "countries = ['BG', 'DE', 'ES', 'FR', 'HU', 'IT', 'NL', 'PL', 'RO']\n",
        "\n",
        "# debug levels\n",
        "debug_levels = [i for i in range(5)]\n",
        "\n",
        "# Keeping these two mappings inside CYPConfiguration leads to SPARK-5063 error\n",
        "# when lambda functions use them. Therefore, they are defined as globals now.\n",
        "\n",
        "# crop name to id mapping\n",
        "crop_id_dict = {\n",
        "    'grain maize': 2,\n",
        "    'sugar beet' : 6,\n",
        "    'sugarbeet' : 6,\n",
        "    'sugarbeets' : 6,\n",
        "    'sugar beets' : 6,\n",
        "    'total potatoes' : 7,\n",
        "    'potatoes' : 7,\n",
        "    'potato' : 7,\n",
        "    'winter wheat' : 90,\n",
        "    'soft wheat' : 90,\n",
        "    'sunflower' : 93,\n",
        "    'spring barley' : 95,\n",
        "}\n",
        "\n",
        "# crop id to name mapping\n",
        "crop_name_dict = {\n",
        "    2 : 'grain maize',\n",
        "    6 : 'sugarbeet',\n",
        "    7 : 'potatoes',\n",
        "    90 : 'soft wheat',\n",
        "    93 : 'sunflower',\n",
        "    95 : 'spring barley',\n",
        "}\n",
        "\n",
        "if (test_env == 'notebook'):\n",
        "  !pip install pyspark > /dev/null\n",
        "  !sudo apt update > /dev/null\n",
        "  !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "  !pip install joblibspark > /dev/null\n",
        "\n",
        "  import os\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import functions as SparkF\n",
        "from pyspark.sql import types as SparkT\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "SparkContext.setSystemProperty('spark.executor.memory', '12g')\n",
        "SparkContext.setSystemProperty('spark.driver.memory', '6g')\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXT68GaJ1jOc"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZtsU16F1jOn"
      },
      "source": [
        "#%%writefile util.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# crop name and id mappings\n",
        "def cropNameToID(crop_id_dict, crop):\n",
        "  \"\"\"\n",
        "  Return id of given crop. Relies on crop_id_dict.\n",
        "  Return 0 if crop name is not in the dictionary.\n",
        "  \"\"\"\n",
        "  crop_lcase = crop.lower()\n",
        "  try:\n",
        "    crop_id = crop_id_dict[crop_lcase]\n",
        "  except KeyError as e:\n",
        "    crop_id = 0\n",
        "\n",
        "  return crop_id\n",
        "\n",
        "def cropIDToName(crop_name_dict, crop_id):\n",
        "  \"\"\"\n",
        "  Return crop name for given crop ID. Relies on crop_name_dict.\n",
        "  Return 'NA' if crop id is not found in the dictionary.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    crop_name = crop_name_dict[crop_id]\n",
        "  except KeyError as e:\n",
        "    crop_name = 'NA'\n",
        "\n",
        "  return crop_name\n",
        "\n",
        "def getYear(date_str):\n",
        "  \"\"\"Extract year from date in yyyyMMdd or dd/MM/yyyy format.\"\"\"\n",
        "  return SparkF.when(SparkF.length(date_str) == 8,\n",
        "                     SparkF.year(SparkF.to_date(date_str, 'yyyyMMdd')))\\\n",
        "                     .otherwise(SparkF.year(SparkF.to_date(date_str, 'dd/MM/yyyy')))\n",
        "\n",
        "def getMonth(date_str):\n",
        "  \"\"\"Extract month from date in yyyyMMdd or dd/MM/yyyy format.\"\"\"\n",
        "  return SparkF.when(SparkF.length(date_str) == 8,\n",
        "                     SparkF.month(SparkF.to_date(date_str, 'yyyyMMdd')))\\\n",
        "                     .otherwise(SparkF.month(SparkF.to_date(date_str, 'dd/MM/yyyy')))\n",
        "\n",
        "def getDay(date_str):\n",
        "  \"\"\"Extract day from date in yyyyMMdd or dd/MM/yyyy format.\"\"\"\n",
        "  return SparkF.when(SparkF.length(date_str) == 8,\n",
        "                     SparkF.dayofmonth(SparkF.to_date(date_str, 'yyyyMMdd')))\\\n",
        "                     .otherwise(SparkF.dayofmonth(SparkF.to_date(date_str, 'dd/MM/yyyy')))\n",
        "\n",
        "# 1-10: Dekad 1\n",
        "# 11-20: Dekad 2\n",
        "# > 20 : Dekad 3\n",
        "def getDekad(date_str):\n",
        "  \"\"\"Extract dekad from date in YYYYMMDD format.\"\"\"\n",
        "  month = getMonth(date_str)\n",
        "  day = getDay(date_str)\n",
        "  return SparkF.when(day < 30, (month - 1)* 3 +\n",
        "                     SparkF.ceil(day/10)).otherwise((month - 1) * 3 + 3)\n",
        "\n",
        "# Machine Learning Utility Functions\n",
        "\n",
        "# This definition is from the suggested answer to:\n",
        "# https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn/294069#294069\n",
        "def mean_absolute_percentage_error(Y_true, Y_pred):\n",
        "  \"\"\"Mean Absolute Percentage Error\"\"\"\n",
        "  Y_true, Y_pred = np.array(Y_true), np.array(Y_pred)\n",
        "  return np.mean(np.abs((Y_true - Y_pred) / Y_true)) * 100\n",
        "\n",
        "def printInGroups(items, indices, item_values=None, log_fh=None):\n",
        "  \"\"\"Print elements at given indices in groups of 5\"\"\"\n",
        "  num_items = len(indices)\n",
        "  groups = int(num_items/5) + 1\n",
        "\n",
        "  items_str = '\\n'\n",
        "  for g in range(groups):\n",
        "    group_start = g * 5\n",
        "    group_end = (g + 1) * 5\n",
        "    if (group_end > num_items):\n",
        "      group_end = num_items\n",
        "\n",
        "    group_indices = indices[group_start:group_end]\n",
        "    for idx in group_indices:\n",
        "      items_str += str(idx+1) + ': ' + items[idx]\n",
        "      if (item_values):\n",
        "        items_str += '=' + item_values[idx]\n",
        "\n",
        "      if (idx != group_indices[-1]):\n",
        "          items_str += ', '\n",
        "\n",
        "    items_str += '\\n'\n",
        "\n",
        "  print(items_str)\n",
        "  if (log_fh is not None):\n",
        "    log_fh.write(items_str)\n",
        "\n",
        "def clusterRegionsUsingYield(pd_yield_df, num_clusters=2, debug_level=0):\n",
        "  \"\"\"Cluster regions based on historical yield data\"\"\"\n",
        "  row_idx = 0\n",
        "  yield_summary = {}\n",
        "  regions = pd_yield_df['IDREGION'].unique()\n",
        "  all_years = pd_yield_df['FYEAR'].unique()\n",
        "  for reg in regions:\n",
        "    pd_reg_df = pd_yield_df[pd_yield_df['IDREGION'] == reg].sort_values(by=['IDREGION', 'FYEAR'])\n",
        "    yield_years = pd_reg_df[['FYEAR', 'YIELD']].values\n",
        "    avg_yield = np.mean(yield_years[:, 1])\n",
        "    reg_row = np.array([avg_yield for yr in all_years])\n",
        "    yield_year_idxs = yield_years[:, 0] - all_years[0]\n",
        "    reg_row[yield_year_idxs.astype('int')] = yield_years[:, 1]\n",
        "    yield_summary['row' + str(row_idx)] = [reg] + list(reg_row)\n",
        "    row_idx += 1\n",
        "\n",
        "  yield_sum_cols = ['IDREGION'] + [str(yr) for yr in all_years]\n",
        "  pd_yield_clu_df = pd.DataFrame.from_dict(yield_summary, orient='index',\n",
        "                                           columns=yield_sum_cols)\n",
        "  pd_yield_clu_df = pd_yield_clu_df.dropna()\n",
        "  if (debug_level > 1):\n",
        "    print('\\n Tranformed yield data')\n",
        "    print(pd_yield_clu_df.head(5))\n",
        "\n",
        "  yield_cols = [str(yr) for yr in pd_yield_df['FYEAR'].unique()]\n",
        "  X_yield = pd_yield_clu_df[yield_cols]\n",
        "\n",
        "  # There is linear connection between cosine distance and Euclidean distance for normalized vectors.\n",
        "  # https://stats.stackexchange.com/questions/299013/cosine-distance-as-similarity-measure-in-kmeans\n",
        "  yield_normalized = normalize(X_yield)\n",
        "  yield_norm_kmeans = KMeans(n_clusters=num_clusters)\n",
        "  yield_norm_kmeans.fit(yield_normalized)\n",
        "  if (debug_level > 1):\n",
        "    score_label = 'Cosine kmeans silhouette_score:'\n",
        "    print('\\n', score_label, silhouette_score(yield_normalized,\n",
        "                                              yield_norm_kmeans.labels_,\n",
        "                                              metric='cosine'))\n",
        "\n",
        "  regions_in_clusters = []\n",
        "  for i in range(num_clusters):\n",
        "    cluster_i_mask = np.where(yield_norm_kmeans.labels_ == i)\n",
        "    regions_cluster_i = regions[cluster_i_mask[0]]\n",
        "    regions_in_clusters.append(regions_cluster_i)\n",
        "\n",
        "  return regions_in_clusters\n",
        "\n",
        "def customFitPredict(args):\n",
        "  \"\"\"\n",
        "  We need this because scikit-learn does not support\n",
        "  cross_val_predict for time series splits.\n",
        "  \"\"\"\n",
        "  X_train = args['X_train']\n",
        "  Y_train = args['Y_train']\n",
        "  X_test = args['X_test']\n",
        "  est = args['estimator']\n",
        "  fit_params = args['fit_params']\n",
        "\n",
        "  est.fit(X_train, Y_train, **fit_params)\n",
        "  return est.predict(X_test)\n",
        "\n",
        "def getPredictionScores(Y_true, Y_predicted, metrics):\n",
        "  \"\"\"Get values of metrics for given Y_predicted and Y_true\"\"\"\n",
        "  pred_scores = {}\n",
        "\n",
        "  for met in metrics:\n",
        "    score_function = metrics[met]\n",
        "    met_score = score_function(Y_true, Y_predicted)\n",
        "    # for RMSE, score_function is mean_squared_error, take square root\n",
        "    # normalize RMSE\n",
        "    if (met == 'RMSE'):\n",
        "      met_score = np.round(100*np.sqrt(met_score)/np.mean(Y_true), 2)\n",
        "      pred_scores['NRMSE'] = met_score\n",
        "    # normalize mean absolute errors except MAPE which is already a percentage\n",
        "    elif ((met == 'MAE') or (met == 'MdAE')):\n",
        "      met_score = np.round(100*met_score/np.mean(Y_true), 2)\n",
        "      pred_scores['N' + met] = met_score\n",
        "    # MAPE, R2, ... : no postprocessing\n",
        "    else:\n",
        "      met_score = np.round(met_score, 2)\n",
        "      pred_scores[met] = met_score\n",
        "\n",
        "  return pred_scores\n",
        "\n",
        "def getFilename(crop, country, yield_trend,\n",
        "                early_season, early_season_end, nuts_level=None):\n",
        "  \"\"\"Get filename based on input arguments\"\"\"\n",
        "  suffix = crop.replace(' ', '_')\n",
        "  suffix += '_' + country\n",
        "\n",
        "  if (nuts_level is not None):\n",
        "    suffix += '_' + nuts_level\n",
        "\n",
        "  if (yield_trend):\n",
        "    suffix += '_trend'\n",
        "  else:\n",
        "    suffix += '_notrend'\n",
        "\n",
        "  if (early_season):\n",
        "    suffix += '_early' + str(early_season_end)\n",
        "\n",
        "  return suffix\n",
        "\n",
        "def getLogFilename(crop, country, yield_trend,\n",
        "                   early_season, early_season_end):\n",
        "  \"\"\"Get filename for experiment log\"\"\"\n",
        "  log_file = getFilename(crop, country, yield_trend,\n",
        "                         early_season, early_season_end)\n",
        "  return log_file + '.log'\n",
        "\n",
        "def getFeatureFilename(crop, country, yield_trend,\n",
        "                       early_season, early_season_end):\n",
        "  \"\"\"Get unique filename for features\"\"\"\n",
        "  feature_file = 'ft_'\n",
        "  suffix = getFilename(crop, country, yield_trend, early_season, early_season_end)\n",
        "  feature_file += suffix\n",
        "  return feature_file\n",
        "\n",
        "def getPredictionFilename(crop, country, nuts_level, yield_trend,\n",
        "                          early_season, early_season_end):\n",
        "  \"\"\"Get unique filename for predictions\"\"\"\n",
        "  pred_file = 'pred_'\n",
        "  suffix = getFilename(crop, country, yield_trend,\n",
        "                       early_season, early_season_end, nuts_level)\n",
        "  pred_file += suffix\n",
        "  return pred_file\n",
        "\n",
        "def plotTrend(years, actual_values, trend_values, trend_label):\n",
        "  \"\"\"Plot a linear trend and scatter plot of actual values\"\"\"\n",
        "  plt.scatter(years, actual_values, color=\"blue\", marker=\"o\")\n",
        "  plt.plot(years, trend_values, '--')\n",
        "  plt.xticks(np.arange(years[0], years[-1] + 1, step=len(years)/5))\n",
        "  ax = plt.axes()\n",
        "  plt.xlabel(\"YEAR\")\n",
        "  plt.ylabel(trend_label)\n",
        "  plt.title(trend_label + ' Trend by YEAR')\n",
        "  plt.show()\n",
        "\n",
        "def plotTrueVSPredicted(actual, predicted):\n",
        "  \"\"\"Plot actual and predicted values\"\"\"\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.scatter(np.asarray(actual), predicted)\n",
        "  ax.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'k--', lw=4)\n",
        "  ax.set_xlabel('Actual')\n",
        "  ax.set_ylabel('Predicted')\n",
        "  plt.show()\n",
        "\n",
        "# Based on\n",
        "# https://stackoverflow.com/questions/39409866/correlation-heatmap\n",
        "def plotCorrelation(df, sel_cols):\n",
        "  corr = df[sel_cols].corr()\n",
        "  mask = np.zeros_like(corr, dtype=np.bool)\n",
        "  mask[np.triu_indices_from(mask)] = True\n",
        "  f, ax = plt.subplots(figsize=(20, 18))\n",
        "  cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "  sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1.0, center=0,\n",
        "              square=True, linewidths=.5, cbar_kws={\"shrink\": .5},\n",
        "              annot=True, fmt='.1g')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8UktClOS9Hs"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjP5SjODS-qS"
      },
      "source": [
        "#%%writefile config.py\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import explained_variance_score\n",
        "from sklearn.metrics import median_absolute_error\n",
        "\n",
        "class CYPConfiguration:\n",
        "  def __init__(self, crop_name='potatoes', country_code='NL', season_cross='N'):\n",
        "    self.config = {\n",
        "        'crop_name' : crop_name,\n",
        "        'crop_id' : cropNameToID(crop_id_dict, crop_name),\n",
        "        'season_crosses_calendar_year' : season_cross,\n",
        "        'country_code' : country_code,\n",
        "        'nuts_level' : 'NUTS2',\n",
        "        'data_sources' : [ 'WOFOST', 'METEO_DAILY', 'SOIL', 'YIELD' ],\n",
        "        'clean_data' : 'N',\n",
        "        'use_yield_trend' : 'N',\n",
        "        'predict_yield_residuals' : 'N',\n",
        "        'find_optimal_trend_window' : 'N',\n",
        "        # set it to a list with one entry for fixed window\n",
        "        'trend_windows' : [5, 7, 10],\n",
        "        'use_centroids' : 'N',\n",
        "        'use_remote_sensing' : 'Y',\n",
        "        'use_gaes' : 'N',\n",
        "        'use_per_year_crop_calendar' : 'N',\n",
        "        'early_season_prediction' : 'N',\n",
        "        'early_season_end_dekad' : 0,\n",
        "        'data_path' : '.',\n",
        "        'output_path' : '.',\n",
        "        'use_features_v2' : 'N',\n",
        "        'save_features' : 'N',\n",
        "        'use_saved_features' : 'N',\n",
        "        'use_sample_weights' : 'N',\n",
        "        'retrain_per_test_year' : 'N',\n",
        "        'save_predictions' : 'Y',\n",
        "        'use_saved_predictions' : 'N',\n",
        "        'compare_with_mcyfs' : 'N',\n",
        "        'debug_level' : 0,\n",
        "    }\n",
        "\n",
        "    # Description of configuration parameters\n",
        "    # This should be in sync with config above\n",
        "    self.config_desc = {\n",
        "        'crop_name' : 'Crop name',\n",
        "        'crop_id' : 'Crop ID',\n",
        "        'season_crosses_calendar_year' : 'Crop growing season crosses calendar year boundary',\n",
        "        'country_code' : 'Country code (e.g. NL)',\n",
        "        'nuts_level' : 'NUTS level for yield prediction',\n",
        "        'data_sources' : 'Input data sources',\n",
        "        'clean_data' : 'Remove data or regions with duplicate or missing values',\n",
        "        'use_yield_trend' : 'Estimate and use yield trend',\n",
        "        'predict_yield_residuals' : 'Predict yield residuals instead of full yield',\n",
        "        'find_optimal_trend_window' : 'Find optimal trend window',\n",
        "        'trend_windows' : 'List of trend window lengths (number of years)',\n",
        "        'use_centroids' : 'Use centroid coordinates and distance to coast',\n",
        "        'use_remote_sensing' : 'Use remote sensing data (FAPAR)',\n",
        "        'use_gaes' : 'Use agro-environmental zones data',\n",
        "        'use_per_year_crop_calendar' : 'Use per region per year crop calendar',\n",
        "        'early_season_prediction' : 'Predict yield early in the season',\n",
        "        'early_season_end_dekad' : 'Early season end dekad relative to harvest',\n",
        "        'data_path' : 'Path to all input data. Default is current directory.',\n",
        "        'output_path' : 'Path to all output files. Default is current directory.',\n",
        "        'use_features_v2' : 'Use feature design v2',\n",
        "        'save_features' : 'Save features to a CSV file',\n",
        "        'use_saved_features' : 'Use features from a CSV file',\n",
        "        'use_sample_weights' : 'Use data sample weights based on crop area',\n",
        "        'retrain_per_test_year' : 'Retrain a model for every test year',\n",
        "        'save_predictions' : 'Save predictions to a CSV file',\n",
        "        'use_saved_predictions' : 'Use predictions from a CSV file',\n",
        "        'compare_with_mcyfs' : 'Compare predictions with MARS Crop Yield Forecasting System',\n",
        "        'debug_level' : 'Debug level to control amount of debug information',\n",
        "    }\n",
        "\n",
        "    ########### Machine learning configuration ###########\n",
        "    # mutual correlation threshold for features\n",
        "    self.feature_correlation_threshold = 0.9\n",
        "\n",
        "    # test fraction\n",
        "    self.test_fraction = 0.3\n",
        "\n",
        "    # scaler\n",
        "    self.scaler = StandardScaler()\n",
        "\n",
        "    # Feature selection algorithms. Initialized in getFeatureSelectors().\n",
        "    self.feature_selectors = {}\n",
        "\n",
        "    # prediction algorithms\n",
        "    self.estimators = {\n",
        "        # linear model\n",
        "        'Ridge' : {\n",
        "            'estimator' : Ridge(alpha=1, random_state=42, max_iter=1000,\n",
        "                                copy_X=True, fit_intercept=True),\n",
        "            'fs_param_grid' : dict(estimator__alpha=[1e-1]),\n",
        "            'param_grid' : dict(estimator__alpha=[1e-5, 1e-2, 1e-1, 0.5, 1, 5, 10])\n",
        "        },\n",
        "        'KNN' : {\n",
        "            'estimator' : KNeighborsRegressor(weights='distance'),\n",
        "            'fs_param_grid' : dict(estimator__n_neighbors=[5]),\n",
        "            'param_grid' : dict(estimator__n_neighbors=[3, 5, 7, 9])\n",
        "        },\n",
        "        # SVM regression\n",
        "        'SVR' : {\n",
        "            'estimator' : SVR(kernel='rbf', gamma='scale', max_iter=-1,\n",
        "                              shrinking=True, tol=0.001),\n",
        "            'fs_param_grid' : dict(estimator__C=[10.0],\n",
        "                                   estimator__epsilon=[0.5]),\n",
        "            'param_grid' : dict(estimator__C=[1e-1, 5e-1, 1.0, 5.0, 10.0, 50.0, 100.0, 200.0],\n",
        "                                estimator__epsilon=[1e-2, 1e-1, 0.5, 1.0, 5.0]),\n",
        "        },\n",
        "        # random forest\n",
        "        #'RF' : {\n",
        "        #    'estimator' : RandomForestRegressor(bootstrap=True, random_state=42,\n",
        "        #                                        oob_score=True, min_samples_leaf=5),\n",
        "        #    'fs_param_grid' : dict(estimator__max_depth=[7],\n",
        "        #                           estimator__n_estimators=[100]),\n",
        "        #    'param_grid' : dict(estimator__max_depth=[5, 7],\n",
        "        #                        estimator__n_estimators=[100, 500])\n",
        "        #},\n",
        "        # extra randomized trees\n",
        "        #'ERT' : {\n",
        "        #    'estimator' : ExtraTreesRegressor(bootstrap=True, random_state=42,\n",
        "        #                                      oob_score=True, min_samples_leaf=5),\n",
        "        #    'fs_param_grid' : dict(estimator__max_depth=[7],\n",
        "        #                           estimator__n_estimators=[100]),\n",
        "        #    'param_grid' : dict(estimator__max_depth=[5, 7],\n",
        "        #                        estimator__n_estimators=[100, 500])\n",
        "        #},\n",
        "        # gradient boosted decision trees\n",
        "        'GBDT' : {\n",
        "            'estimator' : GradientBoostingRegressor(learning_rate=0.01,\n",
        "                                                    subsample=0.8, loss='lad',\n",
        "                                                    min_samples_leaf=5,\n",
        "                                                    random_state=42),\n",
        "            'fs_param_grid' : dict(estimator__max_depth=[5],\n",
        "                                   estimator__n_estimators=[100]),\n",
        "            'param_grid' : dict(estimator__max_depth=[5, 10, 15],\n",
        "                                estimator__n_estimators=[100, 500])\n",
        "        },\n",
        "        #'MLP' : {\n",
        "        #    'estimator' : MLPRegressor(batch_size='auto', learning_rate='adaptive',\n",
        "        #                               solver='sgd', activation='relu',\n",
        "        #                               learning_rate_init=0.01, power_t=0.5,\n",
        "        #                               max_iter=1000, shuffle=True,\n",
        "        #                               random_state=42, tol=0.001,\n",
        "        #                               verbose=False, warm_start=False,\n",
        "        #                               momentum=0.9, nesterovs_momentum=True,\n",
        "        #                               early_stopping=True,\n",
        "        #                               validation_fraction=0.4, beta_1=0.9,\n",
        "        #                               beta_2=0.999, epsilon=1e-08),\n",
        "        #    'fs_param_grid' : dict(estimator__hidden_layer_sizes=[(10, 10), (15,15)],\n",
        "        #                           estimator__alpha=[0.2, 0.3]),\n",
        "        #    'param_grid' : dict(estimator__hidden_layer_sizes=[(10, 10), (15, 15), (20, 20)],\n",
        "        #                        estimator__alpha=[0.1, 0.2, 0.3]),\n",
        "        #},\n",
        "   }\n",
        "\n",
        "    # k-fold validation metric for feature selection\n",
        "    self.fs_cv_metric = 'neg_mean_squared_error'\n",
        "    # k-fold validation metric for training\n",
        "    self.est_cv_metric = 'neg_mean_squared_error'\n",
        "\n",
        "    # Performance evaluation metrics:\n",
        "    # sklearn supports these metrics:\n",
        "    # 'explained_variance', 'max_error', 'neg_mean_absolute_error\n",
        "    # 'neg_mean_squared_error', 'neg_root_mean_squared_error'\n",
        "    # 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'r2'\n",
        "    self.eval_metrics = {\n",
        "        # EXP_VAR (y_true, y_obs) = 1 - ( var(y_true - y_obs) / var (y_true) )\n",
        "        #'EXP_VAR' : explained_variance_score,\n",
        "        # MAE (y_true, y_obs) = ( 1 / n ) * sum_i-n ( | y_true_i - y_obs_i | )\n",
        "        # 'MAE' : mean_absolute_error,\n",
        "        # MdAE (y_true, y_obs) = median ( | y_true_1 - y_obs_1 |, | y_true_2 - y_obs_2 |, ... )\n",
        "        #'MdAE' : median_absolute_error,\n",
        "        # MAPE (y_true, y_obs) = ( 1 / n ) * sum_i-n ( ( y_true_i - y_obs_i ) / y_true_i )\n",
        "        'MAPE' : mean_absolute_percentage_error,\n",
        "        # MSE (y_true, y_obs) = ( 1 / n ) * sum_i-n ( y_true_i - y_obs_i )^2\n",
        "        'RMSE' : mean_squared_error,\n",
        "        # R2 (y_true, y_obs) = 1 - ( ( sum_i-n ( y_true_i - y_obs_i )^2 )\n",
        "        #                           / sum_i-n ( y_true_i - mean(y_true) )^2)\n",
        "        'R2' : r2_score,\n",
        "    }\n",
        "\n",
        "  ########### Setters and getters ###########\n",
        "  def setCropName(self, crop_name):\n",
        "    \"\"\"Set the crop name\"\"\"\n",
        "    crop = crop_name.lower()\n",
        "    assert crop in crop_id_dict\n",
        "    self.config['crop_name'] = crop\n",
        "    self.config['crop_id'] = cropNameToID(crop_id_dict, crop)\n",
        "\n",
        "  def getCropName(self):\n",
        "    \"\"\"Return the crop name\"\"\"\n",
        "    return self.config['crop_name']\n",
        "\n",
        "  def setCropID(self, crop_id):\n",
        "    \"\"\"Set the crop ID\"\"\"\n",
        "    assert crop_id in crop_name_dict\n",
        "    self.config['crop_id'] = crop_id\n",
        "    self.config['crop_name'] = cropIDToName(crop_name_dict, crop_id)\n",
        "\n",
        "  def getCropID(self):\n",
        "    \"\"\"Return the crop ID\"\"\"\n",
        "    return self.config['crop_id']\n",
        "\n",
        "  def setSeasonCrossesCalendarYear(self, season_crosses):\n",
        "    \"\"\"Set whether the season crosses calendar year boundary\"\"\"\n",
        "    scross = season_crosses.upper()\n",
        "    assert scross in ['Y', 'N']\n",
        "    self.config['season_crosses_calendar_year'] = scross\n",
        "\n",
        "  def seasonCrossesCalendarYear(self):\n",
        "    \"\"\"Return whether the season crosses calendar year boundary\"\"\"\n",
        "    return (self.config['season_crosses_calendar_year'] == 'Y')\n",
        "\n",
        "  def setCountryCode(self, country_code):\n",
        "    \"\"\"Set the country code\"\"\"\n",
        "    if (country_code is None):\n",
        "      self.config['country_code'] = None\n",
        "    else:\n",
        "      ccode = country_code.upper()\n",
        "      assert len(ccode) == 2\n",
        "      assert ccode in countries\n",
        "      self.config['country_code'] = ccode\n",
        "\n",
        "  def getCountryCode(self):\n",
        "    \"\"\"Return the country code\"\"\"\n",
        "    return self.config['country_code']\n",
        "\n",
        "  def setNUTSLevel(self, nuts_level):\n",
        "    \"\"\"Set the NUTS level\"\"\"\n",
        "    nuts = nuts_level.upper()\n",
        "    assert nuts in nuts_levels\n",
        "    self.config['nuts_level'] = nuts\n",
        "\n",
        "  def getNUTSLevel(self):\n",
        "    \"\"\"Return the NUTS level\"\"\"\n",
        "    return self.config['nuts_level']\n",
        "\n",
        "  def setDataSources(self, data_sources):\n",
        "    \"\"\"Get the data sources\"\"\"\n",
        "    # TODO: some validation\n",
        "    self.config['data_sources'] = data_sources\n",
        "\n",
        "  def updateDataSources(self, data_src, include_src, nuts_level=None):\n",
        "    \"\"\"add or remove data_src from data sources\"\"\"\n",
        "    src_nuts = self.getNUTSLevel()\n",
        "    if (nuts_level is not None):\n",
        "      src_nuts = nuts_level\n",
        "\n",
        "    data_sources = self.config['data_sources']\n",
        "    # no update required\n",
        "    if (((include_src == 'Y') and (data_src in data_sources)) or\n",
        "        ((include_src == 'N') and (data_src not in data_sources))):\n",
        "      return\n",
        "\n",
        "    if (include_src == 'Y'):\n",
        "      if (isinstance(data_sources, dict)):\n",
        "        data_sources[data_src] = src_nuts\n",
        "      else:\n",
        "        data_sources.append(data_src)\n",
        "    else:\n",
        "      if (isinstance(data_sources, dict)):\n",
        "        del data_sources[data_src]\n",
        "      else:\n",
        "        data_sources.remove(data_src)\n",
        "\n",
        "    self.config['data_sources'] = data_sources\n",
        "\n",
        "  def getDataSources(self):\n",
        "    \"\"\"Return the data sources\"\"\"\n",
        "    return self.config['data_sources']\n",
        "\n",
        "  def setCleanData(self, clean_data):\n",
        "    \"\"\"Set whether to clean data with duplicate or missing values\"\"\"\n",
        "    do_clean = clean_data.upper()\n",
        "    assert do_clean in ['Y', 'N']\n",
        "    self.config['clean_data'] = do_clean\n",
        "\n",
        "  def cleanData(self):\n",
        "    \"\"\"Return whether to clean data with duplicate or missing values\"\"\"\n",
        "    return (self.config['clean_data'] == 'Y')\n",
        "\n",
        "  def setUseYieldTrend(self, use_trend):\n",
        "    \"\"\"Set whether to use yield trend\"\"\"\n",
        "    use_yt = use_trend.upper()\n",
        "    assert use_yt in ['Y', 'N']\n",
        "    self.config['use_yield_trend'] = use_yt\n",
        "\n",
        "  def useYieldTrend(self):\n",
        "    \"\"\"Return whether to use yield trend\"\"\"\n",
        "    return (self.config['use_yield_trend'] == 'Y')\n",
        "\n",
        "  def setPredictYieldResiduals(self, pred_res):\n",
        "    \"\"\"Set whether to use predict yield residuals\"\"\"\n",
        "    pred_yres = pred_res.upper()\n",
        "    assert pred_yres in ['Y', 'N']\n",
        "    self.config['predict_yield_residuals'] = pred_yres\n",
        "\n",
        "  def predictYieldResiduals(self):\n",
        "    \"\"\"Return whether to use predict yield residuals\"\"\"\n",
        "    return (self.config['predict_yield_residuals'] == 'Y')\n",
        "\n",
        "  def setFindOptimalTrendWindow(self, find_opt):\n",
        "    \"\"\"Set whether to find optimal trend window for each year\"\"\"\n",
        "    find_otw = find_opt.upper()\n",
        "    assert find_otw in ['Y', 'N']\n",
        "    self.config['find_optimal_trend_window'] = find_otw\n",
        "\n",
        "  def findOptimalTrendWindow(self):\n",
        "    \"\"\"Return whether to find optimal trend window for each year\"\"\"\n",
        "    return (self.config['find_optimal_trend_window'] == 'Y')\n",
        "\n",
        "  def setTrendWindows(self, trend_windows):\n",
        "    \"\"\"Set trend window lengths (years)\"\"\"\n",
        "    assert isinstance(trend_windows, list)\n",
        "    assert len(trend_windows) > 0\n",
        "\n",
        "    # trend windows less than 2 years do not make sense\n",
        "    for tw in trend_windows:\n",
        "      assert tw > 2\n",
        "\n",
        "    self.config['trend_windows'] = trend_windows\n",
        "\n",
        "  def getTrendWindows(self):\n",
        "    \"\"\"Return trend window lengths (years)\"\"\"\n",
        "    return self.config['trend_windows']\n",
        "\n",
        "  def setUseCentroids(self, use_centroids):\n",
        "    \"\"\"Set whether to use centroid coordinates and distance to coast\"\"\"\n",
        "    use_ct = use_centroids.upper()\n",
        "    assert use_ct in ['Y', 'N']\n",
        "    self.config['use_centroids'] = use_ct\n",
        "    self.updateDataSources('CENTROIDS', use_ct)\n",
        "\n",
        "  def useCentroids(self):\n",
        "    \"\"\"Return whether to use centroid coordinates and distance to coast\"\"\"\n",
        "    return (self.config['use_centroids'] == 'Y')\n",
        "\n",
        "  def setUseRemoteSensing(self, use_remote_sensing):\n",
        "    \"\"\"Set whether to use remote sensing data\"\"\"\n",
        "    use_rs = use_remote_sensing.upper()\n",
        "    assert use_rs in ['Y', 'N']\n",
        "    self.config['use_remote_sensing'] = use_rs\n",
        "    self.updateDataSources('REMOTE_SENSING', use_rs, 'NUTS2')\n",
        "\n",
        "  def useRemoteSensing(self):\n",
        "    \"\"\"Return whether to use remote sensing data\"\"\"\n",
        "    return (self.config['use_remote_sensing'] == 'Y')\n",
        "\n",
        "  def setUseGAES(self, use_gaes):\n",
        "    \"\"\"Set whether to use GAES data\"\"\"\n",
        "    use_aez = use_gaes.upper()\n",
        "    assert use_aez in ['Y', 'N']\n",
        "    self.config['use_gaes'] = use_aez\n",
        "    self.updateDataSources('GAES', use_aez)\n",
        "    self.updateDataSources('CROP_AREA', use_aez)\n",
        "\n",
        "  def useGAES(self):\n",
        "    \"\"\"Return whether to use GAES data\"\"\"\n",
        "    return (self.config['use_gaes'] == 'Y')\n",
        "\n",
        "  def setUsePerYearCropCalendar(self, use_per_year_cc):\n",
        "    \"\"\"Set whether to use per region, per year crop calendar\"\"\"\n",
        "    per_year = use_per_year_cc.upper()\n",
        "    assert per_year in ['Y', 'N']\n",
        "    self.config['use_per_year_crop_calendar'] = per_year\n",
        "\n",
        "  def usePerYearCropCalendar(self):\n",
        "    \"\"\"Return whether to use per region, per year crop calendar\"\"\"\n",
        "    return (self.config['use_per_year_crop_calendar'] == 'Y')\n",
        "\n",
        "  def setEarlySeasonPrediction(self, early_season):\n",
        "    \"\"\"Set whether to do early season prediction\"\"\"\n",
        "    ep = early_season.upper()\n",
        "    assert ep in ['Y', 'N']\n",
        "    self.config['early_season_prediction'] = ep\n",
        "\n",
        "  def earlySeasonPrediction(self):\n",
        "    \"\"\"Return whether to do early season prediction\"\"\"\n",
        "    return (self.config['early_season_prediction'] == 'Y')\n",
        "\n",
        "  def setEarlySeasonEndDekad(self, end_dekad):\n",
        "    \"\"\"Set early season prediction dekad\"\"\"\n",
        "    dekads_range = [dek for dek in range(1, 37)]\n",
        "    assert end_dekad in dekads_range\n",
        "    self.config['early_season_end_dekad'] = end_dekad\n",
        "\n",
        "  def getEarlySeasonEndDekad(self):\n",
        "    \"\"\"Return early season prediction dekad\"\"\"\n",
        "    return self.config['early_season_end_dekad']\n",
        "\n",
        "  def setDataPath(self, data_path):\n",
        "    \"\"\"Set the data path\"\"\"\n",
        "    # TODO: some validation\n",
        "    self.config['data_path'] = data_path\n",
        "\n",
        "  def getDataPath(self):\n",
        "    \"\"\"Return the data path\"\"\"\n",
        "    return self.config['data_path']\n",
        "\n",
        "  def setOutputPath(self, out_path):\n",
        "    \"\"\"Set the path to output files. TODO: some validation.\"\"\"\n",
        "    self.config['output_path'] = out_path\n",
        "\n",
        "  def getOutputPath(self):\n",
        "    \"\"\"Return the path to output files.\"\"\"\n",
        "    return self.config['output_path']\n",
        "\n",
        "  def setUseFeaturesV2(self, use_v2):\n",
        "    \"\"\"Set whether to use features v2\"\"\"\n",
        "    ft_v2 = use_v2.upper()\n",
        "    assert ft_v2 in ['Y', 'N']\n",
        "    self.config['use_features_v2'] = ft_v2\n",
        "\n",
        "  def useFeaturesV2(self):\n",
        "    \"\"\"Return whether to use features v2\"\"\"\n",
        "    return (self.config['use_features_v2'] == 'Y')\n",
        "\n",
        "  def setSaveFeatures(self, save_ft):\n",
        "    \"\"\"Set whether to save features in a CSV file\"\"\"\n",
        "    sft = save_ft.upper()\n",
        "    assert sft in ['Y', 'N']\n",
        "    self.config['save_features'] = sft\n",
        "\n",
        "  def saveFeatures(self):\n",
        "    \"\"\"Return whether to save features in a CSV file\"\"\"\n",
        "    return (self.config['save_features'] == 'Y')\n",
        "\n",
        "  def setUseSavedFeatures(self, use_saved):\n",
        "    \"\"\"Set whether to use features from CSV file\"\"\"\n",
        "    saved = use_saved.upper()\n",
        "    assert saved in ['Y', 'N']\n",
        "    self.config['use_saved_features'] = saved\n",
        "\n",
        "  def useSavedFeatures(self):\n",
        "    \"\"\"Return whether to use to use features from CSV file\"\"\"\n",
        "    return (self.config['use_saved_features'] == 'Y')\n",
        "\n",
        "  def setUseSampleWeights(self, use_weights):\n",
        "    \"\"\"Set whether to use data sample weights\"\"\"\n",
        "    use_sw = use_weights.upper()\n",
        "    assert use_sw in ['Y', 'N']\n",
        "    self.config['use_sample_weights'] = use_sw\n",
        "\n",
        "  def useSampleWeights(self):\n",
        "    \"\"\"Return whether to use data sample weights\"\"\"\n",
        "    return (self.config['use_sample_weights'] == 'Y')\n",
        "\n",
        "  def setRetrainPerTestYear(self, per_test_year):\n",
        "    \"\"\"Set whether to retrain the model for every test year\"\"\"\n",
        "    pty = per_test_year.upper()\n",
        "    assert pty in ['Y', 'N']\n",
        "    self.config['retrain_per_test_year'] = pty\n",
        "\n",
        "  def retrainPerTestYear(self):\n",
        "    \"\"\"Return whether to retrain the model for every test year\"\"\"\n",
        "    return (self.config['retrain_per_test_year'] == 'Y')\n",
        "\n",
        "  def setSavePredictions(self, save_pred):\n",
        "    \"\"\"Set whether to save predictions in a CSV file\"\"\"\n",
        "    spd = save_pred.upper()\n",
        "    assert spd in ['Y', 'N']\n",
        "    self.config['save_predictions'] = spd\n",
        "\n",
        "  def savePredictions(self):\n",
        "    \"\"\"Return whether to save predictions in a CSV file\"\"\"\n",
        "    return (self.config['save_predictions'] == 'Y')\n",
        "\n",
        "  def setUseSavedPredictions(self, use_saved):\n",
        "    \"\"\"Set whether to use predictions from CSV file\"\"\"\n",
        "    saved = use_saved.upper()\n",
        "    assert saved in ['Y', 'N']\n",
        "    self.config['use_saved_predictions'] = saved\n",
        "\n",
        "  def useSavedPredictions(self):\n",
        "    \"\"\"Return whether to use to use predictions from CSV file\"\"\"\n",
        "    return (self.config['use_saved_predictions'] == 'Y')\n",
        "\n",
        "  def setCompareWithMCYFS(self, compare_mcyfs):\n",
        "    \"\"\"Set whether to compare predictions with MCYFS\"\"\"\n",
        "    comp_mcyfs = compare_mcyfs.upper()\n",
        "    assert comp_mcyfs in ['Y', 'N']\n",
        "    self.config['compare_with_mcyfs'] = comp_mcyfs\n",
        "\n",
        "  def compareWithMCYFS(self):\n",
        "    \"\"\"Return whether to compare predictions with MCYFS\"\"\"\n",
        "    return (self.config['compare_with_mcyfs'] == 'Y')\n",
        "\n",
        "  def setDebugLevel(self, debug_level):\n",
        "    \"\"\"Set the debug level\"\"\"\n",
        "    assert debug_level in debug_levels\n",
        "    self.config['debug_level'] = debug_level\n",
        "\n",
        "  def getDebugLevel(self):\n",
        "    \"\"\"Return the debug level\"\"\"\n",
        "    return self.config['debug_level']\n",
        "\n",
        "  def updateConfiguration(self, config_update):\n",
        "    \"\"\"Update configuration\"\"\"\n",
        "    assert isinstance(config_update, dict)\n",
        "    for k in config_update:\n",
        "      assert k in self.config\n",
        "\n",
        "      # keys that need special handling\n",
        "      special_cases = {\n",
        "          'crop_name' : self.setCropName,\n",
        "          'crop_id' : self.setCropID,\n",
        "          'use_centroids' : self.setUseCentroids,\n",
        "          'use_remote_sensing' : self.setUseRemoteSensing,\n",
        "          'use_gaes' : self.setUseGAES,\n",
        "      }\n",
        "\n",
        "      if (k not in special_cases):\n",
        "        self.config[k] = config_update[k]\n",
        "        continue\n",
        "\n",
        "      # special case\n",
        "      special_cases[k](config_update[k])\n",
        "\n",
        "  def printConfig(self, log_fh):\n",
        "    \"\"\"Print current configuration and write configuration to log file.\"\"\"\n",
        "    config_str = '\\nCurrent ML Baseline Configuration'\n",
        "    config_str += '\\n--------------------------------'\n",
        "    for k in self.config:\n",
        "      if (isinstance(self.config[k], dict)):\n",
        "        conf_keys = list(self.config[k].keys())\n",
        "        if (not isinstance(conf_keys[0], str)):\n",
        "          conf_keys = [str(k) for k in conf_keys]\n",
        "\n",
        "        config_str += '\\n' + self.config_desc[k] + ': ' + ', '.join(conf_keys)\n",
        "      elif (isinstance(self.config[k], list)):\n",
        "        conf_vals = self.config[k]\n",
        "        if (not isinstance(conf_vals[0], str)):\n",
        "          conf_vals = [str(k) for k in conf_vals]\n",
        "\n",
        "        config_str += '\\n' + self.config_desc[k] + ': ' + ', '.join(conf_vals)\n",
        "      else:\n",
        "        conf_val = self.config[k]\n",
        "        if (not isinstance(conf_val, str)):\n",
        "          conf_val = str(conf_val)\n",
        "\n",
        "        config_str += '\\n' + self.config_desc[k] + ': ' + conf_val\n",
        "\n",
        "    config_str += '\\n'\n",
        "    log_fh.write(config_str + '\\n')\n",
        "    print(config_str)\n",
        "\n",
        "  # Machine learning configuration\n",
        "  def getFeatureCorrelationThreshold(self):\n",
        "    \"\"\"Return threshold for removing mutually correlated features\"\"\"\n",
        "    return self.feature_correlation_threshold\n",
        "\n",
        "  def setFeatureCorrelationThreshold(self, corr_thresh):\n",
        "    \"\"\"Set threshold for removing mutually correlated features\"\"\"\n",
        "    assert (corr_thresh > 0.0 and corr_thresh < 1.0)\n",
        "    self.feature_correlation_threshold = corr_thresh\n",
        "\n",
        "  def getTestFraction(self):\n",
        "    \"\"\"Return test set fraction (of full dataset)\"\"\"\n",
        "    return self.test_fraction\n",
        "\n",
        "  def setTestFraction(self, test_fraction):\n",
        "    \"\"\"Set test set fraction (of full dataset)\"\"\"\n",
        "    assert (test_fraction > 0.0 and test_fraction < 1.0)\n",
        "    self.test_fraction = test_fraction\n",
        "\n",
        "  def getFeatureScaler(self):\n",
        "    \"\"\"Return feature scaling method\"\"\"\n",
        "    return self.scaler\n",
        "\n",
        "  def setFeatureScaler(self, scaler):\n",
        "    \"\"\"Set feature scaling method\"\"\"\n",
        "    assert (isinstance(scaler, MinMaxScaler) or isinstance(scaler, StandardScaler))\n",
        "    self.scaler = scaler\n",
        "\n",
        "  def getFeatureSelectionCVMetric(self):\n",
        "    \"\"\"Return metric for feature selection using K-fold validation\"\"\"\n",
        "    return self.fs_cv_metric\n",
        "\n",
        "  def setFeatureSelectionCVMetric(self, fs_metric):\n",
        "    \"\"\"Return metric for feature selection using K-fold validation\"\"\"\n",
        "    assert fs_metric in self.eval_metrics\n",
        "    self.fs_cv_metric = fs_metric\n",
        "\n",
        "  def getAlgorithmTrainingCVMetric(self):\n",
        "    \"\"\"Return metric for hyperparameter optimization using K-fold validation\"\"\"\n",
        "    return self.est_cv_metric\n",
        "\n",
        "  def setFeatureSelectionCVMetric(self, est_metric):\n",
        "    \"\"\"Return metric for hyperparameter optimization using K-fold validation\"\"\"\n",
        "    assert est_metric in self.eval_metrics\n",
        "    self.est_cv_metric = est_metric\n",
        "\n",
        "  def getFeatureSelectors(self, num_features):\n",
        "    \"\"\"Feature selection methods\"\"\"\n",
        "    # already defined?\n",
        "    if (len(self.feature_selectors) > 0):\n",
        "      return self.feature_selectors\n",
        "\n",
        "    # Early season prediction can have less than 10 features\n",
        "    min_features = 10 if num_features > 10 else num_features\n",
        "    max_features = [min_features]\n",
        "\n",
        "    if (num_features > 15):\n",
        "      max_features.append(15)\n",
        "    if (num_features > 20):\n",
        "      max_features.append(20)\n",
        "\n",
        "    use_yield_trend = self.useYieldTrend()\n",
        "    if ((num_features > 25) and (use_yield_trend)):\n",
        "      max_features.append(25)\n",
        "\n",
        "    rf = RandomForestRegressor(n_estimators=100, max_depth=5,\n",
        "                               bootstrap=True, random_state=42,\n",
        "                               oob_score=True, min_samples_leaf=5)\n",
        "\n",
        "    lasso = Lasso(alpha=0.1, copy_X=True, fit_intercept=True,\n",
        "                  random_state=42,selection='cyclic', tol=0.01)\n",
        "\n",
        "    self.feature_selectors = {\n",
        "      # random forest\n",
        "      'random_forest' : {\n",
        "          'selector' : SelectFromModel(rf, threshold='median'),\n",
        "          'param_grid' : dict(selector__max_features=max_features)\n",
        "      },\n",
        "      # recursive feature elimination using Lasso\n",
        "      'RFE_Lasso' : {\n",
        "          'selector' : RFE(lasso),\n",
        "          'param_grid' : dict(selector__n_features_to_select=max_features)\n",
        "      },\n",
        "      # NOTE: Mutual info raises an error when used with spark parallel backend.\n",
        "      # univariate feature selection\n",
        "      # 'mutual_info' : {\n",
        "      #     'selector' : SelectKBest(mutual_info_regression),\n",
        "      #     'param_grid' : dict(selector__k=max_features)\n",
        "      # },\n",
        "    }\n",
        "\n",
        "    return self.feature_selectors\n",
        "\n",
        "  def setFeatureSelectors(self, ft_sel):\n",
        "    \"\"\"Set feature selection algorithms\"\"\"\n",
        "    assert isinstance(ft_sel, dict)\n",
        "    assert len(ft_sel) > 0\n",
        "    for sel in ft_sel:\n",
        "      assert isinstance(sel, dict)\n",
        "      assert 'selector' in sel\n",
        "      assert 'param_grid' in sel\n",
        "      # add cases if other feature selection methods are used\n",
        "      assert (isinstance(sel['selector'], SelectKBest) or\n",
        "              isinstance(sel['selector'], SelectFromModel) or\n",
        "              isinstance(sel['selector'], RFE))\n",
        "      assert isinstance(sel['param_grid'], dict)\n",
        "\n",
        "    self.feature_selectors = ft_sel\n",
        "\n",
        "  def getEstimators(self):\n",
        "    \"\"\"Return machine learning algorithms for prediction\"\"\"\n",
        "    return self.estimators\n",
        "  \n",
        "  def setEstimators(self, estimators):\n",
        "    \"\"\"Set machine learning algorithms for prediction\"\"\"\n",
        "    assert isinstance(estimators, dict)\n",
        "    assert len(estimators) > 0\n",
        "    for est in estimators:\n",
        "      assert isinstance(est, dict)\n",
        "      assert 'estimator' in est\n",
        "      assert 'param_grid' in est\n",
        "      assert 'fs_param_grid' in est\n",
        "      assert isinstance(est['param_grid'], dict)\n",
        "      assert isinstance(est['fs_param_grid'], dict)\n",
        "\n",
        "    self.estimators = estimators\n",
        "  \n",
        "  def getEvaluationMetrics(self):\n",
        "    \"\"\"Return metrics to evaluate predictions of algorithms\"\"\"\n",
        "    return self.eval_metrics\n",
        "  \n",
        "  def setEvaluationMetrics(self, metrics):\n",
        "    assert isinstance(metrics, dict)\n",
        "    self.eval_metrics = metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKLbsRZPj_la"
      },
      "source": [
        "## Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJOtnzSYQvPu"
      },
      "source": [
        "### Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDlfa0-RtsA7"
      },
      "source": [
        "#### Data Loader Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk-0JeNdQvP2"
      },
      "source": [
        "#%%writefile data_loading.py\n",
        "class CYPDataLoader:\n",
        "  def __init__(self, spark, cyp_config):\n",
        "    self.spark = spark\n",
        "    self.data_path = cyp_config.getDataPath()\n",
        "    self.country_code = cyp_config.getCountryCode()\n",
        "    self.nuts_level = cyp_config.getNUTSLevel()\n",
        "    self.data_sources = cyp_config.getDataSources()\n",
        "    self.verbose = cyp_config.getDebugLevel()\n",
        "    self.data_dfs = {}\n",
        "\n",
        "  def loadFromCSVFile(self, data_path, src, nuts, country_code):\n",
        "    \"\"\"\n",
        "    The implied filename for each source is:\n",
        "    <data_source>_<nuts_level>_<country_code>.csv\n",
        "    Examples: CENTROIDS_NUTS2_NL.csv, WOFOST_NUTS2_NL.csv.\n",
        "    Schema is inferred from the file. We might want to specify the schema at some point.\n",
        "    \"\"\"\n",
        "    if (country_code is not None):\n",
        "      datafile = data_path + '/' + src  + '_' + nuts + '_' + country_code + '.csv'\n",
        "    else:\n",
        "      datafile = data_path + '/' + src  + '_' + nuts + '.csv'\n",
        "\n",
        "    if (self.verbose > 1):\n",
        "      print('Data file name', '\"' + datafile + '\"')\n",
        "\n",
        "    df = self.spark.read.csv(datafile, header = True, inferSchema = True)\n",
        "    return df\n",
        "\n",
        "  def loadData(self, src, nuts_level):\n",
        "    \"\"\"\n",
        "    Load data for a specific data source.\n",
        "    nuts_level may one level or a list of levels.\n",
        "    \"\"\"\n",
        "    data_path = self.data_path\n",
        "    country_code = self.country_code\n",
        "    assert src in self.data_sources\n",
        "\n",
        "    if (isinstance(nuts_level, list)):\n",
        "      src_dfs = []\n",
        "      for nuts in nuts_level:\n",
        "        df = self.loadFromCSVFile(data_path, src, nuts, country_code)\n",
        "        src_dfs.append(df)\n",
        "\n",
        "    elif (isinstance(nuts_level, str)):\n",
        "      src_dfs = self.loadFromCSVFile(data_path, src, nuts_level, country_code)\n",
        "    else:\n",
        "      src_dfs = None\n",
        "\n",
        "    return src_dfs\n",
        "\n",
        "  def loadAllData(self):\n",
        "    \"\"\"\n",
        "    NOT SUPPORTED:\n",
        "    1. Schema is not defined.\n",
        "    2. Loading data for multiple countries.\n",
        "    3. Loading data from folders.\n",
        "    Ioannis: Spark has a nice way of loading several files from a folder,\n",
        "    and associating the file name on each record, using the function\n",
        "    input_file_name. This allows to extract medatada from the path\n",
        "    into the dataframe. In your case it could be the country name, etc.\n",
        "    \"\"\"\n",
        "    data_dfs = {}\n",
        "    for src in self.data_sources:\n",
        "      nuts_level = self.nuts_level\n",
        "      if (isinstance(self.data_sources, dict)):\n",
        "        nuts_level = self.data_sources[src]\n",
        "      # REMOTE_SENSING data is at NUTS2. If nuts_level is None, leave as is.\n",
        "      elif ((src == 'REMOTE_SENSING') and (nuts_level is not None)):\n",
        "        nuts_level = 'NUTS2'\n",
        "\n",
        "      if ('METEO' in src):\n",
        "        data_dfs['METEO'] = self.loadData(src, nuts_level)\n",
        "      else:\n",
        "        data_dfs[src] = self.loadData(src, nuts_level)\n",
        "\n",
        "    if (self.verbose > 1):\n",
        "      data_sources_str = ''\n",
        "      for src in data_dfs:\n",
        "        data_sources_str = data_sources_str + src + ', '\n",
        "\n",
        "      # remove the comma and space from the end\n",
        "      print('Loaded data:', data_sources_str[:-2])\n",
        "      print('\\n')\n",
        "\n",
        "    return data_dfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30v2QIEh2K9P"
      },
      "source": [
        "#### Data Preprocessor Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo2OHq0x2ID1"
      },
      "source": [
        "#%%writefile data_preprocessing.py\n",
        "from pyspark.sql import Window\n",
        "\n",
        "class CYPDataPreprocessor:\n",
        "  def __init__(self, spark, cyp_config):\n",
        "    self.spark = spark\n",
        "    self.verbose = cyp_config.getDebugLevel()\n",
        "\n",
        "  def extractYearDekad(self, df):\n",
        "    \"\"\"Extract year and dekad from date_col in yyyyMMdd format.\"\"\"\n",
        "    # Conversion to string type is required to make getYear(), getMonth() etc. work correctly.\n",
        "    # They use to_date() function to verify valid dates and to_date() expects the date column to be string.\n",
        "    df = df.withColumn('DATE', df['DATE'].cast(\"string\"))\n",
        "    df = df.select('*',\n",
        "                   getYear('DATE').alias('FYEAR'),\n",
        "                   getDekad('DATE').alias('DEKAD'))\n",
        "\n",
        "    # Bring FYEAR, DEKAD to the front\n",
        "    col_order = df.columns[:2] + df.columns[-2:] + df.columns[2:-2]\n",
        "    df = df.select(col_order).drop('DATE')\n",
        "    return df\n",
        "\n",
        "  def getCropSeasonInformation(self, wofost_df, season_crosses_calyear):\n",
        "    \"\"\"Crop season information based on WOFOST DVS\"\"\"\n",
        "    join_cols = ['IDREGION', 'FYEAR']\n",
        "    if (('DATE' in wofost_df.columns) and ('FYEAR' not in wofost_df.columns)):\n",
        "      wofost_df = self.extractYearDekad(wofost_df)\n",
        "\n",
        "    crop_season = wofost_df.select(join_cols).distinct()\n",
        "    diff_window = Window.partitionBy(join_cols).orderBy('DEKAD')\n",
        "    cs_window = Window.partitionBy('IDREGION').orderBy('FYEAR')\n",
        "\n",
        "    wofost_df = wofost_df.withColumn('VALUE', wofost_df['DVS'])\n",
        "    wofost_df = wofost_df.withColumn('PREV', SparkF.lag(wofost_df['VALUE']).over(diff_window))\n",
        "    wofost_df = wofost_df.withColumn('DIFF', SparkF.when(SparkF.isnull(wofost_df['PREV']), 0)\\\n",
        "                                     .otherwise(wofost_df['VALUE'] - wofost_df['PREV']))\n",
        "    # calculate end of season dekad\n",
        "    dvs_nochange_filter = ((wofost_df['VALUE'] >= 200) & (wofost_df['DIFF'] == 0.0))\n",
        "    year_end_filter = (wofost_df['DEKAD'] == 36)\n",
        "    if (season_crosses_calyear):\n",
        "      value_zero_filter =  (wofost_df['VALUE'] == 0)\n",
        "    else:\n",
        "      value_zero_filter =  ((wofost_df['PREV'] >= 200) & (wofost_df['VALUE'] == 0))\n",
        "\n",
        "    end_season_filter = (dvs_nochange_filter | value_zero_filter | year_end_filter)\n",
        "    crop_season = crop_season.join(wofost_df.filter(end_season_filter).groupBy(join_cols)\\\n",
        "                                   .agg(SparkF.min('DEKAD').alias('SEASON_END_DEKAD')), join_cols)\n",
        "    wofost_df = wofost_df.drop('VALUE', 'PREV', 'DIFF')\n",
        "\n",
        "    # We take the max of SEASON_END_DEKAD for current campaign and next campaign\n",
        "    # to determine which dekads go to next campaign year.\n",
        "    max_year = crop_season.agg(SparkF.max('FYEAR')).collect()[0][0]\n",
        "    min_year = crop_season.agg(SparkF.min('FYEAR')).collect()[0][0]\n",
        "    crop_season = crop_season.withColumn('NEXT_SEASON_END', SparkF.when(crop_season['FYEAR'] == max_year,\n",
        "                                                                        crop_season['SEASON_END_DEKAD'])\\\n",
        "                                         .otherwise(SparkF.lead(crop_season['SEASON_END_DEKAD']).over(cs_window)))\n",
        "    crop_season = crop_season.withColumn('SEASON_END',\n",
        "                                         SparkF.when(crop_season['SEASON_END_DEKAD'] > crop_season['NEXT_SEASON_END'],\n",
        "                                                     crop_season['SEASON_END_DEKAD'])\\\n",
        "                                         .otherwise(crop_season['NEXT_SEASON_END']))\n",
        "    crop_season = crop_season.withColumn('PREV_SEASON_END', SparkF.when(crop_season['FYEAR'] == min_year, 0)\\\n",
        "                                         .otherwise(SparkF.lag(crop_season['SEASON_END']).over(cs_window)))\n",
        "    crop_season = crop_season.select(join_cols + ['PREV_SEASON_END', 'SEASON_END'])\n",
        "\n",
        "    return crop_season\n",
        "\n",
        "  def alignDataToCropSeason(self, df, crop_season, season_crosses_calyear):\n",
        "    \"\"\"Calculate CAMPAIGN_YEAR, CAMPAIGN_DEKAD based on crop_season\"\"\"\n",
        "    join_cols = ['IDREGION', 'FYEAR']\n",
        "    max_year = crop_season.agg(SparkF.max('FYEAR')).collect()[0][0]\n",
        "    min_year = crop_season.agg(SparkF.min('FYEAR')).collect()[0][0]\n",
        "    df = df.join(crop_season, join_cols)\n",
        "\n",
        "    # Dekads > SEASON_END belong to next campaign year\n",
        "    df = df.withColumn('CAMPAIGN_YEAR',\n",
        "                       SparkF.when(df['DEKAD'] > df['SEASON_END'], df['FYEAR'] + 1)\\\n",
        "                       .otherwise(df['FYEAR']))\n",
        "    # min_year has no previous season information. We align CAMPAIGN_DEKAD to end in 36.\n",
        "    # For other years, dekads < SEASON_END are adjusted based on PREV_SEASON_END.\n",
        "    # Dekads > SEASON_END get renumbered from 1 (for next campaign).\n",
        "    df = df.withColumn('CAMPAIGN_DEKAD',\n",
        "                       SparkF.when(df['CAMPAIGN_YEAR'] == min_year, df['DEKAD'] + 36 - df['SEASON_END'])\\\n",
        "                       .otherwise(SparkF.when(df['DEKAD'] > df['SEASON_END'], df['DEKAD'] - df['SEASON_END'])\\\n",
        "                                  .otherwise(df['DEKAD'] + 36 - df['PREV_SEASON_END'])))\n",
        "\n",
        "    # Columns should be IDREGION, FYEAR, DEKAD, ..., CAMPAIGN_YEAR, CAMPAIGN_DEKAD.\n",
        "    # Bring CAMPAIGN_YEAR and CAMPAIGN_DEKAD to the front.\n",
        "    col_order = df.columns[:3] + df.columns[-2:] + df.columns[3:-2]\n",
        "    df = df.select(col_order)\n",
        "    if (season_crosses_calyear):\n",
        "      # For crop with two seasons, remove the first year. Data from the first year\n",
        "      # only contributes to the second year and we have already moved useful data\n",
        "      # to the second year (or first campaign year).\n",
        "      df = df.filter(df['CAMPAIGN_YEAR'] > min_year)\n",
        "\n",
        "    # In both cases, remove extra rows beyond max campaign year\n",
        "    df = df.filter(df['CAMPAIGN_YEAR'] <= max_year)\n",
        "    return df\n",
        "\n",
        "  def preprocessWofost(self, wofost_df, crop_season, season_crosses_calyear):\n",
        "    \"\"\"\n",
        "    Extract year and dekad from date. Use crop_season to compute\n",
        "    CAMPAIGN_YEAR and CAMPAIGN_DEKAD.\n",
        "    \"\"\"\n",
        "    drop_cols = crop_season.columns[2:]\n",
        "    if (('DATE' in wofost_df.columns) and ('FYEAR' not in wofost_df.columns)):\n",
        "      wofost_df = self.extractYearDekad(wofost_df)\n",
        "\n",
        "    join_cols = ['IDREGION', 'FYEAR']\n",
        "    wofost_df = self.alignDataToCropSeason(wofost_df, crop_season, season_crosses_calyear)\n",
        "\n",
        "    # WOFOST indicators come after IDREGION, FYEAR, DEKAD, CAMPAIGN_YEAR, CAMPAIGN_DEKAD.\n",
        "    wofost_inds = wofost_df.columns[5:]\n",
        "    # set indicators values for dekads after end of season to zero.\n",
        "    # TODO - Dilli: Find a way to avoid the for loop.\n",
        "    for ind in wofost_inds:\n",
        "      wofost_df = wofost_df.withColumn(ind,\n",
        "                                       SparkF.when(wofost_df['DEKAD'] < wofost_df['SEASON_END'],\n",
        "                                                   wofost_df[ind])\\\n",
        "                                       .otherwise(0))\n",
        "\n",
        "    wofost_df = wofost_df.drop(*drop_cols)\n",
        "    return wofost_df\n",
        "\n",
        "  def preprocessMeteo(self, meteo_df, crop_season, season_crosses_calyear):\n",
        "    \"\"\"\n",
        "    Extract year and dekad from date, calculate CWB.\n",
        "    Use crop_season to compute CAMPAIGN_YEAR and CAMPAIGN_DEKAD.\n",
        "    \"\"\"\n",
        "    join_cols = ['IDREGION', 'FYEAR']\n",
        "    drop_cols = crop_season.columns[2:]\n",
        "    meteo_df = meteo_df.drop('IDCOVER')\n",
        "    meteo_df = meteo_df.withColumn('CWB',\n",
        "                                   SparkF.bround(meteo_df['PREC'] - meteo_df['ET0'], 2))\n",
        "    if (('DATE' in meteo_df.columns) and ('FYEAR' not in meteo_df.columns)):\n",
        "      meteo_df = self.extractYearDekad(meteo_df)\n",
        "\n",
        "    meteo_df = self.alignDataToCropSeason(meteo_df, crop_season, season_crosses_calyear)\n",
        "    meteo_df = meteo_df.drop(*drop_cols)\n",
        "    return meteo_df\n",
        "\n",
        "  def preprocessMeteoDaily(self, meteo_df):\n",
        "    \"\"\"\n",
        "    Convert daily meteo data to dekadal. Takes avg for all indicators\n",
        "    except TMAX (take max), TMIN (take min), PREC (take sum), ET0 (take sum), CWB (take sum).\n",
        "    \"\"\"\n",
        "    self.spark.catalog.dropTempView('meteo_daily')\n",
        "    meteo_df.createOrReplaceTempView('meteo_daily')\n",
        "    join_cols = ['IDREGION', 'CAMPAIGN_YEAR', 'CAMPAIGN_DEKAD']\n",
        "    join_df = meteo_df.select(join_cols + ['FYEAR', 'DEKAD']).distinct()\n",
        "\n",
        "    # We are ignoring VPRES, WSPD and RELH at the moment\n",
        "    # avg(VPRES) as VPRES1, avg(WSPD) as WSPD1, avg(RELH) as RELH1,\n",
        "    # TMAX| TMIN| TAVG| VPRES| WSPD| PREC| ET0| RAD| RELH| CWB\n",
        "    #\n",
        "    # It seems keeping same name after aggregation is fine. We are using a\n",
        "    # different name just to be sure nothing untoward happens.\n",
        "    query = 'select IDREGION, CAMPAIGN_YEAR, CAMPAIGN_DEKAD, '\n",
        "    query = query + ' max(TMAX) as TMAX1, min(TMIN) as TMIN1, '\n",
        "    query = query + ' bround(avg(TAVG), 2) as TAVG1, bround(sum(PREC), 2) as PREC1, '\n",
        "    query = query + ' bround(sum(ET0), 2) as ET01, bround(avg(RAD), 2) as RAD1, '\n",
        "    query = query + ' bround(sum(CWB), 2) as CWB1 '\n",
        "    query = query + ' from meteo_daily group by IDREGION, CAMPAIGN_YEAR, CAMPAIGN_DEKAD '\n",
        "    query = query + ' order by IDREGION, CAMPAIGN_YEAR, CAMPAIGN_DEKAD'\n",
        "    meteo_df = self.spark.sql(query).cache()\n",
        "\n",
        "    # rename the columns\n",
        "    selected_cols = ['TMAX', 'TMIN', 'TAVG', 'PREC', 'ET0', 'RAD', 'CWB']\n",
        "    for scol in selected_cols:\n",
        "      meteo_df = meteo_df.withColumnRenamed(scol + '1', scol)\n",
        "\n",
        "    meteo_df = meteo_df.join(join_df, join_cols)\n",
        "    # Bring FYEAR, DEKAD to the front\n",
        "    col_order = meteo_df.columns[:1] + meteo_df.columns[-2:] + meteo_df.columns[1:-2]\n",
        "    meteo_df = meteo_df.select(col_order)\n",
        "\n",
        "    return meteo_df\n",
        "\n",
        "  def remoteSensingNUTS2ToNUTS3(self, rs_df, nuts3_regions):\n",
        "    \"\"\"\n",
        "    Convert NUTS2 remote sensing data to NUTS3.\n",
        "    Remote sensing values for NUTS3 regions are inherited from parent regions.\n",
        "    NOTE this function is called before preprocessRemoteSensing.\n",
        "    preprocessRemoteSensing expects crop_season and rs_df to be at the same NUTS level.\n",
        "    \"\"\"\n",
        "    NUTS3_dict = {}\n",
        "\n",
        "    for nuts3 in nuts3_regions:\n",
        "      nuts2 = nuts3[:4]\n",
        "      try:\n",
        "        existing = NUTS3_dict[nuts2]\n",
        "      except KeyError as e:\n",
        "        existing = []\n",
        "\n",
        "      NUTS3_dict[nuts2] = existing + [nuts3]\n",
        "\n",
        "    rs_NUTS3 = rs_df.rdd.map(lambda r: (NUTS3_dict[r[0]] if r[0] in NUTS3_dict else [], r[1], r[2]))\n",
        "    rs_NUTS3_df = rs_NUTS3.toDF(['NUTS3_REG', 'DATE', 'FAPAR'])\n",
        "    rs_NUTS3_df = rs_NUTS3_df.withColumn('IDREGION', SparkF.explode('NUTS3_REG')).drop('NUTS3_REG')\n",
        "    rs_NUTS3_df = rs_NUTS3_df.select('IDREGION', 'DATE', 'FAPAR')\n",
        "\n",
        "    return rs_NUTS3_df\n",
        "\n",
        "  def preprocessRemoteSensing(self, rs_df, crop_season, season_crosses_calyear):\n",
        "    \"\"\"\n",
        "    Extract year and dekad from date.\n",
        "    Use crop_season to compute CAMPAIGN_YEAR and CAMPAIGN_DEKAD.\n",
        "    NOTE crop_season and rs_df must be at the same NUTS level.\n",
        "    \"\"\"\n",
        "    join_cols = ['IDREGION', 'FYEAR']\n",
        "    drop_cols = crop_season.columns[2:]\n",
        "    if (('DATE' in rs_df.columns) and ('FYEAR' not in rs_df.columns)):\n",
        "      rs_df = self.extractYearDekad(rs_df)\n",
        "\n",
        "    rs_df = self.alignDataToCropSeason(rs_df, crop_season, season_crosses_calyear)\n",
        "    rs_df = rs_df.drop(*drop_cols)\n",
        "    return rs_df\n",
        "\n",
        "  def preprocessCentroids(self, centroids_df):\n",
        "    df_cols = centroids_df.columns\n",
        "    centroids_df = centroids_df.withColumn('CENTROID_X', SparkF.bround('CENTROID_X', 2))\n",
        "    centroids_df = centroids_df.withColumn('CENTROID_Y', SparkF.bround('CENTROID_Y', 2))\n",
        "\n",
        "    return centroids_df\n",
        "\n",
        "  def preprocessSoil(self, soil_df):\n",
        "    # SM_WC = water holding capacity\n",
        "    soil_df = soil_df.withColumn('SM_WHC', SparkF.bround(soil_df.SM_FC - soil_df.SM_WP, 2))\n",
        "    soil_df = soil_df.select(['IDREGION', 'SM_WHC'])\n",
        "\n",
        "    return soil_df\n",
        "\n",
        "  def preprocessAreaFractions(self, af_df, crop_id):\n",
        "    \"\"\"Filter area fractions data by crop id\"\"\"\n",
        "    af_df = af_df.withColumn(\"FYEAR\", af_df[\"FYEAR\"].cast(SparkT.IntegerType()))\n",
        "    af_df = af_df.filter(af_df[\"CROP_ID\"] == crop_id).drop('CROP_ID')\n",
        "\n",
        "    return af_df\n",
        "\n",
        "  def preprocessCropArea(self, area_df, crop_id):\n",
        "    \"\"\"Filter area fractions data by crop id\"\"\"\n",
        "    area_df = area_df.withColumn(\"FYEAR\", area_df[\"FYEAR\"].cast(SparkT.IntegerType()))\n",
        "    area_df = area_df.filter(area_df[\"CROP_ID\"] == crop_id).drop('CROP_ID')\n",
        "    area_df = area_df.filter(area_df[\"CROP_AREA\"].isNotNull())\n",
        "    area_df = area_df.drop('FRACTION')\n",
        "\n",
        "    return area_df\n",
        "\n",
        "  def preprocessGAES(self, gaes_df, crop_id):\n",
        "    \"\"\"Select irrigated crop area by crop id\"\"\"\n",
        "    sel_cols = [ c for c in gaes_df.columns if 'IRRIG' not in c]\n",
        "    sel_cols += ['IRRIG_AREA_ALL', 'IRRIG_AREA' + str(crop_id)]\n",
        "\n",
        "    return gaes_df.select(sel_cols)\n",
        "\n",
        "  def removeDuplicateYieldData(self, yield_df, short_seq_len=2, long_seq_len=5,\n",
        "                               max_short_seqs=1, max_long_seqs=0):\n",
        "    \"\"\"\n",
        "    Find and remove duplicate sequences of yield values.\n",
        "    Missing values are replaced with 0.0. So missing values are also handled.\n",
        "    Using some ideas from\n",
        "    https://stackoverflow.com/questions/51291226/finding-length-of-continuous-ones-in-list-in-a-pyspark-column\n",
        "    \"\"\"\n",
        "    w = Window.partitionBy('IDREGION').orderBy('FYEAR')\n",
        "    # check if value changes from one year to next\n",
        "    yield_df = yield_df.select('*',\n",
        "                               (yield_df['YIELD'] != SparkF.lag(yield_df['YIELD'], default=0)\\\n",
        "                                .over(w)).cast(\"int\").alias(\"YIELD_CHANGE\"),\n",
        "                               (yield_df['FYEAR'] - SparkF.lag(yield_df['FYEAR'], default=0)\\\n",
        "                                .over(w)).cast(\"int\").alias(\"FYEAR_CHANGE\"))\n",
        "    # group set of years with the same value\n",
        "    yield_df = yield_df.select('*',\n",
        "                               SparkF.sum(SparkF.col(\"YIELD_CHANGE\"))\\\n",
        "                               .over(w.rangeBetween(Window.unboundedPreceding, 0)).alias(\"YIELD_GROUP\"))\n",
        "\n",
        "    w2 = Window.partitionBy(['IDREGION', 'YIELD_GROUP'])\n",
        "    # compute the start, end and length of duplicate sequence\n",
        "    yield_df = yield_df.select('*',\n",
        "                               SparkF.min(\"FYEAR\").over(w2).alias(\"SEQ_START\"),\n",
        "                               SparkF.max(\"FYEAR\").over(w2).alias(\"SEQ_END\"),\n",
        "                               SparkF.count(\"*\").over(w2).alias(\"SEQ_LEN\"))\n",
        "\n",
        "    w3 = Window.partitionBy('IDREGION')\n",
        "    # compute max year\n",
        "    yield_df = yield_df.select('*',\n",
        "                               SparkF.min(\"FYEAR\").over(w3).alias(\"MIN_FYEAR\"),\n",
        "                               SparkF.max(\"FYEAR\").over(w3).alias(\"MAX_FYEAR\"))\n",
        "    # For sequences ending in max year, we remove data points only.\n",
        "    # So such sequences are not counted here.\n",
        "    yield_df = yield_df.select('*',\n",
        "                               # count number of short sequences except those ending at max(FYEAR)\n",
        "                               SparkF.sum(SparkF.when((yield_df['SEQ_LEN'] > short_seq_len) &\n",
        "                                                      # (yield_df['SEQ_END'] != yield_df['MAX_FYEAR']) &\n",
        "                                                      (yield_df['FYEAR'] == yield_df['SEQ_END']), 1)\\\n",
        "                                          .otherwise(0)).over(w3).alias('COUNT_SHORT_SEQ'),\n",
        "                               # count number of long sequences except those ending at max(FYEAR)\n",
        "                               SparkF.sum(SparkF.when((yield_df['SEQ_LEN'] > long_seq_len) &\n",
        "                                                      (yield_df['SEQ_END'] != yield_df['MAX_FYEAR']) &\n",
        "                                                      (yield_df['FYEAR'] == yield_df['SEQ_END']), 1)\\\n",
        "                                          .otherwise(0)).over(w3).alias('COUNT_LONG_SEQ'),\n",
        "                               # count missing years\n",
        "                               SparkF.sum(SparkF.when((yield_df['FYEAR'] != yield_df['MIN_FYEAR']) &\n",
        "                                                       (yield_df['FYEAR_CHANGE'] > short_seq_len), 1)\\\n",
        "                                          .otherwise(0)).over(w3).alias('COUNT_SHORT_GAPS'),\n",
        "                               SparkF.sum(SparkF.when((yield_df['FYEAR'] != yield_df['MIN_FYEAR']) &\n",
        "                                                        (yield_df['FYEAR_CHANGE'] > long_seq_len), 1)\\\n",
        "                                          .otherwise(0)).over(w3).alias('COUNT_LONG_GAPS'))\n",
        "\n",
        "    if (self.verbose > 2):\n",
        "      print('Data with duplicate sequences')\n",
        "      yield_df.filter(yield_df['COUNT_SHORT_SEQ'] > max_short_seqs).show()\n",
        "      yield_df.filter(yield_df['COUNT_LONG_SEQ'] > max_long_seqs).show()\n",
        "\n",
        "    # remove regions with many short sequences\n",
        "    yield_df = yield_df.filter(yield_df['COUNT_SHORT_SEQ'] <= max_short_seqs)\n",
        "    yield_df = yield_df.filter(yield_df['COUNT_SHORT_GAPS'] <= max_short_seqs)\n",
        "\n",
        "    # remove regions with long sequences\n",
        "    yield_df = yield_df.filter(yield_df['COUNT_LONG_SEQ'] <= max_long_seqs)\n",
        "    yield_df = yield_df.filter(yield_df['COUNT_LONG_GAPS'] <= max_long_seqs)\n",
        "\n",
        "    # remove data points, except SEQ_START, for remaining sequences\n",
        "    yield_df = yield_df.filter((yield_df['FYEAR'] == yield_df['SEQ_START']) |\n",
        "                               (yield_df['SEQ_LEN'] <= short_seq_len))\n",
        "\n",
        "    return yield_df.select(['IDREGION', 'FYEAR', 'YIELD'])\n",
        "\n",
        "  def preprocessYield(self, yield_df, crop_id, clean_data=False):\n",
        "    \"\"\"\n",
        "    Yield preprocessing depends on the data format.\n",
        "    Here we cover preprocessing for France (NUTS3), Germany (NUTS3) and the Netherlands (NUTS2).\n",
        "    \"\"\"\n",
        "    # Delete trailing empty columns\n",
        "    empty_cols = [ c for c in yield_df.columns if c.startswith('_c') ]\n",
        "    for c in empty_cols:\n",
        "      yield_df = yield_df.drop(c)\n",
        "\n",
        "    # Special case for Netherlands and Germany: convert yield columns into rows\n",
        "    years = [int(c) for c in yield_df.columns if c[0].isdigit()]\n",
        "    if (len(years) > 0):\n",
        "      yield_by_year = yield_df.rdd.map(lambda x: (x[0], cropNameToID(crop_id_dict, x[0]), x[1],\n",
        "                                                  [(years[i], x[i+2]) for i in range(len(years))]))\n",
        "\n",
        "      yield_df = yield_by_year.toDF(['CROP', 'CROP_ID', 'IDREGION', 'YIELD'])\n",
        "      yield_df = yield_df.withColumn('YR_YIELD', SparkF.explode('YIELD')).drop('YIELD')\n",
        "      yield_by_year = yield_df.rdd.map(lambda x: (x[0], x[1], x[2], x[3][0], x[3][1]))\n",
        "      yield_df = yield_by_year.toDF(['CROP', 'CROP_ID', 'IDREGION', 'FYEAR', 'YIELD'])\n",
        "    else:\n",
        "      yield_by_year = yield_df.rdd.map(lambda x: (x[0], cropNameToID(crop_id_dict, x[0]), x[1], x[2], x[3]))\n",
        "      yield_df = yield_by_year.toDF(['CROP', 'CROP_ID', 'IDREGION', 'FYEAR', 'YIELD'])\n",
        "\n",
        "    yield_df = yield_df.filter(yield_df.CROP_ID == crop_id).drop('CROP', 'CROP_ID')\n",
        "    if (yield_df.count() == 0):\n",
        "      return None\n",
        "\n",
        "    yield_df = yield_df.filter(yield_df.YIELD.isNotNull())\n",
        "    yield_df = yield_df.withColumn(\"YIELD\", yield_df[\"YIELD\"].cast(SparkT.FloatType()))\n",
        "    if (clean_data):\n",
        "      yield_df = self.removeDuplicateYieldData(yield_df)\n",
        "\n",
        "    yield_df = yield_df.filter(yield_df['YIELD'] > 0.0)\n",
        "\n",
        "    return yield_df\n",
        "\n",
        "  def preprocessYieldMCYFS(self, mcyfs_df, crop_id):\n",
        "    \"\"\"Preprocess MCYFS NUTS0 level yield predictions\"\"\"\n",
        "    # the input columns are IDREGION, CROP, PREDICTION_DATE, FYEAR, YIELD_PRED\n",
        "    mcyfs_df = mcyfs_df.withColumn('PRED_DEKAD', getDekad('PREDICTION_DATE'))\n",
        "    # the columns should now be IDREGION, CROP, PREDICTION_DATE, FYEAR, YIELD_PRED, PRED_DEKAD\n",
        "    yield_by_year = mcyfs_df.rdd.map(lambda x: (x[1], cropNameToID(crop_id_dict, x[1]),\n",
        "                                                x[0], x[3], x[2], x[4], x[5]))\n",
        "    mcyfs_df = yield_by_year.toDF(['CROP', 'CROP_ID', 'IDREGION', 'FYEAR',\n",
        "                                         'PRED_DATE', 'YIELD_PRED', 'PRED_DEKAD'])\n",
        "    mcyfs_df = mcyfs_df.filter(mcyfs_df.CROP_ID == crop_id).drop('CROP', 'CROP_ID')\n",
        "    if (mcyfs_df.count() == 0):\n",
        "      return None\n",
        "\n",
        "    mcyfs_df = mcyfs_df.withColumn(\"YIELD_PRED\", mcyfs_df[\"YIELD_PRED\"].cast(SparkT.FloatType()))\n",
        "\n",
        "    return mcyfs_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2QR2OmysFd1"
      },
      "source": [
        "#### Run Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ajXCUkjsIfK"
      },
      "source": [
        "#%%writefile run_data_preprocessing.py\n",
        "def printPreprocessingInformation(df, data_source, order_cols, crop_season=None):\n",
        "  \"\"\"Print preprocessed data and additional debug information\"\"\"\n",
        "  df_regions = [reg[0] for reg in df.select('IDREGION').distinct().collect()]\n",
        "  print(data_source , 'data available for', len(df_regions), 'region(s)')\n",
        "  if (crop_season is not None):\n",
        "    print('Season end information')\n",
        "    crop_season.orderBy(['IDREGION', 'FYEAR']).show(10)\n",
        "\n",
        "  print(data_source, 'data')\n",
        "  df.orderBy(order_cols).show(10)\n",
        "\n",
        "def preprocessData(cyp_config, cyp_preprocessor, data_dfs):\n",
        "  crop_id = cyp_config.getCropID()\n",
        "  nuts_level = cyp_config.getNUTSLevel()\n",
        "  season_crosses_calyear = cyp_config.seasonCrossesCalendarYear()\n",
        "  clean_data = cyp_config.cleanData()\n",
        "  use_centroids = cyp_config.useCentroids()\n",
        "  use_remote_sensing = cyp_config.useRemoteSensing()\n",
        "  use_gaes = cyp_config.useGAES()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "\n",
        "  order_cols = ['IDREGION', 'CAMPAIGN_YEAR', 'CAMPAIGN_DEKAD']\n",
        "  # wofost data\n",
        "  wofost_df = data_dfs['WOFOST']\n",
        "  wofost_df = wofost_df.filter(wofost_df['CROP_ID'] == crop_id).drop('CROP_ID')\n",
        "  crop_season = cyp_preprocessor.getCropSeasonInformation(wofost_df, season_crosses_calyear)\n",
        "  wofost_df = cyp_preprocessor.preprocessWofost(wofost_df, crop_season, season_crosses_calyear)\n",
        "  wofost_regions = [reg[0] for reg in wofost_df.select('IDREGION').distinct().collect()]\n",
        "  data_dfs['WOFOST'] = wofost_df\n",
        "  if (debug_level > 1):\n",
        "    printPreprocessingInformation(wofost_df, 'WOFOST', order_cols, crop_season)\n",
        "\n",
        "  # meteo data\n",
        "  meteo_df = data_dfs['METEO']\n",
        "  meteo_df = cyp_preprocessor.preprocessMeteo(meteo_df, crop_season, season_crosses_calyear)\n",
        "  assert (meteo_df is not None)\n",
        "  data_dfs['METEO'] = meteo_df\n",
        "  if (debug_level > 1):\n",
        "    printPreprocessingInformation(meteo_df, 'METEO', order_cols)\n",
        "\n",
        "  # remote sensing data\n",
        "  rs_df = None\n",
        "  if (use_remote_sensing):\n",
        "    rs_df = data_dfs['REMOTE_SENSING']\n",
        "    rs_df = rs_df.drop('IDCOVER')\n",
        "\n",
        "    # if other data is at NUTS3, convert rs_df to NUTS3 using parent region data\n",
        "    if (nuts_level == 'NUTS3'):\n",
        "      rs_df = cyp_preprocessor.remoteSensingNUTS2ToNUTS3(rs_df, wofost_regions)\n",
        "\n",
        "    rs_df = cyp_preprocessor.preprocessRemoteSensing(rs_df, crop_season, season_crosses_calyear)\n",
        "    assert (rs_df is not None)\n",
        "    data_dfs['REMOTE_SENSING'] = rs_df\n",
        "    if (debug_level > 1):\n",
        "      printPreprocessingInformation(rs_df, 'REMOTE_SENSING', order_cols)\n",
        "\n",
        "  order_cols = ['IDREGION']\n",
        "  # centroids and distance to coast\n",
        "  centroids_df = None\n",
        "  if (use_centroids):\n",
        "    centroids_df = data_dfs['CENTROIDS']\n",
        "    centroids_df = cyp_preprocessor.preprocessCentroids(centroids_df)\n",
        "    data_dfs['CENTROIDS'] = centroids_df\n",
        "    if (debug_level > 1):\n",
        "      printPreprocessingInformation(centroids_df, 'CENTROIDS', order_cols)\n",
        "\n",
        "  # soil data\n",
        "  soil_df = data_dfs['SOIL']\n",
        "  soil_df = cyp_preprocessor.preprocessSoil(soil_df)\n",
        "  data_dfs['SOIL'] = soil_df\n",
        "  if (debug_level > 1):\n",
        "    printPreprocessingInformation(soil_df, 'SOIL', order_cols)\n",
        "\n",
        "  # agro-environmental zones\n",
        "  if (use_gaes):\n",
        "    aez_df = data_dfs['GAES']\n",
        "    aez_df = cyp_preprocessor.preprocessGAES(aez_df, crop_id)\n",
        "    data_dfs['GAES'] = aez_df\n",
        "    if (debug_level > 1):\n",
        "      printPreprocessingInformation(aez_df, 'GAES', order_cols)\n",
        "\n",
        "    # crop area data\n",
        "    order_cols = ['IDREGION', 'FYEAR']\n",
        "    crop_area_df = data_dfs['CROP_AREA']\n",
        "    crop_area_df = cyp_preprocessor.preprocessCropArea(crop_area_df, crop_id)\n",
        "    data_dfs['CROP_AREA'] = crop_area_df\n",
        "    if (debug_level > 1):\n",
        "      printPreprocessingInformation(crop_area_df, 'CROP_AREA', order_cols)\n",
        "\n",
        "  order_cols = ['IDREGION', 'FYEAR']\n",
        "  # yield_data\n",
        "  yield_df = data_dfs['YIELD']\n",
        "  if (debug_level > 1):\n",
        "    print('Yield before preprocessing')\n",
        "    yield_df.show(10)\n",
        "\n",
        "  yield_df = cyp_preprocessor.preprocessYield(yield_df, crop_id, clean_data)\n",
        "  assert (yield_df is not None)\n",
        "  data_dfs['YIELD'] = yield_df\n",
        "  if (debug_level > 1):\n",
        "    print('Yield after preprocessing')\n",
        "    yield_df.show(10)\n",
        "\n",
        "  return data_dfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krHPVXls8MRd"
      },
      "source": [
        "### Training and Test Split\n",
        "\n",
        "**Custom validation splits**\n",
        "\n",
        "When yield trend is used, custom sliding validation split is used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YPH051S8MRf"
      },
      "source": [
        "#### Training and Test Splitter Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zBSEGkO8MRf"
      },
      "source": [
        "#%%writefile train_test_sets.py\n",
        "import numpy as np\n",
        "\n",
        "class CYPTrainTestSplitter:\n",
        "  def __init__(self, cyp_config):\n",
        "    self.use_yield_trend = cyp_config.useYieldTrend()\n",
        "    self.test_fraction = cyp_config.getTestFraction()\n",
        "    self.verbose = cyp_config.getDebugLevel()\n",
        "\n",
        "  def getTestYears(self, all_years, test_fraction=None, use_yield_trend=None):\n",
        "    num_years = len(all_years)\n",
        "    test_years = []\n",
        "    if (test_fraction is None):\n",
        "      test_fraction = self.test_fraction\n",
        "\n",
        "    if (use_yield_trend is None):\n",
        "      use_yield_trend = self.use_yield_trend\n",
        "\n",
        "    if (use_yield_trend):\n",
        "      # If test_year_start 15, years with index >= 15 are added to the test set\n",
        "      test_year_start = num_years - np.floor(num_years * test_fraction).astype('int')\n",
        "      test_years = all_years[test_year_start:]\n",
        "    else:\n",
        "      # If test_year_pos = 5, every 5th year is added to test set.\n",
        "      # indices start with 0, so test_year_pos'th year has index (test_year_pos - 1)\n",
        "      test_year_pos = np.floor(1/test_fraction).astype('int')\n",
        "      test_years = all_years[test_year_pos - 1::test_year_pos]\n",
        "\n",
        "    return test_years\n",
        "\n",
        "  def trainTestSplit(self, yield_df, test_fraction=None, use_yield_trend=None):\n",
        "    all_years = sorted([yr[0] for yr in yield_df.select('FYEAR').distinct().collect()])\n",
        "    test_years = self.getTestYears(all_years, test_fraction, use_yield_trend)\n",
        "\n",
        "    return test_years\n",
        "\n",
        "  # Returns an array containings tuples (train_idxs, test_idxs) for each fold\n",
        "  # NOTE Y_train should include IDREGION, FYEAR as first two columns.\n",
        "  def customKFoldValidationSplit(self, Y_train_full, num_folds, log_fh=None):\n",
        "    \"\"\"\n",
        "    Custom K-fold Validation Splits:\n",
        "    When using yield trend, we cannot do k-fold cross-validation. The custom\n",
        "    K-Fold validation splits data in time-ordered fashion. The test data\n",
        "    always comes after the training data.\n",
        "    \"\"\"\n",
        "    all_years = sorted(np.unique(Y_train_full[:, 1]))\n",
        "    num_years = len(all_years)\n",
        "    num_test_years = 1\n",
        "    num_train_years = num_years - num_test_years * num_folds\n",
        "\n",
        "    custom_cv = []\n",
        "    custom_split_info = '\\nCustom sliding validation train, test splits'\n",
        "    custom_split_info += '\\n----------------------------------------------'\n",
        "\n",
        "    cv_test_years = []\n",
        "    for k in range(num_folds):\n",
        "      train_years_start = k * num_test_years\n",
        "      test_years_start = train_years_start + num_train_years\n",
        "      train_years = all_years[train_years_start:test_years_start]\n",
        "      test_years = all_years[test_years_start:test_years_start + num_test_years]\n",
        "      cv_test_years += test_years\n",
        "      test_indexes = np.ravel(np.nonzero(np.isin(Y_train_full[:, 1], test_years)))\n",
        "      train_indexes = np.ravel(np.nonzero(np.isin(Y_train_full[:, 1], train_years)))\n",
        "      custom_cv.append(tuple((train_indexes, test_indexes)))\n",
        "\n",
        "      train_years = [str(y) for y in train_years]\n",
        "      test_years = [str(y) for y in test_years]\n",
        "      custom_split_info += '\\nValidation set ' + str(k + 1) + ' training years: ' + ', '.join(train_years)\n",
        "      custom_split_info += '\\nValidation set ' + str(k + 1) + ' test years: ' + ', '.join(test_years)\n",
        "\n",
        "    custom_split_info += '\\n'\n",
        "    if (log_fh is not None):\n",
        "      log_fh.write(custom_split_info)\n",
        "\n",
        "    if (self.verbose > 1):\n",
        "      print(custom_split_info)\n",
        "\n",
        "    return custom_cv, cv_test_years"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URFR5pOxO3XZ"
      },
      "source": [
        "#### Run Training and Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TO9qzRVO3Xb"
      },
      "source": [
        "#%%writefile run_train_test_split.py\n",
        "def printTrainTestSplits(train_df, test_df, src, order_cols):\n",
        "  \"\"\"Print Training and Test Splits\"\"\"\n",
        "  print('\\n', src, 'training data')\n",
        "  train_df.orderBy(order_cols).show(5)\n",
        "  print('\\n', src, 'test data')\n",
        "  test_df.orderBy(order_cols).show(5)\n",
        "\n",
        "# Training, Test Split\n",
        "# --------------------\n",
        "def splitTrainingTest(df, year_col, test_years):\n",
        "  \"\"\"Splitting given df into training and test dataframes.\"\"\"\n",
        "  train_df = df.filter(~df[year_col].isin(test_years))\n",
        "  test_df = df.filter(df[year_col].isin(test_years))\n",
        "\n",
        "  return [train_df, test_df]\n",
        "\n",
        "def splitDataIntoTrainingTestSets(cyp_config, preprocessed_dfs, log_fh):\n",
        "  \"\"\"\n",
        "  Split preprocessed data into training and test sets based on\n",
        "  availability of yield data.\n",
        "  \"\"\"\n",
        "  nuts_level = cyp_config.getNUTSLevel()\n",
        "  use_centroids = cyp_config.useCentroids()\n",
        "  use_remote_sensing = cyp_config.useRemoteSensing()\n",
        "  use_gaes = cyp_config.useGAES()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "\n",
        "  yield_df = preprocessed_dfs['YIELD']\n",
        "  train_test_splitter = CYPTrainTestSplitter(cyp_config)\n",
        "  test_years = train_test_splitter.trainTestSplit(yield_df)\n",
        "  test_years_info = '\\nTest years: ' + ', '.join([str(y) for y in sorted(test_years)]) + '\\n'\n",
        "  log_fh.write(test_years_info + '\\n')\n",
        "  print(test_years_info)\n",
        "\n",
        "  # Times series data used for feature design.\n",
        "  ts_data_sources = {\n",
        "      'WOFOST' : preprocessed_dfs['WOFOST'],\n",
        "      'METEO' : preprocessed_dfs['METEO'],\n",
        "  }\n",
        "\n",
        "  if (use_remote_sensing):\n",
        "    ts_data_sources['REMOTE_SENSING'] = preprocessed_dfs['REMOTE_SENSING']\n",
        "\n",
        "  train_test_dfs = {}\n",
        "  for ts_src in ts_data_sources:\n",
        "    train_test_dfs[ts_src] = splitTrainingTest(ts_data_sources[ts_src], 'CAMPAIGN_YEAR', test_years)\n",
        "\n",
        "  # SOIL, GAES and CENTROIDS data are static.\n",
        "  train_test_dfs['SOIL'] = [preprocessed_dfs['SOIL'], preprocessed_dfs['SOIL']]\n",
        "  if (use_gaes):\n",
        "    train_test_dfs['GAES'] = [preprocessed_dfs['GAES'], preprocessed_dfs['GAES']]\n",
        "\n",
        "  if (use_centroids):\n",
        "    train_test_dfs['CENTROIDS'] = [preprocessed_dfs['CENTROIDS'],\n",
        "                                   preprocessed_dfs['CENTROIDS']]\n",
        "\n",
        "  # crop area\n",
        "  if (use_gaes):\n",
        "    crop_area_df = preprocessed_dfs['CROP_AREA']\n",
        "    train_test_dfs['CROP_AREA'] = splitTrainingTest(crop_area_df, 'FYEAR', test_years)\n",
        "\n",
        "  # yield data\n",
        "  train_test_dfs['YIELD'] = splitTrainingTest(yield_df, 'FYEAR', test_years)\n",
        "\n",
        "  if (debug_level > 2):\n",
        "    for src in train_test_dfs:\n",
        "      if (src in ts_data_sources):\n",
        "        order_cols = ['IDREGION', 'CAMPAIGN_YEAR', 'CAMPAIGN_DEKAD']\n",
        "      elif ((src == 'YIELD') or (src == 'CROP_AREA')):\n",
        "        order_cols = ['IDREGION', 'FYEAR']\n",
        "      else:\n",
        "        order_cols = ['IDREGION']\n",
        "\n",
        "      train_df = train_test_dfs[src][0]\n",
        "      test_df = train_test_dfs[src][1]\n",
        "      printTrainTestSplits(train_df, test_df, src, order_cols)\n",
        "\n",
        "  return train_test_dfs, test_years"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCe_4_v--Ukb"
      },
      "source": [
        "### Data Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaSHuv0du9qf"
      },
      "source": [
        "#### Data Summarizer Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD_QGQJs-Ukl"
      },
      "source": [
        "#%%writefile data_summary.py\n",
        "from pyspark.sql import Window\n",
        "import functools\n",
        "\n",
        "class CYPDataSummarizer:\n",
        "  def __init__(self, cyp_config):\n",
        "    self.verbose = cyp_config.getDebugLevel()\n",
        "\n",
        "  def wofostDVSSummary(self, wofost_df, early_season_end=None):\n",
        "    \"\"\"\n",
        "    Summary of crop calendar based on DVS.\n",
        "    Early season end is relative to end of the season, hence a negative number.\n",
        "    \"\"\"\n",
        "    join_cols = ['IDREGION', 'CAMPAIGN_YEAR']\n",
        "    dvs_summary = wofost_df.select(join_cols).distinct()\n",
        "\n",
        "    # We find the start and end dekads for DVS ranges\n",
        "    my_window = Window.partitionBy(join_cols).orderBy('CAMPAIGN_DEKAD')\n",
        "\n",
        "    wofost_df = wofost_df.withColumn('VALUE', wofost_df['DVS'])\n",
        "    wofost_df = wofost_df.withColumn('PREV', SparkF.lag(wofost_df['VALUE']).over(my_window))\n",
        "    wofost_df = wofost_df.withColumn('DIFF', SparkF.when(SparkF.isnull(wofost_df['PREV']), 0)\\\n",
        "                                     .otherwise(wofost_df['VALUE'] - wofost_df['PREV']))\n",
        "    wofost_df = wofost_df.withColumn('SEASON_ALIGN', wofost_df['CAMPAIGN_DEKAD'] - wofost_df['DEKAD'])\n",
        "\n",
        "    del_cols = ['VALUE', 'PREV', 'DIFF']\n",
        "    dvs_summary = dvs_summary.join(wofost_df.filter(wofost_df['VALUE'] > 0.0).groupBy(join_cols)\\\n",
        "                                   .agg(SparkF.min('CAMPAIGN_DEKAD').alias('START_DVS')), join_cols)\n",
        "    dvs_summary = dvs_summary.join(wofost_df.filter(wofost_df['DVS'] >= 100).groupBy(join_cols)\\\n",
        "                                   .agg(SparkF.min('CAMPAIGN_DEKAD').alias('START_DVS1')), join_cols)\n",
        "    dvs_summary = dvs_summary.join(wofost_df.filter(wofost_df['DVS'] >= 200).groupBy(join_cols)\\\n",
        "                                   .agg(SparkF.min('CAMPAIGN_DEKAD').alias('START_DVS2')), join_cols)\n",
        "    dvs_summary = dvs_summary.join(wofost_df.filter(wofost_df['DVS'] >= 200).groupBy(join_cols)\\\n",
        "                                   .agg(SparkF.min('DEKAD').alias('HARVEST')), join_cols)\n",
        "    dvs_summary = dvs_summary.join(wofost_df.filter(wofost_df['DVS'] >= 200).groupBy(join_cols)\\\n",
        "                                   .agg(SparkF.max('SEASON_ALIGN').alias('SEASON_ALIGN')), join_cols)\n",
        "\n",
        "    # Calendar year end season and early season dekads for comparing with MCYFS\n",
        "    # Campaign year early season dekad to filter data during feature design\n",
        "    dvs_summary = dvs_summary.withColumn('CALENDAR_END_SEASON', dvs_summary['HARVEST'] + 1)\n",
        "    dvs_summary = dvs_summary.withColumn('CAMPAIGN_EARLY_SEASON',\n",
        "                                         dvs_summary['CALENDAR_END_SEASON'] + dvs_summary['SEASON_ALIGN'])\n",
        "    if (early_season_end is not None):\n",
        "      dvs_summary = dvs_summary.withColumn('CALENDAR_EARLY_SEASON',\n",
        "                                         dvs_summary['CALENDAR_END_SEASON'] + early_season_end)\n",
        "      dvs_summary = dvs_summary.withColumn('CAMPAIGN_EARLY_SEASON',\n",
        "                                           dvs_summary['CAMPAIGN_EARLY_SEASON'] + early_season_end)\n",
        "\n",
        "    wofost_df = wofost_df.drop(*del_cols)\n",
        "    dvs_summary = dvs_summary.drop('HARVEST', 'SEASON_ALIGN')\n",
        "\n",
        "    return dvs_summary\n",
        "\n",
        "  def indicatorsSummary(self, df, min_cols, max_cols, avg_cols):\n",
        "    \"\"\"long term min, max and avg values of selected indicators by region\"\"\"\n",
        "    avgs = []\n",
        "    if (avg_cols[1:]):\n",
        "      avgs = [SparkF.bround(SparkF.avg(x), 2).alias('avg(' + x + ')') for x in avg_cols[1:]]\n",
        "    \n",
        "    if (min_cols[:1]):\n",
        "      summary = df.select(min_cols).groupBy('IDREGION').min()\n",
        "    else:\n",
        "      summary = df.select(min_cols).groupBy('IDREGION')\n",
        "\n",
        "    if (max_cols[1:]):\n",
        "      summary = summary.join(df.select(max_cols).groupBy('IDREGION').max(), 'IDREGION')\n",
        "\n",
        "    if (avgs):\n",
        "      summary = summary.join(df.select(avg_cols).groupBy('IDREGION').agg(*avgs), 'IDREGION')\n",
        "    return summary\n",
        "\n",
        "  def yieldSummary(self, yield_df):\n",
        "    \"\"\"long term min, max and avg values of yield by region\"\"\"\n",
        "    select_cols = ['IDREGION', 'YIELD']\n",
        "    yield_summary = yield_df.select(select_cols).groupBy('IDREGION').min('YIELD')\n",
        "    yield_summary = yield_summary.join(yield_df.select(select_cols).groupBy('IDREGION')\\\n",
        "                                       .agg(SparkF.max('YIELD')), 'IDREGION')\n",
        "    yield_summary = yield_summary.join(yield_df.select(select_cols).groupBy('IDREGION')\\\n",
        "                                       .agg(SparkF.bround(SparkF.avg('YIELD'), 2)\\\n",
        "                                            .alias('avg(YIELD)')), 'IDREGION')\n",
        "    return yield_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zym9pU3XsPrt"
      },
      "source": [
        "#### Run Data Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CNwR1WAsSLz"
      },
      "source": [
        "#%%writefile run_data_summary.py\n",
        "def printDataSummary(df, data_source):\n",
        "  \"\"\"Print summary information\"\"\"\n",
        "  if (data_source == 'WOFOST_DVS'):\n",
        "    print('Crop calender information based on WOFOST data')\n",
        "    max_year = df.select('CAMPAIGN_YEAR').agg(SparkF.max('CAMPAIGN_YEAR')).collect()[0][0]\n",
        "    df.filter(df.CAMPAIGN_YEAR == max_year).orderBy('IDREGION').show(10)\n",
        "  else:\n",
        "    print(data_source, 'indicators summary')\n",
        "    df.orderBy('IDREGION').show()\n",
        "\n",
        "def getWOFOSTSummaryCols():\n",
        "  \"\"\"WOFOST columns used for data summary\"\"\"\n",
        "  # only RSM has non-zero min values\n",
        "  min_cols = ['IDREGION', 'RSM']\n",
        "  max_cols = ['IDREGION'] + ['WLIM_YB', 'WLIM_YS', 'DVS',\n",
        "                             'WLAI', 'RSM', 'TWC', 'TWR']\n",
        "  # biomass and DVS values grow over time\n",
        "  avg_cols = ['IDREGION', 'WLAI', 'RSM', 'TWC', 'TWR']\n",
        "\n",
        "  return [min_cols, max_cols, avg_cols]\n",
        "\n",
        "def getMeteoSummaryCols():\n",
        "  \"\"\"Meteo columns used for data summary\"\"\"\n",
        "  col_names = ['TMAX', 'TMIN', 'TAVG', 'PREC', 'ET0', 'CWB', 'RAD']\n",
        "  min_cols = ['IDREGION'] + col_names\n",
        "  max_cols = ['IDREGION'] + col_names\n",
        "  avg_cols = ['IDREGION'] + col_names\n",
        "\n",
        "  return [min_cols, max_cols, avg_cols]\n",
        "\n",
        "def getRemoteSensingSummaryCols():\n",
        "  \"\"\"Remote Sensing columns used for data summary\"\"\"\n",
        "  col_names = ['FAPAR']\n",
        "  min_cols = ['IDREGION'] + col_names\n",
        "  max_cols = ['IDREGION'] + col_names\n",
        "  avg_cols = ['IDREGION'] + col_names\n",
        "\n",
        "  return [min_cols, max_cols, avg_cols]\n",
        "\n",
        "def summarizeData(cyp_config, cyp_summarizer, train_test_dfs):\n",
        "  \"\"\"\n",
        "  Summarize data. Create DVS summary to infer crop calendar.\n",
        "  Summarize selected indicators for each data source.\n",
        "  \"\"\"\n",
        "  wofost_train_df = train_test_dfs['WOFOST'][0]\n",
        "  wofost_test_df = train_test_dfs['WOFOST'][1]\n",
        "  meteo_train_df = train_test_dfs['METEO'][0]\n",
        "  yield_train_df = train_test_dfs['YIELD'][0]\n",
        "\n",
        "  use_remote_sensing = cyp_config.useRemoteSensing()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  early_season = cyp_config.earlySeasonPrediction()\n",
        "  early_season_end = None\n",
        "  if (early_season):\n",
        "    early_season_end = cyp_config.getEarlySeasonEndDekad()\n",
        "\n",
        "  # DVS summary (crop calendar)\n",
        "  # NOTE this summary of crops based on wofost data should be used with caution\n",
        "  # 1. The summary is per region per year.\n",
        "  # 2. The summary is based on wofost simulations not real sowing and harvest dates\n",
        "  dvs_summary_train = cyp_summarizer.wofostDVSSummary(wofost_train_df, early_season_end)\n",
        "  dvs_summary_train = dvs_summary_train.drop('CALENDAR_END_SEASON', 'CALENDAR_EARLY_SEASON')\n",
        "  dvs_summary_test = cyp_summarizer.wofostDVSSummary(wofost_test_df, early_season_end)\n",
        "  dvs_summary_test = dvs_summary_test.drop('CALENDAR_END_SEASON', 'CALENDAR_EARLY_SEASON')\n",
        "  if (debug_level > 1):\n",
        "    printDataSummary(dvs_summary_train, 'WOFOST_DVS')\n",
        "\n",
        "  summary_cols = {\n",
        "      'WOFOST' : getWOFOSTSummaryCols(),\n",
        "      'METEO' : getMeteoSummaryCols(),\n",
        "  }\n",
        "\n",
        "  summary_sources_dfs = {\n",
        "      'WOFOST' : wofost_train_df,\n",
        "      'METEO' : meteo_train_df,\n",
        "  }\n",
        "\n",
        "  if (use_remote_sensing):\n",
        "    rs_train_df = train_test_dfs['REMOTE_SENSING'][0]\n",
        "    summary_cols['REMOTE_SENSING'] = getRemoteSensingSummaryCols()\n",
        "    summary_sources_dfs['REMOTE_SENSING'] = rs_train_df\n",
        "\n",
        "  summary_dfs = {}\n",
        "  for sum_src in summary_sources_dfs:\n",
        "    summary_dfs[sum_src] = cyp_summarizer.indicatorsSummary(summary_sources_dfs[sum_src],\n",
        "                                                            summary_cols[sum_src][0],\n",
        "                                                            summary_cols[sum_src][1],\n",
        "                                                            summary_cols[sum_src][2])\n",
        "\n",
        "  for src in summary_dfs:\n",
        "    if (debug_level > 2):\n",
        "      printDataSummary(summary_dfs[src], src)\n",
        "\n",
        "  yield_summary = cyp_summarizer.yieldSummary(yield_train_df)\n",
        "  if (debug_level > 2):\n",
        "    printDataSummary(yield_summary, 'YIELD')\n",
        "\n",
        "  summary_dfs['WOFOST_DVS'] = [dvs_summary_train, dvs_summary_test]\n",
        "  summary_dfs['YIELD'] = yield_summary\n",
        "\n",
        "  return summary_dfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWwazE4MJI4v"
      },
      "source": [
        "### Crop Calendar\n",
        "\n",
        "We infer crop calendar using WOFOST DVS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBVBgwxSJEIn"
      },
      "source": [
        "#%%writefile crop_calendar.py\n",
        "import numpy as np\n",
        "\n",
        "def getCropCalendarPeriods(df):\n",
        "  \"\"\"Periods for per year crop calendar\"\"\"\n",
        "  # (maximum of 4 months = 12 dekads).\n",
        "  # Subtracting 11 because both ends of the period are included.\n",
        "  # p0 : if CAMPAIGN_EARLY_SEASON > df.START_DVS\n",
        "  #        START_DVS - 11 to START_DVS\n",
        "  #      else\n",
        "  #        START_DVS - 11 to CAMPAIGN_EARLY_SEASON\n",
        "  p0_filter = SparkF.when(df.CAMPAIGN_EARLY_SEASON > df.START_DVS,\n",
        "                          (df.CAMPAIGN_DEKAD >= (df.START_DVS - 11)) &\n",
        "                          (df.CAMPAIGN_DEKAD <= df.START_DVS))\\\n",
        "                          .otherwise((df.CAMPAIGN_DEKAD >= (df.START_DVS - 11)) &\n",
        "                                     (df.CAMPAIGN_DEKAD <= df.CAMPAIGN_EARLY_SEASON))\n",
        "  # p1 : if CAMPAIGN_EARLY_SEASON > (df.START_DVS + 1)\n",
        "  #        (START_DVS - 1) to (START_DVS + 1)\n",
        "  #      else\n",
        "  #        (START_DVS - 1) to CAMPAIGN_EARLY_SEASON\n",
        "  p1_filter = SparkF.when(df.CAMPAIGN_EARLY_SEASON > (df.START_DVS + 1),\n",
        "                          (df.CAMPAIGN_DEKAD >= (df.START_DVS - 1)) &\n",
        "                          (df.CAMPAIGN_DEKAD <= (df.START_DVS + 1)))\\\n",
        "                          .otherwise((df.CAMPAIGN_DEKAD >= (df.START_DVS - 1)) &\n",
        "                                     (df.CAMPAIGN_DEKAD <= df.CAMPAIGN_EARLY_SEASON))\n",
        "  # p2 : if CAMPAIGN_EARLY_SEASON > df.START_DVS1\n",
        "  #        START_DVS to START_DVS1\n",
        "  #      else\n",
        "  #        START_DVS to CAMPAIGN_EARLY_SEASON\n",
        "  p2_filter = SparkF.when(df.CAMPAIGN_EARLY_SEASON > df.START_DVS1,\n",
        "                          (df.CAMPAIGN_DEKAD >= df.START_DVS) &\n",
        "                          (df.CAMPAIGN_DEKAD <= df.START_DVS1))\\\n",
        "                          .otherwise((df.CAMPAIGN_DEKAD >= df.START_DVS) &\n",
        "                                     (df.CAMPAIGN_DEKAD <= df.CAMPAIGN_EARLY_SEASON))\n",
        "  # p3 : if CAMPAIGN_EARLY_SEASON > (df.START_DVS1 + 1)\n",
        "  #        (START_DVS1 - 1) to (START_DVS1 + 1)\n",
        "  #      else\n",
        "  #        (START_DVS1 - 1) to CAMPAIGN_EARLY_SEASON\n",
        "  p3_filter = SparkF.when(df.CAMPAIGN_EARLY_SEASON > (df.START_DVS1 + 1),\n",
        "                          (df.CAMPAIGN_DEKAD >= (df.START_DVS1 - 1)) &\n",
        "                          (df.CAMPAIGN_DEKAD <= (df.START_DVS1 + 1)))\\\n",
        "                          .otherwise((df.CAMPAIGN_DEKAD >= (df.START_DVS1 - 1)) &\n",
        "                                     (df.CAMPAIGN_DEKAD <= df.CAMPAIGN_EARLY_SEASON))\n",
        "  # p4 : if CAMPAIGN_EARLY_SEASON > df.START_DVS2\n",
        "  #        START_DVS1 to START_DVS2\n",
        "  #      else\n",
        "  #        START_DVS1 to CAMPAIGN_EARLY_SEASON\n",
        "  p4_filter = SparkF.when(df.CAMPAIGN_EARLY_SEASON > df.START_DVS2,\n",
        "                          (df.CAMPAIGN_DEKAD >= df.START_DVS1) &\n",
        "                          (df.CAMPAIGN_DEKAD <= df.START_DVS2))\\\n",
        "                          .otherwise((df.CAMPAIGN_DEKAD >= df.START_DVS1) &\n",
        "                                     (df.CAMPAIGN_DEKAD <= df.CAMPAIGN_EARLY_SEASON))\n",
        "  # p5 : if CAMPAIGN_EARLY_SEASON > (df.START_DVS2 + 1)\n",
        "  #        (START_DVS2 - 1) to (START_DVS2 + 1)\n",
        "  #      else\n",
        "  #        (START_DVS2 - 1) to CAMPAIGN_EARLY_SEASON\n",
        "  p5_filter = SparkF.when(df.CAMPAIGN_EARLY_SEASON > (df.START_DVS2 + 1),\n",
        "                          (df.CAMPAIGN_DEKAD >= (df.START_DVS2 - 1)) &\n",
        "                          (df.CAMPAIGN_DEKAD <= (df.START_DVS2 + 1)))\\\n",
        "                          .otherwise((df.CAMPAIGN_DEKAD >= (df.START_DVS2 - 1)) &\n",
        "                                     (df.CAMPAIGN_DEKAD <= df.CAMPAIGN_EARLY_SEASON))\n",
        "\n",
        "  cc_periods = {\n",
        "      'p0' : p0_filter,\n",
        "      'p1' : p1_filter,\n",
        "      'p2' : p2_filter,\n",
        "      'p3' : p3_filter,\n",
        "      'p4' : p4_filter,\n",
        "      'p5' : p5_filter,\n",
        "  }\n",
        "\n",
        "  return cc_periods\n",
        "\n",
        "def getSeasonStartFilter(df):\n",
        "  \"\"\"Filter for dekads after season start\"\"\"\n",
        "  return df.CAMPAIGN_DEKAD >= (df.START_DVS - 11)\n",
        "\n",
        "def getCountryCropCalendar(crop_cal):\n",
        "  \"\"\"Take averages to make the crop calendar per country\"\"\"\n",
        "  crop_cal = crop_cal.withColumn('COUNTRY', SparkF.substring('IDREGION', 1, 2))\n",
        "  aggrs = [ SparkF.bround(SparkF.avg(crop_cal['START_DVS'])).alias('START_DVS'),\n",
        "            SparkF.bround(SparkF.avg(crop_cal['START_DVS1'])).alias('START_DVS1'),\n",
        "            SparkF.bround(SparkF.avg(crop_cal['START_DVS2'])).alias('START_DVS2'),\n",
        "            SparkF.bround(SparkF.avg(crop_cal['CAMPAIGN_EARLY_SEASON'])).alias('CAMPAIGN_EARLY_SEASON') ]\n",
        "\n",
        "  crop_cal = crop_cal.groupBy('COUNTRY').agg(*aggrs)\n",
        "  return crop_cal\n",
        "\n",
        "def getCropCalendar(cyp_config, dvs_summary, log_fh):\n",
        "  \"\"\"Use DVS summary to infer the crop calendar\"\"\"\n",
        "  pd_dvs_summary = dvs_summary.toPandas()\n",
        "  early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "\n",
        "  avg_dvs_start = np.round(pd_dvs_summary['START_DVS'].mean(), 0)\n",
        "  avg_dvs1_start = np.round(pd_dvs_summary['START_DVS1'].mean(), 0)\n",
        "  avg_dvs2_start = np.round(pd_dvs_summary['START_DVS2'].mean(), 0)\n",
        "\n",
        "  # We look at 6 windows\n",
        "  # 0. Preplanting window (maximum of 4 months = 12 dekads).\n",
        "  # Subtracting 11 because both ends of the period are included.\n",
        "  p0_start = 1 if (avg_dvs_start - 11) < 1 else (avg_dvs_start - 11)\n",
        "  p0_end = avg_dvs_start\n",
        "\n",
        "  # 1. Planting window\n",
        "  p1_start = avg_dvs_start - 1\n",
        "  p1_end = avg_dvs_start + 1\n",
        "\n",
        "  # 2. Vegetative phase\n",
        "  p2_start = avg_dvs_start\n",
        "  p2_end = avg_dvs1_start\n",
        "\n",
        "  # 3. Flowering phase\n",
        "  p3_start = avg_dvs1_start - 1\n",
        "  p3_end = avg_dvs1_start + 1\n",
        "\n",
        "  # 4. Yield formation phase\n",
        "  p4_start = avg_dvs1_start\n",
        "  p4_end = avg_dvs2_start\n",
        "\n",
        "  # 5. Harvest window\n",
        "  p5_start = avg_dvs2_start - 1\n",
        "  p5_end = avg_dvs2_start + 1\n",
        "\n",
        "  early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "  early_season_end = 36\n",
        "  if (early_season_prediction):\n",
        "    early_season_end = np.round(pd_dvs_summary['CAMPAIGN_EARLY_SEASON'].mean(), 0)\n",
        "    p0_end = early_season_end if (p0_end > early_season_end) else p0_end\n",
        "    p1_end = early_season_end if (p1_end > early_season_end) else p1_end\n",
        "    p2_end = early_season_end if (p2_end > early_season_end) else p2_end\n",
        "    p3_end = early_season_end if (p3_end > early_season_end) else p3_end\n",
        "    p4_end = early_season_end if (p4_end > early_season_end) else p4_end\n",
        "    p5_end = early_season_end if (p5_end > early_season_end) else p5_end\n",
        "\n",
        "  crop_cal = {}\n",
        "  if (p0_end > p0_start):\n",
        "    crop_cal['p0'] = { 'desc' : 'pre-planting window', 'start' : p0_start, 'end' : p0_end }\n",
        "  if (p1_end > p1_start):\n",
        "    crop_cal['p1'] = { 'desc' : 'planting window', 'start' : p1_start, 'end' : p1_end }\n",
        "  if (p2_end > p2_start):\n",
        "    crop_cal['p2'] = { 'desc' : 'vegetative phase', 'start' : p2_start, 'end' : p2_end }\n",
        "  if (p3_end > p3_start):\n",
        "    crop_cal['p3'] = { 'desc' : 'flowering phase', 'start' : p3_start, 'end' : p3_end }\n",
        "  if (p4_end > p4_start):\n",
        "    crop_cal['p4'] = { 'desc' : 'yield formation phase', 'start' : p4_start, 'end' : p4_end }\n",
        "  if (p5_end > p5_start):\n",
        "    crop_cal['p5'] = { 'desc' : 'harvest window', 'start' : p5_start, 'end' : p5_end }\n",
        "\n",
        "  if (early_season_prediction):\n",
        "    early_season_rel_harvest = cyp_config.getEarlySeasonEndDekad()\n",
        "    early_season_info = '\\nEarly Season Prediction Dekad: ' + str(early_season_rel_harvest)\n",
        "    early_season_info += ', Campaign Dekad: ' + str(early_season_end)\n",
        "    log_fh.write(early_season_info + '\\n')\n",
        "    if (debug_level > 1):\n",
        "      print(early_season_info)\n",
        "\n",
        "  crop_cal_info = '\\nCrop Calendar'\n",
        "  crop_cal_info += '\\n-------------'\n",
        "  for p in crop_cal:\n",
        "    crop_cal_info += '\\nPeriod ' + p + ' (' + crop_cal[p]['desc'] + '): '\n",
        "    crop_cal_info += 'Campaign Dekads ' + str(crop_cal[p]['start']) + '-' + str(crop_cal[p]['end'])\n",
        "\n",
        "  log_fh.write(crop_cal_info + '\\n')\n",
        "  if (debug_level > 1):\n",
        "    print(crop_cal_info)\n",
        "\n",
        "  return crop_cal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QokglosfnQax"
      },
      "source": [
        "### Feature Design\n",
        "\n",
        "For WOFOST, Meteo and Remote Sensing features, we aggregate indicators or count days/dekads with indicators above or below some thresholds. We use 4 thresholds: +/- 1 STD and +/- 2STD above or below the average.\n",
        "\n",
        "We determine the start and end dekads using crop calendar inferred from WOFOST DVS summary. In the case of early season prediction, end dekad is set to the prediction dekad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHVsi9dzv4Pz"
      },
      "source": [
        "#### Featurizer Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e94zGLjVnQa0"
      },
      "source": [
        "#%%writefile feature_design.py\n",
        "from pyspark.sql import Window\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import functools\n",
        "\n",
        "class CYPFeaturizer:\n",
        "  def __init__(self, cyp_config):\n",
        "    self.use_per_year_cc = cyp_config.usePerYearCropCalendar()\n",
        "    self.use_features_v2 = cyp_config.useFeaturesV2()\n",
        "    self.use_gaes = cyp_config.useGAES()\n",
        "    self.verbose = cyp_config.getDebugLevel()\n",
        "    self.lt_stats = {}\n",
        "\n",
        "  def extractFeatures(self, df, data_source, crop_cal,\n",
        "                      max_cols, avg_cols, cum_avg_cols, extreme_cols,\n",
        "                      join_cols, fit=False):\n",
        "    \"\"\"\n",
        "    Extract aggregate and extreme features.\n",
        "    If fit=True, compute and save long-term stats.\n",
        "    \"\"\"\n",
        "    df = df.withColumn('COUNTRY', SparkF.substring('IDREGION', 1, 2))\n",
        "    if (not self.use_per_year_cc):\n",
        "      if (self.use_gaes):\n",
        "        df = df.join(crop_cal.select(['IDREGION', 'AEZ_ID']), 'IDREGION')\n",
        "\n",
        "      crop_cal = getCountryCropCalendar(crop_cal)\n",
        "      df = df.join(SparkF.broadcast(crop_cal), 'COUNTRY')\n",
        "    else:\n",
        "      df = df.join(crop_cal, join_cols)\n",
        "\n",
        "    # Calculate cumulative sums\n",
        "    if (self.use_features_v2 and cum_avg_cols):\n",
        "      w = Window.partitionBy(join_cols).orderBy('CAMPAIGN_DEKAD')\\\n",
        "                .rangeBetween(Window.unboundedPreceding, 0)\n",
        "      after_season_start = getSeasonStartFilter(df)\n",
        "      for c in cum_avg_cols:\n",
        "        df = df.withColumn(c, SparkF.sum(SparkF.when(after_season_start, df[c])\\\n",
        "                                         .otherwise(0.0)).over(w))\n",
        "\n",
        "    cc_periods = getCropCalendarPeriods(df)\n",
        "    aggrs = []\n",
        "    # max aggregation\n",
        "    for p in max_cols:\n",
        "      if (max_cols[p]):\n",
        "        aggrs += [SparkF.bround(SparkF.max(SparkF.when(cc_periods[p], df[x])), 2)\\\n",
        "                  .alias('max' + x + p) for x in max_cols[p] ]\n",
        "\n",
        "    # avg aggregation\n",
        "    for p in avg_cols:\n",
        "      if (avg_cols[p]):\n",
        "        aggrs += [SparkF.bround(SparkF.avg(SparkF.when(cc_periods[p], df[x])), 2)\\\n",
        "                  .alias('avg' + x + p) for x in avg_cols[p] ]\n",
        "\n",
        "    # if not computing extreme features, we can return\n",
        "    if (not extreme_cols):\n",
        "      ft_df = df.groupBy(join_cols).agg(*aggrs)\n",
        "      return ft_df\n",
        "\n",
        "    # compute long-term stats and save them\n",
        "    if (fit):\n",
        "      stat_aggrs = []\n",
        "      for p in extreme_cols:\n",
        "        if (extreme_cols[p]):\n",
        "          stat_aggrs += [ SparkF.bround(SparkF.avg(SparkF.when(cc_periods[p], df[x])), 2)\\\n",
        "                         .alias('avg' + x + p) for x in extreme_cols[p] ]\n",
        "          stat_aggrs += [ SparkF.bround(SparkF.stddev(SparkF.when(cc_periods[p], df[x])), 2)\\\n",
        "                         .alias('std' + x + p) for x in extreme_cols[p] ]\n",
        "\n",
        "      if (stat_aggrs):\n",
        "        if (self.use_gaes):\n",
        "          lt_stats = df.groupBy('AEZ_ID').agg(*stat_aggrs)\n",
        "        else:\n",
        "          lt_stats = df.groupBy('COUNTRY').agg(*stat_aggrs)\n",
        "\n",
        "        self.lt_stats[data_source] = lt_stats\n",
        "\n",
        "    if (self.use_gaes):\n",
        "      df = df.join(SparkF.broadcast(self.lt_stats[data_source]), 'AEZ_ID')\n",
        "    else:\n",
        "      df = df.join(SparkF.broadcast(self.lt_stats[data_source]), 'COUNTRY')\n",
        "\n",
        "    # features for extreme conditions\n",
        "    for p in extreme_cols:\n",
        "      if (extreme_cols[p]):\n",
        "        if (self.use_features_v2):\n",
        "          # sum zscore for values < long-term average\n",
        "          aggrs += [ SparkF.bround(SparkF.sum(SparkF.when(((df[x] - df['avg' + x + p]) < 0) & cc_periods[p],\n",
        "                                                          (df['avg' + x + p] - df[x]) / df['std' + x + p])), 2)\\\n",
        "                     .alias('Z-' + x + p) for x in extreme_cols[p] ]\n",
        "          # sum zscore for values > long-term average\n",
        "          aggrs += [ SparkF.bround(SparkF.sum(SparkF.when(((df[x] - df['avg' + x + p]) > 0) & cc_periods[p],\n",
        "                                                          (df[x] - df['avg' + x + p]) / df['std' + x + p])), 2)\\\n",
        "                     .alias('Z+' + x + p) for x in extreme_cols[p] ]\n",
        "\n",
        "        else:\n",
        "          # Count of days or dekads with values crossing threshold\n",
        "          for i in range(1, 3):\n",
        "            aggrs += [ SparkF.sum(SparkF.when((df[x] > (df['avg' + x + p] + i * df['std' + x + p])) &\n",
        "                                              cc_periods[p], 1))\\\n",
        "                      .alias(x + p + 'gt' + str(i) + 'STD') for x in extreme_cols[p] ]\n",
        "            aggrs += [ SparkF.sum(SparkF.when((df[x] < (df['avg' + x + p] - i * df['std' + x + p])) &\n",
        "                                              cc_periods[p], 1))\\\n",
        "                      .alias(x + p + 'lt' + str(i) + 'STD') for x in extreme_cols[p] ]\n",
        "\n",
        "    ft_df = df.groupBy(join_cols).agg(*aggrs)\n",
        "    ft_df = ft_df.na.fill(0.0)\n",
        "    return ft_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CZS6KDg8raw"
      },
      "source": [
        "#### Yield Trend Estimator Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2O70-hQ8rax"
      },
      "source": [
        "#%%writefile yield_trend.py\n",
        "import numpy as np\n",
        "import functools\n",
        "from pyspark.sql import Window\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "class CYPYieldTrendEstimator:\n",
        "  def __init__(self, cyp_config):\n",
        "    self.verbose = cyp_config.getDebugLevel()\n",
        "    self.trend_windows = cyp_config.getTrendWindows()\n",
        "\n",
        "  def setTrendWindows(self, trend_windows):\n",
        "    \"\"\"Set trend window lengths\"\"\"\n",
        "    assert isinstance(trend_windows, list)\n",
        "    assert len(trend_windows) > 0\n",
        "    assert isinstance(trend_windows[0], int)\n",
        "    self.trend_windows = trend_windows\n",
        "\n",
        "  def getTrendWindowYields(self, df, trend_window, reg_id=None):\n",
        "    \"\"\"Extract previous years' yield values to separate columns\"\"\"\n",
        "    sel_cols = ['IDREGION', 'FYEAR', 'YIELD']\n",
        "    my_window = Window.partitionBy('IDREGION').orderBy('FYEAR')\n",
        "\n",
        "    yield_fts = df.select(sel_cols)\n",
        "    if (reg_id is not None):\n",
        "      yield_fts = yield_fts.filter(yield_fts.IDREGION == reg_id)\n",
        "\n",
        "    for i in range(trend_window):\n",
        "      yield_fts = yield_fts.withColumn('YIELD-' + str(i+1),\n",
        "                                       SparkF.lag(yield_fts.YIELD, i+1).over(my_window))\n",
        "      yield_fts = yield_fts.withColumn('YEAR-' + str(i+1),\n",
        "                                       SparkF.lag(yield_fts.FYEAR, i+1).over(my_window))\n",
        "\n",
        "    # drop columns withs null values\n",
        "    for i in range(trend_window):\n",
        "      yield_fts = yield_fts.filter(SparkF.col('YIELD-' + str(i+1)).isNotNull())\n",
        "\n",
        "    prev_yields = [ 'YIELD-' + str(i) for i in range(trend_window, 0, -1)]\n",
        "    prev_years = [ 'YEAR-' + str(i) for i in range(trend_window, 0, -1)]\n",
        "    sel_cols = ['IDREGION', 'FYEAR'] + prev_years + prev_yields\n",
        "    yield_fts = yield_fts.select(sel_cols)\n",
        "\n",
        "    return yield_fts\n",
        "\n",
        "  # Christos, Ante's suggestion\n",
        "  # - To avoid overfitting, trend estimation could use a window which skips a year in between\n",
        "  # So a window of 3 will use 6 years of data\n",
        "  def printYieldTrendRounds(self, df, reg_id, trend_windows=None):\n",
        "    \"\"\"Print the sequence of years used for yield trend estimation\"\"\"\n",
        "    reg_years = sorted([yr[0] for yr in df.filter(df['IDREGION'] == reg_id).select('FYEAR').distinct().collect()])\n",
        "    num_years = len(reg_years)\n",
        "    if (trend_windows is None):\n",
        "      trend_windows = self.trend_windows\n",
        "\n",
        "    for trend_window in trend_windows:\n",
        "      rounds = (num_years - trend_window)\n",
        "      if ((self.verbose > 2) and (trend_window == trend_windows[0])):\n",
        "        print('Trend window', trend_window)\n",
        "    \n",
        "      for rd in range(rounds):\n",
        "        test_year = reg_years[-(rd + 1)]\n",
        "        start_year = reg_years[-(rd + trend_window + 1)]\n",
        "        end_year = reg_years[-(rd + 2)]\n",
        "\n",
        "        if ((self.verbose > 2) and (trend_window == trend_windows[0])):\n",
        "          print('Round:', rd, 'Test year:', test_year,\n",
        "                'Trend Window:', start_year, '-', end_year)\n",
        "\n",
        "  def getLinearYieldTrend(self, window_years, window_yields, pred_year):\n",
        "    \"\"\"Linear yield trend prediction\"\"\"\n",
        "    coefs = np.polyfit(window_years, window_yields, 1)\n",
        "    return float(np.round(coefs[0] * pred_year + coefs[1], 2))\n",
        "\n",
        "  def getFixedWindowTrendFeatures(self, df, trend_window=None, pred_year=None):\n",
        "    \"\"\"Predict the yield trend for each IDREGION and FYEAR using a fixed window\"\"\"\n",
        "    join_cols = ['IDREGION', 'FYEAR']\n",
        "    if (trend_window is None):\n",
        "      trend_window = self.trend_windows[0]\n",
        "\n",
        "    yield_ft_df = self.getTrendWindowYields(df, trend_window)\n",
        "    if (pred_year is not None):\n",
        "      yield_ft_df = yield_ft_df.filter(yield_ft_df['FYEAR'] == pred_year)\n",
        "\n",
        "    pd_yield_ft_df = yield_ft_df.toPandas()\n",
        "    region_years = pd_yield_ft_df[join_cols].values\n",
        "    prev_year_cols = ['YEAR-' + str(i) for i in range(1, trend_window + 1)]\n",
        "    prev_yield_cols = ['YIELD-' + str(i) for i in range(1, trend_window + 1)]\n",
        "    window_years = pd_yield_ft_df[prev_year_cols].values\n",
        "    window_yields = pd_yield_ft_df[prev_yield_cols].values\n",
        "\n",
        "    yield_trend = []\n",
        "    for i in range(region_years.shape[0]):\n",
        "      yield_trend.append(self.getLinearYieldTrend(window_years[i, :],\n",
        "                                                  window_yields[i, :],\n",
        "                                                  region_years[i, 1]))\n",
        "\n",
        "    pd_yield_ft_df['YIELD_TREND'] = yield_trend\n",
        "    return pd_yield_ft_df\n",
        "\n",
        "  def getFixedWindowTrend(self, df, reg_id, pred_year, trend_window=None):\n",
        "    \"\"\"\n",
        "    Return linear trend prediction for given region and year\n",
        "    using fixed trend window.\n",
        "    \"\"\"\n",
        "    if (trend_window is None):\n",
        "      trend_window = self.trend_windows[0]\n",
        "\n",
        "    reg_df = df.filter((df['IDREGION'] == reg_id) & (df['FYEAR'] <= pred_year))\n",
        "    pd_yield_ft_df = self.getFixedWindowTrendFeatures(reg_df, trend_window, pred_year)\n",
        "    if (len(pd_yield_ft_df.index) == 0):\n",
        "      print('No data to estimate yield trend')\n",
        "      return None\n",
        "\n",
        "    if (len(pd_yield_ft_df.index) == 0):\n",
        "      return None\n",
        "\n",
        "    reg_year_filter = (df['IDREGION'] == reg_id) & (df['FYEAR'] == pred_year)\n",
        "    pd_yield_ft_df['ACTUAL'] = df.filter(reg_year_filter).select('YIELD').collect()[0][0]\n",
        "    pd_yield_ft_df = pd_yield_ft_df.rename(columns={'YIELD_TREND' : 'PREDICTED'})\n",
        "\n",
        "    return pd_yield_ft_df\n",
        "\n",
        "  def getL1OutCVPredictions(self, pd_yield_ft_df, trend_window, join_cols, iter):\n",
        "    \"\"\"1 iteration of leave-one-out cross-validation\"\"\"\n",
        "    iter_year_cols = ['YEAR-' + str(i) for i in range(1, trend_window + 1) if i != iter]\n",
        "    iter_yield_cols = ['YIELD-' + str(i) for i in range(1, trend_window + 1) if i != iter]\n",
        "    window_years = pd_yield_ft_df[iter_year_cols].values\n",
        "    window_yields = pd_yield_ft_df[iter_yield_cols].values\n",
        "\n",
        "    # We are going to predict yield value for YEAR-<iter>.\n",
        "    pred_years = pd_yield_ft_df['YEAR-' + str(iter)].values\n",
        "    predicted_trend = []\n",
        "    for i in range(pred_years.shape[0]):\n",
        "      predicted_trend.append(self.getLinearYieldTrend(window_years[i, :],\n",
        "                                                      window_yields[i, :],\n",
        "                                                      pred_years[i]))\n",
        "\n",
        "    pd_iter_preds = pd_yield_ft_df[join_cols]\n",
        "    pd_iter_preds['YTRUE' + str(iter)] = pd_yield_ft_df['YIELD-' + str(iter)]\n",
        "    pd_iter_preds['YPRED' + str(iter)] = predicted_trend\n",
        "\n",
        "    if (self.verbose > 2):\n",
        "      print('Leave-one-out cross-validation: iteration', iter)\n",
        "      print(pd_iter_preds.head(5))\n",
        "\n",
        "    return pd_iter_preds\n",
        "\n",
        "  def getL1outRMSE(self, cv_actual, cv_predicted):\n",
        "    \"\"\"Compute RMSE for leave-one-out predictions\"\"\"\n",
        "    return float(np.round(np.sqrt(mean_squared_error(cv_actual, cv_predicted)), 2))\n",
        "\n",
        "  def getMinRMSEIndex(self, cv_rmses):\n",
        "    \"\"\"Index of min rmse values\"\"\"\n",
        "    return np.nanargmin(cv_rmses)\n",
        "\n",
        "  def getL1OutCVRMSE(self, df, trend_window, join_cols, pred_year=None):\n",
        "    \"\"\"Run leave-one-out cross-validation and compute RMSE\"\"\"\n",
        "    join_cols = ['IDREGION', 'FYEAR']\n",
        "    pd_yield_ft_df = self.getFixedWindowTrendFeatures(df, trend_window, pred_year)\n",
        "    pd_l1out_preds = None\n",
        "    for i in range(1, trend_window + 1):\n",
        "      pd_iter_preds = self.getL1OutCVPredictions(pd_yield_ft_df, trend_window,\n",
        "                                                 join_cols, i)\n",
        "      if (pd_l1out_preds is None):\n",
        "        pd_l1out_preds = pd_iter_preds\n",
        "      else:\n",
        "        pd_l1out_preds = pd_l1out_preds.merge(pd_iter_preds, on=join_cols)\n",
        "\n",
        "    region_years = pd_l1out_preds[join_cols].values\n",
        "    ytrue_cols = ['YTRUE' + str(i) for i in range(1, trend_window + 1)]\n",
        "    ypred_cols = ['YPRED' + str(i) for i in range(1, trend_window + 1)]\n",
        "    l1out_ytrue = pd_l1out_preds[ytrue_cols].values\n",
        "    l1out_ypred = pd_l1out_preds[ypred_cols].values\n",
        "    cv_rmse = []\n",
        "    for i in range(region_years.shape[0]):\n",
        "      cv_rmse.append(self.getL1outRMSE(l1out_ytrue[i, :],\n",
        "                                       l1out_ypred[i, :]))\n",
        "\n",
        "    pd_l1out_rmse = pd_yield_ft_df[join_cols]\n",
        "    pd_l1out_rmse['YIELD_TREND' + str(trend_window)] = pd_yield_ft_df['YIELD_TREND']\n",
        "    pd_l1out_rmse['CV_RMSE' + str(trend_window)] = cv_rmse\n",
        "\n",
        "    return pd_l1out_rmse\n",
        "\n",
        "  def getOptimalTrendWindows(self, df, pred_year=None, trend_windows=None):\n",
        "    \"\"\"\n",
        "    Compute optimal yield trend values based on leave-one-out\n",
        "    cross validation errors for different trend windows.\n",
        "    \"\"\"\n",
        "    join_cols = ['IDREGION', 'FYEAR']\n",
        "    if (trend_windows is None):\n",
        "      trend_windows = self.trend_windows\n",
        "\n",
        "    pd_tw_rmses = None\n",
        "    for tw in trend_windows:\n",
        "      pd_l1out_rmse = self.getL1OutCVRMSE(df, tw, join_cols, pred_year)\n",
        "      if (pd_tw_rmses is None):\n",
        "        pd_tw_rmses = pd_l1out_rmse\n",
        "      else:\n",
        "        pd_tw_rmses = pd_tw_rmses.merge(pd_l1out_rmse, on=join_cols, how='left')\n",
        "\n",
        "    if (self.verbose > 2):\n",
        "      print('Leave-one-out cross-validation: RMSE')\n",
        "      print(pd_tw_rmses.sort_values(by=join_cols).head(5))\n",
        "\n",
        "    region_years = pd_tw_rmses[join_cols].values\n",
        "    tw_rmse_cols = ['CV_RMSE' + str(tw) for tw in trend_windows]\n",
        "    tw_trend_cols = ['YIELD_TREND' + str(tw) for tw in trend_windows]\n",
        "    tw_cv_rmses = pd_tw_rmses[tw_rmse_cols].values\n",
        "    tw_yield_trend = pd_tw_rmses[tw_trend_cols].values\n",
        "\n",
        "    opt_windows = []\n",
        "    yield_trend_preds = []\n",
        "    for i in range(region_years.shape[0]):\n",
        "      min_rmse_index = self.getMinRMSEIndex(tw_cv_rmses[i, :])\n",
        "      opt_windows.append(trend_windows[min_rmse_index])\n",
        "      yield_trend_preds.append(tw_yield_trend[i, min_rmse_index])\n",
        "\n",
        "    pd_opt_win_df = pd_tw_rmses[join_cols]\n",
        "    pd_opt_win_df['OPT_TW'] = opt_windows\n",
        "    pd_opt_win_df['YIELD_TREND'] = yield_trend_preds\n",
        "    if (self.verbose > 2):\n",
        "      print('Optimal trend windows')\n",
        "      print(pd_opt_win_df.sort_values(by=join_cols).head(5))\n",
        "\n",
        "    return pd_opt_win_df\n",
        "\n",
        "  def getOptimalWindowTrendFeatures(self, df, trend_windows=None):\n",
        "    \"\"\"\n",
        "    Get previous year yield values and predicted yield trend\n",
        "    by determining optimal trend window for each region and year.\n",
        "    NOTE: We have to select the same number of features, so we\n",
        "    select previous trend_windows[0] yield values.\n",
        "    \"\"\"\n",
        "    join_cols = ['IDREGION', 'FYEAR']\n",
        "    if (trend_windows is None):\n",
        "      trend_windows = self.trend_windows\n",
        "\n",
        "    pd_yield_ft_df = self.getTrendWindowYields(df, trend_windows[0]).toPandas()\n",
        "    pd_opt_win_df = self.getOptimalTrendWindows(df, trend_windows=trend_windows)\n",
        "    pd_opt_win_df = pd_opt_win_df.drop(columns=['OPT_TW'])\n",
        "    pd_yield_ft_df = pd_yield_ft_df.merge(pd_opt_win_df, on=join_cols)\n",
        "\n",
        "    return pd_yield_ft_df\n",
        "\n",
        "  def getOptimalWindowTrend(self, df, reg_id, pred_year, trend_windows=None):\n",
        "    \"\"\"\n",
        "    Compute the optimal trend window for given region and year based on\n",
        "    leave-one-out cross validation errors for different trend windows.\n",
        "    \"\"\"\n",
        "    df = df.filter(df['IDREGION'] == reg_id)\n",
        "    pd_opt_win_df = self.getOptimalTrendWindows(df, pred_year, trend_windows)\n",
        "    if (len(pd_opt_win_df.index) == 0):\n",
        "      return None\n",
        "\n",
        "    reg_year_filter = (df['IDREGION'] == reg_id) & (df['FYEAR'] == pred_year)\n",
        "    pd_opt_win_df['ACTUAL'] = df.filter(reg_year_filter).select('YIELD').collect()[0][0]\n",
        "    pd_opt_win_df = pd_opt_win_df.rename(columns={'YIELD_TREND' : 'PREDICTED'})\n",
        "\n",
        "    return pd_opt_win_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGNO_QZCsYf2"
      },
      "source": [
        "#### Create WOFOST, Meteo and Remote Sensing Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NpzcktZse_8"
      },
      "source": [
        "#%%writefile run_feature_design.py\n",
        "def getTrendFeatureCols(ft_src):\n",
        "  \"\"\"Features from previous years to capture feature trend\"\"\"\n",
        "  if (ft_src == 'WOFOST'):\n",
        "    return []\n",
        "  elif (ft_src == 'METEO'):\n",
        "    return ['avgPRECp5', 'Z-PRECp5', 'Z+PRECp5']\n",
        "  elif (ft_src == 'REMOTE_SENSING'):\n",
        "    return []\n",
        "\n",
        "def getCumulativeAvgCols(ft_src):\n",
        "  \"\"\"columns or indicators using avg of cumulative values\"\"\"\n",
        "  cum_cols = []\n",
        "  if (ft_src == 'METEO'):\n",
        "    cum_cols = ['CWB']\n",
        "\n",
        "  return cum_cols\n",
        "\n",
        "def wofostMaxFeatureCols():\n",
        "  \"\"\"columns or indicators using max aggregation\"\"\"\n",
        "  # must be in sync with crop calendar periods\n",
        "  max_cols = {\n",
        "      'p0' : [],\n",
        "      'p1' : [],\n",
        "      'p2' : ['WLIM_YB', 'TWC', 'WLAI'],\n",
        "      'p3' : [],\n",
        "      'p4' : ['WLIM_YB', 'WLIM_YS', 'TWC', 'WLAI'],\n",
        "      'p5' : [],\n",
        "  }\n",
        "\n",
        "  return max_cols\n",
        "\n",
        "def wofostAvgFeatureCols():\n",
        "  \"\"\"columns or indicators using avg aggregation\"\"\"\n",
        "  # must be in sync with crop calendar periods\n",
        "  avg_cols = {\n",
        "      'p0' : [],\n",
        "      'p1' : [],\n",
        "      'p2' : ['RSM'],\n",
        "      'p3' : [],\n",
        "      'p4' : ['RSM'],\n",
        "      'p5' : [],\n",
        "  }\n",
        "\n",
        "  return avg_cols\n",
        "\n",
        "def wofostCountFeatureCols():\n",
        "  \"\"\"columns or indicators using count aggregation\"\"\"\n",
        "  # must be in sync with crop calendar periods\n",
        "  count_cols = {\n",
        "      'p0' : [],\n",
        "      'p1' : ['RSM'],\n",
        "      'p2' : ['RSM'],\n",
        "      'p3' : ['RSM'],\n",
        "      'p4' : ['RSM'],\n",
        "      'p5' : [],\n",
        "  }\n",
        "\n",
        "  return count_cols\n",
        "\n",
        "# Meteo Feature ideas:\n",
        "# Two dry summers caused drop in ground water level:\n",
        "#   rainfall sums going back to second of half of previous year\n",
        "# Previous year: high production, prices low, invest less in crop\n",
        "#   Focus on another crop\n",
        "def meteoMaxFeatureCols():\n",
        "  \"\"\"columns or indicators using max aggregation\"\"\"\n",
        "  # must be in sync with crop calendar periods\n",
        "  max_cols = { 'p0' : [], 'p1' : [], 'p2' : [], 'p3' : [], 'p4' : [], 'p5' : [] }\n",
        "\n",
        "  return max_cols\n",
        "\n",
        "def meteoAvgFeatureCols(features_v2=False):\n",
        "  \"\"\"columns or indicators using avg aggregation\"\"\"\n",
        "  # must be in sync with crop calendar periods\n",
        "  avg_cols = {\n",
        "      'p0' : ['TAVG', 'PREC', 'CWB'],\n",
        "      'p1' : ['TAVG', 'PREC'],\n",
        "      'p2' : ['TAVG', 'CWB'],\n",
        "      'p3' : ['PREC'],\n",
        "      'p4' : ['CWB'],\n",
        "      'p5' : ['PREC'],\n",
        "  }\n",
        "\n",
        "  if (features_v2):\n",
        "    avg_cols['p2'] = avg_cols['p2'] + ['RAD']\n",
        "    avg_cols['p4'] = avg_cols['p4'] + ['RAD']\n",
        "\n",
        "  return avg_cols\n",
        "\n",
        "def meteoCountFeatureCols():\n",
        "  \"\"\"columns or indicators using count aggregation\"\"\"\n",
        "  # must be in sync with crop calendar periods\n",
        "  count_cols = {\n",
        "      'p0' : [],\n",
        "      'p1' : ['TMIN', 'PREC'],\n",
        "      'p2' : [],\n",
        "      'p3' : ['PREC', 'TMAX'],\n",
        "      'p4' : [],\n",
        "      'p5' : ['PREC'],\n",
        "  }\n",
        "\n",
        "  return count_cols\n",
        "\n",
        "def rsMaxFeatureCols():\n",
        "  \"\"\"columns or indicators using max aggregation\"\"\"\n",
        "  # must be in sync with crop calendar periods\n",
        "  max_cols = { 'p0' : [], 'p1' : [], 'p2' : [], 'p3' : [], 'p4' : [], 'p5' : [] }\n",
        "\n",
        "  return max_cols\n",
        "\n",
        "def rsAvgFeatureCols():\n",
        "  \"\"\"columns or indicators using avg aggregation\"\"\"\n",
        "  # must be in sync with crop calendar periods\n",
        "  avg_cols = {\n",
        "      'p0' : [],\n",
        "      'p1' : [],\n",
        "      'p2' : ['FAPAR'],\n",
        "      'p3' : [],\n",
        "      'p4' : ['FAPAR'],\n",
        "      'p5' : [],\n",
        "  }\n",
        "\n",
        "  return avg_cols\n",
        "\n",
        "def convertFeaturesToPandas(ft_dfs, join_cols):\n",
        "  \"\"\"Convert features to pandas and merge\"\"\"\n",
        "  train_ft_df = ft_dfs[0]\n",
        "  test_ft_df = ft_dfs[1]\n",
        "  train_ft_df = train_ft_df.withColumnRenamed('CAMPAIGN_YEAR', 'FYEAR')\n",
        "  test_ft_df = test_ft_df.withColumnRenamed('CAMPAIGN_YEAR', 'FYEAR')\n",
        "  pd_train_df = train_ft_df.toPandas()\n",
        "  pd_test_df = test_ft_df.toPandas()\n",
        "\n",
        "  return [pd_train_df, pd_test_df]\n",
        "\n",
        "def dropZeroColumns(pd_ft_dfs):\n",
        "  \"\"\"Drop columns which have all zeros in training data\"\"\"\n",
        "  pd_train_df = pd_ft_dfs[0]\n",
        "  pd_test_df = pd_ft_dfs[1]\n",
        "\n",
        "  pd_train_df = pd_train_df.loc[:, (pd_train_df != 0.0).any(axis=0)]\n",
        "  pd_train_df = pd_train_df.dropna(axis=1)\n",
        "  pd_test_df = pd_test_df[pd_train_df.columns]\n",
        "\n",
        "  return [pd_train_df, pd_test_df]\n",
        "\n",
        "def printFeatureData(pd_feature_dfs, join_cols):\n",
        "  for src in pd_feature_dfs:\n",
        "    pd_train_fts = pd_feature_dfs[src][0]\n",
        "    if (pd_train_fts is None):\n",
        "      continue\n",
        "\n",
        "    pd_test_fts = pd_feature_dfs[src][1]\n",
        "    all_cols = list(pd_train_fts.columns)\n",
        "    aggr_cols = [ c for c in all_cols if (('avg' in c) or ('max' in c))]\n",
        "    if (len(aggr_cols) > 0):\n",
        "      print('\\n', src, 'Aggregate Features: Training')\n",
        "      print(pd_train_fts[join_cols + aggr_cols].head(5))\n",
        "      print('\\n', src, 'Aggregate Features: Test')\n",
        "      print(pd_test_fts[join_cols + aggr_cols].head(5))\n",
        "\n",
        "    ext_cols = [ c for c in all_cols if (('Z+' in c) or ('Z-' in c) or\n",
        "                                         ('lt' in c) or ('gt' in c))]\n",
        "    if (len(ext_cols) > 0):\n",
        "      print('\\n', src, 'Features for Extreme Conditions: Training')\n",
        "      print(pd_train_fts[join_cols + ext_cols].head(5))\n",
        "      print('\\n', src, 'Features for Extreme Conditions: Test')\n",
        "      print(pd_test_fts[join_cols + ext_cols].head(5))\n",
        "\n",
        "def createFeatures(cyp_config, cyp_featurizer, train_test_dfs,\n",
        "                   summary_dfs, log_fh):\n",
        "  \"\"\"Create WOFOST, Meteo and Remote Sensing features\"\"\"\n",
        "  nuts_level = cyp_config.getNUTSLevel()\n",
        "  use_remote_sensing = cyp_config.useRemoteSensing()\n",
        "  use_gaes = cyp_config.useGAES()\n",
        "  use_per_year_cc = cyp_config.usePerYearCropCalendar()\n",
        "  use_features_v2 = cyp_config.useFeaturesV2()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "\n",
        "  wofost_train_df = train_test_dfs['WOFOST'][0]\n",
        "  wofost_test_df = train_test_dfs['WOFOST'][1]\n",
        "  meteo_train_df = train_test_dfs['METEO'][0]\n",
        "  meteo_test_df = train_test_dfs['METEO'][1]\n",
        "  yield_train_df = train_test_dfs['YIELD'][0]\n",
        "\n",
        "  dvs_train = summary_dfs['WOFOST_DVS'][0]\n",
        "  dvs_test = summary_dfs['WOFOST_DVS'][1]\n",
        "\n",
        "  if (debug_level > 2):\n",
        "    print('WOFOST training data size',\n",
        "          wofost_train_df.select(['IDREGION', 'CAMPAIGN_YEAR']).distinct().count())\n",
        "    print('WOFOST test data size',\n",
        "          wofost_test_df.select(['IDREGION', 'CAMPAIGN_YEAR']).distinct().count())\n",
        "    print('Meteo training data size',\n",
        "          meteo_train_df.select(['IDREGION', 'CAMPAIGN_YEAR']).distinct().count())\n",
        "    print('Meteo test data size',\n",
        "          meteo_test_df.select(['IDREGION', 'CAMPAIGN_YEAR']).distinct().count())\n",
        "    print('DVS Summary of training data',\n",
        "          dvs_summary_train.select(['IDREGION', 'CAMPAIGN_YEAR']).distinct().count())\n",
        "    print('DVS Summary of test data',\n",
        "          dvs_summary_test.select(['IDREGION', 'CAMPAIGN_YEAR']).distinct().count())\n",
        "\n",
        "  rs_train_df = None\n",
        "  rs_test_df = None\n",
        "  if (use_remote_sensing):\n",
        "    rs_train_df = train_test_dfs['REMOTE_SENSING'][0]\n",
        "    rs_test_df = train_test_dfs['REMOTE_SENSING'][1]\n",
        "    if (debug_level > 2):\n",
        "      print('Remote sensing training data size',\n",
        "            rs_train_df.select(['IDREGION', 'CAMPAIGN_YEAR']).distinct().count())\n",
        "      print('Remote sensing test data size',\n",
        "            rs_test_df.select(['IDREGION', 'CAMPAIGN_YEAR']).distinct().count())\n",
        "\n",
        "  join_cols = ['IDREGION', 'CAMPAIGN_YEAR']\n",
        "  aggr_ft_cols = {\n",
        "      'WOFOST' : [wofostMaxFeatureCols(), wofostAvgFeatureCols()],\n",
        "      'METEO' : [meteoMaxFeatureCols(), meteoAvgFeatureCols(use_features_v2)],\n",
        "  }\n",
        "\n",
        "  count_ft_cols = {\n",
        "      'WOFOST' : wofostCountFeatureCols(),\n",
        "      'METEO' : meteoCountFeatureCols(),\n",
        "  }\n",
        "\n",
        "  train_ft_src_dfs = {\n",
        "      'WOFOST' : wofost_train_df,\n",
        "      'METEO' : meteo_train_df,\n",
        "  }\n",
        "\n",
        "  test_ft_src_dfs = {\n",
        "      'WOFOST' : wofost_test_df,\n",
        "      'METEO' : meteo_test_df,\n",
        "  }\n",
        "\n",
        "  if (use_remote_sensing):\n",
        "    train_ft_src_dfs['REMOTE_SENSING'] = rs_train_df\n",
        "    test_ft_src_dfs['REMOTE_SENSING'] = rs_test_df\n",
        "    aggr_ft_cols['REMOTE_SENSING'] = [rsMaxFeatureCols(), rsAvgFeatureCols()]\n",
        "    count_ft_cols['REMOTE_SENSING'] = {}\n",
        "\n",
        "  if (use_gaes):\n",
        "    aez_df = train_test_dfs['GAES'][0]\n",
        "    dvs_train = dvs_train.join(aez_df.select(['IDREGION', 'AEZ_ID']), 'IDREGION')\n",
        "    dvs_test = dvs_test.join(aez_df.select(['IDREGION', 'AEZ_ID']), 'IDREGION')\n",
        "\n",
        "  crop_cal_train = dvs_train\n",
        "  crop_cal_test = dvs_test\n",
        "  if (not use_per_year_cc):\n",
        "    crop_cal_test = dvs_train\n",
        "\n",
        "  train_ft_dfs = {}\n",
        "  test_ft_dfs = {}\n",
        "  for ft_src in train_ft_src_dfs:\n",
        "    cum_avg_cols = []\n",
        "    if (use_features_v2):\n",
        "      cum_avg_cols = getCumulativeAvgCols(ft_src)\n",
        "\n",
        "    train_ft_dfs[ft_src] = cyp_featurizer.extractFeatures(train_ft_src_dfs[ft_src],\n",
        "                                                          ft_src,\n",
        "                                                          crop_cal_train,\n",
        "                                                          aggr_ft_cols[ft_src][0],\n",
        "                                                          aggr_ft_cols[ft_src][1],\n",
        "                                                          cum_avg_cols,\n",
        "                                                          count_ft_cols[ft_src],\n",
        "                                                          join_cols,\n",
        "                                                          True)\n",
        "    test_ft_dfs[ft_src] = cyp_featurizer.extractFeatures(test_ft_src_dfs[ft_src],\n",
        "                                                         ft_src,\n",
        "                                                         crop_cal_test,\n",
        "                                                         aggr_ft_cols[ft_src][0],\n",
        "                                                         aggr_ft_cols[ft_src][1],\n",
        "                                                         cum_avg_cols,\n",
        "                                                         count_ft_cols[ft_src],\n",
        "                                                         join_cols)\n",
        "\n",
        "  pd_conversion_dict = {\n",
        "      'WOFOST' : [ train_ft_dfs['WOFOST'], test_ft_dfs['WOFOST'] ],\n",
        "      'METEO' : [ train_ft_dfs['METEO'], test_ft_dfs['METEO'] ],\n",
        "  }\n",
        "\n",
        "  if (use_remote_sensing):\n",
        "      pd_conversion_dict['REMOTE_SENSING'] = [ train_ft_dfs['REMOTE_SENSING'], test_ft_dfs['REMOTE_SENSING'] ]\n",
        "\n",
        "  pd_feature_dfs = {}\n",
        "  for ft_src in pd_conversion_dict:\n",
        "    pd_feature_dfs[ft_src] = convertFeaturesToPandas(pd_conversion_dict[ft_src], join_cols)\n",
        "\n",
        "  # Check and drop features with all zeros (possible in early season prediction).\n",
        "  for ft_src in pd_feature_dfs:\n",
        "    pd_feature_dfs[ft_src] = dropZeroColumns(pd_feature_dfs[ft_src])\n",
        "\n",
        "  if (debug_level > 1):\n",
        "    join_cols = ['IDREGION', 'FYEAR']\n",
        "    printFeatureData(pd_feature_dfs, join_cols)\n",
        "\n",
        "  return pd_feature_dfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERZ4JYeyss7m"
      },
      "source": [
        "#### Create Trend Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbqouti4swJ0"
      },
      "source": [
        "#%%writefile run_trend_feature_design.py\n",
        "def createYieldTrendFeatures(cyp_config, cyp_trend_est,\n",
        "                             yield_train_df, yield_test_df, test_years):\n",
        "  \"\"\"Create yield trend features\"\"\"\n",
        "  join_cols = ['IDREGION', 'FYEAR']\n",
        "  find_optimal = cyp_config.findOptimalTrendWindow()\n",
        "  trend_window = cyp_config.getTrendWindows()[0]\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  yield_df = yield_train_df.union(yield_test_df.select(yield_train_df.columns))\n",
        "\n",
        "  if (find_optimal):\n",
        "    pd_train_features = cyp_trend_est.getOptimalWindowTrendFeatures(yield_train_df)\n",
        "    pd_test_features = cyp_trend_est.getOptimalWindowTrendFeatures(yield_df)\n",
        "  else:\n",
        "    pd_train_features = cyp_trend_est.getFixedWindowTrendFeatures(yield_train_df)\n",
        "    pd_test_features = cyp_trend_est.getFixedWindowTrendFeatures(yield_df)\n",
        "\n",
        "  pd_test_features = pd_test_features[pd_test_features['FYEAR'].isin(test_years)]\n",
        "  prev_year_cols = ['YEAR-' + str(i) for i in range(1, trend_window + 1)]\n",
        "  pd_train_features = pd_train_features.drop(columns=prev_year_cols)\n",
        "  pd_test_features = pd_test_features.drop(columns=prev_year_cols)\n",
        "\n",
        "  if (debug_level > 1):\n",
        "    print('\\nYield Trend Features: Train')\n",
        "    join_cols = ['IDREGION', 'FYEAR']\n",
        "    print(pd_train_features.sort_values(by=join_cols).head(5))\n",
        "    print('Total', len(pd_train_features.index), 'rows')\n",
        "    print('\\nYield Trend Features: Test')\n",
        "    print(pd_test_features.sort_values(by=join_cols).head(5))\n",
        "    print('Total', len(pd_test_features.index), 'rows')\n",
        "\n",
        "  return pd_train_features, pd_test_features\n",
        "\n",
        "def addFeaturesFromPreviousYears(cyp_config, pd_feature_dfs,\n",
        "                                 trend_window, test_years, join_cols):\n",
        "  \"\"\"Add features from previous years as trend features\"\"\"\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "\n",
        "  for ft_src in pd_feature_dfs:\n",
        "    trend_cols = getTrendFeatureCols(ft_src)\n",
        "    if (not trend_cols):\n",
        "      continue\n",
        "\n",
        "    pd_ft_train_df = pd_feature_dfs[ft_src][0]\n",
        "    pd_ft_test_df = pd_feature_dfs[ft_src][1]\n",
        "    pd_all_df = pd_ft_train_df.append(pd_ft_test_df).sort_values(by=join_cols)\n",
        "    all_cols = pd_ft_train_df.columns\n",
        "    common_cols = [c for c in trend_cols if c in all_cols]\n",
        "    if (not common_cols):\n",
        "      continue\n",
        "\n",
        "    for tc in trend_cols:\n",
        "      if (tc not in all_cols):\n",
        "        continue\n",
        "\n",
        "      for yr in range(1, trend_window + 1):\n",
        "        pd_all_df[tc + '-' + str(yr)] = pd_all_df.groupby(['IDREGION'])[tc].shift(yr)\n",
        "\n",
        "    pd_ft_train_df = pd_all_df[~pd_all_df['FYEAR'].isin(test_years)]\n",
        "    pd_ft_train_df = pd_ft_train_df.dropna(axis=0)\n",
        "    pd_ft_test_df = pd_all_df[pd_all_df['FYEAR'].isin(test_years)]\n",
        "    pd_ft_test_df = pd_ft_test_df.dropna(axis=0)\n",
        "\n",
        "    sel_cols = ['IDREGION', 'FYEAR']\n",
        "    all_cols = pd_ft_train_df.columns\n",
        "    for tc in trend_cols:\n",
        "      tc_cols = [c for c in all_cols if tc in c]\n",
        "      sel_cols += tc_cols\n",
        "\n",
        "    if ((debug_level > 1) and (len(sel_cols) > 2)):\n",
        "      print('\\n' + ft_src + ' Trend Features: Train')\n",
        "      print(pd_ft_train_df[sel_cols].sort_values(by=join_cols).head(5).to_string(index=False))\n",
        "      print('\\n' + ft_src + ' Trend Features: Test')\n",
        "      print(pd_ft_test_df[sel_cols].sort_values(by=join_cols).head(5).to_string(index=False))\n",
        "\n",
        "    pd_feature_dfs[ft_src] = [pd_ft_train_df, pd_ft_test_df]\n",
        "\n",
        "  return pd_feature_dfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIDmOfx9KKuF"
      },
      "source": [
        "### Combine features and labels\n",
        "\n",
        "Combine wofost, meteo and soil with remote sensing. Combine with centroids or yield trend both if configured. Combine with yield data in the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsoMBrpIKKuH"
      },
      "source": [
        "#%%writefile combine_features.py\n",
        "def combineFeaturesLabels(cyp_config, sqlCtx,\n",
        "                          prep_train_test_dfs, pd_feature_dfs,\n",
        "                          join_cols, log_fh):\n",
        "  \"\"\"\n",
        "  Combine wofost, meteo and soil with remote sensing. Combine centroids\n",
        "  and yield trend if configured. Combine with yield data in the end.\n",
        "  If configured, save features to a CSV file.\n",
        "  \"\"\"\n",
        "  pd_soil_df = prep_train_test_dfs['SOIL'][0].toPandas()\n",
        "  pd_yield_train_df = prep_train_test_dfs['YIELD'][0].toPandas()\n",
        "  pd_yield_test_df = prep_train_test_dfs['YIELD'][1].toPandas()\n",
        "\n",
        "  # Feature dataframes have already been converted to pandas\n",
        "  pd_wofost_train_ft = pd_feature_dfs['WOFOST'][0]\n",
        "  pd_wofost_test_ft = pd_feature_dfs['WOFOST'][1]\n",
        "  pd_meteo_train_ft = pd_feature_dfs['METEO'][0]\n",
        "  pd_meteo_test_ft = pd_feature_dfs['METEO'][1]\n",
        "\n",
        "  crop = cyp_config.getCropName()\n",
        "  country = cyp_config.getCountryCode()\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  use_centroids = cyp_config.useCentroids()\n",
        "  use_remote_sensing = cyp_config.useRemoteSensing()\n",
        "  use_gaes = cyp_config.useGAES()\n",
        "  save_features = cyp_config.saveFeatures()\n",
        "  use_sample_weights = cyp_config.useSampleWeights()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  \n",
        "  combine_info = '\\nCombine Features and Labels'\n",
        "  combine_info += '\\n---------------------------'\n",
        "  yield_min_year = pd_yield_train_df['FYEAR'].min()\n",
        "  combine_info += '\\nYield min year ' + str(yield_min_year) + '\\n'\n",
        "\n",
        "  # start with static SOIL data\n",
        "  pd_train_df = pd_soil_df.copy(deep=True)\n",
        "  pd_test_df = pd_soil_df.copy(deep=True)\n",
        "  combine_info += '\\nData size after including SOIL data: '\n",
        "  combine_info += '\\nTrain ' + str(len(pd_train_df.index)) + ' rows.'\n",
        "  combine_info += '\\nTest ' + str(len(pd_test_df.index)) + ' rows.\\n'\n",
        "\n",
        "  if (use_gaes):\n",
        "    pd_aez_df = prep_train_test_dfs['GAES'][0].toPandas()\n",
        "    pd_aez_df = pd_aez_df.drop(columns=['AEZ_ID'])\n",
        "    pd_train_df = pd_train_df.merge(pd_aez_df, on=['IDREGION'])\n",
        "    pd_test_df = pd_test_df.merge(pd_aez_df, on=['IDREGION'])\n",
        "    combine_info += '\\nData size after including GAES data: '\n",
        "    combine_info += '\\nTrain ' + str(len(pd_train_df.index)) + ' rows.'\n",
        "    combine_info += '\\nTest ' + str(len(pd_test_df.index)) + ' rows.\\n'\n",
        "\n",
        "  if (use_centroids):\n",
        "    # combine with region centroids\n",
        "    pd_centroids_df = prep_train_test_dfs['CENTROIDS'][0].toPandas()\n",
        "    pd_train_df = pd_train_df.merge(pd_centroids_df, on=['IDREGION'])\n",
        "    pd_test_df = pd_test_df.merge(pd_centroids_df, on='IDREGION')\n",
        "    combine_info += '\\nData size after including CENTROIDS data: '\n",
        "    combine_info += '\\nTrain ' + str(len(pd_train_df.index)) + ' rows.'\n",
        "    combine_info += '\\nTest ' + str(len(pd_test_df.index)) + ' rows.\\n'\n",
        "\n",
        "  # combine with WOFOST features\n",
        "  static_cols = list(pd_train_df.columns)\n",
        "  pd_train_df = pd_train_df.merge(pd_wofost_train_ft, on=['IDREGION'])\n",
        "  pd_test_df = pd_test_df.merge(pd_wofost_test_ft, on=['IDREGION'])\n",
        "  wofost_cols = list(pd_wofost_train_ft.columns)\n",
        "  col_order = ['IDREGION', 'FYEAR'] + static_cols[1:] + wofost_cols[2:]\n",
        "  pd_train_df = pd_train_df[col_order]\n",
        "  pd_test_df = pd_test_df[col_order]\n",
        "  combine_info += '\\nData size after including WOFOST features: '\n",
        "  combine_info += '\\nTrain ' + str(len(pd_train_df.index)) + ' rows.'\n",
        "  combine_info += '\\nTest ' + str(len(pd_test_df.index)) + ' rows.\\n'\n",
        "\n",
        "  # combine with METEO features\n",
        "  pd_train_df = pd_train_df.merge(pd_meteo_train_ft, on=join_cols)\n",
        "  pd_test_df = pd_test_df.merge(pd_meteo_test_ft, on=join_cols)\n",
        "  combine_info += '\\nData size after including METEO features: '\n",
        "  combine_info += '\\nTrain ' + str(len(pd_train_df.index)) + ' rows.'\n",
        "  combine_info += '\\nTest ' + str(len(pd_test_df.index)) + ' rows.\\n'\n",
        "\n",
        "  # combine with remote sensing features\n",
        "  if (use_remote_sensing):\n",
        "    pd_rs_train_ft = pd_feature_dfs['REMOTE_SENSING'][0]\n",
        "    pd_rs_test_ft = pd_feature_dfs['REMOTE_SENSING'][1]\n",
        "\n",
        "    pd_train_df = pd_train_df.merge(pd_rs_train_ft, on=join_cols)\n",
        "    pd_test_df = pd_test_df.merge(pd_rs_test_ft, on=join_cols)\n",
        "    combine_info += '\\nData size after including REMOTE_SENSING features: '\n",
        "    combine_info += '\\nTrain ' + str(len(pd_train_df.index)) + ' rows.'\n",
        "    combine_info += '\\nTest ' + str(len(pd_test_df.index)) + ' rows.\\n'\n",
        "\n",
        "  if (use_gaes):\n",
        "    # combine with crop area\n",
        "    pd_area_train_df = prep_train_test_dfs['CROP_AREA'][0].toPandas()\n",
        "    pd_area_test_df = prep_train_test_dfs['CROP_AREA'][1].toPandas()\n",
        "    pd_train_df = pd_train_df.merge(pd_area_train_df, on=join_cols)\n",
        "    pd_test_df = pd_test_df.merge(pd_area_test_df, on=join_cols)\n",
        "    combine_info += '\\nData size after including CROP_AREA data: '\n",
        "    combine_info += '\\nTrain ' + str(len(pd_train_df.index)) + ' rows.'\n",
        "    combine_info += '\\nTest ' + str(len(pd_test_df.index)) + ' rows.\\n'\n",
        "\n",
        "  if (use_yield_trend):\n",
        "    # combine with yield trend features\n",
        "    pd_yield_trend_train_ft = pd_feature_dfs['YIELD_TREND'][0]\n",
        "    pd_yield_trend_test_ft = pd_feature_dfs['YIELD_TREND'][1]\n",
        "    pd_train_df = pd_train_df.merge(pd_yield_trend_train_ft, on=join_cols)\n",
        "    pd_test_df = pd_test_df.merge(pd_yield_trend_test_ft, on=join_cols)\n",
        "    combine_info += '\\nData size after including yield trend features: '\n",
        "    combine_info += '\\nTrain ' + str(len(pd_train_df.index)) + ' rows.'\n",
        "    combine_info += '\\nTest ' + str(len(pd_test_df.index)) + ' rows.\\n'\n",
        "\n",
        "  # combine with yield data\n",
        "  pd_train_df = pd_train_df.merge(pd_yield_train_df, on=join_cols)\n",
        "  pd_test_df = pd_test_df.merge(pd_yield_test_df, on=join_cols)\n",
        "  pd_train_df = pd_train_df.sort_values(by=join_cols)\n",
        "  pd_test_df = pd_test_df.sort_values(by=join_cols)\n",
        "  combine_info += '\\nData size after including yield (label) data: '\n",
        "  combine_info += '\\nTrain ' + str(len(pd_train_df.index)) + ' rows.'\n",
        "  combine_info += '\\nTest ' + str(len(pd_test_df.index)) + ' rows.\\n'\n",
        "\n",
        "  # sample weights\n",
        "  if (use_sample_weights):\n",
        "    assert use_gaes\n",
        "    pd_train_df['SAMPLE_WEIGHT'] = pd_train_df['CROP_AREA']\n",
        "    pd_test_df['SAMPLE_WEIGHT'] = pd_test_df['CROP_AREA']\n",
        "\n",
        "  log_fh.write(combine_info + '\\n')\n",
        "  if (debug_level > 1):\n",
        "    print(combine_info)\n",
        "    print('\\nAll Features and labels: Training')\n",
        "    print(pd_train_df.head(5))\n",
        "    print('\\nAll Features and labels: Test')\n",
        "    print(pd_test_df.head(5))\n",
        "\n",
        "  if (save_features):\n",
        "    early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "    early_season_end = cyp_config.getEarlySeasonEndDekad()\n",
        "    feature_file_path = cyp_config.getOutputPath()\n",
        "    features_file = getFeatureFilename(crop, country, use_yield_trend,\n",
        "                                       early_season_prediction, early_season_end)\n",
        "    save_ft_path = feature_file_path + '/' + features_file\n",
        "    save_ft_info = '\\nSaving features to: ' + save_ft_path + '[train, test].csv'\n",
        "    log_fh.write(save_ft_info + '\\n')\n",
        "    if (debug_level > 1):\n",
        "      print(save_ft_info)\n",
        "\n",
        "    pd_train_df.to_csv(save_ft_path + '_train.csv', index=False, header=True)\n",
        "    pd_test_df.to_csv(save_ft_path + '_test.csv', index=False, header=True)\n",
        "\n",
        "    # NOTE: In some environments, Spark can write, but pandas cannot.\n",
        "    # In such cases, use the following code.\n",
        "    # spark_train_df = sqlCtx.createDataFrame(pd_train_df)\n",
        "    # spark_train_df.coalesce(1).write.option('header','true').mode('overwrite').csv(save_ft_path + '_train')\n",
        "    # spark_test_df = sqlCtx.createDataFrame(pd_test_df)\n",
        "    # spark_test_df.coalesce(1).write.option('header','true').mode('overwrite').csv(save_ft_path + '_test')\n",
        "\n",
        "  return pd_train_df, pd_test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQb85mf_wgvS"
      },
      "source": [
        "### Load Saved Features and Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5TyGb2iwnFr"
      },
      "source": [
        "#%%writefile load_saved_features.py\n",
        "import pandas as pd\n",
        "\n",
        "def loadSavedFeaturesLabels(cyp_config, spark):\n",
        "  \"\"\"Load saved features from a CSV file\"\"\"\n",
        "  crop = cyp_config.getCropName()\n",
        "  country = cyp_config.getCountryCode()\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "  early_season_end = cyp_config.getEarlySeasonEndDekad()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "\n",
        "  feature_file_path = cyp_config.getOutputPath()\n",
        "  feature_file = getFeatureFilename(crop, country, use_yield_trend,\n",
        "                                    early_season_prediction, early_season_end)\n",
        "\n",
        "  load_ft_path = feature_file_path + '/' + feature_file\n",
        "  pd_train_df = pd.read_csv(load_ft_path + '_train.csv', header=0)\n",
        "  pd_test_df = pd.read_csv(load_ft_path + '_test.csv', header=0)\n",
        "\n",
        "  # NOTE: In some environments, Spark can read, but pandas cannot.\n",
        "  # In such cases, use the following code.\n",
        "  # spark_train_df = spark.read.csv(load_ft_path + '_train.csv', header=True, inferSchema=True)\n",
        "  # spark_test_df = spark.read.csv(load_ft_path + '_test.csv', header=True, inferSchema=True)\n",
        "  # pd_train_df = spark_train_df.toPandas()\n",
        "  # pd_test_df = spark_test_df.toPandas()\n",
        "\n",
        "  if (debug_level > 1):\n",
        "    print('\\nAll Features and labels')\n",
        "    print(pd_train_df.head(5))\n",
        "    print(pd_test_df.head(5))\n",
        "\n",
        "  return pd_train_df, pd_test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PjL0Nv1jEpR"
      },
      "source": [
        "### Machine Learning using scikit learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drvMnHJBuBwK"
      },
      "source": [
        "#### Feature Selector Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBcbtxArL2Ld"
      },
      "source": [
        "#%%writefile feature_selection.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.utils import parallel_backend\n",
        "\n",
        "class CYPFeatureSelector:\n",
        "  def __init__(self, cyp_config, X_train, Y_train, all_features,\n",
        "               custom_cv, train_weights=None):\n",
        "    self.X_train = X_train\n",
        "    self.Y_train = Y_train\n",
        "    self.train_weights = train_weights\n",
        "    self.custom_cv = custom_cv\n",
        "    self.all_features = all_features\n",
        "    self.scaler = cyp_config.getFeatureScaler()\n",
        "    self.cv_metric = cyp_config.getFeatureSelectionCVMetric()\n",
        "    self.feature_selectors = cyp_config.getFeatureSelectors(len(all_features))\n",
        "    self.use_sample_weights = cyp_config.useSampleWeights()\n",
        "    self.verbose = cyp_config.getDebugLevel()\n",
        "\n",
        "  def setCustomCV(self, custom_cv):\n",
        "    \"\"\"Set custom K-Fold validation splits\"\"\"\n",
        "    self.custom_cv = custom_cv\n",
        "\n",
        "  def setFeatures(self, features):\n",
        "    \"\"\"Set the list of all features\"\"\"\n",
        "    self.all_features = all_features\n",
        "\n",
        "  # K-fold validation to find the optimal number of features\n",
        "  # and optimal hyperparameters for estimator.\n",
        "  def featureSelectionParameterSearch(self, selector, est, param_grid, fit_params):\n",
        "    \"\"\"Use grid search with k-fold validation to optimize the number of features\"\"\"\n",
        "    X_train_copy = np.copy(self.X_train)\n",
        "    pipeline = Pipeline([(\"scaler\", self.scaler),\n",
        "                         (\"selector\", selector),\n",
        "                         (\"estimator\", est)])\n",
        "\n",
        "    grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid,\n",
        "                               scoring=self.cv_metric, cv=self.custom_cv)\n",
        "    with parallel_backend('spark', n_jobs=-1):\n",
        "      grid_search.fit(X_train_copy, np.ravel(self.Y_train), **fit_params)\n",
        "\n",
        "    best_score = grid_search.best_score_\n",
        "    best_estimator = grid_search.best_estimator_\n",
        "    indices = []\n",
        "    # feature selectors should have 'get_support' function\n",
        "    if ((isinstance(selector, SelectFromModel)) or (isinstance(selector, SelectKBest)) or\n",
        "        (isinstance(selector, RFE))):\n",
        "      sel = grid_search.best_estimator_.named_steps['selector']\n",
        "      indices = sel.get_support(indices=True)\n",
        "\n",
        "    result = {\n",
        "        'indices' : indices,\n",
        "        'best_score' : best_score,\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "  # Compare different feature selectors using cross validation score.\n",
        "  # Also compare combined features with the best individual feature selector.\n",
        "  def compareFeatureSelectors(self, est_name, est, est_param_grid):\n",
        "    \"\"\"Compare feature selectors based on K-fold validation scores\"\"\"\n",
        "    combined_indices = []\n",
        "    # set it to a large negative value\n",
        "    best_score = -1000\n",
        "    best_indices = []\n",
        "    best_selector = ''\n",
        "    fs_scores = {}\n",
        "    for sel_name in self.feature_selectors:\n",
        "      selector = self.feature_selectors[sel_name]['selector']\n",
        "      sel_param_grid = self.feature_selectors[sel_name]['param_grid']\n",
        "      param_grid = sel_param_grid.copy()\n",
        "      param_grid.update(est_param_grid)\n",
        "\n",
        "      fit_params = {}\n",
        "      if (self.use_sample_weights and (est_name != 'KNN')):\n",
        "        fit_params['estimator__sample_weight'] = self.train_weights\n",
        "\n",
        "      result = self.featureSelectionParameterSearch(selector, est,\n",
        "                                                    param_grid, fit_params)\n",
        "      param_grid.clear()\n",
        "\n",
        "      if (self.verbose > 2):\n",
        "        print('\\nFeature selection using', sel_name)\n",
        "        print('Best cross-validation', self.cv_metric + ':',\n",
        "              np.round(result['best_score'], 3))\n",
        "\n",
        "        print('\\nSelected Features:')\n",
        "        print('-------------------')\n",
        "        printInGroups(self.all_features, result['indices'])\n",
        "\n",
        "      combined_indices = list(set(combined_indices) | set(result['indices']))\n",
        "\n",
        "      fs_scores[sel_name] = result['best_score']\n",
        "\n",
        "      if (result['best_score'] > best_score):\n",
        "        best_indices = result['indices']\n",
        "        best_score = result['best_score']\n",
        "        best_selector = sel_name\n",
        "\n",
        "    result = {\n",
        "        'best_selector' : best_selector,\n",
        "        'best_score' : best_score,\n",
        "        'best_indices' : best_indices,\n",
        "        'combined_indices' : combined_indices,\n",
        "        'scores' : fs_scores\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "  # Between the optimal sets for each estimator select the set with the higher score.\n",
        "  def selectOptimalFeatures(self, est, est_name, est_param_grid, log_fh):\n",
        "    \"\"\"\n",
        "    Select optimal features by comparing individual feature selectors\n",
        "    and combined features\n",
        "    \"\"\"\n",
        "    X_train_selected = None\n",
        "    # set it to a large negative value\n",
        "    best_score = -1000\n",
        "    fs_summary = {}\n",
        "    row_count = 1\n",
        "\n",
        "    est_info = '\\nEstimator: ' + est_name\n",
        "    est_info += '\\n---------------------------'\n",
        "    log_fh.write(est_info)\n",
        "    print(est_info)\n",
        "\n",
        "    result = self.compareFeatureSelectors(est_name, est, est_param_grid)\n",
        "\n",
        "    # result includes\n",
        "    # - 'best_selector' : name of the best selector\n",
        "    # - 'best_score' : cv score of features selected with estimator\n",
        "    # - 'best_indices' : indices of features selected\n",
        "    # - 'best_estimator' : estimator with settings that gave best score\n",
        "    # - 'combined_indices' : indices of combined features\n",
        "    # - 'scores' : dict with scores of all feature selectors\n",
        "\n",
        "    for sel_name in result['scores']:\n",
        "      est_sel_row = [est_name, sel_name, np.round(result['scores'][sel_name], 2)]\n",
        "      fs_summary['row' + str(row_count)] = est_sel_row\n",
        "      row_count += 1\n",
        "\n",
        "    # calculate cross-validation score for combined features\n",
        "    X_train_selected = self.X_train[:, result['combined_indices']]\n",
        "    pipeline = Pipeline([(\"scaler\", self.scaler), (\"estimator\", est)])\n",
        "    grid_search = GridSearchCV(estimator=pipeline, param_grid=est_param_grid,\n",
        "                               scoring=self.cv_metric, cv=self.custom_cv)\n",
        "    X_train_selected_copy = np.copy(X_train_selected)\n",
        "    with parallel_backend('spark', n_jobs=-1):\n",
        "      grid_search.fit(X_train_selected_copy, np.ravel(self.Y_train))\n",
        "\n",
        "    combined_score = grid_search.best_score_\n",
        "    combo_sel_row = [est_name, 'combined', np.round(combined_score, 2)]\n",
        "\n",
        "    fs_summary['row' + str(row_count)] = combo_sel_row\n",
        "    row_count += 1\n",
        "\n",
        "    # We check if combined features give us a better score\n",
        "    # than the best feature selection method\n",
        "    if (combined_score < result['best_score']):\n",
        "      selector = result['best_selector']\n",
        "      selected_indices = result['best_indices']\n",
        "    else:\n",
        "      selected_indices = result['combined_indices']\n",
        "\n",
        "    fs_df_columns = ['estimator', 'selector', self.cv_metric]\n",
        "    fs_df = pd.DataFrame.from_dict(fs_summary, orient='index', columns=fs_df_columns)\n",
        "\n",
        "    ftsel_summary_info = '\\nFeature Selection Summary'\n",
        "    ftsel_summary_info += '\\n---------------------------'\n",
        "    ftsel_summary_info += '\\n' + fs_df.to_string(index=False) + '\\n'\n",
        "    log_fh.write(ftsel_summary_info)\n",
        "    print(ftsel_summary_info)\n",
        "\n",
        "    return selected_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWdWViKkuFM1"
      },
      "source": [
        "#### Algorithm Evaluator Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LutX8Ih9MsGx"
      },
      "source": [
        "#%%writefile algorithm_evaluation.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "import multiprocessing as mp\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.utils import parallel_backend\n",
        "\n",
        "class CYPAlgorithmEvaluator:\n",
        "  def __init__(self, cyp_config, custom_cv=None,\n",
        "               train_weights=None, test_weights=None):\n",
        "    self.scaler = cyp_config.getFeatureScaler()\n",
        "    self.estimators = cyp_config.getEstimators()\n",
        "    self.custom_cv = custom_cv\n",
        "    self.cv_metric = cyp_config.getAlgorithmTrainingCVMetric()\n",
        "    self.train_weights = train_weights\n",
        "    self.test_weights = test_weights\n",
        "    self.metrics = cyp_config.getEvaluationMetrics()\n",
        "    self.verbose = cyp_config.getDebugLevel()\n",
        "    self.use_yield_trend = cyp_config.useYieldTrend()\n",
        "    self.trend_windows = cyp_config.getTrendWindows()\n",
        "    self.predict_residuals = cyp_config.predictYieldResiduals()\n",
        "    self.use_sample_weights = cyp_config.useSampleWeights()\n",
        "    self.retrain_per_test_year = cyp_config.retrainPerTestYear()\n",
        "\n",
        "  def setCustomCV(self, custom_cv):\n",
        "    \"\"\"Set custom K-Fold validation splits\"\"\"\n",
        "    self.custom_cv = custom_cv\n",
        "\n",
        "  # Nash-Sutcliffe Model Efficiency\n",
        "  def nse(self, Y_true, Y_pred):\n",
        "    \"\"\"\n",
        "        Nash Sutcliffe efficiency coefficient\n",
        "        input:\n",
        "          Y_pred: predicted\n",
        "          Y_true: observed\n",
        "        output:\n",
        "          nse: Nash Sutcliffe efficient coefficient\n",
        "        \"\"\"\n",
        "    return (1 - np.sum(np.square(Y_pred - Y_true))/np.sum(np.square(Y_true - np.mean(Y_true))))\n",
        "\n",
        "  def updateAlgorithmsSummary(self, alg_summary, alg_name, scores_list):\n",
        "    \"\"\"Update algorithms summary with scores for given algorithm\"\"\"\n",
        "    alg_row = [alg_name]\n",
        "    alg_index = len(alg_summary)\n",
        "    assert (len(scores_list) > 0)\n",
        "    for met in scores_list[0]:\n",
        "      for pred_scores in scores_list:\n",
        "        alg_row.append(pred_scores[met])\n",
        "\n",
        "    alg_summary['row' + str(alg_index)] = alg_row\n",
        "\n",
        "  def createPredictionDataFrames(self, Y_pred_arrays, data_cols):\n",
        "    \"\"\"\"Create pandas data frames from true and predicted values\"\"\"\n",
        "    pd_pred_dfs = []\n",
        "    for ar in Y_pred_arrays:\n",
        "      pd_df = pd.DataFrame(data=ar, columns=data_cols)\n",
        "      pd_pred_dfs.append(pd_df)\n",
        "\n",
        "    return pd_pred_dfs\n",
        "\n",
        "  def printPredictionDataFrames(self, pd_pred_dfs, pred_set_info, log_fh):\n",
        "    \"\"\"\"Print true and predicted values from pandas data frames\"\"\"\n",
        "    for i in range(len(pd_pred_dfs)):\n",
        "      pd_df = pd_pred_dfs[i]\n",
        "      set_info = pred_set_info[i]\n",
        "      df_info = '\\n Yield Predictions ' + set_info\n",
        "      df_info += '\\n--------------------------------'\n",
        "      df_info += '\\n' + pd_df.head(6).to_string(index=False)\n",
        "      log_fh.write(df_info + '\\n')\n",
        "      print(df_info)\n",
        "\n",
        "  def getNullMethodPredictions(self, Y_train_full, Y_test_full, cv_test_years, log_fh):\n",
        "    \"\"\"\n",
        "    The Null method or poor man's prediction. Y_*_full includes IDREGION, FYEAR.\n",
        "    If using yield trend, Y_*_full also include YIELD_TREND.\n",
        "    The null method predicts the YIELD_TREND or the average of the training set.\n",
        "    \"\"\"\n",
        "    Y_train = Y_train_full[:, 2]\n",
        "    if (self.use_yield_trend):\n",
        "      Y_train = Y_train_full[:, 3]\n",
        "\n",
        "    min_yield = np.round(np.min(Y_train), 2)\n",
        "    max_yield = np.round(np.max(Y_train), 2)\n",
        "    avg_yield = np.round(np.mean(Y_train), 2)\n",
        "    median_yield = np.round(np.median(np.ravel(Y_train)), 2)\n",
        "    cv_test_years = np.array(cv_test_years)\n",
        "\n",
        "    null_method_label = 'Null Method: '\n",
        "    if (self.use_yield_trend):\n",
        "      null_method_label += 'Predicting linear yield trend:'\n",
        "      data_cols = ['IDREGION', 'FYEAR', 'YIELD_TREND', 'YIELD']\n",
        "      Y_cv_full = Y_train_full[np.in1d(Y_train_full[:, 1], cv_test_years)]\n",
        "      Y_pred_arrays = [Y_train_full, Y_cv_full, Y_test_full]\n",
        "    else:\n",
        "      Y_train_full_n = np.insert(Y_train_full, 2, avg_yield, axis=1)\n",
        "      Y_test_full_n = np.insert(Y_test_full, 2, avg_yield, axis=1)\n",
        "      null_method_label += 'Predicting average of the training set:'\n",
        "      data_cols = ['IDREGION', 'FYEAR', 'YIELD_PRED', 'YIELD']\n",
        "      Y_cv_full = Y_train_full_n[np.in1d(Y_train_full_n[:, 1], cv_test_years)]\n",
        "      Y_pred_arrays = [Y_train_full_n, Y_cv_full, Y_test_full_n]\n",
        "\n",
        "    pd_pred_dfs = self.createPredictionDataFrames(Y_pred_arrays, data_cols)\n",
        "    null_method_info = '\\n' + null_method_label\n",
        "    null_method_info += '\\nMin Yield: ' + str(min_yield) + ', Max Yield: ' + str(max_yield)\n",
        "    null_method_info += '\\nMedian Yield: ' + str(median_yield) + ', Mean Yield: ' + str(avg_yield)\n",
        "    log_fh.write(null_method_info + '\\n')\n",
        "    print(null_method_info)\n",
        "    pred_set_info = ['Training Set', 'Validation Test Set', 'Test Set']\n",
        "    self.printPredictionDataFrames(pd_pred_dfs, pred_set_info, log_fh)\n",
        "\n",
        "    result = {\n",
        "        'train' : pd_pred_dfs[0],\n",
        "        'custom_cv' : pd_pred_dfs[1],\n",
        "        'test' : pd_pred_dfs[2],\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "  def evaluateNullMethodPredictions(self, pd_pred_dfs, alg_summary):\n",
        "    \"\"\"Evaluate predictions of the Null method\"\"\"\n",
        "    if (self.use_yield_trend):\n",
        "      alg_name = 'trend'\n",
        "      pred_col_name = 'YIELD_TREND'\n",
        "    else:\n",
        "      alg_name = 'average'\n",
        "      pred_col_name = 'YIELD_PRED'\n",
        "\n",
        "    scores_list = []\n",
        "    for pred_set in pd_pred_dfs:\n",
        "      pred_df = pd_pred_dfs[pred_set]\n",
        "      Y_true = pred_df['YIELD'].values\n",
        "      Y_pred = pred_df[pred_col_name].values\n",
        "      pred_scores = getPredictionScores(Y_true, Y_pred, self.metrics)\n",
        "      scores_list.append(pred_scores)\n",
        "\n",
        "    self.updateAlgorithmsSummary(alg_summary, alg_name, scores_list)\n",
        "\n",
        "  def getYieldTrendML(self, X_train, Y_train, X_test):\n",
        "    \"\"\"\n",
        "    Predict yield trend using a linear model.\n",
        "    No need to scale features. They are all yield values.\n",
        "    \"\"\"\n",
        "    est = Ridge(alpha=1, random_state=42, max_iter=1000,\n",
        "                copy_X=True, fit_intercept=True)\n",
        "    est_param_grid = dict(alpha=[1e-3, 1e-2, 1e-1, 1, 10])\n",
        "\n",
        "    grid_search = GridSearchCV(estimator=est, param_grid=est_param_grid,\n",
        "                               scoring=self.cv_metric, cv=self.custom_cv)\n",
        "    X_train_copy = np.copy(X_train)\n",
        "    fit_params = {}\n",
        "    if (self.use_sample_weights):\n",
        "      fit_params = { 'estimator__sample_weight' : self.train_weights }\n",
        "\n",
        "    with parallel_backend('spark', n_jobs=-1):\n",
        "      grid_search.fit(X_train_copy, np.ravel(Y_train), **fit_params)\n",
        "\n",
        "    best_params = grid_search.best_params_\n",
        "    best_estimator = grid_search.best_estimator_\n",
        "\n",
        "    if (self.verbose > 1):\n",
        "      for param in est_param_grid:\n",
        "        print(param + '=', best_params[param])\n",
        "\n",
        "    Y_pred_train = np.reshape(best_estimator.predict(X_train), (X_train.shape[0], 1))\n",
        "    Y_pred_test = np.reshape(best_estimator.predict(X_test), (X_test.shape[0], 1))\n",
        "\n",
        "    return Y_pred_train, Y_pred_test\n",
        "\n",
        "  def estimateYieldTrendAndDetrend(self, X_train, Y_train, X_test, Y_test, features):\n",
        "    \"\"\"Estimate yield trend using machine learning and detrend\"\"\"\n",
        "    trend_window = self.trend_windows[0]\n",
        "    # NOTE assuming previous years' yield values are at the end\n",
        "    X_train_trend = X_train[:, -trend_window:]\n",
        "    X_test_trend = X_test[:, -trend_window:]\n",
        "\n",
        "    Y_train_trend, Y_test_trend = self.getYieldTrendML(X_train_trend, Y_train, X_test_trend)\n",
        "    # New features exclude previous years' yield and include YIELD_TREND\n",
        "    features_n = features[:-trend_window] + ['YIELD_TREND']\n",
        "    X_train_n = np.append(X_train[:, :-trend_window], Y_train_trend, axis=1)\n",
        "    X_test_n = np.append(X_test[:, :-trend_window], Y_test_trend, axis=1)\n",
        "    Y_train_res = np.reshape(Y_train, (X_train.shape[0], 1)) - Y_train_trend\n",
        "    Y_test_res = np.reshape(Y_test, (X_test.shape[0], 1)) - Y_test_trend\n",
        "\n",
        "    result =  {\n",
        "        'X_train' : X_train_n,\n",
        "        'Y_train' : Y_train_res,\n",
        "        'Y_train_trend' : Y_train_trend,\n",
        "        'X_test' : X_test_n,\n",
        "        'Y_test' : Y_test_res,\n",
        "        'Y_test_trend' : Y_test_trend,\n",
        "        'features' : features_n,\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "  def yieldPredictionsFromResiduals(self, pd_train_df, Y_train, pd_test_df, Y_test,\n",
        "                                      pd_cv_df, cv_test_years):\n",
        "    \"\"\"Predictions are residuals. Add trend back to get yield predictions.\"\"\"\n",
        "    pd_train_df['YIELD_RES'] = pd_train_df['YIELD']\n",
        "    pd_train_df['YIELD'] = Y_train\n",
        "    Y_custom_cv = pd_train_df[pd_train_df['FYEAR'].isin(cv_test_years)]['YIELD'].values\n",
        "    pd_cv_df['YIELD_RES'] = pd_cv_df['YIELD']\n",
        "    pd_cv_df['YIELD'] = Y_custom_cv\n",
        "    pd_test_df['YIELD_RES'] = pd_test_df['YIELD']\n",
        "    pd_test_df['YIELD'] = Y_test\n",
        "\n",
        "    for alg in self.estimators:\n",
        "      pd_train_df['YIELD_RES_PRED_' + alg] = pd_train_df['YIELD_PRED_' + alg]\n",
        "      pd_train_df['YIELD_PRED_' + alg] = pd_train_df['YIELD_RES_PRED_' + alg] + pd_train_df['YIELD_TREND']\n",
        "      pd_test_df['YIELD_RES_PRED_' + alg] = pd_test_df['YIELD_PRED_' + alg]\n",
        "      pd_test_df['YIELD_PRED_' + alg] = pd_test_df['YIELD_RES_PRED_' + alg] + pd_test_df['YIELD_TREND']\n",
        "      pd_cv_df['YIELD_RES_PRED_' + alg] = pd_cv_df['YIELD_PRED_' + alg]\n",
        "      pd_cv_df['YIELD_PRED_' + alg] = pd_cv_df['YIELD_RES_PRED_' + alg] + pd_cv_df['YIELD_TREND']\n",
        "\n",
        "    sel_cols = ['IDREGION', 'FYEAR', 'YIELD_TREND', 'YIELD_RES']\n",
        "    for alg in self.estimators:\n",
        "      sel_cols += ['YIELD_RES_PRED_' + alg, 'YIELD_PRED_' + alg]\n",
        "\n",
        "    sel_cols.append('YIELD')\n",
        "    result = {\n",
        "        'train' : pd_train_df[sel_cols],\n",
        "        'custom_cv' : pd_cv_df[sel_cols],\n",
        "        'test' : pd_test_df[sel_cols],\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "  def updateFeatureSelectionInfo(self, est_name, features, selected_indices,\n",
        "                                 ft_selection_counts, ft_importances, log_fh):\n",
        "    \"\"\"\n",
        "    Update feature selection counts.\n",
        "    Print selected features and importance.\n",
        "    \"\"\"\n",
        "    # update feature selection counts\n",
        "    for idx in selected_indices:\n",
        "      ft_count = 0\n",
        "      ft = features[idx]\n",
        "      ft_period = 'static'\n",
        "      for p in ft_selection_counts:\n",
        "        if p in ft:\n",
        "          ft_period = p\n",
        "\n",
        "      if (ft in ft_selection_counts[ft_period]):\n",
        "        ft_count = ft_selection_counts[ft_period][ft]\n",
        "\n",
        "      ft_selection_counts[ft_period][ft] = ft_count + 1\n",
        "\n",
        "    if (ft_importances is not None):\n",
        "      ft_importance_indices = []\n",
        "      ft_importance_values = [0.0 for i in range(len(features))]\n",
        "      for idx in reversed(np.argsort(ft_importances)):\n",
        "        ft_importance_indices.append(selected_indices[idx])\n",
        "        ft_importance_values[selected_indices[idx]] = str(np.round(ft_importances[idx], 2))\n",
        "\n",
        "      ft_importance_info = '\\nSelected features with importance:'\n",
        "      ft_importance_info += '\\n----------------------------------'\n",
        "      log_fh.write(ft_importance_info)\n",
        "      print(ft_importance_info)\n",
        "      printInGroups(features, ft_importance_indices, ft_importance_values, log_fh)\n",
        "    else:\n",
        "      sel_fts_info = '\\nSelected Features:'\n",
        "      sel_fts_info += '\\n-------------------'\n",
        "      log_fh.write(sel_fts_info)\n",
        "      print(sel_fts_info)\n",
        "      printInGroups(features, selected_indices, log_fh=log_fh)\n",
        "\n",
        "  def getCustomCVPredictions(self, est_name, best_est,\n",
        "                             X_train, Y_train, Y_cv_full):\n",
        "    \"\"\"Get predictions for custom cv test years\"\"\"\n",
        "    Y_pred_cv = np.zeros(Y_cv_full.shape[0])\n",
        "    fit_predict_args = []\n",
        "    for i in range(len(self.custom_cv)):\n",
        "      cv_train_idxs, cv_test_idxs = self.custom_cv[i]\n",
        "      sample_weights = None\n",
        "      if (self.use_sample_weights):\n",
        "        sample_weights = np.copy(self.train_weights[cv_train_idxs])\n",
        "\n",
        "      fit_params = {}\n",
        "      if (self.use_sample_weights and (est_name != 'KNN')):\n",
        "        fit_params = { 'estimator__sample_weight' : sample_weights }\n",
        "\n",
        "      fit_predict_args.append(\n",
        "          {\n",
        "              'X_train' : np.copy(X_train[cv_train_idxs, :]),\n",
        "              'Y_train' : np.copy(Y_train[cv_train_idxs]),\n",
        "              'X_test' : np.copy(X_train[cv_test_idxs, :]),\n",
        "              'fit_params' : fit_params,\n",
        "              'estimator' : deepcopy(best_est),\n",
        "          }\n",
        "      )\n",
        "\n",
        "    pool = mp.Pool(len(self.custom_cv))\n",
        "    Y_preds = pool.map(customFitPredict, fit_predict_args)\n",
        "    for i in range(len(self.custom_cv)):\n",
        "      cv_train_idxs, cv_test_idxs = self.custom_cv[i]\n",
        "      Y_pred_cv[cv_test_idxs] = Y_preds[i]\n",
        "\n",
        "    # clean up\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    return Y_pred_cv\n",
        "\n",
        "  def getPerTestYearPredictions(self, est_name, best_est,\n",
        "                                X_train, Y_train_full,\n",
        "                                X_test, Y_test_full, test_years):\n",
        "    \"\"\"For each test year, fit best_est on all previous years and predict\"\"\"\n",
        "    Y_train = Y_train_full[:, -1]\n",
        "    Y_test = Y_test_full[:, -1]\n",
        "    Y_pred_test = np.zeros(Y_test_full.shape[0])\n",
        "\n",
        "    # For the first test year, X_train and Y_train do not change. No need to refit.\n",
        "    test_indexes = np.where(Y_test_full[:, 1] == test_years[0])[0]\n",
        "    Y_pred_first_yr = best_est.predict(X_test[test_indexes, :])\n",
        "    Y_pred_test[test_indexes] = Y_pred_first_yr\n",
        "\n",
        "    if (est_name == 'GBDT'):\n",
        "      best_est.named_steps['estimator'].set_params(**{ 'warm_start' : True })\n",
        "\n",
        "    fit_predict_args = []\n",
        "    for i in range(1, len(test_years)):\n",
        "      extra_train_years = test_years[:i]\n",
        "      test_indexes = np.where(Y_test_full[:, 1] == test_years[i])[0]\n",
        "      sample_weights = None\n",
        "      if (self.use_sample_weights):\n",
        "        sample_weights = self.train_weights\n",
        "\n",
        "      train_indexes_n = np.ravel(np.nonzero(np.isin(Y_test_full[:, 1], extra_train_years)))\n",
        "      X_train_n = np.append(X_train, X_test[train_indexes_n, :], axis=0)\n",
        "      Y_train_n = np.append(Y_train, Y_test[train_indexes_n])\n",
        "      if (self.use_sample_weights):\n",
        "        sample_weights = np.append(sample_weights, self.test_weights[train_indexes_n], axis=0)\n",
        "\n",
        "      fit_params = {}\n",
        "      if (self.use_sample_weights and (est_name != 'KNN')):\n",
        "        fit_params['estimator__sample_weight'] = sample_weights\n",
        "\n",
        "      fit_predict_args.append(\n",
        "          {\n",
        "              'X_train' : np.copy(X_train_n),\n",
        "              'Y_train' : np.copy(Y_train_n),\n",
        "              'X_test' : np.copy(X_test[test_indexes, :]),\n",
        "              'fit_params' : fit_params,\n",
        "              'estimator' : deepcopy(best_est),\n",
        "          }\n",
        "      )\n",
        "\n",
        "    pool = mp.Pool(len(test_years) - 1)\n",
        "    Y_preds = pool.map(customFitPredict, fit_predict_args)\n",
        "    for i in range(1, len(test_years)):\n",
        "      test_indexes = np.where(Y_test_full[:, 1] == test_years[i])[0]\n",
        "      Y_pred_test[test_indexes] = Y_preds[i-1]\n",
        "\n",
        "    # clean up\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    return Y_pred_test\n",
        "\n",
        "  def combineAlgorithmPredictions(self, pd_ml_predictions, pd_alg_predictions, alg):\n",
        "    \"\"\"Combine predictions of ML algorithms.\"\"\"\n",
        "    join_cols = ['IDREGION', 'FYEAR']\n",
        "\n",
        "    if (pd_ml_predictions is None):\n",
        "      pd_ml_predictions = pd_alg_predictions\n",
        "      pd_ml_predictions = pd_ml_predictions.rename(columns={'YIELD_PRED': 'YIELD_PRED_' + alg })\n",
        "    else:\n",
        "      pd_alg_predictions = pd_alg_predictions[join_cols + ['YIELD_PRED']]\n",
        "      pd_ml_predictions = pd_ml_predictions.merge(pd_alg_predictions, on=join_cols)\n",
        "      pd_ml_predictions = pd_ml_predictions.rename(columns={'YIELD_PRED': 'YIELD_PRED_' + alg })\n",
        "      # Put YIELD at the end\n",
        "      all_cols = list(pd_ml_predictions.columns)\n",
        "      col_order = all_cols[:-2] + ['YIELD_PRED_' + alg, 'YIELD']\n",
        "      pd_ml_predictions = pd_ml_predictions[col_order]\n",
        "\n",
        "    return pd_ml_predictions\n",
        "\n",
        "  def getMLPredictions(self, X_train, Y_train_full, X_test, Y_test_full,\n",
        "                       cv_test_years, cyp_ftsel, features, log_fh):\n",
        "    \"\"\"Train and evaluate crop yield prediction algorithms\"\"\"\n",
        "    # Y_*_full\n",
        "    # IDREGION, FYEAR, YIELD_TREND, YIELD_PRED_Ridge, ..., YIELD_PRED_GBDT, YIELD\n",
        "    # NL11, NL12, NL13 (some regions can have missing values)\n",
        "    # 1999, ..., 2011 => training\n",
        "    # 2012, ..., 2018 => Test\n",
        "    # We need to aggregate to national level. Need predictions from all regions.\n",
        "    #\n",
        "    # cv_test_years\n",
        "    # 1999, ..., 2006 => 2007\n",
        "    # 1999, ..., 2007 => 2008\n",
        "    # ...\n",
        "    # cv_test_years : [2007, 2008, ..., 2011]\n",
        "\n",
        "    Y_train = Y_train_full[:, -1]\n",
        "    train_years = sorted(np.unique(Y_train_full[:, 1]))\n",
        "    Y_cv_full = np.copy(Y_train_full)\n",
        "    Y_test = Y_test_full[:, -1]\n",
        "    test_years = sorted(np.unique(Y_test_full[:, 1]))\n",
        "\n",
        "    pd_test_predictions = None\n",
        "    pd_cv_predictions = None\n",
        "    pd_train_predictions = None\n",
        "\n",
        "    # feature selection frequency\n",
        "    # NOTE must be in sync with crop calendar periods\n",
        "    ft_selection_counts = {\n",
        "        'static' : {}, 'p0' : {}, 'p1' : {}, 'p2' : {}, 'p3' : {}, 'p4' : {}, 'p5' : {},\n",
        "    }\n",
        "\n",
        "    for est_name in self.estimators:\n",
        "      # feature selection\n",
        "      est = self.estimators[est_name]['estimator']\n",
        "      est_param_grid = self.estimators[est_name]['param_grid']\n",
        "      sel_indices = cyp_ftsel.selectOptimalFeatures(est, est_name, est_param_grid, log_fh)\n",
        "\n",
        "      X_train_sel = X_train[:, sel_indices]\n",
        "      X_test_sel = X_test[:, sel_indices]\n",
        "\n",
        "      # Training and testing\n",
        "      pipeline = Pipeline([(\"scaler\", self.scaler), (\"estimator\", est)])\n",
        "      grid_search = GridSearchCV(estimator=pipeline, param_grid=est_param_grid,\n",
        "                                 scoring=self.cv_metric, cv=self.custom_cv)\n",
        "      X_train_copy = np.copy(X_train_sel)\n",
        "      with parallel_backend('spark', n_jobs=-1):\n",
        "        grid_search.fit(X_train_copy, np.ravel(Y_train))\n",
        "\n",
        "      best_params = grid_search.best_params_\n",
        "      best_est = grid_search.best_estimator_\n",
        "\n",
        "      if (self.verbose > 1):\n",
        "        for param in est_param_grid:\n",
        "          print(param + '=', best_params[param])\n",
        "\n",
        "      # feature importance\n",
        "      ft_importances = None\n",
        "      if ((est_name == 'Ridge') or (est_name == 'Lasso')):\n",
        "        ft_importances = best_est.named_steps['estimator'].coef_\n",
        "      elif (est_name == 'SVR'):\n",
        "        try:\n",
        "          ft_importances = np.ravel(best_est.named_steps['estimator'].coef_)\n",
        "        except AttributeError as e:\n",
        "          ft_importances = None\n",
        "      elif ((est_name == 'GBDT') or (est_name == 'RF') or (est_name == 'ERT')):\n",
        "        ft_importances = best_est.named_steps['estimator'].feature_importances_\n",
        "\n",
        "      self.updateFeatureSelectionInfo(est_name, features, sel_indices, ft_selection_counts,\n",
        "                                      ft_importances, log_fh)\n",
        "\n",
        "      # Predictions\n",
        "      Y_pred_train = best_est.predict(X_train_sel)\n",
        "      Y_pred_test = best_est.predict(X_test_sel)\n",
        "\n",
        "      # custom cv predictions for cv metrics\n",
        "      Y_pred_cv = self.getCustomCVPredictions(est_name, best_est,\n",
        "                                              X_train_sel, Y_train, Y_cv_full)\n",
        "\n",
        "      # per test year predictions\n",
        "      # 1999, ..., 2011 => 2012\n",
        "      # 1999, ..., 2012 => 2013\n",
        "      # ...\n",
        "      if (self.retrain_per_test_year):\n",
        "        Y_pred_test = self.getPerTestYearPredictions(est_name, best_est,\n",
        "                                                     X_train_sel, Y_train_full,\n",
        "                                                     X_test_sel, Y_test_full, test_years)\n",
        "\n",
        "      data_cols = ['IDREGION', 'FYEAR']\n",
        "      if (self.use_yield_trend):\n",
        "        data_cols.append('YIELD_TREND')\n",
        "        Y_train_full_n = np.insert(Y_train_full, 3, Y_pred_train, axis=1)\n",
        "        Y_cv_full_n = np.insert(Y_cv_full, 3, Y_pred_cv, axis=1)\n",
        "        Y_test_full_n = np.insert(Y_test_full, 3, Y_pred_test, axis=1)\n",
        "      else:\n",
        "        Y_train_full_n = np.insert(Y_train_full, 2, Y_pred_train, axis=1)\n",
        "        Y_cv_full_n = np.insert(Y_cv_full, 2, Y_pred_cv, axis=1)\n",
        "        Y_test_full_n = np.insert(Y_test_full, 2, Y_pred_test, axis=1)\n",
        "\n",
        "      data_cols += ['YIELD_PRED', 'YIELD']\n",
        "      Y_cv_full_n = Y_cv_full_n[np.in1d(Y_cv_full_n[:, 1], cv_test_years)]\n",
        "      Y_pred_arrays = [Y_train_full_n, Y_cv_full_n, Y_test_full_n]\n",
        "      pd_pred_dfs = self.createPredictionDataFrames(Y_pred_arrays, data_cols)\n",
        "      pd_train_predictions = self.combineAlgorithmPredictions(pd_train_predictions,\n",
        "                                                              pd_pred_dfs[0], est_name)\n",
        "      pd_cv_predictions = self.combineAlgorithmPredictions(pd_cv_predictions,\n",
        "                                                           pd_pred_dfs[1], est_name)\n",
        "      pd_test_predictions = self.combineAlgorithmPredictions(pd_test_predictions,\n",
        "                                                             pd_pred_dfs[2], est_name)\n",
        "\n",
        "    ft_counts_info = '\\nFeature Selection Frequencies'\n",
        "    ft_counts_info += '\\n-------------------------------'\n",
        "    for ft_period in ft_selection_counts:\n",
        "      ft_count_str = ft_period + ': '\n",
        "      for ft in sorted(ft_selection_counts[ft_period],\n",
        "                       key=ft_selection_counts[ft_period].get, reverse=True):\n",
        "        ft_count_str += ft + '(' + str(ft_selection_counts[ft_period][ft]) + '), '\n",
        "\n",
        "      if (len(ft_selection_counts[ft_period]) > 0):\n",
        "        # drop ', ' from the end\n",
        "        ft_count_str = ft_count_str[:-2]\n",
        "\n",
        "      ft_counts_info += '\\n' + ft_count_str\n",
        "\n",
        "    ft_counts_info += '\\n'\n",
        "    log_fh.write(ft_counts_info)\n",
        "    if (self.verbose > 1):\n",
        "      print(ft_counts_info)\n",
        "\n",
        "    result = {\n",
        "        'train' : pd_train_predictions,\n",
        "        'custom_cv' : pd_cv_predictions,\n",
        "        'test' : pd_test_predictions,\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "  def evaluateMLPredictions(self, pd_pred_dfs, alg_summary):\n",
        "    \"\"\"Evaluate predictions of ML algorithms and add entries to alg_summary.\"\"\"\n",
        "    for alg in self.estimators:\n",
        "      pred_col = 'YIELD_PRED_' + alg\n",
        "      scores_list = []\n",
        "      for pred_set in pd_pred_dfs:\n",
        "        pred_df = pd_pred_dfs[pred_set]\n",
        "        Y_true = pred_df['YIELD'].values\n",
        "        Y_pred = pred_df[pred_col].values\n",
        "        pred_scores = getPredictionScores(Y_true, Y_pred, self.metrics)\n",
        "        scores_list.append(pred_scores)\n",
        "\n",
        "      self.updateAlgorithmsSummary(alg_summary, alg, scores_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpxrWSjsxMdx"
      },
      "source": [
        "#### Run Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HbC8rSxjEpT"
      },
      "source": [
        "#%%writefile run_machine_learning.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from joblibspark import register_spark\n",
        "\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "\n",
        "def dropHighlyCorrelatedFeatures(cyp_config, pd_train_df, pd_test_df,\n",
        "                                 log_fh, corr_method='pearson', corr_thresh=0.95):\n",
        "  \"\"\"Plot correlations. Drop columns that are highly correlated.\"\"\"\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  all_cols = list(pd_train_df.columns)[2:]\n",
        "  avg_cols = [c for c in all_cols if 'avg' in c] + ['YIELD']\n",
        "  max_cols = [c for c in all_cols if 'max' in c] + ['YIELD']\n",
        "  lt_th_cols = [c for c in all_cols if 'Z-' in c] + ['YIELD']\n",
        "  gt_th_cols = [c for c in all_cols if 'Z+' in c] + ['YIELD']\n",
        "  yt_cols = ['YIELD-' + str(i) for i in range(1, 6)]  + ['YIELD']\n",
        "\n",
        "  if (debug_level > 2):\n",
        "    plotCorrelation(pd_train_df, avg_cols)\n",
        "    plotCorrelation(pd_train_df, max_cols)\n",
        "    plotCorrelation(pd_train_df, lt_th_cols)\n",
        "    plotCorrelation(pd_train_df, gt_th_cols)\n",
        "    plotCorrelation(pd_train_df, yt_cols)\n",
        "\n",
        "  # drop highly correlated features\n",
        "  # Based on https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/\n",
        "\n",
        "  corr_columns = [c for c in all_cols if ((c != 'YIELD') and (c != 'YIELD_TREND'))]\n",
        "  corr_matrix = pd_train_df[corr_columns].corr(method=corr_method).abs()\n",
        "  ut_mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "  ut_matrix = corr_matrix.mask(ut_mask)\n",
        "  to_drop = [c for c in ut_matrix.columns if any(ut_matrix[c] > corr_thresh)]\n",
        "  drop_info = '\\nDropping highly correlated features'\n",
        "  drop_info += '\\n' + ', '.join(to_drop)\n",
        "\n",
        "  log_fh.write(drop_info + '\\n')\n",
        "  if ((debug_level > 1) and (to_drop)):\n",
        "    print(drop_info)\n",
        "\n",
        "  pd_train_df = pd_train_df.drop(columns=to_drop)\n",
        "  pd_test_df = pd_test_df.drop(columns=to_drop)\n",
        "\n",
        "  return pd_train_df, pd_test_df\n",
        "\n",
        "def getValidationSplits(cyp_config, pd_train_df, pd_test_df, log_fh):\n",
        "  \"\"\"Split features and label into training and test sets\"\"\"\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  use_sample_weights = cyp_config.useSampleWeights()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "\n",
        "  regions = [reg for reg in pd_train_df['IDREGION'].unique()]\n",
        "  num_regions = len(regions)\n",
        "\n",
        "  original_headers = list(pd_train_df.columns.values)\n",
        "  features = []\n",
        "  labels = []\n",
        "  if (use_yield_trend):\n",
        "    if (use_sample_weights):\n",
        "      features = original_headers[2:-3]\n",
        "      labels = original_headers[:2] + original_headers[-3:-1]\n",
        "    else:\n",
        "      features = original_headers[2:-2]\n",
        "      labels = original_headers[:2] + original_headers[-2:]\n",
        "  else:\n",
        "    if (use_sample_weights):\n",
        "      features = original_headers[2:-2]\n",
        "      labels = original_headers[:2] + original_headers[-2:-1]\n",
        "    else:\n",
        "      features = original_headers[2:-1]\n",
        "      labels = original_headers[:2] + original_headers[-1:]\n",
        "\n",
        "  X_train = pd_train_df[features].values\n",
        "  Y_train = pd_train_df[labels].values\n",
        "  train_weights = None\n",
        "  if (use_sample_weights):\n",
        "    train_weights = pd_train_df['SAMPLE_WEIGHT'].values\n",
        "\n",
        "  train_info = '\\nTraining Data Size: ' + str(len(pd_train_df.index)) + ' rows'\n",
        "  train_info += '\\nX cols: ' + str(X_train.shape[1]) + ', Y cols: ' + str(Y_train.shape[1])\n",
        "  train_info += '\\n' + pd_train_df.head(5).to_string(index=False)\n",
        "  log_fh.write(train_info + '\\n')\n",
        "  if (debug_level > 1):\n",
        "    print(train_info)\n",
        "\n",
        "  X_test = pd_test_df[features].values\n",
        "  Y_test = pd_test_df[labels].values\n",
        "  test_weights = None\n",
        "  if (use_sample_weights):\n",
        "    test_weights = pd_test_df['SAMPLE_WEIGHT'].values\n",
        "\n",
        "  test_info = '\\nTest Data Size: ' + str(len(pd_test_df.index)) + ' rows'\n",
        "  test_info += '\\nX cols: ' + str(X_test.shape[1]) + ', Y cols: ' + str(Y_test.shape[1])\n",
        "  test_info += '\\n' + pd_test_df.head(5).to_string(index=False)\n",
        "  log_fh.write(test_info + '\\n')\n",
        "  if (debug_level > 1):\n",
        "    print(test_info)\n",
        "\n",
        "  # print feature names\n",
        "  num_features = len(features)\n",
        "  indices = [idx for idx in range(num_features)]\n",
        "  feature_info = '\\nAll features'\n",
        "  feature_info += '\\n-------------'\n",
        "  log_fh.write(feature_info)\n",
        "  print(feature_info)\n",
        "  printInGroups(features, indices, log_fh=log_fh)\n",
        "\n",
        "  # num_folds for k-fold cv\n",
        "  num_folds = 5\n",
        "  custom_cv = num_folds\n",
        "  cv_test_years = []\n",
        "  if (use_yield_trend):\n",
        "    cyp_cv_splitter = CYPTrainTestSplitter(cyp_config)\n",
        "    custom_cv, cv_test_years = cyp_cv_splitter.customKFoldValidationSplit(Y_train, num_folds, log_fh)\n",
        "\n",
        "  result = {\n",
        "      'X_train' : X_train,\n",
        "      'Y_train_full' : Y_train,\n",
        "      'train_weights' : train_weights,\n",
        "      'X_test' : X_test,\n",
        "      'Y_test_full' : Y_test,\n",
        "      'test_weights' : test_weights,\n",
        "      'custom_cv' : custom_cv,\n",
        "      'cv_test_years' : cv_test_years,\n",
        "      'features' : features,\n",
        "  }\n",
        "\n",
        "  return result\n",
        "\n",
        "def printAlgorithmsEvaluationSummary(cyp_config, null_preds, ml_preds,\n",
        "                                     log_fh, country_code=None):\n",
        "  \"\"\"Print summary of algorithm evaluation\"\"\"\n",
        "  metrics = cyp_config.getEvaluationMetrics()\n",
        "  country = country_code\n",
        "  if (country_code is None):\n",
        "    country = cyp_config.getCountryCode()\n",
        "\n",
        "  alg_summary = {}\n",
        "  cyp_algeval = CYPAlgorithmEvaluator(cyp_config)\n",
        "  cyp_algeval.evaluateNullMethodPredictions(null_preds, alg_summary)\n",
        "  cyp_algeval.evaluateMLPredictions(ml_preds, alg_summary)\n",
        "  pd_pred_dfs = [ml_preds['train'], ml_preds['custom_cv'], ml_preds['test']]\n",
        "  pred_sets_info = ['Training Set', 'Validation Test Set', 'Test Set']\n",
        "  cyp_algeval.printPredictionDataFrames(pd_pred_dfs, pred_sets_info, log_fh)\n",
        "\n",
        "  alg_df_columns = ['algorithm']\n",
        "  for met in metrics:\n",
        "    alg_df_columns += ['train_' + met, 'cv_' + met, 'test_' + met]\n",
        "\n",
        "  alg_df = pd.DataFrame.from_dict(alg_summary, orient='index', columns=alg_df_columns)\n",
        "\n",
        "  eval_summary_info = '\\nAlgorithm Evaluation Summary for ' + country\n",
        "  eval_summary_info += '\\n-----------------------------------------'\n",
        "  eval_summary_info += '\\n' + alg_df.to_string(index=False) + '\\n'\n",
        "  log_fh.write(eval_summary_info)\n",
        "  print(eval_summary_info)\n",
        "\n",
        "def getMachineLearningPredictions(cyp_config, pd_train_df, pd_test_df, log_fh):\n",
        "  \"\"\"Train and evaluate algorithms\"\"\"\n",
        "  metrics = cyp_config.getEvaluationMetrics()\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  predict_residuals = cyp_config.predictYieldResiduals()\n",
        "  alg_names = list(cyp_config.getEstimators().keys())\n",
        "  use_sample_weights = cyp_config.useSampleWeights()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  country_code = cyp_config.getCountryCode()\n",
        "\n",
        "  # register spark parallel backend\n",
        "  register_spark()\n",
        "\n",
        "  eval_info = '\\nTraining and Evaluation'\n",
        "  eval_info += '\\n-------------------------'\n",
        "  log_fh.write(eval_info)\n",
        "  if (debug_level > 1):\n",
        "    print(eval_info)\n",
        "\n",
        "  data_splits = getValidationSplits(cyp_config, pd_train_df, pd_test_df, log_fh)\n",
        "  X_train = data_splits['X_train']\n",
        "  Y_train_full = data_splits['Y_train_full']\n",
        "  X_test = data_splits['X_test']\n",
        "  Y_test_full = data_splits['Y_test_full']\n",
        "  features = data_splits['features']\n",
        "  custom_cv = data_splits['custom_cv']\n",
        "  cv_test_years = data_splits['cv_test_years']\n",
        "\n",
        "  train_weights = None\n",
        "  test_weights = None\n",
        "  if (use_sample_weights):\n",
        "    train_weights = data_splits['train_weights']\n",
        "    test_weights = data_splits['test_weights']\n",
        "\n",
        "  cyp_algeval = CYPAlgorithmEvaluator(cyp_config, custom_cv, train_weights, test_weights)\n",
        "  null_preds = cyp_algeval.getNullMethodPredictions(Y_train_full, Y_test_full,\n",
        "                                                    cv_test_years, log_fh)\n",
        "\n",
        "  Y_train_full_n = Y_train_full\n",
        "  Y_test_full_n = Y_test_full\n",
        "  if (use_yield_trend and predict_residuals):\n",
        "    result = cyp_algeval.estimateYieldTrendAndDetrend(X_train, Y_train_full[:, -1],\n",
        "                                                      X_test, Y_test_full[:, -1], features)\n",
        "    X_train = result['X_train']\n",
        "    Y_train_full_n = np.copy(Y_train_full)\n",
        "    Y_train_full_n[:, -1] = result['Y_train'][:, 0]\n",
        "    Y_train_full_n[:, -2] = result['Y_train_trend'][:, 0]\n",
        "    X_test = result['X_test']\n",
        "    Y_test_full_n = np.copy(Y_test_full)\n",
        "    Y_test_full_n[:, -1] = result['Y_test'][:,0]\n",
        "    Y_test_full_n[:, -2] = result['Y_test_trend'][:, 0]\n",
        "    features = result['features']\n",
        "\n",
        "  cyp_ftsel = CYPFeatureSelector(cyp_config, X_train, Y_train_full_n[:, -1], features,\n",
        "                                 custom_cv, train_weights)\n",
        "  ml_preds = cyp_algeval.getMLPredictions(X_train, Y_train_full_n, X_test, Y_test_full_n,\n",
        "                                          cv_test_years, cyp_ftsel, features, log_fh)\n",
        "\n",
        "  if (use_yield_trend and predict_residuals):\n",
        "    ml_preds = cyp_algeval.yieldPredictionsFromResiduals(ml_preds['train'],\n",
        "                                                         Y_train_full[:, -1],\n",
        "                                                         ml_preds['test'],\n",
        "                                                         Y_test_full[:, -1],\n",
        "                                                         ml_preds['custom_cv'],\n",
        "                                                         cv_test_years)\n",
        "\n",
        "  if (debug_level > 1):\n",
        "    sel_cols = ['IDREGION', 'FYEAR', 'YIELD'] + ['YIELD_PRED_' + alg for alg in alg_names]\n",
        "    print('\\n', ml_preds['test'][sel_cols].head(5))\n",
        "\n",
        "  ml_preds_test = ml_preds['test']\n",
        "\n",
        "  # print per country evaluation summary\n",
        "  printAlgorithmsEvaluationSummary(cyp_config, null_preds, ml_preds, log_fh)\n",
        "  return ml_preds['test']\n",
        "\n",
        "def saveMLPredictions(cyp_config, sqlCtx, pd_ml_predictions):\n",
        "  \"\"\"Save ML predictions to a CSV file\"\"\"\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  crop = cyp_config.getCropName()\n",
        "  country = cyp_config.getCountryCode()\n",
        "  nuts_level = cyp_config.getNUTSLevel()\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "  early_season_end = cyp_config.getEarlySeasonEndDekad()\n",
        "  ml_algs = cyp_config.getEstimators()\n",
        "\n",
        "  output_path = cyp_config.getOutputPath()\n",
        "  output_file = getPredictionFilename(crop, country, nuts_level, use_yield_trend,\n",
        "                                      early_season_prediction, early_season_end)\n",
        "\n",
        "  save_pred_path = output_path + '/' + output_file\n",
        "  if (debug_level > 1):\n",
        "    print('\\nSaving predictions to', save_pred_path + '.csv')\n",
        "    print(pd_ml_predictions.head(5))\n",
        "\n",
        "  pd_ml_predictions.to_csv(save_pred_path + '.csv', index=False, header=True)\n",
        "\n",
        "  # NOTE: In some environments, Spark can write, but pandas cannot.\n",
        "  # In such cases, use the following code.\n",
        "  # spark_predictions_df = sqlCtx.createDataFrame(pd_ml_predictions)\n",
        "  # spark_predictions_df.coalesce(1)\\\n",
        "  #                     .write.option('header','true')\\\n",
        "  #                     .mode(\"overwrite\").csv(save_pred_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jDs9IO_xfPu"
      },
      "source": [
        "### Load Saved Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOFLzsa8xjT-"
      },
      "source": [
        "#%%writefile load_saved_predictions.py\n",
        "import pandas as pd\n",
        "\n",
        "def loadSavedPredictions(cyp_config, spark):\n",
        "  \"\"\"Load machine learning predictions from saved CSV file\"\"\"\n",
        "  crop = cyp_config.getCropName()\n",
        "  country = cyp_config.getCountryCode()\n",
        "  nuts_level = cyp_config.getNUTSLevel()\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "  early_season_end = cyp_config.getEarlySeasonEndDekad()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "\n",
        "  pred_file_path = cyp_config.getOutputPath()\n",
        "  pred_file = getPredictionFilename(crop, country, nuts_level, use_yield_trend,\n",
        "                                    early_season_prediction, early_season_end)\n",
        "  \n",
        "  pred_file += '.csv'\n",
        "  pd_ml_predictions = pd.read_csv(pred_file_path + '/' + pred_file, header=0)\n",
        "\n",
        "  # NOTE: In some environments, Spark can read, but pandas cannot.\n",
        "  # In such cases, use the following code.\n",
        "  # all_pred_df = spark.read.csv(pred_file_path + '/' + pred_file, header=True, inferSchema=True)\n",
        "  # pd_ml_predictions = all_pred_df.toPandas()\n",
        "\n",
        "  if (debug_level > 1):\n",
        "    print(pd_ml_predictions.head(5))\n",
        "\n",
        "  return pd_ml_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uvKP54QxrUK"
      },
      "source": [
        "### Compare Predictions with MCYFS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj_ZCXfaxuxt"
      },
      "source": [
        "#%%writefile compare_with_mcyfs.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def saveNUTS0Predictions(cyp_config, sqlCtx, nuts0_ml_predictions):\n",
        "  \"\"\"Save predictions aggregated to NUTS0\"\"\"\n",
        "  crop = cyp_config.getCropName()\n",
        "  country = cyp_config.getCountryCode()\n",
        "  nuts_level = 'NUTS0'\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "  early_season_end = cyp_config.getEarlySeasonEndDekad()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "\n",
        "  output_path = cyp_config.getOutputPath()\n",
        "  output_file = getPredictionFilename(crop, country, nuts_level, use_yield_trend,\n",
        "                                      early_season_prediction, early_season_end)\n",
        "\n",
        "  save_pred_path = output_path + '/' + output_file\n",
        "  if (debug_level > 1):\n",
        "    print('\\nNUTS0 Predictions of ML algorithms')\n",
        "    print(nuts0_ml_predictions.head(5))\n",
        "    print('\\nSaving predictions to', save_pred_path + '.csv')\n",
        "\n",
        "  nuts0_ml_predictions.to_csv(save_pred_path + '.csv', index=False, header=True)\n",
        "\n",
        "  # NOTE: In some environments, Spark can write, but pandas cannot.\n",
        "  # In such cases, use the following code.\n",
        "  # spark_predictions_df = sqlCtx.createDataFrame(nuts0_ml_predictions)\n",
        "  # spark_predictions_df.coalesce(1)\\\n",
        "  #                     .write.option('header','true')\\\n",
        "  #                     .mode(\"overwrite\").csv(save_pred_path)\n",
        "\n",
        "def getDataForMCYFSComparison(spark, cyp_config, test_years):\n",
        "  \"\"\"Load and preprocess data for MCYFS comparison\"\"\"\n",
        "  data_path = cyp_config.getDataPath()\n",
        "  crop_id = cyp_config.getCropID()\n",
        "  nuts_level = cyp_config.getNUTSLevel()\n",
        "  season_crosses_calyear = cyp_config.seasonCrossesCalendarYear()\n",
        "  early_season_end = cyp_config.getEarlySeasonEndDekad()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  area_nuts = ['NUTS' + str(i) for i in range(int(nuts_level[-1]), 0, -1)]\n",
        "  data_sources = {\n",
        "      'WOFOST' : nuts_level,\n",
        "      'AREA_FRACTIONS' : area_nuts,\n",
        "      'YIELD' : 'NUTS0',\n",
        "      'YIELD_PRED_MCYFS' : 'NUTS0',\n",
        "  }\n",
        "\n",
        "  if (run_tests):\n",
        "    test_util = TestUtil(spark)\n",
        "    test_util.runAllTests()\n",
        "\n",
        "  print('##############')\n",
        "  print('# Load Data  #')\n",
        "  print('##############')\n",
        "\n",
        "  if (run_tests):\n",
        "    test_loader = TestDataLoader(spark)\n",
        "    test_loader.runAllTests()\n",
        "\n",
        "  cyp_config.setDataSources(data_sources)\n",
        "  cyp_loader = CYPDataLoader(spark, cyp_config)\n",
        "  data_dfs = cyp_loader.loadAllData()\n",
        "\n",
        "  wofost_df = data_dfs['WOFOST']\n",
        "  area_dfs = data_dfs['AREA_FRACTIONS']\n",
        "  nuts0_yield_df = data_dfs['YIELD']\n",
        "  mcyfs_yield_df = data_dfs['YIELD_PRED_MCYFS']\n",
        "\n",
        "  print('####################')\n",
        "  print('# Preprocess Data  #')\n",
        "  print('####################')\n",
        "\n",
        "  if (run_tests):\n",
        "    test_preprocessor = TestDataPreprocessor(spark)\n",
        "    test_preprocessor.runAllTests()\n",
        "\n",
        "  cyp_preprocessor = CYPDataPreprocessor(spark, cyp_config)\n",
        "  wofost_df = wofost_df.filter(wofost_df['CROP_ID'] == crop_id).drop('CROP_ID')\n",
        "  crop_season = cyp_preprocessor.getCropSeasonInformation(wofost_df, season_crosses_calyear)\n",
        "  wofost_df = cyp_preprocessor.preprocessWofost(wofost_df, crop_season, season_crosses_calyear)\n",
        "\n",
        "  for i in range(len(area_dfs)):\n",
        "    af_df = area_dfs[i]\n",
        "    af_df = cyp_preprocessor.preprocessAreaFractions(af_df, crop_id)\n",
        "    af_df = af_df.filter(af_df['FYEAR'].isin(test_years))\n",
        "    area_dfs[i] = af_df\n",
        "\n",
        "  if (debug_level > 1):\n",
        "    print('NUTS0 Yield before preprocessing')\n",
        "    nuts0_yield_df.show(10)\n",
        "\n",
        "  nuts0_yield_df = cyp_preprocessor.preprocessYield(nuts0_yield_df, crop_id)\n",
        "  nuts0_yield_df = nuts0_yield_df.filter(nuts0_yield_df['FYEAR'].isin(test_years))\n",
        "  if (debug_level > 1):\n",
        "    print('NUTS0 Yield after preprocessing')\n",
        "    nuts0_yield_df.show(10)\n",
        "\n",
        "  if (debug_level > 1):\n",
        "    print('MCYFS yield predictions before preprocessing')\n",
        "    mcyfs_yield_df.show(10)\n",
        "\n",
        "  mcyfs_yield_df = cyp_preprocessor.preprocessYieldMCYFS(mcyfs_yield_df, crop_id)\n",
        "  mcyfs_yield_df = mcyfs_yield_df.filter(mcyfs_yield_df['FYEAR'].isin(test_years))\n",
        "  if (debug_level > 1):\n",
        "    print('MCYFS yield predictions after preprocessing')\n",
        "    mcyfs_yield_df.show(10)\n",
        "\n",
        "  # Check we have yield data for crop\n",
        "  assert (nuts0_yield_df is not None)\n",
        "  assert (mcyfs_yield_df is not None)\n",
        "\n",
        "  if (run_tests):\n",
        "    test_summarizer = TestDataSummarizer(spark)\n",
        "    test_summarizer.runAllTests()\n",
        "\n",
        "  cyp_summarizer = CYPDataSummarizer(cyp_config)\n",
        "  dvs_summary = cyp_summarizer.wofostDVSSummary(wofost_df, early_season_end)\n",
        "  dvs_summary = dvs_summary.filter(dvs_summary['CAMPAIGN_YEAR'].isin(test_years))\n",
        "\n",
        "  data_dfs = {\n",
        "      'WOFOST_DVS' : dvs_summary,\n",
        "      'AREA_FRACTIONS' : area_dfs,\n",
        "      'YIELD_NUTS0' : nuts0_yield_df,\n",
        "      'YIELD_PRED_MCYFS' : mcyfs_yield_df\n",
        "  }\n",
        "\n",
        "  return data_dfs\n",
        "\n",
        "def fillMissingDataWithAverage(pd_pred_df, print_debug):\n",
        "  \"\"\"Fill missing data with regional average or zero\"\"\"\n",
        "  regions = pd_pred_df['IDREGION'].unique()\n",
        "\n",
        "  for reg_id in regions:\n",
        "    reg_filter = (pd_pred_df['IDREGION'] == reg_id)\n",
        "    pd_reg_pred_df = pd_pred_df[reg_filter]\n",
        "\n",
        "    if (len(pd_reg_pred_df[pd_reg_pred_df['YIELD_PRED'].notnull()].index) == 0):\n",
        "      if (print_debug):\n",
        "        print('No data for', reg_id)\n",
        "\n",
        "      pd_pred_df.loc[reg_filter, 'FRACTION'] = 0.0\n",
        "      pd_pred_df.loc[reg_filter, 'YIELD_PRED'] = 0.0\n",
        "    else:\n",
        "      reg_avg_yield_pred = pd_pred_df.loc[reg_filter, 'YIELD_PRED'].mean()\n",
        "      pd_pred_df.loc[reg_filter, 'YIELD_PRED'] = pd_pred_df.loc[reg_filter, 'YIELD_PRED']\\\n",
        "                                                           .fillna(reg_avg_yield_pred)\n",
        "\n",
        "  return pd_pred_df\n",
        "\n",
        "def recalculateAreaFractions(pd_pred_df, print_debug):\n",
        "  \"\"\"Recalculate area fractions by excluding regions with missing data\"\"\"\n",
        "  join_cols = ['IDREG_PARENT', 'FYEAR']\n",
        "  pd_af_sum = pd_pred_df.groupby(join_cols).agg(FRACTION_SUM=('FRACTION', 'sum')).reset_index()\n",
        "  pd_pred_df = pd_pred_df.merge(pd_af_sum, on=join_cols, how='left')\n",
        "  pd_pred_df['FRACTION'] = pd_pred_df['FRACTION'] / pd_pred_df['FRACTION_SUM']\n",
        "  pd_pred_df = pd_pred_df.drop(columns=['FRACTION_SUM'])\n",
        "\n",
        "  return pd_pred_df\n",
        "\n",
        "def aggregatePredictionsToNUTS0(cyp_config, pd_ml_predictions,\n",
        "                                area_dfs, test_years, join_cols):\n",
        "  \"\"\"Aggregate regional predictions to national level\"\"\"\n",
        "  pd_area_dfs = []\n",
        "  nuts_level = cyp_config.getNUTSLevel()\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  crop_id = cyp_config.getCropID()\n",
        "  alg_names = list(cyp_config.getEstimators().keys())\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "\n",
        "  for af_df in area_dfs:\n",
        "    pd_af_df = af_df.toPandas()\n",
        "    pd_af_df = pd_af_df[pd_af_df['FYEAR'].isin(test_years)]\n",
        "    pd_area_dfs.append(pd_af_df)\n",
        "\n",
        "  nuts0_pred_df = None\n",
        "  for alg in alg_names:\n",
        "    sel_cols = ['IDREGION', 'FYEAR', 'YIELD_PRED_' + alg]\n",
        "    pd_alg_pred_df = pd_ml_predictions[sel_cols]\n",
        "    pd_alg_pred_df = pd_alg_pred_df.rename(columns={'YIELD_PRED_' + alg : 'YIELD_PRED'})\n",
        "\n",
        "    for idx in range(len(pd_area_dfs)):\n",
        "      pd_af_df = pd_area_dfs[idx]\n",
        "      # merge with area fractions to get all regions and years\n",
        "      pd_alg_pred_df = pd_af_df.merge(pd_alg_pred_df, on=join_cols)\n",
        "      print_debug = (debug_level > 2) and (alg == alg_names[0])\n",
        "      pd_alg_pred_df = fillMissingDataWithAverage(pd_alg_pred_df, print_debug)\n",
        "      pd_alg_pred_df['IDREG_PARENT'] = pd_alg_pred_df['IDREGION'].str[:-1]\n",
        "      pd_alg_pred_df = recalculateAreaFractions(pd_alg_pred_df, print_debug)\n",
        "      if (print_debug):\n",
        "        print('\\nAggregation to NUTS' + str(len(pd_area_dfs) - (idx + 1)))\n",
        "        print(pd_alg_pred_df[pd_alg_pred_df['FYEAR'] == test_years[0]].head(10))\n",
        "\n",
        "      pd_alg_pred_df['YPRED_WEIGHTED'] = pd_alg_pred_df['YIELD_PRED'] * pd_alg_pred_df['FRACTION']\n",
        "      pd_alg_pred_df = pd_alg_pred_df.groupby(by=['IDREG_PARENT', 'FYEAR'])\\\n",
        "                                     .agg(YPRED_WEIGHTED=('YPRED_WEIGHTED', 'sum')).reset_index()\n",
        "      pd_alg_pred_df = pd_alg_pred_df.rename(columns={'IDREG_PARENT': 'IDREGION',\n",
        "                                                      'YPRED_WEIGHTED': 'YIELD_PRED' })\n",
        "\n",
        "    pd_alg_pred_df = pd_alg_pred_df.rename(columns={ 'YIELD_PRED': 'YIELD_PRED_' + alg })\n",
        "    if (nuts0_pred_df is None):\n",
        "      nuts0_pred_df = pd_alg_pred_df\n",
        "    else:\n",
        "      nuts0_pred_df = nuts0_pred_df.merge(pd_alg_pred_df, on=join_cols)\n",
        "\n",
        "  return nuts0_pred_df\n",
        "\n",
        "def getMCYFSPrediction(pd_mcyfs_pred_df, pred_year, pred_dekad, print_debug):\n",
        "  \"\"\"Get MCYFS prediction for given year with prediction date close to pred_dekad\"\"\"\n",
        "  pd_pred_year = pd_mcyfs_pred_df[pd_mcyfs_pred_df['FYEAR'] == pred_year]\n",
        "  mcyfs_pred_dekads = pd_pred_year['PRED_DEKAD'].unique()\n",
        "  if (len(mcyfs_pred_dekads) == 0):\n",
        "    return 0.0\n",
        "\n",
        "  mcyfs_pred_dekads = sorted(mcyfs_pred_dekads)\n",
        "  mcyfs_pred_dekad = mcyfs_pred_dekads[-1]\n",
        "  if (pred_dekad < mcyfs_pred_dekad):\n",
        "    for dek in mcyfs_pred_dekads:\n",
        "      if dek >= pred_dekad:\n",
        "        mcyfs_pred_dekad = dek\n",
        "        break\n",
        "\n",
        "  pd_pred_dek = pd_pred_year[pd_pred_year['PRED_DEKAD'] == mcyfs_pred_dekad]\n",
        "  yield_pred_list = pd_pred_dek['YIELD_PRED'].values\n",
        "\n",
        "  if (print_debug):\n",
        "    print('\\nAll MCYFS dekads for', pred_year, ':', mcyfs_pred_dekads)\n",
        "    print('MCYFS prediction dekad', mcyfs_pred_dekad)\n",
        "    print('ML Baseline prediction dekad', pred_dekad)\n",
        "    print('MCYFS prediction:', yield_pred_list[0], '\\n')\n",
        "\n",
        "  return yield_pred_list[0]\n",
        "\n",
        "def getNUTS0Yield(pd_nuts0_yield_df, pred_year, print_debug):\n",
        "  \"\"\"Get the true (reported) Eurostat yield value\"\"\"\n",
        "  nuts0_yield_year = pd_nuts0_yield_df[pd_nuts0_yield_df['FYEAR'] == pred_year]\n",
        "  pred_year_yield = nuts0_yield_year['YIELD'].values\n",
        "  if (len(pred_year_yield) == 0):\n",
        "    return 0.0\n",
        "\n",
        "  if (print_debug):\n",
        "    print(pred_year, 'Eurostat yield', pred_year_yield[0])\n",
        "\n",
        "  return pred_year_yield[0]\n",
        "\n",
        "def comparePredictionsWithMCYFS(sqlCtx, cyp_config, pd_ml_predictions, log_fh):\n",
        "  \"\"\"Compare ML Baseline predictions with MCYFS predictions\"\"\"\n",
        "  # We need AREA_FRACTIONS, MCYFS yield predictions and NUTS0 Eurostat YIELD\n",
        "  # for comparison with MCYFS\n",
        "  country_code = cyp_config.getCountryCode()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  alg_names = list(cyp_config.getEstimators().keys())\n",
        "  test_years = list(pd_ml_predictions['FYEAR'].unique())\n",
        "  early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "\n",
        "  spark = sqlCtx.sparkSession\n",
        "  data_dfs = getDataForMCYFSComparison(spark, cyp_config, test_years)\n",
        "  pd_dvs_summary = data_dfs['WOFOST_DVS'].toPandas()\n",
        "  pd_nuts0_yield_df = data_dfs['YIELD_NUTS0'].toPandas()\n",
        "  pd_mcyfs_pred_df = data_dfs['YIELD_PRED_MCYFS'].toPandas()\n",
        "  area_dfs = data_dfs['AREA_FRACTIONS']\n",
        "  join_cols = ['IDREGION', 'FYEAR']\n",
        "  test_years = pd_ml_predictions['FYEAR'].unique()\n",
        "  metrics = cyp_config.getEvaluationMetrics()\n",
        "  nuts0_pred_df = aggregatePredictionsToNUTS0(cyp_config, pd_ml_predictions,\n",
        "                                              area_dfs, test_years, join_cols)\n",
        "\n",
        "  crop_season_cols = ['IDREGION', 'CAMPAIGN_YEAR', 'CALENDAR_END_SEASON', 'CALENDAR_EARLY_SEASON']\n",
        "  pd_dvs_summary = pd_dvs_summary[crop_season_cols].rename(columns={ 'CAMPAIGN_YEAR' : 'FYEAR' })\n",
        "  pd_dvs_summary = pd_dvs_summary.groupby('FYEAR').agg(END_SEASON=('CALENDAR_END_SEASON', 'mean'),\n",
        "                                                       EARLY_SEASON=('CALENDAR_EARLY_SEASON', 'mean'))\\\n",
        "                                                       .round(0).reset_index()\n",
        "  if (debug_level > 1):\n",
        "    print(pd_dvs_summary.head(5).to_string(index=False))\n",
        "\n",
        "  alg_summary = {}\n",
        "  Y_pred_mcyfs = []\n",
        "  Y_true = []\n",
        "  nuts0_pred_df['YIELD_PRED_MCYFS'] = 0.0\n",
        "  nuts0_pred_df['YIELD'] = 0.0\n",
        "  nuts0_pred_df = nuts0_pred_df.sort_values(by=join_cols)\n",
        "  ml_pred_years = nuts0_pred_df['FYEAR'].unique()\n",
        "  mcyfs_pred_years = []\n",
        "  print_debug = (debug_level > 2)\n",
        "  if (print_debug):\n",
        "    print('\\nPredictions and true values for', country_code)\n",
        "\n",
        "  for yr in ml_pred_years:\n",
        "    pred_dekad = pd_dvs_summary[pd_dvs_summary['FYEAR'] == yr]['END_SEASON'].values[0]\n",
        "    if (early_season_prediction):\n",
        "      pred_dekad = pd_dvs_summary[pd_dvs_summary['FYEAR'] == yr]['EARLY_SEASON'].values[0]\n",
        "\n",
        "    mcyfs_pred = getMCYFSPrediction(pd_mcyfs_pred_df, yr, pred_dekad, print_debug)\n",
        "    nuts0_yield = getNUTS0Yield(pd_nuts0_yield_df, yr, print_debug)\n",
        "    if ((mcyfs_pred > 0.0) and (nuts0_yield > 0.0)):\n",
        "      nuts0_pred_df.loc[nuts0_pred_df['FYEAR'] == yr, 'YIELD'] = nuts0_yield\n",
        "      nuts0_pred_df.loc[nuts0_pred_df['FYEAR'] == yr, 'YIELD_PRED_MCYFS'] = mcyfs_pred\n",
        "      mcyfs_pred_years.append(yr)\n",
        "\n",
        "  nuts0_pred_df = nuts0_pred_df[nuts0_pred_df['FYEAR'].isin(mcyfs_pred_years)]\n",
        "  Y_true = nuts0_pred_df['YIELD'].values\n",
        "\n",
        "  if (print_debug):\n",
        "    print(nuts0_pred_df.head(5))\n",
        "\n",
        "  if (len(mcyfs_pred_years) > 0):\n",
        "    for alg in alg_names:\n",
        "      Y_pred_alg = nuts0_pred_df['YIELD_PRED_' + alg].values\n",
        "      alg_nuts0_scores = getPredictionScores(Y_true, Y_pred_alg, metrics)\n",
        "\n",
        "      alg_row = [alg]\n",
        "      for met in alg_nuts0_scores:\n",
        "        alg_row.append(alg_nuts0_scores[met])\n",
        "\n",
        "      alg_index = len(alg_summary)\n",
        "      alg_summary['row' + str(alg_index)] = alg_row\n",
        "\n",
        "    Y_pred_mcyfs = nuts0_pred_df['YIELD_PRED_MCYFS'].values\n",
        "    mcyfs_nuts0_scores = getPredictionScores(Y_true, Y_pred_mcyfs, metrics)\n",
        "    alg_row = ['MCYFS_Predictions']\n",
        "    for met in mcyfs_nuts0_scores:\n",
        "      alg_row.append(mcyfs_nuts0_scores[met])\n",
        "\n",
        "    alg_index = len(alg_summary)\n",
        "    alg_summary['row' + str(alg_index)] = alg_row\n",
        "\n",
        "    alg_df_columns = ['algorithm']\n",
        "    for met in metrics:\n",
        "      alg_df_columns += ['test_' + met]\n",
        "\n",
        "    alg_df = pd.DataFrame.from_dict(alg_summary, orient='index',\n",
        "                                    columns=alg_df_columns)\n",
        "    eval_summary_info = '\\nAlgorithm Evaluation Summary (NUTS0) for ' + country_code\n",
        "    eval_summary_info += '\\n-------------------------------------------'\n",
        "    eval_summary_info += '\\n' + alg_df.to_string(index=False) + '\\n'\n",
        "    log_fh.write(eval_summary_info)\n",
        "    print(eval_summary_info)\n",
        "\n",
        "  save_predictions = cyp_config.savePredictions()\n",
        "  if (save_predictions):\n",
        "    saveNUTS0Predictions(cyp_config, sqlCtx, nuts0_pred_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7FqdmXOjYUm"
      },
      "source": [
        "## Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPwEyntUYHsS"
      },
      "source": [
        "### Test Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx6fS731YHsV"
      },
      "source": [
        "#%%writefile test_util.py\n",
        "import numpy as np\n",
        "\n",
        "class TestUtil():\n",
        "  def __init__(self, spark):\n",
        "    self.good_date = spark.createDataFrame([(1, '19940102'),\n",
        "                                          (2, '15831224')],\n",
        "                                         ['ID', 'DATE'])\n",
        "    self.bad_date = spark.createDataFrame([(1, '14341224'),\n",
        "                                           (2, '12345678'),\n",
        "                                          (3, '123-12-24')],\n",
        "                                         ['ID', 'DATE'])\n",
        "\n",
        "  def testDateFormat(self):\n",
        "    print('\\n Test Date Format')\n",
        "    self.good_date = self.good_date.withColumn('FYEAR', getYear('DATE'))\n",
        "    self.good_date.show()\n",
        "    self.bad_date = self.bad_date.withColumn('FYEAR', getYear('DATE'))\n",
        "    self.bad_date.show()\n",
        "    assert (self.bad_date.filter(self.bad_date.FYEAR.isNull()).count() == 2)\n",
        "    self.bad_date = self.bad_date.withColumn('MONTH', getMonth('DATE'))\n",
        "    self.bad_date.show()\n",
        "    assert (self.bad_date.filter(self.bad_date.MONTH.isNull()).count() == 2)\n",
        "    self.bad_date = self.bad_date.withColumn('DAY', getDay('DATE'))\n",
        "    # check the day here for first date, it's incorrect\n",
        "    # seems to be a Spark issue\n",
        "    self.bad_date.show()\n",
        "    assert (self.bad_date.filter(self.bad_date.DAY.isNull()).count() == 2)\n",
        "    self.bad_date = self.bad_date.withColumn('DEKAD', getDekad('DATE'))\n",
        "    self.bad_date.show()\n",
        "    assert (self.bad_date.filter(self.bad_date.DEKAD.isNull()).count() == 2)\n",
        "\n",
        "  def testGetYear(self):\n",
        "    print('\\n Test getYear')\n",
        "    self.good_date = self.good_date.withColumn('FYEAR', getYear('DATE'))\n",
        "    self.good_date.show()\n",
        "    year1 = self.good_date.filter(self.good_date.ID == 1).select('FYEAR').collect()[0][0]\n",
        "    assert year1 == 1994\n",
        "    year2 = self.good_date.filter(self.good_date.ID == 2).select('FYEAR').collect()[0][0]\n",
        "    assert year2 == 1583\n",
        "\n",
        "  def testGetMonth(self):\n",
        "    print('\\n Test getMonth')\n",
        "    self.good_date = self.good_date.withColumn('MONTH', getMonth('DATE'))\n",
        "    self.good_date.show()\n",
        "    month1 = self.good_date.filter(self.good_date.ID == 1).select('MONTH').collect()[0][0]\n",
        "    assert month1 == 1\n",
        "    month2 = self.good_date.filter(self.good_date.ID == 2).select('MONTH').collect()[0][0]\n",
        "    assert month2 == 12\n",
        "\n",
        "  def testGetDay(self):\n",
        "    print('\\n Test getDay')\n",
        "    self.good_date = self.good_date.withColumn('DAY', getDay('DATE'))\n",
        "    self.good_date.show()\n",
        "    day1 = self.good_date.filter(self.good_date.ID == 1).select('DAY').collect()[0][0]\n",
        "    assert day1 == 2\n",
        "    day2 = self.good_date.filter(self.good_date.ID == 2).select('DAY').collect()[0][0]\n",
        "    assert day2 == 24\n",
        "\n",
        "  def testGetDekad(self):\n",
        "    print('\\n Test getDekad')\n",
        "    self.good_date = self.good_date.withColumn('DEKAD', getDekad('DATE'))\n",
        "    self.good_date.show()\n",
        "    dekad1 = self.good_date.filter(self.good_date.ID == 1).select('DEKAD').collect()[0][0]\n",
        "    assert dekad1 == 1\n",
        "    dekad2 = self.good_date.filter(self.good_date.ID == 2).select('DEKAD').collect()[0][0]\n",
        "    assert dekad2 == 36\n",
        "\n",
        "  def testCropIDToName(self):\n",
        "    print('\\n Test cropIDToName')\n",
        "    crop_name = cropIDToName(crop_name_dict, 6)\n",
        "    print(6, ':' + crop_name)\n",
        "    assert crop_name == 'sugarbeet'\n",
        "    crop_name = cropIDToName(crop_name_dict, 8)\n",
        "    print(8, ':' + crop_name)\n",
        "    assert crop_name == 'NA'\n",
        "\n",
        "  def testCropNameToID(self):\n",
        "    print('\\n Test cropNameToID')\n",
        "    crop_id = cropNameToID(crop_id_dict, 'Potatoes')\n",
        "    print('Potatoes:', crop_id)\n",
        "    assert crop_id == 7\n",
        "    crop_id = cropNameToID(crop_id_dict, 'Soybean')\n",
        "    print('Soybean:', crop_id)\n",
        "    assert crop_id == 0\n",
        "\n",
        "  def testPrintInGroups(self):\n",
        "    print('\\n Test printInGroups')\n",
        "    features = ['feat' + str(i+1) for i in range(15)]\n",
        "    num_features = len(features)\n",
        "    num_half = np.cast['int64'](np.floor(num_features/2))\n",
        "    indices1 = [ i for i in range(num_features)]\n",
        "    indices2 = [ 2*i for i in range(num_half)]\n",
        "    indices3 = [ (2*i + 1) for i in range(num_half)]\n",
        "\n",
        "    printInGroups(features, indices1)\n",
        "    printInGroups(features, indices2)\n",
        "    printInGroups(features, indices3)\n",
        "\n",
        "  def testPlotTrend(self):\n",
        "    print('\\n Test plotTrend')\n",
        "    years = [yr for yr in range(2000, 2010)]\n",
        "    trend_values = [ (i + 1) for i in range(50, 60)]\n",
        "    actual_values = []\n",
        "    for tval in trend_values:\n",
        "      if (tval % 2) == 0:\n",
        "        actual_values.append(tval + 0.5)\n",
        "      else:\n",
        "        actual_values.append(tval - 0.5)\n",
        "\n",
        "    plotTrend(years, actual_values, trend_values, 'YIELD')\n",
        "\n",
        "  def testPlotTrueVSPredicted(self):\n",
        "    print('\\n Test plotTrueVSPredicted')\n",
        "    Y_true = [ (i + 1) for i in range(50, 60)]\n",
        "    Y_predicted = []\n",
        "    for tval in Y_true:\n",
        "      if (tval % 2) == 0:\n",
        "        Y_predicted.append(tval + 0.5)\n",
        "      else:\n",
        "        Y_predicted.append(tval - 0.5)\n",
        "\n",
        "    Y_true = np.asarray(Y_true)\n",
        "    Y_predicted = np.asarray(Y_predicted)\n",
        "\n",
        "    plotTrueVSPredicted(Y_true, Y_predicted)\n",
        "\n",
        "  def runAllTests(self):\n",
        "    print('\\nTest Utility Functions BEGIN\\n')\n",
        "    self.testDateFormat()\n",
        "    self.testGetYear()\n",
        "    self.testGetMonth()\n",
        "    self.testGetDay()\n",
        "    self.testGetDekad()\n",
        "    self.testCropIDToName()\n",
        "    self.testCropNameToID()\n",
        "    self.testPrintInGroups()\n",
        "    self.testPlotTrend()\n",
        "    self.testPlotTrueVSPredicted()\n",
        "    print('\\nTest Utility Functions END\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iW23SQzYN09"
      },
      "source": [
        "### Test Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWKTClTNYN0_"
      },
      "source": [
        "#%%writefile test_data_loading.py\n",
        "class TestDataLoader():\n",
        "  def __init__(self, spark):\n",
        "    cyp_config = CYPConfiguration()\n",
        "    self.nuts_level = cyp_config.getNUTSLevel()\n",
        "    data_sources = { 'SOIL' : self.nuts_level }\n",
        "    cyp_config.setDataSources(data_sources)\n",
        "    cyp_config.setDebugLevel(2)\n",
        "\n",
        "    self.data_loader = CYPDataLoader(spark, cyp_config)\n",
        "\n",
        "  def testDataLoad(self):\n",
        "    print('\\nTest loadData, loadAllData')\n",
        "    soil_df = self.data_loader.loadData('SOIL', self.nuts_level)\n",
        "    assert soil_df is not None\n",
        "    soil_df.show(5)\n",
        "\n",
        "    all_dfs = self.data_loader.loadAllData()\n",
        "    soil_df = all_dfs['SOIL']\n",
        "    assert soil_df is not None\n",
        "    soil_df.show(5)\n",
        "\n",
        "  def runAllTests(self):\n",
        "    print('\\nTest Data Loader BEGIN\\n')\n",
        "    self.testDataLoad()\n",
        "    print('\\nTest Data Loader END\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vil46Q08YQ9v"
      },
      "source": [
        "### Test Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNZQd9kEYQ9w"
      },
      "source": [
        "#%%writefile test_data_preprocessing.py\n",
        "class TestDataPreprocessor():\n",
        "  def __init__(self, spark):\n",
        "    cyp_config = CYPConfiguration()\n",
        "    cyp_config.setDebugLevel(2)\n",
        "    self.preprocessor = CYPDataPreprocessor(spark, cyp_config)\n",
        "\n",
        "    # create a small wofost data set\n",
        "    # preprocessing currently extracts the year and dekad only\n",
        "    self.wofost_df = spark.createDataFrame([(6, 'NL11', '19790110', 0.0, 0),\n",
        "                                            (6, 'NL11', '19790121', 0.0, 0),\n",
        "                                            (6, 'NL11', '19790331', 0.0, 50),\n",
        "                                            (6, 'NL11', '19790510', 0.0, 100),\n",
        "                                            (6, 'NL11', '19790821', 0.0, 150),\n",
        "                                            (6, 'NL11', '19790831', 0.0, 201),\n",
        "                                            (6, 'NL11', '19790910', 0.0, 201),\n",
        "                                            (6, 'NL11', '19800110', 0.0, 0),\n",
        "                                            (6, 'NL11', '19800121', 0.0, 0),\n",
        "                                            (6, 'NL11', '19800331', 0.0, 50),\n",
        "                                            (6, 'NL11', '19800610', 0.0, 100),\n",
        "                                            (6, 'NL11', '19800721', 0.0, 150),\n",
        "                                            (6, 'NL11', '19800831', 0.0, 200),\n",
        "                                            (6, 'NL11', '19800910', 0.0, 201),\n",
        "                                            (6, 'NL11', '19800921', 0.0, 201)],\n",
        "                                           ['CROP_ID', 'IDREGION', 'DATE', 'POT_YB', 'DVS'])\n",
        "\n",
        "    self.crop_season = None\n",
        "\n",
        "    # Create a small meteo dekadal data set\n",
        "    # Preprocessing currently extracts the year and dekad, and computes climate\n",
        "    # water balance.\n",
        "    self.meteo_dekdf = spark.createDataFrame([('NL11', '19790110', 1.2, 0.2),\n",
        "                                              ('NL11', '19790121', 2.1, 0.2),\n",
        "                                              ('NL11', '19790131', 0.2, 1.2),\n",
        "                                              ('NL11', '19790210', 0.1, 2.0),\n",
        "                                              ('NL11', '19790221', 0.0, 2.1),\n",
        "                                              ('NL11', '19790228', 1.0, 0.8),\n",
        "                                              ('NL11', '19790310', 1.1, 1.0),\n",
        "                                              ('NL11', '19800110', 1.2, 0.2),\n",
        "                                              ('NL11', '19800121', 2.1, 0.2),\n",
        "                                              ('NL11', '19800131', 0.2, 1.2),\n",
        "                                              ('NL11', '19800210', 0.1, 2.0),\n",
        "                                              ('NL11', '19800221', 0.0, 2.1),\n",
        "                                              ('NL11', '19800228', 1.0, 0.8),\n",
        "                                              ('NL11', '19800310', 1.1, 1.0)],\n",
        "                                             ['IDREGION', 'DATE', 'PREC', 'ET0'])\n",
        "\n",
        "    # Create a small meteo daily data set\n",
        "    # Preprocessing currently converts daily data to dekadal data by taking AVG\n",
        "    # for all indicators except TMAX (MAX is used instead) and TMIN (MIN is used instead).\n",
        "    query = 'select IDREGION, FYEAR, DEKAD, max(TMAX) as TMAX, min(TMIN) as TMIN, '\n",
        "    query = query + ' bround(avg(TAVG), 2) as TAVG, bround(sum(PREC), 2) as PREC, '\n",
        "    query = query + ' bround(sum(ET0), 2) as ET0, bround(avg(RAD), 2) as RAD, '\n",
        "    query = query + ' bround(sum(CWB), 2) as CWB '\n",
        "    self.meteo_daydf = spark.createDataFrame([('NL11', '19790101', 1.2, 0.2, 8.5, -1.2, 5.5, 10000.0),\n",
        "                                              ('NL11', '19790102', 2.1, 0.2, 9.1, 0.3, 6.1, 12000.0),\n",
        "                                              ('NL11', '19790103', 0.2, 1.2, 10.4, 1.2, 7.2, 14000.0),\n",
        "                                              ('NL11', '19790104', 0.1, 2.0, 8.1, -1.5, 5.2, 10000.0),\n",
        "                                              ('NL11', '19790105', 0.0, 2.1, 10.2, 1.0, 7.5, 13000.0),\n",
        "                                              ('NL11', '19790106', 1.2, 0.2, 11.2, 2.5, 8.2, 16000.0),\n",
        "                                              ('NL11', '19790112', 2.1, 0.5, 9.2, 0.5, 5.5, 12000.0),\n",
        "                                              ('NL11', '19790113', 0.2, 1.1, 10.2, 1.4, 7.1, 14000.0),\n",
        "                                              ('NL11', '19790114', 0.1, 2.0, 12.0, 3.2, 8.3, 15000.0),\n",
        "                                              ('NL11', '19790115', 0.0, 1.5, 13.1, 4.5, 9.2, 17000.0),\n",
        "                                              ('NL11', '19790122', 2.1, 0.5, 9.2, 0.5, 5.5, 12000.0),\n",
        "                                              ('NL11', '19790123', 0.2, 1.1, 10.2, 1.4, 7.1, 14000.0),\n",
        "                                              ('NL11', '19790124', 0.1, 2.0, 12.0, 3.2, 8.3, 15000.0),\n",
        "                                              ('NL11', '19790125', 0.0, 1.5, 13.1, 4.5, 9.2, 17000.0),\n",
        "                                              ('NL11', '19800101', 1.2, 0.2, 8.5, -1.2, 5.5, 10000.0),\n",
        "                                              ('NL11', '19800102', 2.1, 0.2, 9.1, 0.3, 6.1, 12000.0),\n",
        "                                              ('NL11', '19800103', 0.2, 1.2, 10.4, 1.2, 7.2, 14000.0),\n",
        "                                              ('NL11', '19800104', 0.1, 2.0, 8.1, -1.5, 5.2, 10000.0),\n",
        "                                              ('NL11', '19800105', 0.0, 2.1, 10.2, 1.0, 7.5, 13000.0),\n",
        "                                              ('NL11', '19800106', 1.2, 0.2, 11.2, 2.5, 8.2, 16000.0),\n",
        "                                              ('NL11', '19800112', 2.1, 0.5, 9.2, 0.5, 5.5, 12000.0),\n",
        "                                              ('NL11', '19800113', 0.2, 1.1, 10.2, 1.4, 7.1, 14000.0),\n",
        "                                              ('NL11', '19800114', 0.1, 2.0, 12.0, 3.2, 8.3, 15000.0),\n",
        "                                              ('NL11', '19800115', 0.0, 1.5, 13.1, 4.5, 9.2, 17000.0),\n",
        "                                              ('NL11', '19800122', 2.1, 0.5, 9.2, 0.5, 5.5, 12000.0),\n",
        "                                              ('NL11', '19800123', 0.2, 1.1, 10.2, 1.4, 7.1, 14000.0),\n",
        "                                              ('NL11', '19800124', 0.1, 2.0, 12.0, 3.2, 8.3, 15000.0),\n",
        "                                              ('NL11', '19800125', 0.0, 1.5, 13.1, 4.5, 9.2, 17000.0)],\n",
        "                                             ['IDREGION', 'DATE', 'PREC', 'ET0', 'TMAX', 'TMIN', 'TAVG', 'RAD'])\n",
        "\n",
        "    # Create a small remote sensing data set\n",
        "    # Preprocessing currently extracts the year and dekad\n",
        "    self.rs_df1 = spark.createDataFrame([('NL11', '19790321', 0.47),\n",
        "                                         ('NL11', '19790331', 0.49),\n",
        "                                         ('NL11', '19790410', 0.55),\n",
        "                                         ('NL11', '19790421', 0.49),\n",
        "                                         ('NL11', '19790430', 0.64),\n",
        "                                         ('NL11', '19800110', 0.42),\n",
        "                                         ('NL11', '19800121', 2.43),\n",
        "                                         ('NL11', '19800131', 0.41),\n",
        "                                         ('NL11', '19800210', 0.42),\n",
        "                                         ('NL11', '19800221', 0.44),\n",
        "                                         ('NL11', '19800228', 0.45),\n",
        "                                         ('NL11', '19800310', 2.43)],\n",
        "                                        ['IDREGION', 'DATE', 'FAPAR'])\n",
        "\n",
        "    self.rs_df2 = spark.createDataFrame([('FR10', '19790110', 0.42),\n",
        "                                         ('FR10', '19790121', 0.43),\n",
        "                                         ('FR10', '19790131', 0.41),\n",
        "                                         ('FR10', '19790210', 0.42),\n",
        "                                         ('FR10', '19790221', 0.44),\n",
        "                                         ('FR10', '19790228', 0.45),\n",
        "                                         ('FR10', '19790310', 0.47),\n",
        "                                         ('FR10', '19790321', 0.49),\n",
        "                                         ('FR10', '19790331', 0.55),\n",
        "                                         ('FR10', '19790410', 0.62),\n",
        "                                         ('FR10', '19790421', 0.66),\n",
        "                                         ('FR10', '19800110', 0.42),\n",
        "                                         ('FR10', '19800121', 2.43),\n",
        "                                         ('FR10', '19800131', 0.41),\n",
        "                                         ('FR10', '19800210', 0.42),\n",
        "                                         ('FR10', '19800221', 0.44),\n",
        "                                         ('FR10', '19800228', 0.45),\n",
        "                                         ('FR10', '19800310', 2.43)],\n",
        "                                        ['IDREGION', 'DATE', 'FAPAR'])\n",
        "  \n",
        "    self.crop_season_nuts3 = spark.createDataFrame([('FR101', '1979', 0, 27),\n",
        "                                                    ('FR101', '1980', 27, 28),\n",
        "                                                    ('FR102', '1979', 0, 27),\n",
        "                                                    ('FR102', '1980', 27, 29)],\n",
        "                                                   ['IDREGION', 'FYEAR', 'PREV_SEASON_END', 'SEASON_END'])\n",
        "\n",
        "    # Create small yield data sets\n",
        "    # Two formats are preprocessed: (1) year and yield are columns,\n",
        "    # (2) years are columns with yield values in rows\n",
        "    # Preprocessing currently converts (2) into 1\n",
        "    self.yield_df1 = spark.createDataFrame([('potatoes', 'FR102', '1989', 29.75),\n",
        "                                            ('potatoes', 'FR102', '1990', 25.44),\n",
        "                                            ('potatoes', 'FR103', '1989', 30.2),\n",
        "                                            ('potatoes', 'FR103', '1990', 29.9),\n",
        "                                            ('sugarbeet', 'FR102', '1989', 66.0),\n",
        "                                            ('sugarbeet', 'FR102', '1990', 55.0),\n",
        "                                            ('sugarbeet', 'FR103', '1989', 69.3),\n",
        "                                            ('sugarbeet', 'FR103', '1990', 59.1)],\n",
        "                                           ['Crop', 'IDREGION', 'FYEAR', 'YIELD'])\n",
        "\n",
        "    self.yield_df2 = spark.createDataFrame([('Total potatoes', 'NL11', 38.0, 40.5, 40.0),\n",
        "                                            ('Total potatoes', 'NL12', 49.0, 44.0, 46.8),\n",
        "                                            ('Spring barley', 'NL13', 4.6, 5.5, 6.6),\n",
        "                                            ('Spring barley', 'NL12', 5.6, 6.1, 7.0)],\n",
        "                                           ['Crop', 'IDREGION', '1994', '1995', '1996'])\n",
        "\n",
        "  def testExtractYearDekad(self):\n",
        "    print('WOFOST data after extracting year and dekad')\n",
        "    print('-------------------------------------------')\n",
        "    self.preprocessor.extractYearDekad(self.wofost_df).show(10)\n",
        "\n",
        "  def testPreprocessWofost(self):\n",
        "    print('WOFOST data after preprocessing')\n",
        "    print('--------------------------------')\n",
        "    self.wofost_df = self.wofost_df.filter(self.wofost_df['CROP_ID'] == 6).drop('CROP_ID')\n",
        "    self.crop_season = self.preprocessor.getCropSeasonInformation(self.wofost_df,\n",
        "                                                                  False)\n",
        "    self.wofost_df = self.preprocessor.preprocessWofost(self.wofost_df,\n",
        "                                                        self.crop_season,\n",
        "                                                        False)\n",
        "    self.wofost_df.show(5)\n",
        "    self.crop_season.show(5)\n",
        "\n",
        "  def testPreprocessMeteo(self):\n",
        "    print('Meteo dekadal data after preprocessing')\n",
        "    print('--------------------------------------')\n",
        "    self.meteo_dekdf = self.preprocessor.preprocessMeteo(self.meteo_dekdf,\n",
        "                                                         self.crop_season,\n",
        "                                                         False)\n",
        "    self.meteo_dekdf.show(5)\n",
        "\n",
        "  def testPreprocessMeteoDaily(self):\n",
        "    self.meteo_daydf = self.preprocessor.preprocessMeteo(self.meteo_daydf,\n",
        "                                                         self.crop_season,\n",
        "                                                         False)\n",
        "    self.meteo_daydf = self.preprocessor.preprocessMeteoDaily(self.meteo_daydf)\n",
        "    print('Meteo daily data after preprocessing')\n",
        "    print('------------------------------------')\n",
        "    self.meteo_daydf.show(5)\n",
        "\n",
        "  def testPreprocessRemoteSensing(self):\n",
        "    self.rs_df1 = self.preprocessor.preprocessRemoteSensing(self.rs_df1,\n",
        "                                                            self.crop_season,\n",
        "                                                            False)\n",
        "    print('Remote sensing data after preprocessing')\n",
        "    print('---------------------------------------')\n",
        "    self.rs_df1.show(5)\n",
        "\n",
        "  def testRemoteSensingNUTS2ToNUTS3(self):\n",
        "    print('Remote sensing data before preprocessing')\n",
        "    print('---------------------------------------')\n",
        "    self.rs_df2.show()\n",
        "    nuts3_regions = [reg[0] for reg in self.yield_df1.select('IDREGION').distinct().collect()]\n",
        "    self.rs_df2 = self.preprocessor.remoteSensingNUTS2ToNUTS3(self.rs_df2, nuts3_regions)\n",
        "    print('Remote sensing data at NUTS3')\n",
        "    print('-----------------------------')\n",
        "    self.rs_df2.show(5)\n",
        "\n",
        "    self.rs_df2 = self.preprocessor.preprocessRemoteSensing(self.rs_df2,\n",
        "                                                            self.crop_season_nuts3,\n",
        "                                                            False)\n",
        "    print('Remote sensing data after preprocessing')\n",
        "    print('---------------------------------------')\n",
        "    self.rs_df2.show(5)\n",
        "\n",
        "  def testPreprocessYield(self):\n",
        "    self.yield_df1 = self.preprocessor.preprocessYield(self.yield_df1, 7)\n",
        "    print('Yield data format 1 after preprocessing')\n",
        "    print('--------------------------------------')\n",
        "    self.yield_df1.show(5)\n",
        "\n",
        "    print('Yield data format 2 before preprocessing')\n",
        "    print('----------------------------------------')\n",
        "    self.yield_df2.show(5)\n",
        "\n",
        "    self.yield_df2 = self.preprocessor.preprocessYield(self.yield_df2, 7)\n",
        "    print('Yield data format 2 after preprocessing')\n",
        "    print('----------------------------------------')\n",
        "    self.yield_df2.show(5)\n",
        "\n",
        "  def runAllTests(self):\n",
        "    print('\\nTest Data Preprocessor BEGIN\\n')\n",
        "    self.testExtractYearDekad()\n",
        "    self.testPreprocessWofost()\n",
        "    self.testPreprocessMeteo()\n",
        "    self.testPreprocessMeteoDaily()\n",
        "    self.testPreprocessRemoteSensing()\n",
        "    self.testRemoteSensingNUTS2ToNUTS3()\n",
        "    self.testPreprocessYield()\n",
        "    print('\\nTest Data Preprocessor END\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylzks219YUwm"
      },
      "source": [
        "### Test Data Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD-xHJSiYUwn"
      },
      "source": [
        "#%%writefile test_data_summary.py\n",
        "class TestDataSummarizer():\n",
        "  def __init__(self, spark):\n",
        "    cyp_config = CYPConfiguration()\n",
        "    cyp_config.setDebugLevel(2)\n",
        "    self.data_summarizer = CYPDataSummarizer(cyp_config)\n",
        "\n",
        "    # create a small wofost data set\n",
        "    self.wofost_df = spark.createDataFrame([('NL11', '1979', 14, 0.0, 0.0, 5.0),\n",
        "                                            ('NL11', '1979', 15, 2.0, 1.0, 10.0),\n",
        "                                            ('NL11', '1979', 16, 5.0, 4.0, 7.0),\n",
        "                                            ('NL11', '1979', 17, 15.0, 12.0, 4.0),\n",
        "                                            ('NL11', '1979', 18, 40.0, 35.0, 6.0),\n",
        "                                            ('NL11', '1979', 19, 100.0, 80.0, 5.0),\n",
        "                                            ('NL11', '1979', 20, 150.0, 120.0, 3.0),\n",
        "                                            ('NL11', '1980', 13, 0.0, 0.0, 15.0),\n",
        "                                            ('NL11', '1980', 14, 5.0, 2.0, 12.0),\n",
        "                                            ('NL11', '1980', 15, 15.0, 12.0, 10.0),\n",
        "                                            ('NL11', '1980', 16, 50.0, 40.0, 8.0),\n",
        "                                            ('NL11', '1980', 17, 100.0, 80.0, 4.0),\n",
        "                                            ('NL11', '1980', 18, 200.0, 140.0, 5.0),\n",
        "                                            ('NL11', '1980', 19, 200.0, 150.0, 12.0),\n",
        "                                            ('NL11', '1980', 20, 200.0, 150.0, 12.0)],\n",
        "                                           ['IDREGION', 'FYEAR', 'DEKAD', 'POT_YB', 'WLIM_YB', 'RSM'])\n",
        "\n",
        "    self.wofost_df2 = spark.createDataFrame([('NL11', '1979', 12, '1979', 22, 0),\n",
        "                                             ('NL11', '1979', 13, '1979', 23, 1),\n",
        "                                             ('NL11', '1979', 14, '1979', 24, 4),\n",
        "                                             ('NL11', '1979', 15, '1979', 25, 70),\n",
        "                                             ('NL11', '1979', 16, '1979', 26, 101),\n",
        "                                             ('NL11', '1979', 19, '1979', 29, 150),\n",
        "                                             ('NL11', '1979', 21, '1979', 31, 180),\n",
        "                                             ('NL11', '1979', 23, '1979', 33, 200),\n",
        "                                             ('NL11', '1979', 24, '1979', 34, 201),\n",
        "                                             ('NL11', '1979', 25, '1979', 35, 201),\n",
        "                                             ('NL11', '1980', 12, '1980', 22, 0),\n",
        "                                             ('NL11', '1980', 13, '1980', 23, 2),\n",
        "                                             ('NL11', '1980', 14, '1980', 24, 15),\n",
        "                                             ('NL11', '1980', 15, '1980', 25, 80),\n",
        "                                             ('NL11', '1980', 16, '1980', 26, 99),\n",
        "                                             ('NL11', '1980', 19, '1980', 29, 140),\n",
        "                                             ('NL11', '1980', 21, '1980', 31, 170),\n",
        "                                             ('NL11', '1980', 23, '1980', 33, 195),\n",
        "                                             ('NL11', '1980', 24, '1980', 34, 201),\n",
        "                                             ('NL11', '1980', 25, '1980', 35, 201)],\n",
        "                                           ['IDREGION', 'FYEAR', 'DEKAD', 'CAMPAIGN_YEAR', 'CAMPAIGN_DEKAD', 'DVS'])\n",
        "\n",
        "    # Create a small meteo dekadal data set\n",
        "    self.meteo_df = spark.createDataFrame([('NL11', '1979', 1, '1979', 11, 1.2, 8.5, -1.2, 5.5, 10000.0),\n",
        "                                           ('NL11', '1979', 2, '1979', 12, 2.1, 9.1, 0.3, 6.1, 12000.0),\n",
        "                                           ('NL11', '1979', 3, '1979', 13, 0.2, 10.4, 1.2, 7.2, 14000.0),\n",
        "                                           ('NL11', '1979', 4, '1979', 14, 0.1, 8.1, -1.5, 5.2, 10000.0),\n",
        "                                           ('NL11', '1979', 5, '1979', 15, 0.0, 10.2, 1.0, 7.5, 13000.0),\n",
        "                                           ('NL12', '1979', 1, '1979', 12, 1.2, 11.2, 2.5, 8.2, 16000.0),\n",
        "                                           ('NL12', '1979', 2, '1979', 13, 2.1, 9.2, 0.5, 5.5, 12000.0),\n",
        "                                           ('NL12', '1979', 3, '1979', 14, 0.2, 10.2, 1.4, 7.1, 14000.0),\n",
        "                                           ('NL12', '1979', 4, '1979', 15, 0.1, 12.0, 3.2, 8.3, 15000.0),\n",
        "                                           ('NL12', '1979', 5, '1979', 16, 0.0, 13.1, 4.5, 9.2, 17000.0)],\n",
        "                                          ['IDREGION', 'FYEAR', 'DEKAD', 'CAMPAIGN_YEAR', 'CAMPAIGN_DEKAD', 'PREC', 'TMAX', 'TMIN', 'TAVG', 'RAD'])\n",
        "\n",
        "    # Create a small remote sensing data set\n",
        "    # Preprocessing currently extracts the year and dekad\n",
        "    self.rs_df = spark.createDataFrame([('NL11', '1979', 1, 0.42),\n",
        "                                         ('NL11', '1979', 2, 0.41),\n",
        "                                         ('NL11', '1979', 3, 0.42),\n",
        "                                         ('NL11', '1980', 1, 0.44),\n",
        "                                         ('NL11', '1980', 2, 0.45),\n",
        "                                        ('NL11', '1980', 3, 0.43)],\n",
        "                                        ['IDREGION', 'FYEAR', 'DEKAD', 'FAPAR'])\n",
        "\n",
        "    # Create a small yield data set\n",
        "    self.yield_df = spark.createDataFrame([(7, 'FR102', '1989', 29.75),\n",
        "                                           (7, 'FR102', '1990', 25.44),\n",
        "                                           (7, 'FR103', '1989', 30.2),\n",
        "                                           (7, 'FR103', '1990', 29.9),\n",
        "                                           (6, 'FR102', '1989', 66.0),\n",
        "                                           (6, 'FR102', '1990', 55.0),\n",
        "                                           (6, 'FR103', '1989', 69.3),\n",
        "                                           (6, 'FR103', '1990', 59.1)],\n",
        "                                          ['CROP_ID', 'IDREGION', 'FYEAR', 'YIELD'])\n",
        "\n",
        "  def testWofostDVSSummary(self):\n",
        "    print('WOFOST Crop Calendar Summary using DVS')\n",
        "    print('-----------------------------')\n",
        "    self.data_summarizer.wofostDVSSummary(self.wofost_df2).show()\n",
        "\n",
        "  def testWofostIndicatorsSummary(self):\n",
        "    print('WOFOST indicators summary')\n",
        "    print('--------------------------')\n",
        "    min_cols = ['IDREGION']\n",
        "    max_cols = ['IDREGION', 'POT_YB', 'WLIM_YB']\n",
        "    avg_cols = ['IDREGION', 'RSM']\n",
        "    self.data_summarizer.indicatorsSummary(self.wofost_df, min_cols, max_cols, avg_cols).show()\n",
        "\n",
        "  def testMeteoIndicatorsSummary(self):\n",
        "    print('Meteo indicators summary')\n",
        "    print('-------------------------')\n",
        "    meteo_cols = self.meteo_df.columns[3:]\n",
        "    min_cols = ['IDREGION'] + meteo_cols\n",
        "    max_cols = ['IDREGION'] + meteo_cols\n",
        "    avg_cols = ['IDREGION'] + meteo_cols\n",
        "    self.data_summarizer.indicatorsSummary(self.meteo_df, min_cols, max_cols, avg_cols).show()\n",
        "\n",
        "  def testRemoteSensingSummary(self):\n",
        "    print('Remote sensing indicators summary')\n",
        "    print('----------------------------------')\n",
        "    rs_cols = ['FAPAR']\n",
        "    min_cols = ['IDREGION'] + rs_cols\n",
        "    max_cols = ['IDREGION'] + rs_cols\n",
        "    avg_cols = ['IDREGION'] + rs_cols\n",
        "    self.data_summarizer.indicatorsSummary(self.rs_df, min_cols, max_cols, avg_cols).show()\n",
        "\n",
        "  def testYieldSummary(self):\n",
        "    crop = 'potatoes'\n",
        "    print('Yield summary for', crop)\n",
        "    print('-----------------------------')\n",
        "    crop_id = cropNameToID(crop_id_dict, crop)\n",
        "    self.yield_df = self.yield_df.filter(self.yield_df.CROP_ID == crop_id)\n",
        "    self.data_summarizer.yieldSummary(self.yield_df).show()\n",
        "\n",
        "  def runAllTests(self):\n",
        "    print('\\nTest Data Summarizer BEGIN\\n')\n",
        "    self.testWofostDVSSummary()\n",
        "    self.testWofostIndicatorsSummary()\n",
        "    self.testMeteoIndicatorsSummary()\n",
        "    self.testRemoteSensingSummary()\n",
        "    self.testYieldSummary()\n",
        "    print('\\nTest Data Summarizer END\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjepG1gAYXrc"
      },
      "source": [
        "### Test Yield Trend Estimation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRoEhIT6YXre"
      },
      "source": [
        "#%%writefile test_yield_trend.py\n",
        "class TestYieldTrendEstimator():\n",
        "  def __init__(self, yield_df):\n",
        "    # TODO: Create a small yield data set\n",
        "    self.yield_df = yield_df\n",
        "    cyp_config = CYPConfiguration()\n",
        "    self.verbose = 2\n",
        "    cyp_config.setDebugLevel(self.verbose)\n",
        "    self.trend_est = CYPYieldTrendEstimator(cyp_config)\n",
        "\n",
        "  def testYieldTrendTwoRegions(self):\n",
        "    print('\\nFind the optimal trend window and estimate trend for first 2 regions')\n",
        "    pd_yield_df = self.yield_df.toPandas()\n",
        "    regions = sorted(pd_yield_df['IDREGION'].unique())\n",
        "    reg1 = regions[0]\n",
        "    pd_reg1_df = pd_yield_df[pd_yield_df['IDREGION'] == reg1]\n",
        "    reg1_num_years = len(pd_reg1_df.index)\n",
        "    reg1_max_year = pd_reg1_df['FYEAR'].max()\n",
        "    reg1_min_year = pd_reg1_df['FYEAR'].min()\n",
        "\n",
        "    if (self.verbose > 2):\n",
        "      print('\\nPrint Yield Trend Rounds')\n",
        "      print('------------------------')\n",
        "\n",
        "    trend_windows = [5]\n",
        "    self.trend_est.printYieldTrendRounds(self.yield_df, reg1, trend_windows)\n",
        "\n",
        "    if (self.verbose > 1):\n",
        "      print('\\n Fixed Trend Window prediction for region 1')\n",
        "      print('---------------------------------------------------')\n",
        "    trend_window = 5\n",
        "    pd_fixed_win_df = self.trend_est.getFixedWindowTrend(self.yield_df, reg1, reg1_max_year,\n",
        "                                                         trend_window)\n",
        "    if (self.verbose > 1):\n",
        "      print(pd_fixed_win_df.head(1))\n",
        "\n",
        "    if (self.verbose > 1):\n",
        "      print('\\n Optimal Trend Window and prediction for region 1')\n",
        "      print('---------------------------------------------------')\n",
        "  \n",
        "    trend_windows = [5, 7]\n",
        "    pd_opt_win_df = self.trend_est.getOptimalWindowTrend(self.yield_df, reg1, reg1_max_year,\n",
        "                                                         trend_windows)\n",
        "    if (self.verbose > 1):\n",
        "      print(pd_opt_win_df.head(1))\n",
        "\n",
        "    reg2 = regions[1]\n",
        "    pd_reg2_df = pd_yield_df[pd_yield_df['IDREGION'] == reg2]\n",
        "    reg2_num_years = len(pd_reg2_df.index)\n",
        "    reg2_max_year = pd_reg2_df['FYEAR'].max()\n",
        "    reg2_min_year = pd_reg2_df['FYEAR'].min()\n",
        "\n",
        "    if (self.verbose > 1):\n",
        "      print('\\n Fixed Trend Window prediction for region 2')\n",
        "      print('---------------------------------------------------')\n",
        "    trend_window = 5\n",
        "    pd_fixed_win_df = self.trend_est.getFixedWindowTrend(self.yield_df, reg2, reg2_max_year,\n",
        "                                                         trend_window)\n",
        "    if (self.verbose > 1):\n",
        "      print(pd_fixed_win_df.head(1))\n",
        "\n",
        "    if (self.verbose > 1):\n",
        "      print('\\n Optimal Trend Window and prediction for region 2')\n",
        "      print('---------------------------------------------------')\n",
        "\n",
        "    pd_opt_win_df = self.trend_est.getOptimalWindowTrend(self.yield_df, reg2, reg2_max_year,\n",
        "                                                         trend_windows)\n",
        "    if (self.verbose > 1):\n",
        "      print(pd_opt_win_df.head(1))\n",
        "\n",
        "  def testYieldTrendAllRegions(self):\n",
        "    print('\\nYield trend estimation for all regions')\n",
        "\n",
        "    print('\\nOptimal Trend Windows')\n",
        "    pd_trend_df = self.trend_est.getOptimalWindowTrendFeatures(self.yield_df)\n",
        "    print(pd_trend_df.head(5))\n",
        "\n",
        "    print('\\nFixed Trend Window')\n",
        "    pd_trend_df = self.trend_est.getFixedWindowTrendFeatures(self.yield_df)\n",
        "    print(pd_trend_df.head(5))\n",
        "\n",
        "  def runAllTests(self):\n",
        "    print('\\nTest Yield Trend Estimator BEGIN\\n')\n",
        "    self.testYieldTrendTwoRegions()\n",
        "    self.testYieldTrendAllRegions()\n",
        "    print('\\nTest Yield Trend Estimator END\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih1GgFhZe13K"
      },
      "source": [
        "### Test custom train, test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQNoWy51e13M"
      },
      "source": [
        "#%%writefile test_train_test_split.py\n",
        "class TestCustomTrainTestSplit:\n",
        "  def __init__(self, yield_df):\n",
        "    cyp_config = CYPConfiguration()\n",
        "    self.verbose = 2\n",
        "    cyp_config.setDebugLevel(self.verbose)\n",
        "    self.yield_df = yield_df\n",
        "    self.trTsSplitter = CYPTrainTestSplitter(cyp_config)\n",
        "\n",
        "  def testCustomTrainTestSplit(self):\n",
        "    print('\\nTest customTrainTestSplit')\n",
        "    test_fraction = 0.2\n",
        "    regions = [reg[0] for reg in self.yield_df.select('IDREGION').distinct().collect()]\n",
        "    num_regions = len(regions)\n",
        "    test_years = self.trTsSplitter.trainTestSplit(self.yield_df, test_fraction, True)\n",
        "    all_years = [yr[0] for yr in self.yield_df.select('FYEAR').distinct().collect()]\n",
        "    yield_train_df = self.yield_df.filter(~self.yield_df['FYEAR'].isin(test_years))\n",
        "    yield_test_df = self.yield_df.filter(self.yield_df['FYEAR'].isin(test_years))\n",
        "\n",
        "    if(self.verbose > 1):\n",
        "      print('\\nCustom training, test split using yield trend')\n",
        "      print('---------------------------------------------')\n",
        "      print('Estimated size of test data', num_regions * np.floor(len(all_years) * test_fraction))\n",
        "      print('Data Size:', yield_train_df.count(), yield_test_df.count())\n",
        "      print('Test years:', test_years)\n",
        "\n",
        "    test_years = self.trTsSplitter.trainTestSplit(self.yield_df, test_fraction, False)\n",
        "\n",
        "    if(self.verbose > 1):\n",
        "      print('\\ncustom training, test split without yield trend')\n",
        "      print('------------------------------------------------')\n",
        "      print('Estimated size of test data', num_regions * np.floor(len(all_years) * test_fraction))\n",
        "      print('Data Size:', yield_train_df.count(), yield_test_df.count())\n",
        "      print('Test years:', test_years)\n",
        "\n",
        "  def testCustomKFoldValidationSplit(self):\n",
        "    print('\\nTest customKFoldValidationSplit')\n",
        "    test_fraction = 0.2\n",
        "    num_folds = 5\n",
        "    test_years = self.trTsSplitter.trainTestSplit(self.yield_df, test_fraction, 'Y')\n",
        "    yield_train_df = self.yield_df.filter(~self.yield_df['FYEAR'].isin(test_years))\n",
        "    yield_test_df = self.yield_df.filter(self.yield_df['FYEAR'].isin(test_years))\n",
        "    yield_cols = yield_train_df.columns\n",
        "    pd_yield_train_df = yield_train_df.toPandas()\n",
        "    Y_train_full = pd_yield_train_df[yield_cols].values\n",
        "\n",
        "    custom_cv, _ = self.trTsSplitter.customKFoldValidationSplit(Y_train_full, num_folds)\n",
        "\n",
        "  def runAllTests(self):\n",
        "    print('\\nTest Custom Train, Test Splitter BEGIN\\n')\n",
        "    self.testCustomTrainTestSplit()\n",
        "    self.testCustomKFoldValidationSplit()\n",
        "    print('\\nTest Custom Train, Test Splitter END\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NrPZSmEViO5"
      },
      "source": [
        "## Run Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWnqPIU4cM_z"
      },
      "source": [
        "### Set Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2r5-pmuVgS1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fbe5ec5-1fd5-4bec-912b-a45410841db5"
      },
      "source": [
        "if (test_env == 'notebook'):\n",
        "  cyp_config = CYPConfiguration()\n",
        "\n",
        "  if (run_tests):\n",
        "    test_util = TestUtil(spark)\n",
        "    test_util.runAllTests()\n",
        "\n",
        "  my_config = {\n",
        "      'crop_name' : 'sugarbeet',\n",
        "      'season_crosses_calendar_year' : 'N',\n",
        "      'country_code' : 'NL',\n",
        "      'data_sources' : [ 'WOFOST', 'METEO_DAILY', 'SOIL', 'YIELD'],\n",
        "      'clean_data' : 'Y',\n",
        "      'data_path' : '.',\n",
        "      'output_path' : '.',\n",
        "      'nuts_level' : 'NUTS2',\n",
        "      'use_yield_trend' : 'Y',\n",
        "      'predict_yield_residuals' : 'N',\n",
        "      'trend_windows' : [5, 7, 10],\n",
        "      'use_centroids' : 'N',\n",
        "      'use_remote_sensing' : 'Y',\n",
        "      'use_gaes' : 'N',\n",
        "      'use_per_year_crop_calendar' : 'Y',\n",
        "      'early_season_prediction' : 'N',\n",
        "      'early_season_end_dekad' : 0,\n",
        "      'use_features_v2' : 'Y',\n",
        "      'save_features' : 'N',\n",
        "      'use_saved_features' : 'Y',\n",
        "      'use_sample_weights' : 'N',\n",
        "      'retrain_per_test_year' : 'N',\n",
        "      'save_predictions' : 'Y',\n",
        "      'use_saved_predictions' : 'N',\n",
        "      'compare_with_mcyfs' : 'Y',\n",
        "      'debug_level' : 2,\n",
        "  }\n",
        "\n",
        "  cyp_config.updateConfiguration(my_config)\n",
        "  crop = cyp_config.getCropName()\n",
        "  country = cyp_config.getCountryCode()\n",
        "  nuts_level = cyp_config.getNUTSLevel()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  use_saved_predictions = cyp_config.useSavedPredictions()\n",
        "  use_saved_features = cyp_config.useSavedFeatures()\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "  early_season_end = cyp_config.getEarlySeasonEndDekad()\n",
        "\n",
        "  print('##################')\n",
        "  print('# Configuration  #')\n",
        "  print('##################')\n",
        "  output_path = cyp_config.getOutputPath()\n",
        "  log_file = getLogFilename(crop, country, use_yield_trend,\n",
        "                            early_season_prediction, early_season_end)\n",
        "  log_fh = open(output_path + '/' + log_file, 'w+')\n",
        "  cyp_config.printConfig(log_fh)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##################\n",
            "# Configuration  #\n",
            "##################\n",
            "\n",
            "Current ML Baseline Configuration\n",
            "--------------------------------\n",
            "Crop name: sugarbeet\n",
            "Crop ID: 6\n",
            "Crop growing season crosses calendar year boundary: N\n",
            "Country code (e.g. NL): NL\n",
            "NUTS level for yield prediction: NUTS2\n",
            "Input data sources: WOFOST, METEO_DAILY, SOIL, YIELD, REMOTE_SENSING\n",
            "Remove data or regions with duplicate or missing values: Y\n",
            "Estimate and use yield trend: Y\n",
            "Predict yield residuals instead of full yield: N\n",
            "Find optimal trend window: N\n",
            "List of trend window lengths (number of years): 5, 7, 10\n",
            "Use centroid coordinates and distance to coast: N\n",
            "Use remote sensing data (FAPAR): Y\n",
            "Use agro-environmental zones data: N\n",
            "Use per region per year crop calendar: Y\n",
            "Predict yield early in the season: N\n",
            "Early season end dekad relative to harvest: 0\n",
            "Path to all input data. Default is current directory.: .\n",
            "Path to all output files. Default is current directory.: .\n",
            "Use feature design v2: Y\n",
            "Save features to a CSV file: N\n",
            "Use features from a CSV file: Y\n",
            "Use data sample weights based on crop area: N\n",
            "Retrain a model for every test year: N\n",
            "Save predictions to a CSV file: Y\n",
            "Use predictions from a CSV file: N\n",
            "Compare predictions with MARS Crop Yield Forecasting System: Y\n",
            "Debug level to control amount of debug information: 2\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cytBAKTO2SfR"
      },
      "source": [
        "### Load and Preprocess Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpkFFtVi2SfU"
      },
      "source": [
        "if ((test_env == 'notebook') and\n",
        "    (not use_saved_predictions) and\n",
        "    (not use_saved_features)):\n",
        "\n",
        "  print('#################')\n",
        "  print('# Data Loading  #')\n",
        "  print('#################')\n",
        "\n",
        "  if (run_tests):\n",
        "    test_loader = TestDataLoader(spark)\n",
        "    test_loader.runAllTests()\n",
        "\n",
        "  cyp_loader = CYPDataLoader(spark, cyp_config)\n",
        "  data_dfs = cyp_loader.loadAllData()\n",
        "\n",
        "  print('#######################')\n",
        "  print('# Data Preprocessing  #')\n",
        "  print('#######################')\n",
        "\n",
        "  if (run_tests):\n",
        "    test_preprocessor = TestDataPreprocessor(spark)\n",
        "    test_preprocessor.runAllTests()\n",
        "\n",
        "  cyp_preprocessor = CYPDataPreprocessor(spark, cyp_config)\n",
        "  data_dfs = preprocessData(cyp_config, cyp_preprocessor, data_dfs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePa1-zrp-Vay"
      },
      "source": [
        "### Split Data into Training and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtLALA3gtbiQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e897f43a-d004-467e-df5a-5be20f2e1155"
      },
      "source": [
        "if ((test_env == 'notebook') and\n",
        "    (not use_saved_predictions) and\n",
        "    (not use_saved_features)):\n",
        "\n",
        "  print('###########################')\n",
        "  print('# Training and Test Split #')\n",
        "  print('###########################')\n",
        "\n",
        "  if (run_tests):\n",
        "    yield_df = data_dfs['YIELD']\n",
        "    test_custom = TestCustomTrainTestSplit(yield_df)\n",
        "    test_custom.runAllTests()\n",
        "\n",
        "  prep_train_test_dfs, test_years = splitDataIntoTrainingTestSets(cyp_config, data_dfs, log_fh)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "###########################\n",
            "# Training and Test Split #\n",
            "###########################\n",
            "\n",
            "Test years: 2012, 2013, 2014, 2015, 2016, 2017, 2018\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7w13GLYExv1"
      },
      "source": [
        "### Summarize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxYQ_BnPExv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1aaecfb-6bd5-4457-d5b3-55a0127b9806"
      },
      "source": [
        "if ((test_env == 'notebook') and\n",
        "    (not use_saved_predictions) and\n",
        "    (not use_saved_features)):\n",
        "\n",
        "  print('#################')\n",
        "  print('# Data Summary  #')\n",
        "  print('#################')\n",
        "\n",
        "  if (run_tests):\n",
        "    test_summarizer = TestDataSummarizer(spark)\n",
        "    test_summarizer.runAllTests()\n",
        "\n",
        "  cyp_summarizer = CYPDataSummarizer(cyp_config)\n",
        "  summary_dfs = summarizeData(cyp_config, cyp_summarizer, prep_train_test_dfs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#################\n",
            "# Data Summary  #\n",
            "#################\n",
            "Crop calender information based on WOFOST data\n",
            "+--------+-------------+---------+----------+----------+---------------------+\n",
            "|IDREGION|CAMPAIGN_YEAR|START_DVS|START_DVS1|START_DVS2|CAMPAIGN_EARLY_SEASON|\n",
            "+--------+-------------+---------+----------+----------+---------------------+\n",
            "|    NL11|         2011|       13|        19|        30|                   31|\n",
            "|    NL12|         2011|       15|        20|        31|                   32|\n",
            "|    NL13|         2011|       13|        19|        29|                   30|\n",
            "|    NL21|         2011|       11|        17|        28|                   29|\n",
            "|    NL22|         2011|       11|        17|        27|                   28|\n",
            "|    NL23|         2011|       12|        17|        28|                   29|\n",
            "|    NL31|         2011|       12|        17|        27|                   28|\n",
            "|    NL32|         2011|       12|        17|        28|                   29|\n",
            "|    NL33|         2011|       12|        17|        27|                   28|\n",
            "|    NL34|         2011|       12|        17|        27|                   28|\n",
            "+--------+-------------+---------+----------+----------+---------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvVoVMRgcejF"
      },
      "source": [
        "### Create Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucxoQlMXwKEg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa588deb-cf8a-4452-b2ea-bce676356323"
      },
      "source": [
        "if ((test_env == 'notebook') and\n",
        "    (not use_saved_predictions) and\n",
        "    (not use_saved_features)):\n",
        "\n",
        "  print('###################')\n",
        "  print('# Feature Design  #')\n",
        "  print('###################')\n",
        "\n",
        "  # WOFOST, Meteo and Remote Sensing Features\n",
        "  cyp_featurizer = CYPFeaturizer(cyp_config)\n",
        "  pd_feature_dfs = createFeatures(cyp_config, cyp_featurizer,\n",
        "                                  prep_train_test_dfs, summary_dfs, log_fh)\n",
        "\n",
        "  # trend features\n",
        "  join_cols = ['IDREGION', 'FYEAR']\n",
        "  if (use_yield_trend):\n",
        "    yield_train_df = prep_train_test_dfs['YIELD'][0]\n",
        "    yield_test_df = prep_train_test_dfs['YIELD'][1]\n",
        "\n",
        "    # Trend features from feature data\n",
        "    use_features_v2 = cyp_config.useFeaturesV2()\n",
        "    if (use_features_v2):\n",
        "      pd_feature_dfs = addFeaturesFromPreviousYears(cyp_config, pd_feature_dfs,\n",
        "                                                    1, test_years, join_cols)\n",
        "\n",
        "    if (run_tests):\n",
        "      test_yield_trend = TestYieldTrendEstimator(yield_train_df)\n",
        "      test_yield_trend.runAllTests()\n",
        "\n",
        "    # Trend features from label data\n",
        "    cyp_trend_est = CYPYieldTrendEstimator(cyp_config)\n",
        "    pd_yield_train_ft, pd_yield_test_ft = createYieldTrendFeatures(cyp_config, cyp_trend_est,\n",
        "                                                                   yield_train_df, yield_test_df,\n",
        "                                                                   test_years)\n",
        "    pd_feature_dfs['YIELD_TREND'] = [pd_yield_train_ft, pd_yield_test_ft]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "###################\n",
            "# Feature Design  #\n",
            "###################\n",
            "\n",
            " WOFOST Aggregate Features: Training\n",
            "  IDREGION  FYEAR  maxWLIM_YBp2  ...  maxWLAIp4  avgRSMp2  avgRSMp4\n",
            "0     NL34   1987       4016.67  ...       5.66     90.20     70.26\n",
            "1     NL42   1995       2615.32  ...       3.68     89.34     29.10\n",
            "2     NL12   1999       4411.05  ...       5.82     94.00     64.00\n",
            "3     NL13   2006       3007.10  ...       4.16     90.38     65.28\n",
            "4     NL34   2000       1356.84  ...       4.60     94.75     69.73\n",
            "\n",
            "[5 rows x 11 columns]\n",
            "\n",
            " WOFOST Aggregate Features: Test\n",
            "  IDREGION  FYEAR  maxWLIM_YBp2  ...  maxWLAIp4  avgRSMp2  avgRSMp4\n",
            "0     NL13   2016       2175.01  ...       4.36     95.82     67.05\n",
            "1     NL21   2018       1038.62  ...       3.10     91.10     35.68\n",
            "2     NL32   2014       2763.33  ...       4.26     93.49     64.05\n",
            "3     NL42   2015       3801.85  ...       4.16     84.48     58.02\n",
            "4     NL33   2012       2129.41  ...       4.85    103.96     77.55\n",
            "\n",
            "[5 rows x 11 columns]\n",
            "\n",
            " WOFOST Features for Extreme Conditions: Training\n",
            "  IDREGION  FYEAR  Z-RSMp1  Z+RSMp1  ...  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4\n",
            "0     NL34   1987     0.96     0.57  ...     0.61     0.93     0.86     3.45\n",
            "1     NL42   1995     0.55     0.39  ...     3.05     0.96    12.81     0.00\n",
            "2     NL12   1999     0.09     0.89  ...     0.43     1.38     2.73     2.40\n",
            "3     NL13   2006     0.31     0.84  ...     2.20     0.00     2.53     2.71\n",
            "4     NL34   2000     0.00     0.85  ...     0.33     1.11     1.71     3.86\n",
            "\n",
            "[5 rows x 10 columns]\n",
            "\n",
            " WOFOST Features for Extreme Conditions: Test\n",
            "  IDREGION  FYEAR  Z-RSMp1  Z+RSMp1  ...  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4\n",
            "0     NL13   2016     0.00     1.10  ...     0.00     3.75     2.57     3.45\n",
            "1     NL21   2018     4.33     1.18  ...     0.99     0.71    12.23     0.62\n",
            "2     NL32   2014     0.00     4.05  ...     3.15     0.00     2.58     2.26\n",
            "3     NL42   2015     0.00     1.04  ...     4.81     0.00     4.90     2.18\n",
            "4     NL33   2012     0.00     4.15  ...     0.00     2.64     0.96     6.53\n",
            "\n",
            "[5 rows x 10 columns]\n",
            "\n",
            " METEO Aggregate Features: Training\n",
            "  IDREGION  FYEAR  avgTAVGp0  ...  avgCWBp4  avgRADp4  avgPRECp5\n",
            "0     NL34   1987       3.73  ...    -92.92  12419.08       1.14\n",
            "1     NL42   1995       6.53  ...   -135.37  18347.00       2.17\n",
            "2     NL12   1999       5.79  ...     -2.27  13769.05       3.34\n",
            "3     NL13   2006       3.67  ...   -146.59  15605.83       1.16\n",
            "4     NL34   2000       7.54  ...    -12.36  14271.95       3.14\n",
            "\n",
            "[5 rows x 14 columns]\n",
            "\n",
            " METEO Aggregate Features: Test\n",
            "  IDREGION  FYEAR  avgTAVGp0  ...  avgCWBp4  avgRADp4  avgPRECp5\n",
            "0     NL13   2016       5.01  ...      3.04  15050.67       0.46\n",
            "1     NL21   2018       4.90  ...   -226.18  18978.63       1.25\n",
            "2     NL32   2014       7.95  ...    -53.87  16278.82       1.34\n",
            "3     NL42   2015       5.41  ...   -165.13  15476.45       1.81\n",
            "4     NL33   2012       5.77  ...     -4.57  15289.06       4.54\n",
            "\n",
            "[5 rows x 14 columns]\n",
            "\n",
            " METEO Features for Extreme Conditions: Training\n",
            "  IDREGION  FYEAR  Z-TMINp1  Z-PRECp1  ...  Z+PRECp3  Z+TMAXp3  Z-PRECp5  Z+PRECp5\n",
            "0     NL34   1987      4.88     12.09  ...     12.37     12.61     13.51      5.35\n",
            "1     NL42   1995      8.43     10.53  ...      5.33     27.72      9.41      9.93\n",
            "2     NL12   1999     15.19      8.40  ...      8.45     10.62      7.44     17.44\n",
            "3     NL13   2006     10.54      9.39  ...      4.32     32.42     11.75      4.00\n",
            "4     NL34   2000      2.58      7.38  ...      7.01     10.56      8.37     16.74\n",
            "\n",
            "[5 rows x 12 columns]\n",
            "\n",
            " METEO Features for Extreme Conditions: Test\n",
            "  IDREGION  FYEAR  Z-TMINp1  Z-PRECp1  ...  Z+PRECp3  Z+TMAXp3  Z-PRECp5  Z+PRECp5\n",
            "0     NL13   2016     24.85      7.38  ...     18.51      6.89     14.44      1.02\n",
            "1     NL21   2018      7.70      9.35  ...      6.93     26.05     12.12      4.94\n",
            "2     NL32   2014      6.66      9.06  ...      5.71      4.85     12.42      6.13\n",
            "3     NL42   2015     23.60     11.58  ...      6.49     32.82     10.71      8.27\n",
            "4     NL33   2012      7.24      4.61  ...     20.55      7.87      5.51     25.24\n",
            "\n",
            "[5 rows x 12 columns]\n",
            "\n",
            " REMOTE_SENSING Aggregate Features: Training\n",
            "  IDREGION  FYEAR  avgFAPARp2  avgFAPARp4\n",
            "0     NL12   1999        0.57        0.65\n",
            "1     NL13   2006        0.61        0.72\n",
            "2     NL34   2000        0.64        0.64\n",
            "3     NL22   2004        0.66        0.66\n",
            "4     NL22   2000        0.65        0.70\n",
            "\n",
            " REMOTE_SENSING Aggregate Features: Test\n",
            "  IDREGION  FYEAR  avgFAPARp2  avgFAPARp4\n",
            "0     NL13   2016        0.59        0.78\n",
            "1     NL21   2018        0.62        0.67\n",
            "2     NL32   2014        0.65        0.61\n",
            "3     NL42   2015        0.53        0.70\n",
            "4     NL33   2012        0.61        0.59\n",
            "\n",
            "METEO Trend Features: Train\n",
            "IDREGION  FYEAR  avgPRECp5  avgPRECp5-1  Z-PRECp5  Z-PRECp5-1  Z+PRECp5  Z+PRECp5-1\n",
            "    NL11   1980       2.88         2.83      5.44        7.04     11.72       12.89\n",
            "    NL11   1981       1.82         2.88      8.11        5.44      5.67       11.72\n",
            "    NL11   1982       2.41         1.82      8.75        8.11     11.27        5.67\n",
            "    NL11   1983       2.50         2.41      8.76        8.75     11.95       11.27\n",
            "    NL11   1984       2.14         2.50     12.51        8.76     12.75       11.95\n",
            "\n",
            "METEO Trend Features: Test\n",
            "IDREGION  FYEAR  avgPRECp5  avgPRECp5-1  Z-PRECp5  Z-PRECp5-1  Z+PRECp5  Z+PRECp5-1\n",
            "    NL11   2012       2.53         2.41      9.17       10.41     12.66       12.87\n",
            "    NL11   2013       2.29         2.53      8.16        9.17      9.65       12.66\n",
            "    NL11   2014       0.94         2.29     12.05        8.16      2.53        9.65\n",
            "    NL11   2015       1.64         0.94     11.29       12.05      7.33        2.53\n",
            "    NL11   2016       0.57         1.64     13.84       11.29      1.32        7.33\n",
            "\n",
            "Yield Trend Features: Train\n",
            "    IDREGION  FYEAR    YIELD-5  ...    YIELD-2    YIELD-1  YIELD_TREND\n",
            "104     NL11   1999  51.000000  ...  55.700001  47.000000        50.01\n",
            "105     NL11   2000  54.400002  ...  47.000000  60.299999        55.92\n",
            "106     NL11   2001  52.000000  ...  60.299999  59.099998        60.46\n",
            "107     NL11   2002  55.700001  ...  59.099998  54.400002        58.15\n",
            "108     NL11   2003  47.000000  ...  54.400002  55.299999        58.43\n",
            "\n",
            "[5 rows x 8 columns]\n",
            "Total 156 rows\n",
            "\n",
            "Yield Trend Features: Test\n",
            "    IDREGION  FYEAR    YIELD-5  ...    YIELD-2    YIELD-1  YIELD_TREND\n",
            "173     NL11   2012  65.599998  ...  71.199997  75.800003        77.87\n",
            "174     NL11   2013  70.900002  ...  75.800003  75.199997        76.46\n",
            "175     NL11   2014  74.800003  ...  75.199997  74.599998        75.40\n",
            "176     NL11   2015  71.199997  ...  74.599998  86.800003        85.72\n",
            "177     NL11   2016  75.800003  ...  86.800003  76.000000        81.28\n",
            "\n",
            "[5 rows x 8 columns]\n",
            "Total 84 rows\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOy8NcAtfnhu"
      },
      "source": [
        "### Combine Features and Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1brZTniBfpaO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c3c7608-5e4c-434c-966b-735458db581f"
      },
      "source": [
        "if ((test_env == 'notebook') and\n",
        "    (not use_saved_predictions) and\n",
        "    (not use_saved_features)):\n",
        "\n",
        "  pd_train_df, pd_test_df = combineFeaturesLabels(cyp_config, sqlContext,\n",
        "                                                  prep_train_test_dfs, pd_feature_dfs,\n",
        "                                                  join_cols, log_fh)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Combine Features and Labels\n",
            "---------------------------\n",
            "Yield min year 1994\n",
            "\n",
            "Data size after including SOIL data: \n",
            "Train 12 rows.\n",
            "Test 12 rows.\n",
            "\n",
            "Data size after including WOFOST features: \n",
            "Train 392 rows.\n",
            "Test 84 rows.\n",
            "\n",
            "Data size after including METEO features: \n",
            "Train 380 rows.\n",
            "Test 84 rows.\n",
            "\n",
            "Data size after including REMOTE_SENSING features: \n",
            "Train 143 rows.\n",
            "Test 77 rows.\n",
            "\n",
            "Data size after including yield trend features: \n",
            "Train 143 rows.\n",
            "Test 77 rows.\n",
            "\n",
            "Data size after including yield (label) data: \n",
            "Train 143 rows.\n",
            "Test 77 rows.\n",
            "\n",
            "\n",
            "All Features and labels: Training\n",
            "   IDREGION  FYEAR  SM_WHC  ...    YIELD-1  YIELD_TREND      YIELD\n",
            "51     NL11   1999    0.22  ...  47.000000        50.01  60.299999\n",
            "42     NL11   2000    0.22  ...  60.299999        55.92  59.099998\n",
            "49     NL11   2001    0.22  ...  59.099998        60.46  54.400002\n",
            "48     NL11   2002    0.22  ...  54.400002        58.15  55.299999\n",
            "47     NL11   2003    0.22  ...  55.299999        58.43  59.200001\n",
            "\n",
            "[5 rows x 54 columns]\n",
            "\n",
            "All Features and labels: Test\n",
            "   IDREGION  FYEAR  SM_WHC  ...    YIELD-1  YIELD_TREND      YIELD\n",
            "21     NL11   2012    0.22  ...  75.800003        77.87  75.199997\n",
            "26     NL11   2013    0.22  ...  75.199997        76.46  74.599998\n",
            "25     NL11   2014    0.22  ...  74.599998        75.40  86.800003\n",
            "23     NL11   2015    0.22  ...  86.800003        85.72  76.000000\n",
            "27     NL11   2016    0.22  ...  76.000000        81.28  74.800003\n",
            "\n",
            "[5 rows x 54 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lzdenOWYxYr"
      },
      "source": [
        "### Apply Machine Learning using scikit learn\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1AKH4dbtR1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f11339dd-b99d-40b7-8268-df672d84d1fa"
      },
      "source": [
        "if ((test_env == 'notebook') and\n",
        "    (not use_saved_predictions)):\n",
        "\n",
        "  if (use_saved_features):\n",
        "    pd_train_df, pd_test_df = loadSavedFeaturesLabels(cyp_config, spark)\n",
        "\n",
        "  print('\\n###################################')\n",
        "  print('# Machine Learning using sklearn  #')\n",
        "  print('###################################')\n",
        "\n",
        "  # # drop mutually correlated features\n",
        "  corr_threshold = cyp_config.getFeatureCorrelationThreshold()\n",
        "  pd_train_df, pd_test_df = dropHighlyCorrelatedFeatures(cyp_config, pd_train_df, pd_test_df,\n",
        "                                                         log_fh, corr_thresh=corr_threshold)\n",
        "\n",
        "  pd_ml_predictions = getMachineLearningPredictions(cyp_config, pd_train_df, pd_test_df, log_fh)\n",
        "  save_predictions = cyp_config.savePredictions()\n",
        "  if (save_predictions):\n",
        "    saveMLPredictions(cyp_config, sqlContext, pd_ml_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "All Features and labels\n",
            "  IDREGION  FYEAR  SM_WHC  AVG_ELEV  ...  YIELD-2  YIELD-1  YIELD_TREND  YIELD\n",
            "0     NL11   1999    0.22  1.761161  ...     55.7     47.0        50.01   60.3\n",
            "1     NL11   2000    0.22  1.761161  ...     47.0     60.3        55.92   59.1\n",
            "2     NL11   2001    0.22  1.761161  ...     60.3     59.1        60.46   54.4\n",
            "3     NL11   2002    0.22  1.761161  ...     59.1     54.4        58.15   55.3\n",
            "4     NL11   2003    0.22  1.761161  ...     54.4     55.3        58.43   59.2\n",
            "\n",
            "[5 rows x 63 columns]\n",
            "  IDREGION  FYEAR  SM_WHC  AVG_ELEV  ...  YIELD-2  YIELD-1  YIELD_TREND  YIELD\n",
            "0     NL11   2012    0.22  1.761161  ...     71.2     75.8        77.87   75.2\n",
            "1     NL11   2013    0.22  1.761161  ...     75.8     75.2        76.46   74.6\n",
            "2     NL11   2014    0.22  1.761161  ...     75.2     74.6        75.40   86.8\n",
            "3     NL11   2015    0.22  1.761161  ...     74.6     86.8        85.72   76.0\n",
            "4     NL11   2016    0.22  1.761161  ...     86.8     76.0        81.28   74.8\n",
            "\n",
            "[5 rows x 63 columns]\n",
            "\n",
            "###################################\n",
            "# Machine Learning using sklearn  #\n",
            "###################################\n",
            "\n",
            "Dropping highly correlated features\n",
            "AVG_ELEV, STD_ELEV, AVG_SLOPE, maxWLIM_YBp2, maxTWCp2, maxWLIM_YBp4, avgRSMp2, avgRSMp4, avgPRECp1, avgPRECp3, avgPRECp5, avgPRECp5-1\n",
            "\n",
            "Training and Evaluation\n",
            "-------------------------\n",
            "\n",
            "Training Data Size: 143 rows\n",
            "X cols: 47, Y cols: 4\n",
            "IDREGION  FYEAR  SM_WHC  STD_SLOPE  AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  IRRIG_AREA6  maxWLAIp2  maxWLIM_YSp4  maxTWCp4  maxWLAIp4  Z-RSMp1  Z+RSMp1  Z-RSMp2  Z+RSMp2  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4  avgTAVGp0  avgPRECp0  avgCWBp0  avgTAVGp1  avgTAVGp2  avgCWBp2  avgRADp2  avgCWBp4  avgRADp4  Z-TMINp1  Z-PRECp1  Z+TMINp1  Z+PRECp1  Z-PRECp3  Z-TMAXp3  Z+PRECp3  Z+TMAXp3  Z-PRECp5  Z+PRECp5  Z-PRECp5-1  Z+PRECp5-1  avgFAPARp2  avgFAPARp4  CROP_AREA  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  YIELD\n",
            "    NL11   1999    0.22   0.026759            28.0        24.84236          2119.0        160.0       3.19      13020.90     27.02       5.92     0.07     0.68     0.08     2.36     0.00     1.78     1.98     2.75       5.76       1.79     88.43       9.57      13.20     60.83  18514.10    -42.65  14995.26     15.23      8.49      9.17     10.01     10.20     10.79     10.14      7.75      7.15      8.83        3.35       33.59        0.60        0.64      15621     51.0     54.4     52.0     55.7     47.0        50.01   60.3\n",
            "    NL11   2000    0.22   0.026759            28.0        24.84236          2119.0        160.0       3.44      15033.47     26.38       5.53     0.31     0.42     1.36     1.27     0.18     0.46     0.00     8.37       6.28       1.83     79.80      12.29      14.45     53.51  17680.69      6.51  12032.57      6.65      8.70     22.80     10.00     10.18     21.17     11.37     10.01      7.80      9.05        7.15        8.83        0.70        0.61      14282     54.4     52.0     55.7     47.0     60.3        55.92   59.1\n",
            "    NL11   2001    0.22   0.026759            28.0        24.84236          2119.0        160.0       3.69      13541.39     26.56       5.80     0.31     0.85     0.76     3.42     0.00     2.32     0.00    12.76       4.15       1.85     79.30       8.01      13.48     63.47  18755.01     68.28  12979.92     23.25      8.05      4.56     13.93      8.24      6.31     19.82     12.92     11.78      6.65        7.80        9.05        0.66        0.61      13659     52.0     55.7     47.0     60.3     59.1        60.46   54.4\n",
            "    NL11   2002    0.22   0.026759            28.0        24.84236          2119.0        160.0       2.66      13979.16     25.64       5.17     0.00     1.45     0.00     3.73     0.00     3.49     0.01     5.93       6.30       2.18     94.93       9.79      14.23     90.31  16490.58     28.78  14749.80      7.46      8.16      8.51     25.06      6.46     13.34     16.00      8.50     12.02      6.49       11.78        6.65        0.65        0.62      13752     55.7     47.0     60.3     59.1     54.4        58.15   55.3\n",
            "    NL11   2003    0.22   0.026759            28.0        24.84236          2119.0        160.0       2.09      10864.88     24.71       5.22     0.00     1.57     0.18     3.70     0.49     1.37     6.79     1.17       4.44       1.30     41.63      12.01      14.45     -7.11  18366.59   -156.91  16804.43      8.40     10.38     13.35     16.79     11.84      3.72      7.67     16.51     11.22     13.58       12.02        6.49        0.66        0.61      13042     47.0     60.3     59.1     54.4     55.3        58.43   59.2\n",
            "\n",
            "Test Data Size: 66 rows\n",
            "X cols: 47, Y cols: 4\n",
            "IDREGION  FYEAR  SM_WHC  STD_SLOPE  AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  IRRIG_AREA6  maxWLAIp2  maxWLIM_YSp4  maxTWCp4  maxWLAIp4  Z-RSMp1  Z+RSMp1  Z-RSMp2  Z+RSMp2  Z-RSMp3  Z+RSMp3  Z-RSMp4  Z+RSMp4  avgTAVGp0  avgPRECp0  avgCWBp0  avgTAVGp1  avgTAVGp2  avgCWBp2  avgRADp2  avgCWBp4  avgRADp4  Z-TMINp1  Z-PRECp1  Z+TMINp1  Z+PRECp1  Z-PRECp3  Z-TMAXp3  Z+PRECp3  Z+TMAXp3  Z-PRECp5  Z+PRECp5  Z-PRECp5-1  Z+PRECp5-1  avgFAPARp2  avgFAPARp4  CROP_AREA  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND  YIELD\n",
            "    NL11   2012    0.22   0.026759            28.0        24.84236          2119.0        160.0       2.30      15253.26     29.07       5.39      0.0     1.72     0.00     5.19     0.00     2.42     0.00     9.41       5.03       1.51     74.08       9.89      13.47     12.95  16935.89    -33.66  13932.66     11.34      6.05     12.10     13.90      7.95     11.06     16.43      8.32      9.17     12.66       10.41       12.87        0.67        0.65      10710     65.6     70.9     74.8     71.2     75.8        77.87   75.2\n",
            "    NL11   2013    0.22   0.026759            28.0        24.84236          2119.0        160.0       3.65      13295.19     27.27       5.43      0.0     0.68     0.42     4.27     0.97     1.29     1.00     3.18       2.79       1.03     31.46      10.43      12.91    -35.08  17323.78   -115.54  14212.99     13.16     11.82     10.25      7.83     13.15     13.96     10.02      5.52      8.16      9.65        9.17       12.66        0.65        0.63      10849     70.9     74.8     71.2     75.8     75.2        76.46   74.6\n",
            "    NL11   2014    0.22   0.026759            28.0        24.84236          2119.0        160.0       3.51      13180.74     27.44       5.27      0.0     1.31     1.12     4.40     0.66     0.09     4.08     0.71       6.62       1.39     40.45      11.33      13.60     15.29  17601.09    -94.25  15925.70      6.76      7.18     16.85     14.90      9.53     10.91      7.10      8.69     12.05      2.53        8.16        9.65        0.69        0.62      11171     74.8     71.2     75.8     75.2     74.6        75.40   86.8\n",
            "    NL11   2015    0.22   0.026759            28.0        24.84236          2119.0        160.0       4.23      14120.36     28.66       5.65      0.0     0.96     0.61     3.09     0.18     0.46     0.00    12.81       5.30       1.85     86.03       9.78      13.27     31.96  18306.59     -8.94  12287.56     15.25      8.52      9.04     12.24      8.51      7.00     15.05     18.26     11.29      7.33       12.05        2.53        0.70        0.62       8176     71.2     75.8     75.2     74.6     86.8        85.72   76.0\n",
            "    NL11   2016    0.22   0.026759            28.0        24.84236          2119.0        160.0       1.83      13200.18     24.07       4.65      0.0     1.09     1.17     3.83     0.00     3.74     2.74     3.53       4.88       1.96     90.45       8.10      13.38     62.46  17361.70    -23.22  15328.88     20.95      7.44      4.45     18.47      4.38     10.07     19.03      5.20     13.84      1.32       11.29        7.33        0.63        0.66      10717     75.8     75.2     74.6     86.8     76.0        81.28   74.8\n",
            "\n",
            "All features\n",
            "-------------\n",
            "\n",
            "1: SM_WHC, 2: STD_SLOPE, 3: AVG_FIELD_SIZE, 4: STD_FIELD_SIZE, 5: IRRIG_AREA_ALL\n",
            "6: IRRIG_AREA6, 7: maxWLAIp2, 8: maxWLIM_YSp4, 9: maxTWCp4, 10: maxWLAIp4\n",
            "11: Z-RSMp1, 12: Z+RSMp1, 13: Z-RSMp2, 14: Z+RSMp2, 15: Z-RSMp3\n",
            "16: Z+RSMp3, 17: Z-RSMp4, 18: Z+RSMp4, 19: avgTAVGp0, 20: avgPRECp0\n",
            "21: avgCWBp0, 22: avgTAVGp1, 23: avgTAVGp2, 24: avgCWBp2, 25: avgRADp2\n",
            "26: avgCWBp4, 27: avgRADp4, 28: Z-TMINp1, 29: Z-PRECp1, 30: Z+TMINp1\n",
            "31: Z+PRECp1, 32: Z-PRECp3, 33: Z-TMAXp3, 34: Z+PRECp3, 35: Z+TMAXp3\n",
            "36: Z-PRECp5, 37: Z+PRECp5, 38: Z-PRECp5-1, 39: Z+PRECp5-1, 40: avgFAPARp2\n",
            "41: avgFAPARp4, 42: CROP_AREA, 43: YIELD-5, 44: YIELD-4, 45: YIELD-3\n",
            "46: YIELD-2, 47: YIELD-1\n",
            "\n",
            "\n",
            "Custom sliding validation train, test splits\n",
            "----------------------------------------------\n",
            "Validation set 1 training years: 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006\n",
            "Validation set 1 test years: 2007\n",
            "Validation set 2 training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007\n",
            "Validation set 2 test years: 2008\n",
            "Validation set 3 training years: 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008\n",
            "Validation set 3 test years: 2009\n",
            "Validation set 4 training years: 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009\n",
            "Validation set 4 test years: 2010\n",
            "Validation set 5 training years: 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010\n",
            "Validation set 5 test years: 2011\n",
            "\n",
            "\n",
            "Null Method: Predicting linear yield trend:\n",
            "Min Yield: 50.6, Max Yield: 92.9\n",
            "Median Yield: 64.6, Mean Yield: 66.03\n",
            "\n",
            " Yield Predictions Training Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "    NL11  1999       50.01  60.3\n",
            "    NL11  2000       55.92  59.1\n",
            "    NL11  2001       60.46  54.4\n",
            "    NL11  2002       58.15  55.3\n",
            "    NL11  2003       58.43  59.2\n",
            "    NL11  2004       55.86    62\n",
            "\n",
            " Yield Predictions Validation Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "    NL11  2007       68.55  65.6\n",
            "    NL11  2008       68.12  70.9\n",
            "    NL11  2009       71.57  74.8\n",
            "    NL11  2010       76.59  71.2\n",
            "    NL11  2011       75.66  75.8\n",
            "    NL12  2007       68.57    65\n",
            "\n",
            " Yield Predictions Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD\n",
            "    NL11  2012       77.87  75.2\n",
            "    NL11  2013       76.46  74.6\n",
            "    NL11  2014        75.4  86.8\n",
            "    NL11  2015       85.72    76\n",
            "    NL11  2016       81.28  74.8\n",
            "    NL11  2017       77.66  87.7\n",
            "\n",
            "Estimator: Ridge\n",
            "---------------------------\n",
            "\n",
            "Feature Selection Summary\n",
            "---------------------------\n",
            "estimator       selector  neg_mean_squared_error\n",
            "    Ridge  random_forest                  -26.41\n",
            "    Ridge      RFE_Lasso                  -31.68\n",
            "    Ridge       combined                  -24.13\n",
            "\n",
            "estimator__alpha= 10\n",
            "\n",
            "Selected features with importance:\n",
            "----------------------------------\n",
            "\n",
            "45: YIELD-3=2.22, 47: YIELD-1=1.92, 43: YIELD-5=1.1, 46: YIELD-2=0.85, 25: avgRADp2=0.79\n",
            "34: Z+PRECp3=0.69, 44: YIELD-4=0.57, 10: maxWLAIp4=0.54, 33: Z-TMAXp3=0.51, 9: maxTWCp4=0.44\n",
            "6: IRRIG_AREA6=0.44, 41: avgFAPARp4=0.41, 37: Z+PRECp5=0.39, 27: avgRADp4=0.32, 29: Z-PRECp1=0.11\n",
            "22: avgTAVGp1=0.01, 24: avgCWBp2=0.0, 12: Z+RSMp1=-0.2, 14: Z+RSMp2=-0.24, 42: CROP_AREA=-0.36\n",
            "35: Z+TMAXp3=-0.4, 3: AVG_FIELD_SIZE=-0.43, 28: Z-TMINp1=-0.46, 11: Z-RSMp1=-0.56, 20: avgPRECp0=-0.61\n",
            "26: avgCWBp4=-0.63, 18: Z+RSMp4=-0.98, 30: Z+TMINp1=-1.07, 31: Z+PRECp1=-1.67\n",
            "\n",
            "\n",
            "Estimator: KNN\n",
            "---------------------------\n",
            "\n",
            "Feature Selection Summary\n",
            "---------------------------\n",
            "estimator       selector  neg_mean_squared_error\n",
            "      KNN  random_forest                  -24.84\n",
            "      KNN      RFE_Lasso                  -47.13\n",
            "      KNN       combined                  -40.22\n",
            "\n",
            "estimator__n_neighbors= 3\n",
            "\n",
            "Selected Features:\n",
            "-------------------\n",
            "\n",
            "12: Z+RSMp1, 20: avgPRECp0, 22: avgTAVGp1, 24: avgCWBp2, 31: Z+PRECp1\n",
            "43: YIELD-5, 44: YIELD-4, 45: YIELD-3, 46: YIELD-2, 47: YIELD-1\n",
            "\n",
            "\n",
            "\n",
            "Estimator: SVR\n",
            "---------------------------\n",
            "\n",
            "Feature Selection Summary\n",
            "---------------------------\n",
            "estimator       selector  neg_mean_squared_error\n",
            "      SVR  random_forest                  -36.27\n",
            "      SVR      RFE_Lasso                  -55.27\n",
            "      SVR       combined                  -48.01\n",
            "\n",
            "estimator__C= 50.0\n",
            "estimator__epsilon= 0.01\n",
            "\n",
            "Selected Features:\n",
            "-------------------\n",
            "\n",
            "10: maxWLAIp4, 12: Z+RSMp1, 20: avgPRECp0, 22: avgTAVGp1, 24: avgCWBp2\n",
            "25: avgRADp2, 26: avgCWBp4, 29: Z-PRECp1, 31: Z+PRECp1, 37: Z+PRECp5\n",
            "43: YIELD-5, 44: YIELD-4, 45: YIELD-3, 46: YIELD-2, 47: YIELD-1\n",
            "\n",
            "\n",
            "\n",
            "Estimator: GBDT\n",
            "---------------------------\n",
            "\n",
            "Feature Selection Summary\n",
            "---------------------------\n",
            "estimator       selector  neg_mean_squared_error\n",
            "     GBDT  random_forest                  -44.67\n",
            "     GBDT      RFE_Lasso                  -47.47\n",
            "     GBDT       combined                  -47.41\n",
            "\n",
            "estimator__max_depth= 10\n",
            "estimator__n_estimators= 500\n",
            "\n",
            "Selected features with importance:\n",
            "----------------------------------\n",
            "\n",
            "47: YIELD-1=0.18, 45: YIELD-3=0.12, 44: YIELD-4=0.12, 43: YIELD-5=0.1, 22: avgTAVGp1=0.1\n",
            "31: Z+PRECp1=0.09, 46: YIELD-2=0.09, 24: avgCWBp2=0.08, 12: Z+RSMp1=0.06, 20: avgPRECp0=0.06\n",
            "\n",
            "\n",
            "\n",
            "Feature Selection Frequencies\n",
            "-------------------------------\n",
            "static: YIELD-5(4), YIELD-4(4), YIELD-3(4), YIELD-2(4), YIELD-1(4), AVG_FIELD_SIZE(1), IRRIG_AREA6(1), CROP_AREA(1)\n",
            "p0: avgPRECp0(4)\n",
            "p1: Z+RSMp1(4), avgTAVGp1(4), Z+PRECp1(4), Z-PRECp1(2), Z-RSMp1(1), Z-TMINp1(1), Z+TMINp1(1)\n",
            "p2: avgCWBp2(4), avgRADp2(2), Z+RSMp2(1)\n",
            "p3: Z-TMAXp3(1), Z+PRECp3(1), Z+TMAXp3(1)\n",
            "p4: maxWLAIp4(2), avgCWBp4(2), maxTWCp4(1), Z+RSMp4(1), avgRADp4(1), avgFAPARp4(1)\n",
            "p5: Z+PRECp5(2)\n",
            "\n",
            "\n",
            "   IDREGION FYEAR YIELD  ... YIELD_PRED_KNN YIELD_PRED_SVR YIELD_PRED_GBDT\n",
            "0     NL11  2012  75.2  ...        75.0993        69.6442         75.6569\n",
            "1     NL11  2013  74.6  ...        83.2016        80.9559         81.7156\n",
            "2     NL11  2014  86.8  ...        83.4849        78.4801         78.1675\n",
            "3     NL11  2015    76  ...        81.8633        84.2034         79.7438\n",
            "4     NL11  2016  74.8  ...        80.4189        71.6265         75.6541\n",
            "\n",
            "[5 rows x 7 columns]\n",
            "\n",
            " Yield Predictions Training Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_Ridge YIELD_PRED_KNN YIELD_PRED_SVR YIELD_PRED_GBDT YIELD\n",
            "    NL11  1999       50.01          58.3044           60.3        60.2899         59.7277  60.3\n",
            "    NL11  2000       55.92          57.8428           59.1        59.1102         59.1056  59.1\n",
            "    NL11  2001       60.46          55.2594           54.4        54.4101         55.3627  54.4\n",
            "    NL11  2002       58.15          55.9361           55.3          55.31          55.936  55.3\n",
            "    NL11  2003       58.43          60.1999           59.2        59.2102         58.4446  59.2\n",
            "    NL11  2004       55.86          64.8274             62        62.0102         62.4322    62\n",
            "\n",
            " Yield Predictions Validation Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_Ridge YIELD_PRED_KNN YIELD_PRED_SVR YIELD_PRED_GBDT YIELD\n",
            "    NL11  2007       68.55           66.302        62.5934        62.9906         63.1748  65.6\n",
            "    NL11  2008       68.12          65.5007        66.8272        69.3086         68.2141  70.9\n",
            "    NL11  2009       71.57          73.8351        71.9994        73.3184         70.9862  74.8\n",
            "    NL11  2010       76.59          74.3976        73.7645        71.7605         76.3965  71.2\n",
            "    NL11  2011       75.66          77.0929        76.9795        74.3392         74.9314  75.8\n",
            "    NL12  2007       68.57          65.0597        60.6777        64.1428         64.3037    65\n",
            "\n",
            " Yield Predictions Test Set\n",
            "--------------------------------\n",
            "IDREGION FYEAR YIELD_TREND YIELD_PRED_Ridge YIELD_PRED_KNN YIELD_PRED_SVR YIELD_PRED_GBDT YIELD\n",
            "    NL11  2012       77.87          74.0864        75.0993        69.6442         75.6569  75.2\n",
            "    NL11  2013       76.46          79.2103        83.2016        80.9559         81.7156  74.6\n",
            "    NL11  2014        75.4          76.9974        83.4849        78.4801         78.1675  86.8\n",
            "    NL11  2015       85.72          77.4898        81.8633        84.2034         79.7438    76\n",
            "    NL11  2016       81.28          77.0113        80.4189        71.6265         75.6541  74.8\n",
            "    NL11  2017       77.66           80.572          79.98        76.0993         81.3685  87.7\n",
            "\n",
            "Algorithm Evaluation Summary for NL\n",
            "-----------------------------------------\n",
            "algorithm  train_MAPE  cv_MAPE  test_MAPE  train_RMSE  cv_RMSE  test_RMSE  train_R2  cv_R2  test_R2\n",
            "    trend        6.89     6.45       9.44        8.38     8.13      11.32      0.57   0.19    -0.08\n",
            "    Ridge        3.40     5.40       6.77        4.20     6.65       8.36      0.89   0.46     0.41\n",
            "      KNN        0.00     5.90       7.78        0.00     7.05       9.25      1.00   0.39     0.28\n",
            "      SVR        0.29     5.93      12.50        1.46     7.83      16.42      0.99   0.25    -1.27\n",
            "     GBDT        1.32     6.92       8.58        2.67     9.06      10.99      0.96  -0.00    -0.02\n",
            "\n",
            "\n",
            "Saving predictions to ./pred_sugarbeet_NL_NUTS2_trend.csv\n",
            "  IDREGION FYEAR YIELD_TREND  ... YIELD_PRED_SVR YIELD_PRED_GBDT YIELD\n",
            "0     NL11  2012       77.87  ...        69.6442         75.6569  75.2\n",
            "1     NL11  2013       76.46  ...        80.9559         81.7156  74.6\n",
            "2     NL11  2014        75.4  ...        78.4801         78.1675  86.8\n",
            "3     NL11  2015       85.72  ...        84.2034         79.7438    76\n",
            "4     NL11  2016       81.28  ...        71.6265         75.6541  74.8\n",
            "\n",
            "[5 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivn9xbXlPs4d"
      },
      "source": [
        "### Compare Predictions with JRC Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyzPXCxEtpC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "604e4461-cbb9-44d5-d6d9-543948381d8e"
      },
      "source": [
        "if (test_env == 'notebook'):\n",
        "  if (use_saved_predictions):\n",
        "    pd_ml_predictions = loadSavedPredictions(cyp_config, spark)\n",
        "\n",
        "  compareWithMCYFS = cyp_config.compareWithMCYFS()\n",
        "  if (compareWithMCYFS):\n",
        "    comparePredictionsWithMCYFS(sqlContext, cyp_config, pd_ml_predictions, log_fh)\n",
        "\n",
        "  log_fh.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##############\n",
            "# Load Data  #\n",
            "##############\n",
            "Data file name \"./WOFOST_NUTS2_NL.csv\"\n",
            "Data file name \"./AREA_FRACTIONS_NUTS2_NL.csv\"\n",
            "Data file name \"./AREA_FRACTIONS_NUTS1_NL.csv\"\n",
            "Data file name \"./YIELD_NUTS0_NL.csv\"\n",
            "Data file name \"./YIELD_PRED_MCYFS_NUTS0_NL.csv\"\n",
            "Loaded data: WOFOST, AREA_FRACTIONS, YIELD, YIELD_PRED_MCYFS\n",
            "\n",
            "\n",
            "####################\n",
            "# Preprocess Data  #\n",
            "####################\n",
            "NUTS0 Yield before preprocessing\n",
            "+------+--------+-----+-----+\n",
            "|  CROP|IDREGION|FYEAR|YIELD|\n",
            "+------+--------+-----+-----+\n",
            "|potato|      NL| 1971| 37.3|\n",
            "|potato|      NL| 1972| 37.5|\n",
            "|potato|      NL| 1973| 36.8|\n",
            "|potato|      NL| 1974| 38.4|\n",
            "|potato|      NL| 1975| 33.1|\n",
            "|potato|      NL| 1976| 29.8|\n",
            "|potato|      NL| 1977| 33.8|\n",
            "|potato|      NL| 1978| 38.6|\n",
            "|potato|      NL| 1979| 37.8|\n",
            "|potato|      NL| 1980| 36.3|\n",
            "+------+--------+-----+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "NUTS0 Yield after preprocessing\n",
            "+--------+-----+-----+\n",
            "|IDREGION|FYEAR|YIELD|\n",
            "+--------+-----+-----+\n",
            "|      NL| 2012| 78.9|\n",
            "|      NL| 2013| 76.0|\n",
            "|      NL| 2014| 87.4|\n",
            "|      NL| 2015| 83.3|\n",
            "|      NL| 2016| 77.8|\n",
            "|      NL| 2017| 93.3|\n",
            "+--------+-----+-----+\n",
            "\n",
            "MCYFS yield predictions before preprocessing\n",
            "+--------+------+---------------+-----+----------+\n",
            "|IDREGION|  CROP|PREDICTION_DATE|FYEAR|YIELD_PRED|\n",
            "+--------+------+---------------+-----+----------+\n",
            "|      NL|Potato|     01/08/2005| 2005|     45.91|\n",
            "|      NL|Potato|     01/10/2013| 2013|     43.68|\n",
            "|      NL|Potato|     02/03/2018| 2018|     44.54|\n",
            "|      NL|Potato|     02/09/2013| 2013|     43.37|\n",
            "|      NL|Potato|     03/06/2019| 2019|     44.27|\n",
            "|      NL|Potato|     03/07/2017| 2017|     43.55|\n",
            "|      NL|Potato|     03/09/2019| 2019|     41.94|\n",
            "|      NL|Potato|     04/04/2019| 2019|     44.82|\n",
            "|      NL|Potato|     04/07/2006| 2006|     43.63|\n",
            "|      NL|Potato|     05/04/2018| 2018|     44.54|\n",
            "+--------+------+---------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "MCYFS yield predictions after preprocessing\n",
            "+--------+-----+----------+----------+----------+\n",
            "|IDREGION|FYEAR| PRED_DATE|YIELD_PRED|PRED_DEKAD|\n",
            "+--------+-----+----------+----------+----------+\n",
            "|      NL| 2013|01/10/2013|      76.7|        28|\n",
            "|      NL| 2013|02/09/2013|     74.92|        25|\n",
            "|      NL| 2017|03/07/2017|     83.71|        19|\n",
            "|      NL| 2016|06/05/2016|     84.31|        13|\n",
            "|      NL| 2016|06/06/2016|     82.83|        16|\n",
            "|      NL| 2016|07/03/2016|     84.39|         7|\n",
            "|      NL| 2013|08/03/2013|     79.24|         7|\n",
            "|      NL| 2013|08/05/2013|     79.24|        13|\n",
            "|      NL| 2017|08/09/2017|     85.71|        25|\n",
            "|      NL| 2013|09/04/2013|     79.24|        10|\n",
            "+--------+-----+----------+----------+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            " FYEAR  END_SEASON  EARLY_SEASON\n",
            "  2012        29.0          29.0\n",
            "  2013        30.0          30.0\n",
            "  2014        28.0          28.0\n",
            "  2015        30.0          30.0\n",
            "  2016        28.0          28.0\n",
            "\n",
            "Algorithm Evaluation Summary (NUTS0) for NL\n",
            "-------------------------------------------\n",
            "         algorithm  test_MAPE  test_RMSE  test_R2\n",
            "             Ridge       5.44       6.23     0.27\n",
            "               KNN       5.78       6.66     0.16\n",
            "               SVR      10.27      13.30    -2.34\n",
            "              GBDT       6.40       8.08    -0.23\n",
            " MCYFS_Predictions       3.59       4.60     0.60\n",
            "\n",
            "\n",
            "NUTS0 Predictions of ML algorithms\n",
            "  IDREGION  FYEAR  ...  YIELD_PRED_MCYFS      YIELD\n",
            "0       NL   2012  ...         78.750000  78.900002\n",
            "1       NL   2013  ...         76.699997  76.000000\n",
            "2       NL   2014  ...         82.709999  87.400002\n",
            "3       NL   2015  ...         80.919998  83.300003\n",
            "4       NL   2016  ...         81.860001  77.800003\n",
            "\n",
            "[5 rows x 8 columns]\n",
            "\n",
            "Saving predictions to ./pred_sugarbeet_NL_NUTS0_trend.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ1Ya_s0bVbZ"
      },
      "source": [
        "### Python Script Main\n",
        "\n",
        "To be used in environment supporting command-line arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo3wAY6AWdId",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0755a758-a789-49ee-f632-849cbbc97c5c"
      },
      "source": [
        "#%%writefile main.py\n",
        "import sys\n",
        "import argparse\n",
        "\n",
        "def main():\n",
        "  print('##################')\n",
        "  print('# Configuration  #')\n",
        "  print('##################')\n",
        "\n",
        "  parser = argparse.ArgumentParser(prog='mlbaseline_plus.py')\n",
        "\n",
        "  # Some command-line argument names are slightly different\n",
        "  # from configuration option names for brevity.\n",
        "  args_dict = {\n",
        "      '--crop' : { 'type' : str,\n",
        "                   'default' : 'potatoes',\n",
        "                   'help' : 'crop name (default: potatoes)',\n",
        "                 },\n",
        "      '--crosses-calendar-year' : { 'type' : str,\n",
        "                                    'default' : 'N',\n",
        "                                    'choices' : ['Y', 'N'],\n",
        "                                    'help' : 'crop growing season crosses calendar year boundary (default: N)',\n",
        "                                  },\n",
        "      '--country' : { 'type' : str,\n",
        "                      'default' : 'NL',\n",
        "                      'choices' : ['BG', 'DE', 'ES', 'FR', 'HU', 'IT', 'NL', 'PL', 'RO'],\n",
        "                      'help' : 'country code (default: NL)',\n",
        "                    },\n",
        "      '--nuts-level' : { 'type' : str,\n",
        "                         'default' : 'NUTS2',\n",
        "                         'choices' : ['NUTS2', 'NUTS3'],\n",
        "                         'help' : 'country code (default: NL)',\n",
        "                       },\n",
        "      '--data-path' : { 'type' : str,\n",
        "                        'default' : '.',\n",
        "                        'help' : 'path to data files (default: .)',\n",
        "                       },\n",
        "      '--output-path' : { 'type' : str,\n",
        "                          'default' : '.',\n",
        "                          'help' : 'path to output files (default: .)',\n",
        "                        },\n",
        "      '--clean-data' : { 'type' : str,\n",
        "                         'default' : 'Y',\n",
        "                         'choices' : ['Y', 'N'],\n",
        "                         'help' : 'remove data or regions with duplicate or missing values (default: Y)',\n",
        "                       },\n",
        "      '--yield-trend' : { 'type' : str,\n",
        "                          'default' : 'Y',\n",
        "                          'choices' : ['Y', 'N'],\n",
        "                          'help' : 'estimate and use yield trend (default: Y)',\n",
        "                        },\n",
        "      '--optimal-trend-window' : { 'type' : str,\n",
        "                                   'default' : 'N',\n",
        "                                   'choices' : ['Y', 'N'],\n",
        "                                   'help' : 'find optimal trend window for each year (default: N)',\n",
        "                                 },\n",
        "      '--predict-residuals' : { 'type' : str,\n",
        "                                'default' : 'N',\n",
        "                                'choices' : ['Y', 'N'],\n",
        "                                'help' : 'predict yield residuals instead of full yield (default: N)',\n",
        "                              },\n",
        "      '--per-year-crop-calendar' : { 'type' : str,\n",
        "                                     'default' : 'Y',\n",
        "                                     'choices' : ['Y', 'N'],\n",
        "                                     'help' : 'use per region per year crop calendar (default: Y)',\n",
        "                                   },\n",
        "      '--early-season' : { 'type' : str,\n",
        "                           'default' : 'N',\n",
        "                           'choices' : ['Y', 'N'],\n",
        "                           'help' : 'early season prediction (default: N)',\n",
        "                         },\n",
        "      '--early-season-end' : { 'type' : int,\n",
        "                               'default' : 0,\n",
        "                               'help' : 'early season end dekad (default: 0)',\n",
        "                             },\n",
        "      '--centroids' : { 'type' : str,\n",
        "                        'default' : 'N',\n",
        "                        'choices' : ['Y', 'N'],\n",
        "                        'help' : 'use centroid coordinates and distance to coast (default: N)',\n",
        "                      },\n",
        "      '--remote-sensing' : { 'type' : str,\n",
        "                             'default' : 'Y',\n",
        "                             'choices' : ['Y', 'N'],\n",
        "                             'help' : 'use remote sensing data (default: Y)',\n",
        "                           },\n",
        "      '--gaes' : { 'type' : str,\n",
        "                   'default' : 'N',\n",
        "                   'choices' : ['Y', 'N'],\n",
        "                   'help' : 'use agro-environmental zones data',\n",
        "                 },\n",
        "      '--use-features-v2' : { 'type' : str,\n",
        "                              'default' : 'Y',\n",
        "                              'choices' : ['Y', 'N'],\n",
        "                              'help' : 'use feature design v2 (default: Y)',\n",
        "                            },\n",
        "      '--save-features' : { 'type' : str,\n",
        "                            'default' : 'N',\n",
        "                            'choices' : ['Y', 'N'],\n",
        "                            'help' : 'save features to a CSV file (default: N)',\n",
        "                          },\n",
        "      '--use-saved-features' : { 'type' : str,\n",
        "                                 'default' : 'N',\n",
        "                                 'choices' : ['Y', 'N'],\n",
        "                                 'help' : 'use features from a CSV file (default: N)',\n",
        "                               },\n",
        "      '--use-sample-weights' : { 'type' : str,\n",
        "                                 'default' : 'N',\n",
        "                                 'choices' : ['Y', 'N'],\n",
        "                                 'help' : 'Use recency as data sample weight (default N)',\n",
        "                               }, \n",
        "      '--retrain-per-test-year' : { 'type' : str,\n",
        "                                    'default' : 'N',\n",
        "                                    'choices' : ['Y', 'N'],\n",
        "                                    'help' : 'retrain a model for every test year (default: N)',\n",
        "                                  },\n",
        "      '--save-predictions' : { 'type' : str,\n",
        "                               'default' : 'Y',\n",
        "                               'choices' : ['Y', 'N'],\n",
        "                               'help' : 'save predictions to a CSV file (default: Y)',\n",
        "                             },\n",
        "      '--use-saved-predictions' : { 'type' : str,\n",
        "                                    'default' : 'N',\n",
        "                                    'choices' : ['Y', 'N'],\n",
        "                                    'help' : 'use predictions from a CSV file (default: N)',\n",
        "                                  },\n",
        "      '--compare-with-mcyfs' : { 'type' : str,\n",
        "                                 'default' : 'N',\n",
        "                                 'choices' : ['Y', 'N'],\n",
        "                                 'help' : 'compare predictions with MCYFS (default: N)',\n",
        "                               },\n",
        "      '--debug-level' : { 'type' : int,\n",
        "                          'default' : 0,\n",
        "                          'choices' : range(4),\n",
        "                          'help' : 'amount of debug information to print (default: 0)',\n",
        "                        },\n",
        "  }\n",
        "\n",
        "  for arg in args_dict:\n",
        "    arg_config = args_dict[arg]\n",
        "    # add cases if other argument settings are used\n",
        "    if ('choices' in arg_config):\n",
        "      parser.add_argument(arg, type=arg_config['type'], default=arg_config['default'],\n",
        "                          choices=arg_config['choices'], help=arg_config['help'])\n",
        "    else:\n",
        "      parser.add_argument(arg, type=arg_config['type'], default=arg_config['default'],\n",
        "                          help=arg_config['help'])\n",
        "\n",
        "  if (run_tests):\n",
        "    test_util = TestUtil(spark)\n",
        "    test_util.runAllTests()\n",
        "\n",
        "  args = parser.parse_args()\n",
        "  cyp_config = CYPConfiguration()\n",
        "\n",
        "  # must be in sync with args_dict used to parse args\n",
        "  config_update = {\n",
        "      'crop_name' : args.crop,\n",
        "      'season_crosses_calendar_year' : args.crosses_calendar_year,\n",
        "      'country_code' : args.country,\n",
        "      'nuts_level' : args.nuts_level,\n",
        "      'data_path' : args.data_path,\n",
        "      'output_path' : args.output_path,\n",
        "      'clean_data' : args.clean_data,\n",
        "      'use_yield_trend' : args.yield_trend,\n",
        "      'find_optimal_trend_window' : args.optimal_trend_window,\n",
        "      'predict_yield_residuals' : args.predict_residuals,\n",
        "      'use_centroids' : args.centroids,\n",
        "      'use_remote_sensing' : args.remote_sensing,\n",
        "      'use_gaes' : args.gaes,\n",
        "      'use_per_year_crop_calendar' : args.per_year_crop_calendar,\n",
        "      'early_season_prediction' : args.early_season,\n",
        "      'early_season_end_dekad' : args.early_season_end,\n",
        "      'use_features_v2' : args.use_features_v2,\n",
        "      'save_features' : args.save_features,\n",
        "      'use_saved_features' : args.use_saved_features,\n",
        "      'use_sample_weights' : args.use_sample_weights,\n",
        "      'retrain_per_test_year' : args.retrain_per_test_year,\n",
        "      'save_predictions' : args.save_predictions,\n",
        "      'use_saved_predictions' : args.use_saved_predictions,\n",
        "      'compare_with_mcyfs' : args.compare_with_mcyfs,\n",
        "      'debug_level' : args.debug_level,\n",
        "  }\n",
        "\n",
        "  cyp_config.updateConfiguration(config_update)\n",
        "  crop = cyp_config.getCropName()\n",
        "  country = cyp_config.getCountryCode()\n",
        "  nuts_level = cyp_config.getNUTSLevel()\n",
        "  debug_level = cyp_config.getDebugLevel()\n",
        "  use_saved_predictions = cyp_config.useSavedPredictions()\n",
        "  use_saved_features = cyp_config.useSavedFeatures()\n",
        "  use_yield_trend = cyp_config.useYieldTrend()\n",
        "  early_season_prediction = cyp_config.earlySeasonPrediction()\n",
        "  early_season_end = cyp_config.getEarlySeasonEndDekad()\n",
        "\n",
        "  output_path = cyp_config.getOutputPath()\n",
        "  log_file = getLogFilename(crop, country, use_yield_trend,\n",
        "                            early_season_prediction, early_season_end)\n",
        "  log_fh = open(output_path + '/' + log_file, 'w+')\n",
        "  cyp_config.printConfig(log_fh)\n",
        "\n",
        "  if (not use_saved_predictions):\n",
        "    if (not use_saved_features):\n",
        "      print('#################')\n",
        "      print('# Data Loading  #')\n",
        "      print('#################')\n",
        "\n",
        "      if (run_tests):\n",
        "        test_loader = TestDataLoader(spark)\n",
        "        test_loader.runAllTests()\n",
        "\n",
        "      cyp_loader = CYPDataLoader(spark, cyp_config)\n",
        "      data_dfs = cyp_loader.loadAllData()\n",
        "\n",
        "      print('#######################')\n",
        "      print('# Data Preprocessing  #')\n",
        "      print('#######################')\n",
        "\n",
        "      if (run_tests):\n",
        "        test_preprocessor = TestDataPreprocessor(spark)\n",
        "        test_preprocessor.runAllTests()\n",
        "\n",
        "      cyp_preprocessor = CYPDataPreprocessor(spark, cyp_config)\n",
        "      data_dfs = preprocessData(cyp_config, cyp_preprocessor, data_dfs)\n",
        "\n",
        "      print('###########################')\n",
        "      print('# Training and Test Split #')\n",
        "      print('###########################')\n",
        "\n",
        "      if (run_tests):\n",
        "        yield_df = data_dfs['YIELD']\n",
        "        test_custom = TestCustomTrainTestSplit(yield_df)\n",
        "        test_custom.runAllTests()\n",
        "\n",
        "      prep_train_test_dfs, test_years = splitDataIntoTrainingTestSets(cyp_config, data_dfs, log_fh)\n",
        "\n",
        "      print('#################')\n",
        "      print('# Data Summary  #')\n",
        "      print('#################')\n",
        "\n",
        "      if (run_tests):\n",
        "        test_summarizer = TestDataSummarizer(spark)\n",
        "        test_summarizer.runAllTests()\n",
        "\n",
        "      cyp_summarizer = CYPDataSummarizer(cyp_config)\n",
        "      summary_dfs = summarizeData(cyp_config, cyp_summarizer, prep_train_test_dfs)\n",
        "\n",
        "      print('###################')\n",
        "      print('# Feature Design  #')\n",
        "      print('###################')\n",
        "\n",
        "      # WOFOST, Meteo and Remote Sensing Features\n",
        "      cyp_featurizer = CYPFeaturizer(cyp_config)\n",
        "      pd_feature_dfs = createFeatures(cyp_config, cyp_featurizer,\n",
        "                                      prep_train_test_dfs, summary_dfs, log_fh)\n",
        "\n",
        "      # trend features\n",
        "      join_cols = ['IDREGION', 'FYEAR']\n",
        "      if (use_yield_trend):\n",
        "        yield_train_df = prep_train_test_dfs['YIELD'][0]\n",
        "        yield_test_df = prep_train_test_dfs['YIELD'][1]\n",
        "\n",
        "        # Trend features from feature data\n",
        "        use_features_v2 = cyp_config.useFeaturesV2()\n",
        "        if (use_features_v2):\n",
        "          pd_feature_dfs = addFeaturesFromPreviousYears(cyp_config, pd_feature_dfs,\n",
        "                                                        1, test_years, join_cols)\n",
        "\n",
        "        if (run_tests):\n",
        "          test_yield_trend = TestYieldTrendEstimator(yield_train_df)\n",
        "          test_yield_trend.runAllTests()\n",
        "\n",
        "        # Trend features from label data\n",
        "        cyp_trend_est = CYPYieldTrendEstimator(cyp_config)\n",
        "        pd_yield_train_ft, pd_yield_test_ft = createYieldTrendFeatures(cyp_config, cyp_trend_est,\n",
        "                                                                       yield_train_df, yield_test_df,\n",
        "                                                                       test_years)\n",
        "        pd_feature_dfs['YIELD_TREND'] = [pd_yield_train_ft, pd_yield_test_ft]\n",
        "\n",
        "      # combine features\n",
        "      pd_train_df, pd_test_df = combineFeaturesLabels(cyp_config, sqlContext,\n",
        "                                                      prep_train_test_dfs, pd_feature_dfs,\n",
        "                                                      join_cols, log_fh)\n",
        "\n",
        "    # use saved features\n",
        "    else:\n",
        "      pd_train_df, pd_test_df = loadSavedFeaturesLabels(cyp_config, spark)\n",
        "\n",
        "    print('###################################')\n",
        "    print('# Machine Learning using sklearn  #')\n",
        "    print('###################################')\n",
        "\n",
        "    # drop mutually correlated features\n",
        "    corr_threshold = cyp_config.getFeatureCorrelationThreshold()\n",
        "    pd_train_df, pd_test_df = dropHighlyCorrelatedFeatures(cyp_config, pd_train_df, pd_test_df,\n",
        "                                                           log_fh, corr_thresh=corr_threshold)\n",
        "\n",
        "    pd_ml_predictions = getMachineLearningPredictions(cyp_config, pd_train_df, pd_test_df, log_fh)\n",
        "    save_predictions = cyp_config.savePredictions()\n",
        "    if (save_predictions):\n",
        "      saveMLPredictions(cyp_config, sqlContext, pd_ml_predictions)\n",
        "\n",
        "  # use saved predictions\n",
        "  else:\n",
        "    pd_ml_predictions = loadSavedPredictions(cyp_config, spark)\n",
        "\n",
        "  # compare with MCYFS\n",
        "  compareWithMCYFS = cyp_config.compareWithMCYFS()\n",
        "  if (compareWithMCYFS):\n",
        "    comparePredictionsWithMCYFS(sqlContext, cyp_config, pd_ml_predictions, log_fh)\n",
        "\n",
        "  log_fh.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing main.py\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}