{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ksndBerUYL_"
      },
      "source": [
        "# Crop Yield Prediction - Deep Learning\n",
        "\n",
        "Use NUTS3 input data to produce NUTS3 crop yield forecasts. Aggregate forecasts to NUTS2 level and train model using weak supervision from NUTS2 yield statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF2EPBzXz7IP",
        "outputId": "61bf6591-f614-4439-9634-4cfc890dc30a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_env = 'notebook'\n",
        "if (test_env == 'notebook'):\n",
        "  !pip install d2l==0.16.1 >/dev/null\n",
        "\n",
        "  from d2l import torch as d2l\n",
        "  import numpy as np\n",
        "  import torch\n",
        "  from torch import nn\n",
        "  import pandas as pd\n",
        "  import matplotlib.pyplot as plt\n",
        "  %matplotlib inline\n",
        "  plt.rcParams['figure.figsize'] = [30,25]\n",
        "  plt.rcParams.update({'font.size': 12})\n",
        "\n",
        "  !pip install pyspark > /dev/null\n",
        "  !sudo apt update > /dev/null\n",
        "  !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "  !pip install joblibspark > /dev/null\n",
        "\n",
        "  import os\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "else:  \n",
        "  import findspark\n",
        "  findspark.init()\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import functions as SparkF\n",
        "from pyspark.sql import types as SparkT\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "SparkContext.setSystemProperty('spark.executor.memory', '12g')\n",
        "SparkContext.setSystemProperty('spark.driver.memory', '6g')\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
        "\n",
        "# crop name to id mapping\n",
        "crop_id_dict = {\n",
        "    'grain maize': 2,\n",
        "    'sugar beet' : 6,\n",
        "    'sugarbeet' : 6,\n",
        "    'sugarbeets' : 6,\n",
        "    'sugar beets' : 6,\n",
        "    'total potatoes' : 7,\n",
        "    'potatoes' : 7,\n",
        "    'potato' : 7,\n",
        "    'winter wheat' : 90,\n",
        "    'soft wheat' : 90,\n",
        "    'sunflower' : 93,\n",
        "    'spring barley' : 95,\n",
        "}\n",
        "\n",
        "# crop id to name mapping\n",
        "crop_name_dict = {\n",
        "    2 : 'grain maize',\n",
        "    6 : 'sugarbeet',\n",
        "    7 : 'potatoes',\n",
        "    90 : 'soft wheat',\n",
        "    93 : 'sunflower',\n",
        "    95 : 'spring barley',\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vdmh5cvWm0yB"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pcqgv5_m2rF"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from pyspark.sql import Window\n",
        "\n",
        "# crop name and id mappings\n",
        "def cropNameToID(crop_id_dict, crop):\n",
        "  \"\"\"\n",
        "  Return id of given crop. Relies on crop_id_dict.\n",
        "  Return 0 if crop name is not in the dictionary.\n",
        "  \"\"\"\n",
        "  crop_lcase = crop.lower()\n",
        "  try:\n",
        "    crop_id = crop_id_dict[crop_lcase]\n",
        "  except KeyError as e:\n",
        "    crop_id = 0\n",
        "\n",
        "  return crop_id\n",
        "\n",
        "def cropIDToName(crop_name_dict, crop_id):\n",
        "  \"\"\"\n",
        "  Return crop name for given crop ID. Relies on crop_name_dict.\n",
        "  Return 'NA' if crop id is not found in the dictionary.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    crop_name = crop_name_dict[crop_id]\n",
        "  except KeyError as e:\n",
        "    crop_name = 'NA'\n",
        "\n",
        "  return crop_name\n",
        "\n",
        "def getYear(date_str):\n",
        "  \"\"\"Extract year from date in yyyyMMdd or dd/MM/yyyy format.\"\"\"\n",
        "  return SparkF.when(SparkF.length(date_str) == 8,\n",
        "                     SparkF.year(SparkF.to_date(date_str, 'yyyyMMdd')))\\\n",
        "                     .otherwise(SparkF.year(SparkF.to_date(date_str, 'dd/MM/yyyy')))\n",
        "\n",
        "def getMonth(date_str):\n",
        "  \"\"\"Extract month from date in yyyyMMdd or dd/MM/yyyy format.\"\"\"\n",
        "  return SparkF.when(SparkF.length(date_str) == 8,\n",
        "                     SparkF.month(SparkF.to_date(date_str, 'yyyyMMdd')))\\\n",
        "                     .otherwise(SparkF.month(SparkF.to_date(date_str, 'dd/MM/yyyy')))\n",
        "\n",
        "def getDay(date_str):\n",
        "  \"\"\"Extract day from date in yyyyMMdd or dd/MM/yyyy format.\"\"\"\n",
        "  return SparkF.when(SparkF.length(date_str) == 8,\n",
        "                     SparkF.dayofmonth(SparkF.to_date(date_str, 'yyyyMMdd')))\\\n",
        "                     .otherwise(SparkF.dayofmonth(SparkF.to_date(date_str, 'dd/MM/yyyy')))\n",
        "\n",
        "# 1-10: Dekad 1\n",
        "# 11-20: Dekad 2\n",
        "# > 20 : Dekad 3\n",
        "def getDekad(date_str):\n",
        "  \"\"\"Extract dekad from date in YYYYMMDD format.\"\"\"\n",
        "  month = getMonth(date_str)\n",
        "  day = getDay(date_str)\n",
        "  return SparkF.when(day < 30, (month - 1)* 3 +\n",
        "                     SparkF.ceil(day/10)).otherwise((month - 1) * 3 + 3)\n",
        "\n",
        "def getFilename(crop, yield_trend, early_season_end,\n",
        "                country=None, spatial_level=None, architecture=None):\n",
        "  \"\"\"Get filename based on input arguments\"\"\"\n",
        "  suffix = crop.replace(' ', '_')\n",
        "\n",
        "  if (country is not None):\n",
        "    suffix += '_' + country\n",
        "\n",
        "  if (spatial_level is not None):\n",
        "    suffix += '_' + spatial_level\n",
        "\n",
        "  if (yield_trend):\n",
        "    suffix += '_trend'\n",
        "  else:\n",
        "    suffix += '_notrend'\n",
        "\n",
        "  if (early_season_end < 0):\n",
        "    suffix += '_early' + str(early_season_end)\n",
        "\n",
        "  if (architecture is not None):\n",
        "    suffix += '-' + architecture\n",
        "\n",
        "  return suffix\n",
        "\n",
        "def getLogFilename(crop, yield_trend, early_season_end,\n",
        "                   country=None, architecture=None):\n",
        "  \"\"\"Get filename for experiment log\"\"\"\n",
        "  log_file = getFilename(crop, yield_trend, early_season_end,\n",
        "                         country=country,\n",
        "                         architecture=architecture)\n",
        "  return log_file + '.log'\n",
        "\n",
        "def getPredictionFilename(crop, yield_trend, early_season_end,\n",
        "                          country=None, spatial_level=None, architecture=None):\n",
        "  \"\"\"Get unique filename for predictions\"\"\"\n",
        "  pred_file = 'pred_'\n",
        "  suffix = getFilename(crop, yield_trend, early_season_end,\n",
        "                       country=country,\n",
        "                       spatial_level=spatial_level,\n",
        "                       architecture=architecture)\n",
        "  pred_file += suffix\n",
        "  return pred_file\n",
        "\n",
        "def printConfig(cyp_config, log_fh=None):\n",
        "  config_str = '\\nCurrent DL Configuration'\n",
        "  config_str += '\\n-------------------------'\n",
        "  for k in cyp_config:\n",
        "    conf_val = cyp_config[k]\n",
        "    if (not isinstance(conf_val, str)):\n",
        "      conf_val = str(conf_val)\n",
        "\n",
        "    config_str += '\\n' + k + ': ' + conf_val\n",
        "  \n",
        "  config_str += '\\n'\n",
        "  if (log_fh is not None):\n",
        "    log_fh.write(config_str)\n",
        "\n",
        "  print(config_str)\n",
        "\n",
        "def printPreprocessingInformation(df, data_source, id_cols,\n",
        "                                  order_cols, crop_season=None):\n",
        "  \"\"\"Print preprocessed data and additional debug information\"\"\"\n",
        "  df_regions = [reg[0] for reg in df.select(id_cols).distinct().collect()]\n",
        "  print(data_source , 'data available for', len(df_regions), 'region(s)')\n",
        "  if (crop_season is not None):\n",
        "    print('Season end information')\n",
        "    crop_season.orderBy(id_cols + ['FYEAR']).show(10)\n",
        "\n",
        "  print(data_source, 'data')\n",
        "  df.orderBy(order_cols).show(10)\n",
        "\n",
        "def getTrendWindowYields(df, id_col, trend_window):\n",
        "  \"\"\"Extract previous years' yield values to separate columns\"\"\"\n",
        "  sel_cols = [id_col, 'FYEAR', 'YIELD']\n",
        "  my_window = Window.partitionBy(id_col).orderBy('FYEAR')\n",
        "\n",
        "  yield_fts = df.select(sel_cols)\n",
        "  for i in range(trend_window):\n",
        "    yield_fts = yield_fts.withColumn('YIELD-' + str(i+1),\n",
        "                                     SparkF.lag(yield_fts.YIELD, i+1).over(my_window))\n",
        "    yield_fts = yield_fts.withColumn('YEAR-' + str(i+1),\n",
        "                                     SparkF.lag(yield_fts.FYEAR, i+1).over(my_window))\n",
        "\n",
        "  # drop columns withs null values\n",
        "  for i in range(trend_window):\n",
        "    yield_fts = yield_fts.filter(SparkF.col('YIELD-' + str(i+1)).isNotNull())\n",
        "\n",
        "  prev_yields = [ 'YIELD-' + str(i) for i in range(trend_window, 0, -1)]\n",
        "  prev_years = [ 'YEAR-' + str(i) for i in range(trend_window, 0, -1)]\n",
        "  sel_cols = [id_col, 'FYEAR'] + prev_years + prev_yields\n",
        "  yield_fts = yield_fts.select(sel_cols)\n",
        "\n",
        "  return yield_fts\n",
        "\n",
        "def getNumericIDS(src_df, reg_id_col, num_id_col):\n",
        "  \"\"\"Assigns monotonically increasing unique ids to NUTS regions\"\"\"\n",
        "  id_df = src_df.select(reg_id_col).distinct().orderBy(reg_id_col)\n",
        "  id_df = id_df.withColumn(num_id_col, SparkF.monotonically_increasing_id())\n",
        "\n",
        "  return id_df\n",
        "\n",
        "def NormalizedRMSE(y_true, y_pred):\n",
        "  y_true = y_true.astype('float32')\n",
        "  y_pred = y_pred.astype('float32')\n",
        "  return 100 * np.sqrt(mean_squared_error(y_true, y_pred))/np.mean(y_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJgYxi8L1w_9"
      },
      "source": [
        "## Data Preprocessor Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfUbfgid10FR"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Window\n",
        "\n",
        "class CYPDataPreprocessor:\n",
        "  def __init__(self, spark):\n",
        "    self.spark = spark\n",
        "    self.verbose = 1\n",
        "\n",
        "  def extractYearDekad(self, df):\n",
        "    \"\"\"Extract year and dekad from date_col in yyyyMMdd format.\"\"\"\n",
        "    # Conversion to string type is required to make getYear(), getMonth() etc. work correctly.\n",
        "    # They use to_date() function to verify valid dates and to_date() expects the date column to be string.\n",
        "    df = df.withColumn('DATE', df['DATE'].cast(\"string\"))\n",
        "    df = df.select('*',\n",
        "                   getYear('DATE').alias('FYEAR'),\n",
        "                   getDekad('DATE').alias('DEKAD'))\n",
        "\n",
        "    # Bring FYEAR, DEKAD to the front\n",
        "    col_order = df.columns[:2] + df.columns[-2:] + df.columns[2:-2]\n",
        "    df = df.select(col_order).drop('DATE')\n",
        "    return df\n",
        "\n",
        "  def getCropSeasonInformation(self, wofost_df, id_cols,\n",
        "                               season_crosses_calyear):\n",
        "    \"\"\"Crop season information based on WOFOST DVS\"\"\"\n",
        "    join_cols = id_cols + ['FYEAR']\n",
        "    if (('DATE' in wofost_df.columns) and ('FYEAR' not in wofost_df.columns)):\n",
        "      wofost_df = self.extractYearDekad(wofost_df)\n",
        "\n",
        "    crop_season = wofost_df.select(join_cols).distinct()\n",
        "    diff_window = Window.partitionBy(join_cols).orderBy('DEKAD')\n",
        "    cs_window = Window.partitionBy(id_cols).orderBy('FYEAR')\n",
        "\n",
        "    wofost_df = wofost_df.withColumn('VALUE', wofost_df['DVS'])\n",
        "    wofost_df = wofost_df.withColumn('PREV', SparkF.lag(wofost_df['VALUE']).over(diff_window))\n",
        "    wofost_df = wofost_df.withColumn('DIFF', SparkF.when(SparkF.isnull(wofost_df['PREV']), 0)\\\n",
        "                                     .otherwise(wofost_df['VALUE'] - wofost_df['PREV']))\n",
        "    # calculate end of season dekad\n",
        "    dvs_nochange_filter = ((wofost_df['VALUE'] >= 200) & (wofost_df['DIFF'] == 0.0))\n",
        "    year_end_filter = (wofost_df['DEKAD'] == 36)\n",
        "    if (season_crosses_calyear):\n",
        "      value_zero_filter =  (wofost_df['VALUE'] == 0)\n",
        "    else:\n",
        "      value_zero_filter =  ((wofost_df['PREV'] >= 200) & (wofost_df['VALUE'] == 0))\n",
        "\n",
        "    end_season_filter = (dvs_nochange_filter | value_zero_filter | year_end_filter)\n",
        "    crop_season = crop_season.join(wofost_df.filter(end_season_filter).groupBy(join_cols)\\\n",
        "                                   .agg(SparkF.min('DEKAD').alias('SEASON_END_DEKAD')), join_cols)\n",
        "    wofost_df = wofost_df.drop('VALUE', 'PREV', 'DIFF')\n",
        "\n",
        "    # We take the max of SEASON_END_DEKAD for current campaign and next campaign\n",
        "    # to determine which dekads go to next campaign year.\n",
        "    max_year = crop_season.agg(SparkF.max('FYEAR')).collect()[0][0]\n",
        "    min_year = crop_season.agg(SparkF.min('FYEAR')).collect()[0][0]\n",
        "    crop_season = crop_season.withColumn('NEXT_SEASON_END', SparkF.when(crop_season['FYEAR'] == max_year,\n",
        "                                                                        crop_season['SEASON_END_DEKAD'])\\\n",
        "                                         .otherwise(SparkF.lead(crop_season['SEASON_END_DEKAD']).over(cs_window)))\n",
        "    crop_season = crop_season.withColumn('SEASON_END',\n",
        "                                         SparkF.when(crop_season['SEASON_END_DEKAD'] > crop_season['NEXT_SEASON_END'],\n",
        "                                                     crop_season['SEASON_END_DEKAD'])\\\n",
        "                                         .otherwise(crop_season['NEXT_SEASON_END']))\n",
        "    crop_season = crop_season.withColumn('PREV_SEASON_END', SparkF.when(crop_season['FYEAR'] == min_year, 0)\\\n",
        "                                         .otherwise(SparkF.lag(crop_season['SEASON_END']).over(cs_window)))\n",
        "    crop_season = crop_season.select(join_cols + ['PREV_SEASON_END', 'SEASON_END'])\n",
        "\n",
        "    return crop_season\n",
        "\n",
        "  def alignDataToCropSeason(self, df, id_cols, crop_season,\n",
        "                            season_crosses_calyear):\n",
        "    \"\"\"Calculate CAMPAIGN_YEAR, CAMPAIGN_DEKAD based on crop_season\"\"\"\n",
        "    join_cols = id_cols + ['FYEAR']\n",
        "    max_year = crop_season.agg(SparkF.max('FYEAR')).collect()[0][0]\n",
        "    min_year = crop_season.agg(SparkF.min('FYEAR')).collect()[0][0]\n",
        "    df = df.join(crop_season, join_cols)\n",
        "\n",
        "    # Dekads > SEASON_END belong to next campaign year\n",
        "    df = df.withColumn('CAMPAIGN_YEAR',\n",
        "                       SparkF.when(df['DEKAD'] > df['SEASON_END'], df['FYEAR'] + 1)\\\n",
        "                       .otherwise(df['FYEAR']))\n",
        "    # min_year has no previous season information. We align CAMPAIGN_DEKAD to end in 36.\n",
        "    # For other years, dekads < SEASON_END are adjusted based on PREV_SEASON_END.\n",
        "    # Dekads > SEASON_END get renumbered from 1 (for next campaign).\n",
        "    df = df.withColumn('CAMPAIGN_DEKAD',\n",
        "                       SparkF.when(df['CAMPAIGN_YEAR'] == min_year, df['DEKAD'] + 36 - df['SEASON_END'])\\\n",
        "                       .otherwise(SparkF.when(df['DEKAD'] > df['SEASON_END'], df['DEKAD'] - df['SEASON_END'])\\\n",
        "                                  .otherwise(df['DEKAD'] + 36 - df['PREV_SEASON_END'])))\n",
        "\n",
        "    # Columns should be id_cols, FYEAR, DEKAD, ..., CAMPAIGN_YEAR, CAMPAIGN_DEKAD.\n",
        "    # Bring CAMPAIGN_YEAR and CAMPAIGN_DEKAD to the front.\n",
        "    col_order = df.columns[:len(id_cols) + 2] + df.columns[-2:] + df.columns[len(id_cols) + 2:-2]\n",
        "    df = df.select(col_order)\n",
        "    if (season_crosses_calyear):\n",
        "      # For crop with two seasons, remove the first year. Data from the first year\n",
        "      # only contributes to the second year and we have already moved useful data\n",
        "      # to the second year (or first campaign year).\n",
        "      df = df.filter(df['CAMPAIGN_YEAR'] > min_year)\n",
        "\n",
        "    # In both cases, remove extra rows beyond max campaign year\n",
        "    df = df.filter(df['CAMPAIGN_YEAR'] <= max_year)\n",
        "    return df\n",
        "\n",
        "  def preprocessWofost(self, wofost_df, id_cols,\n",
        "                       crop_season, season_crosses_calyear):\n",
        "    \"\"\"\n",
        "    Extract year and dekad from date. Use crop_season to compute\n",
        "    CAMPAIGN_YEAR and CAMPAIGN_DEKAD.\n",
        "    \"\"\"\n",
        "    drop_cols = crop_season.columns[len(id_cols) + 1:]\n",
        "    if (('DATE' in wofost_df.columns) and ('FYEAR' not in wofost_df.columns)):\n",
        "      wofost_df = self.extractYearDekad(wofost_df)\n",
        "\n",
        "    wofost_df = self.alignDataToCropSeason(wofost_df, id_cols,\n",
        "                                           crop_season, season_crosses_calyear)\n",
        "\n",
        "    # WOFOST indicators come after id_cols, FYEAR, DEKAD, CAMPAIGN_YEAR, CAMPAIGN_DEKAD.\n",
        "    indicators_idx = len(id_cols) + 4\n",
        "    wofost_inds = wofost_df.columns[indicators_idx:]\n",
        "    # set indicators values for dekads after end of season to zero.\n",
        "    # TODO - Dilli: Find a way to avoid the for loop.\n",
        "    for ind in wofost_inds:\n",
        "      wofost_df = wofost_df.withColumn(ind,\n",
        "                                       SparkF.when(wofost_df['DEKAD'] < wofost_df['SEASON_END'],\n",
        "                                                   wofost_df[ind])\\\n",
        "                                       .otherwise(0))\n",
        "\n",
        "    wofost_df = wofost_df.drop(*drop_cols)\n",
        "    return wofost_df\n",
        "\n",
        "  def preprocessMeteo(self, meteo_df, id_cols,\n",
        "                      crop_season, season_crosses_calyear):\n",
        "    \"\"\"\n",
        "    Extract year and dekad from date, calculate CWB.\n",
        "    Use crop_season to compute CAMPAIGN_YEAR and CAMPAIGN_DEKAD.\n",
        "    \"\"\"\n",
        "    drop_cols = crop_season.columns[len(id_cols) + 1:]\n",
        "    meteo_df = meteo_df.drop('IDCOVER')\n",
        "    meteo_df = meteo_df.withColumn('CWB',\n",
        "                                   SparkF.bround(meteo_df['PREC'] - meteo_df['ET0'], 2))\n",
        "    if (('DATE' in meteo_df.columns) and ('FYEAR' not in meteo_df.columns)):\n",
        "      meteo_df = self.extractYearDekad(meteo_df)\n",
        "\n",
        "    meteo_df = self.alignDataToCropSeason(meteo_df, id_cols,\n",
        "                                          crop_season, season_crosses_calyear)\n",
        "    meteo_df = meteo_df.drop(*drop_cols)\n",
        "    return meteo_df\n",
        "\n",
        "  def preprocessMeteoDaily(self, meteo_df, id_cols):\n",
        "    \"\"\"\n",
        "    Convert daily meteo data to dekadal. Takes avg for all indicators\n",
        "    except TMAX (take max), TMIN (take min), PREC (take sum), ET0 (take sum), CWB (take sum).\n",
        "    \"\"\"\n",
        "    self.spark.catalog.dropTempView('meteo_daily')\n",
        "    meteo_df.createOrReplaceTempView('meteo_daily')\n",
        "    join_cols = id_cols + ['CAMPAIGN_YEAR', 'CAMPAIGN_DEKAD']\n",
        "    join_df = meteo_df.select(join_cols + ['FYEAR', 'DEKAD']).distinct()\n",
        "\n",
        "    # We are ignoring VPRES, WSPD and RELH at the moment\n",
        "    # avg(VPRES) as VPRES1, avg(WSPD) as WSPD1, avg(RELH) as RELH1,\n",
        "    # TMAX| TMIN| TAVG| VPRES| WSPD| PREC| ET0| RAD| RELH| CWB\n",
        "    #\n",
        "    # It seems keeping same name after aggregation is fine. We are using a\n",
        "    # different name just to be sure nothing untoward happens.\n",
        "    query = 'select ' + ', '.join(id_cols) + ', CAMPAIGN_YEAR, CAMPAIGN_DEKAD, '\n",
        "    query = query + ' max(TMAX) as TMAX1, min(TMIN) as TMIN1, '\n",
        "    query = query + ' bround(avg(TAVG), 2) as TAVG1, bround(sum(PREC), 2) as PREC1, '\n",
        "    query = query + ' bround(sum(ET0), 2) as ET01, bround(avg(RAD), 2) as RAD1, '\n",
        "    query = query + ' bround(sum(CWB), 2) as CWB1 from meteo_daily'\n",
        "    query = query + ' group by ' + ', '.join(id_cols) + ', CAMPAIGN_YEAR, CAMPAIGN_DEKAD '\n",
        "    query = query + ' order by ' + ', '.join(id_cols) + ', CAMPAIGN_YEAR, CAMPAIGN_DEKAD'\n",
        "    meteo_df = self.spark.sql(query).cache()\n",
        "\n",
        "    # rename the columns\n",
        "    selected_cols = ['TMAX', 'TMIN', 'TAVG', 'PREC', 'ET0', 'RAD', 'CWB']\n",
        "    for scol in selected_cols:\n",
        "      meteo_df = meteo_df.withColumnRenamed(scol + '1', scol)\n",
        "\n",
        "    meteo_df = meteo_df.join(join_df, join_cols)\n",
        "    # Bring FYEAR, DEKAD to the front\n",
        "    col_order = meteo_df.columns[:len(id_cols)] + meteo_df.columns[-2:] + meteo_df.columns[len(id_cols):-2]\n",
        "    meteo_df = meteo_df.select(col_order)\n",
        "\n",
        "    return meteo_df\n",
        "\n",
        "  def preprocessRemoteSensing(self, rs_df, id_cols,\n",
        "                              crop_season, season_crosses_calyear):\n",
        "    \"\"\"\n",
        "    Extract year and dekad from date.\n",
        "    Use crop_season to compute CAMPAIGN_YEAR and CAMPAIGN_DEKAD.\n",
        "    NOTE crop_season and rs_df must be at the same NUTS level.\n",
        "    \"\"\"\n",
        "    drop_cols = crop_season.columns[len(id_cols) + 1:]\n",
        "    if (('DATE' in rs_df.columns) and ('FYEAR' not in rs_df.columns)):\n",
        "      rs_df = self.extractYearDekad(rs_df)\n",
        "\n",
        "    rs_df = self.alignDataToCropSeason(rs_df, id_cols,\n",
        "                                       crop_season, season_crosses_calyear)\n",
        "    rs_df = rs_df.drop(*drop_cols)\n",
        "    return rs_df\n",
        "\n",
        "  def preprocessSoil(self, soil_df, id_cols):\n",
        "    # SM_WC = water holding capacity\n",
        "    soil_df = soil_df.withColumn('SM_WHC', SparkF.bround(soil_df['SM_FC'] - soil_df['SM_WP'], 2))\n",
        "    soil_df = soil_df.select(id_cols + ['SM_WHC'])\n",
        "\n",
        "    return soil_df\n",
        "\n",
        "  def preprocessAreaFractions(self, af_df, crop_id):\n",
        "    \"\"\"Filter area fractions data by crop id\"\"\"\n",
        "    af_df = af_df.withColumn(\"FYEAR\", af_df[\"FYEAR\"].cast(SparkT.IntegerType()))\n",
        "    af_df = af_df.filter(af_df[\"CROP_ID\"] == crop_id).drop('CROP_ID')\n",
        "\n",
        "    return af_df\n",
        "\n",
        "  def preprocessCropArea(self, area_df, crop_id):\n",
        "    \"\"\"Filter area fractions data by crop id\"\"\"\n",
        "    area_df = area_df.withColumn(\"FYEAR\", area_df[\"FYEAR\"].cast(SparkT.IntegerType()))\n",
        "    area_df = area_df.filter(area_df[\"CROP_ID\"] == crop_id).drop('CROP_ID')\n",
        "    area_df = area_df.filter(area_df[\"CROP_AREA\"].isNotNull())\n",
        "    area_df = area_df.drop('FRACTION')\n",
        "\n",
        "    return area_df\n",
        "\n",
        "  def preprocessGAES(self, gaes_df, crop_id):\n",
        "    \"\"\"Select irrigated crop area by crop id\"\"\"\n",
        "    sel_cols = [ c for c in gaes_df.columns if 'IRRIG' not in c]\n",
        "    sel_cols += ['IRRIG_AREA_ALL', 'IRRIG_AREA' + str(crop_id)]\n",
        "\n",
        "    return gaes_df.select(sel_cols)\n",
        "\n",
        "  def preprocessLabels(self, yield_df, id_col, crop_id):\n",
        "    \"\"\"\n",
        "    Yield preprocessing depends on the data format.\n",
        "    Here we cover preprocessing for France (NUTS3), Germany (NUTS3) and the Netherlands (NUTS2).\n",
        "    \"\"\"\n",
        "    # Delete trailing empty columns\n",
        "    empty_cols = [ c for c in yield_df.columns if c.startswith('_c') ]\n",
        "    for c in empty_cols:\n",
        "      yield_df = yield_df.drop(c)\n",
        "\n",
        "    # Special case for Netherlands and Germany: convert yield columns into rows\n",
        "    years = [int(c) for c in yield_df.columns if c[0].isdigit()]\n",
        "    if (len(years) > 0):\n",
        "      yield_by_year = yield_df.rdd.map(lambda x: (x[0], cropNameToID(crop_id_dict, x[0]), x[1],\n",
        "                                                  [(years[i], x[i+2]) for i in range(len(years))]))\n",
        "\n",
        "      yield_df = yield_by_year.toDF(['CROP', 'CROP_ID', id_col, 'YIELD'])\n",
        "      yield_df = yield_df.withColumn('YR_YIELD', SparkF.explode('YIELD')).drop('YIELD')\n",
        "      yield_by_year = yield_df.rdd.map(lambda x: (x[0], x[1], x[2], x[3][0], x[3][1]))\n",
        "      yield_df = yield_by_year.toDF(['CROP', 'CROP_ID', id_col, 'FYEAR', 'YIELD'])\n",
        "    else:\n",
        "      yield_by_year = yield_df.rdd.map(lambda x: (x[0], cropNameToID(crop_id_dict, x[0]), x[1], x[2], x[3]))\n",
        "      yield_df = yield_by_year.toDF(['CROP', 'CROP_ID', id_col, 'FYEAR', 'YIELD'])\n",
        "\n",
        "    yield_df = yield_df.filter(yield_df.CROP_ID == crop_id).drop('CROP', 'CROP_ID')\n",
        "    if (yield_df.count() == 0):\n",
        "      return None\n",
        "\n",
        "    yield_df = yield_df.filter(yield_df.YIELD.isNotNull())\n",
        "    yield_df = yield_df.withColumn(\"YIELD\", yield_df[\"YIELD\"].cast(SparkT.FloatType()))\n",
        "    yield_df = yield_df.withColumn(\"FYEAR\", yield_df[\"FYEAR\"].cast(SparkT.IntegerType()))\n",
        "    yield_df = yield_df.filter(yield_df['YIELD'] > 0.0)\n",
        "\n",
        "    return yield_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91RKRiDfsGdZ"
      },
      "source": [
        "## Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6kkDgvJsJFh"
      },
      "outputs": [],
      "source": [
        "from datetime import date\n",
        "\n",
        "def loadDataFromCSVFile(spark, data_path, src, spatial_level, country_code):\n",
        "    \"\"\"\n",
        "    The implied filename for each source is:\n",
        "    <data_source>_<spatial_level>_<country_code>.csv\n",
        "    Examples: WOFOST_NUTS2_NL.csv.\n",
        "    Schema is inferred from the file. We might want to specify the schema at some point.\n",
        "    \"\"\"\n",
        "    if (country_code is not None):\n",
        "      datafile = data_path + '/' + src  + '_' + spatial_level + '_' + country_code + '.csv'\n",
        "    elif (spatial_level is not None):\n",
        "      datafile = data_path + '/' + src  + '_' + spatial_level + '.csv'\n",
        "    else:\n",
        "      datafile = data_path + '/' + src  + '.csv'\n",
        "\n",
        "    print('Data file name', '\"' + datafile + '\"')\n",
        "\n",
        "    df = spark.read.csv(datafile, header = True, inferSchema = True)\n",
        "    return df\n",
        "\n",
        "def loadAllData(spark, data_sources, data_path='.', country=None):\n",
        "  #################\n",
        "  # Load data     #\n",
        "  #################\n",
        "  data_dfs = {}\n",
        "  for src in data_sources:\n",
        "    spatial_level = data_sources[src]['spatial_level']\n",
        "    data_dfs[src] = loadDataFromCSVFile(spark, data_path, src, spatial_level, country)\n",
        "\n",
        "  data_sources_str = ''\n",
        "  for src in data_dfs:\n",
        "    data_sources_str = data_sources_str + src + ', '\n",
        "\n",
        "  # remove the comma and space from the end\n",
        "  print('Loaded data:', data_sources_str[:-2])\n",
        "  print('\\n')\n",
        "\n",
        "  return data_dfs\n",
        "\n",
        "def preprocessData(spark, data_dfs, data_sources,\n",
        "                   high_res_id_col, low_res_id_col,\n",
        "                   crop, countries, season_crosses_calyear=False,\n",
        "                   early_season=False, early_season_end=0, print_debug=False):\n",
        "  ######################\n",
        "  # Preprocess Data    #\n",
        "  ######################\n",
        "  crop_id = cropNameToID(crop_id_dict, crop)\n",
        "  cyp_preprocessor = CYPDataPreprocessor(spark)\n",
        "  crop_season = None\n",
        "  id_cols = [low_res_id_col, high_res_id_col]\n",
        "  for src in data_sources:\n",
        "    src_df = data_dfs[src]\n",
        "    spatial_level = data_sources[src]['spatial_level']\n",
        "    if ((spatial_level == 'NUTS2') and\n",
        "        ('IDREGION' in src_df.columns)):\n",
        "      src_df = src_df.withColumnRenamed('IDREGION', 'NUTS2_ID')\n",
        "    elif ((spatial_level == 'NUTS3') and\n",
        "        ('IDREGION' in src_df.columns) and\n",
        "        ('NUTS2_ID' not in src_df.columns)):\n",
        "      sel_cols = ['NUTS2_ID', 'NUTS3_ID'] + [c for c in src_df.columns if c != 'IDREGION']\n",
        "      src_df = src_df.withColumnRenamed('IDREGION', 'NUTS3_ID')\n",
        "      src_df = src_df.withColumn('NUTS2_ID', SparkF.substring(src_df['NUTS3_ID'], 1, 4))\n",
        "      src_df = src_df.select(sel_cols)\n",
        "    elif ((spatial_level == 'GRIDS') and\n",
        "          ('IDREGION' in src_df.columns) and\n",
        "          ('NUTS3_ID' not in src_df.columns)):\n",
        "      sel_cols = ['NUTS3_ID', 'GRID_ID'] + [c for c in src_df.columns if ((c != 'IDREGION') and\n",
        "                                                                          ('GRID' not in c))]\n",
        "      src_df = src_df.withColumnRenamed('IDREGION', 'NUTS3_ID')\n",
        "      src_df = src_df.select(sel_cols)\n",
        "\n",
        "    if (src == 'WOFOST'):\n",
        "      # wofost data\n",
        "      wofost_df = src_df.filter(src_df['CROP_ID'] == crop_id).drop('CROP_ID')\n",
        "      wofost_df = wofost_df.filter(SparkF.substring(wofost_df[high_res_id_col], 1, 2).isin(countries))\n",
        "      crop_season = cyp_preprocessor.getCropSeasonInformation(wofost_df, id_cols,\n",
        "                                                              season_crosses_calyear)\n",
        "      wofost_df = cyp_preprocessor.preprocessWofost(wofost_df, id_cols,\n",
        "                                                    crop_season, season_crosses_calyear)\n",
        "      data_dfs[src] = wofost_df\n",
        "\n",
        "    if ('METEO' in src):\n",
        "      # meteo data\n",
        "      meteo_df = cyp_preprocessor.preprocessMeteo(src_df, id_cols,\n",
        "                                                  crop_season, season_crosses_calyear)\n",
        "\n",
        "      if (src == 'METEO_DAILY'):\n",
        "        meteo_df = cyp_preprocessor.preprocessMeteoDaily(meteo_df, id_cols)\n",
        "\n",
        "      assert (meteo_df is not None)\n",
        "      data_dfs[src] = meteo_df.filter(SparkF.substring(meteo_df[high_res_id_col], 1, 2).isin(countries))\n",
        "\n",
        "    # remote sensing data\n",
        "    if (src == 'REMOTE_SENSING'):\n",
        "      rs_df = src_df.drop('IDCOVER')\n",
        "      rs_df = cyp_preprocessor.preprocessRemoteSensing(rs_df, id_cols,\n",
        "                                                       crop_season, season_crosses_calyear)\n",
        "      assert (rs_df is not None)\n",
        "      data_dfs[src] = rs_df.filter(SparkF.substring(rs_df[high_res_id_col], 1, 2).isin(countries))\n",
        "\n",
        "    # soil data\n",
        "    if (src == 'SOIL'):\n",
        "      soil_df = cyp_preprocessor.preprocessSoil(src_df, id_cols)\n",
        "      data_dfs['SOIL'] = soil_df.filter(SparkF.substring(soil_df[high_res_id_col], 1, 2).isin(countries))\n",
        "\n",
        "    # agro-environmental zones\n",
        "    if (src == 'GAES'):\n",
        "      gaes_df = cyp_preprocessor.preprocessGAES(src_df, crop_id)\n",
        "      data_dfs['GAES'] = gaes_df.filter(SparkF.substring(gaes_df[high_res_id_col], 1, 2).isin(countries))\n",
        "\n",
        "    # crop area data\n",
        "    if (src == 'CROP_AREA'):\n",
        "      crop_area_df = cyp_preprocessor.preprocessCropArea(src_df, crop_id)\n",
        "      data_dfs['CROP_AREA'] = crop_area_df.filter(SparkF.substring(crop_area_df[low_res_id_col], 1, 2).isin(countries))\n",
        "\n",
        "    # label data\n",
        "    if (src == 'YIELD'):\n",
        "      label_df = cyp_preprocessor.preprocessLabels(src_df, low_res_id_col, crop_id)\n",
        "      assert (label_df is not None)\n",
        "      data_dfs['YIELD'] = label_df.filter(SparkF.substring(label_df[low_res_id_col], 1, 2).isin(countries))\n",
        "\n",
        "  # Print debug information\n",
        "  for src in data_dfs:\n",
        "    src_df = data_dfs[src]\n",
        "    order_cols = data_sources[src]['order_cols']\n",
        "    if (high_res_id_col in src_df.columns):\n",
        "      if (src == 'WOFOST' and print_debug):\n",
        "        printPreprocessingInformation(src_df, src, id_cols,\n",
        "                                      order_cols, crop_season=crop_season)\n",
        "      elif (print_debug):\n",
        "        printPreprocessingInformation(src_df, src, id_cols, order_cols)\n",
        "    elif (print_debug):\n",
        "        printPreprocessingInformation(src_df, src, low_res_id_col, order_cols)\n",
        "\n",
        "  return data_dfs\n",
        "\n",
        "def getLinearYieldTrend(pd_yield_ft_df, id_cols, trend_window):\n",
        "  \"\"\"Fits a linear trend model to yields from 5 previous years\"\"\"\n",
        "  join_cols = id_cols + ['FYEAR']\n",
        "  region_years = pd_yield_ft_df[join_cols].values\n",
        "  prev_year_cols = ['YEAR-' + str(i) for i in range(1, trend_window + 1)]\n",
        "  prev_yield_cols = ['YIELD-' + str(i) for i in range(1, trend_window + 1)]\n",
        "  window_years = pd_yield_ft_df[prev_year_cols].values\n",
        "  window_yields = pd_yield_ft_df[prev_yield_cols].values\n",
        "\n",
        "  yield_trend = []\n",
        "  for i in range(region_years.shape[0]):\n",
        "    coefs = np.polyfit(window_years[i, :], window_yields[i, :], 1)\n",
        "    yield_trend.append(float(np.round(coefs[0] * region_years[i, 1] + coefs[1], 2)))\n",
        "\n",
        "  pd_yield_ft_df['YIELD_TREND'] = yield_trend\n",
        "  drop_cols = ['YEAR-' + str(i) for i in range(1, 6)]\n",
        "  pd_yield_ft_df = pd_yield_ft_df.drop(columns=drop_cols)\n",
        "\n",
        "  return pd_yield_ft_df\n",
        "\n",
        "def combineInputData(data_sources, data_dfs, countries,\n",
        "                     ts_data_sources,\n",
        "                     static_data_sources,\n",
        "                     high_res_id_col, low_res_id_col,\n",
        "                     early_season_end=None,\n",
        "                     trend_window=5,\n",
        "                     print_debug=False):\n",
        "  \"\"\"Combine dekadal and static data\"\"\"\n",
        "  input_min_year = 1900\n",
        "  input_max_year = date.today().year\n",
        "  for src in ts_data_sources:\n",
        "    input_df = data_dfs[src]\n",
        "    min_year = input_df.agg(SparkF.min('CAMPAIGN_YEAR')).collect()[0][0]\n",
        "    max_year = input_df.agg(SparkF.max('CAMPAIGN_YEAR')).collect()[0][0]\n",
        "    # max of min years (earliest year after join, not min of min)\n",
        "    if (min_year > input_min_year):\n",
        "      input_min_year = min_year\n",
        "    # min of max years (latest year after join, not max of max)\n",
        "    if (max_year < input_max_year):\n",
        "      input_max_year = max_year\n",
        "\n",
        "  # combine dekadal data\n",
        "  dekadal_df = None\n",
        "  for src in ts_data_sources:\n",
        "    input_df = data_dfs[src].select(data_sources[src]['sel_cols'])\n",
        "    input_df = input_df.filter((input_df['CAMPAIGN_YEAR'] >= input_min_year) &\n",
        "                               (input_df['CAMPAIGN_YEAR'] <= input_max_year))\n",
        "    if (dekadal_df is None):\n",
        "      dekadal_df = input_df\n",
        "    else:\n",
        "      dekadal_df = dekadal_df.join(input_df, data_sources[src]['order_cols'], 'full')\n",
        "      dekadal_df = dekadal_df.na.fill(0.0)\n",
        "\n",
        "  dekadal_df = dekadal_df.withColumnRenamed('CAMPAIGN_YEAR', 'FYEAR')\n",
        "  dekadal_df = dekadal_df.withColumnRenamed('CAMPAIGN_DEKAD', 'DEKAD')\n",
        "  max_dekad = 36\n",
        "  if (early_season_end is not None):\n",
        "    # early_season_end is relative to harvest (so 0 or negative)\n",
        "    max_dekad += early_season_end\n",
        "    dekadal_df = dekadal_df.filter(dekadal_df['DEKAD'] <= max_dekad)\n",
        "\n",
        "  static_df = None\n",
        "  for src in static_data_sources:\n",
        "    input_df = data_dfs[src].select(data_sources[src]['sel_cols'])\n",
        "    if (static_df is None):\n",
        "      static_df = input_df\n",
        "    else:\n",
        "      static_df = static_df.join(input_df, data_sources[src]['order_cols'])\n",
        "\n",
        "  label_df = data_dfs['YIELD']\n",
        "  crop_area_df = data_dfs['CROP_AREA']\n",
        "\n",
        "  # get trend feature values: basically values of 5 previous years\n",
        "  trend_ft_df = getTrendWindowYields(label_df, low_res_id_col, trend_window)\n",
        "  year_cols = ['YEAR-' + str(i) for i in range(1, trend_window + 1)]\n",
        "  trend_ft_df = trend_ft_df.drop(*year_cols)\n",
        "\n",
        "  # Training, test splits are decided based on label years.\n",
        "  country_years = {}\n",
        "  label_df = label_df.withColumn('COUNTRY', SparkF.substring(label_df[low_res_id_col], 1, 2))\n",
        "  for cn in countries:\n",
        "    label_cn_df = label_df.filter(label_df['COUNTRY'] == cn)\n",
        "    cn_years = [yr[0] for yr in label_cn_df.select('FYEAR').distinct().collect()]\n",
        "    country_years[cn] = sorted(cn_years)\n",
        "\n",
        "  # Align spatial units and years\n",
        "  label_df = label_df.join(static_df.select(low_res_id_col).distinct(), [low_res_id_col])\n",
        "  label_df = label_df.join(trend_ft_df.select([low_res_id_col, 'FYEAR']),\n",
        "                           [low_res_id_col, 'FYEAR'])\n",
        "  # NOTE: CROP_AREA is added to label_df\n",
        "  label_df = label_df.join(crop_area_df, [low_res_id_col, 'FYEAR'])\n",
        "\n",
        "  label_df = label_df.join(dekadal_df.select([low_res_id_col, 'FYEAR']).distinct(),\n",
        "                           [low_res_id_col, 'FYEAR'])\n",
        "  label_reg_years = label_df.select([low_res_id_col, 'FYEAR']).distinct()\n",
        "  trend_ft_df = trend_ft_df.join(label_reg_years, [low_res_id_col, 'FYEAR'])\n",
        "  dekadal_df = dekadal_df.join(label_reg_years, [low_res_id_col, 'FYEAR'])\n",
        "  label_regions = label_df.select(low_res_id_col).distinct()\n",
        "  static_df = static_df.join(label_regions, [low_res_id_col])\n",
        "\n",
        "  # Create numeric ids for regions and countries\n",
        "  input_id_df = getNumericIDS(label_df, low_res_id_col, 'id_y')\n",
        "  label_id_df = getNumericIDS(label_df, low_res_id_col, 'id_y')\n",
        "  input_id_df = input_id_df.join(static_df.select([low_res_id_col, high_res_id_col]), [low_res_id_col])\n",
        "  if (high_res_id_col == 'NUTS3_ID'):\n",
        "    nuts3_id_df = getNumericIDS(input_id_df, high_res_id_col, 'id_x')\n",
        "    input_id_df = input_id_df.join(nuts3_id_df, high_res_id_col)\n",
        "\n",
        "  input_id_df = input_id_df.withColumn('COUNTRY', SparkF.substring(input_id_df[high_res_id_col], 1, 2))\n",
        "  label_id_df = label_id_df.withColumn('COUNTRY', SparkF.substring(label_id_df[low_res_id_col], 1, 2))\n",
        "  cnid_df = getNumericIDS(label_id_df, 'COUNTRY', 'id0')\n",
        "  input_id_df = input_id_df.join(cnid_df, ['COUNTRY'])\n",
        "  label_id_df = label_id_df.join(cnid_df, ['COUNTRY'])\n",
        "\n",
        "  # Add numeric id columns, remove IDREGION, COUNTRY and reorder columns\n",
        "  label_num_id_cols = ['id0', 'id_y']\n",
        "  label_join_cols = [low_res_id_col]\n",
        "  label_drop_cols = [low_res_id_col, 'COUNTRY']\n",
        "  if (low_res_id_col == 'NUTS2_ID'):\n",
        "    input_num_id_cols = ['id0', 'id_y', 'id_x']\n",
        "    input_join_cols = ['NUTS2_ID', 'NUTS3_ID']\n",
        "    input_drop_cols = ['NUTS2_ID', 'NUTS3_ID', 'COUNTRY']\n",
        "  else:\n",
        "    input_num_id_cols = ['id0', 'id_y', 'GRID_ID']\n",
        "    input_join_cols = ['NUTS3_ID', 'GRID_ID']\n",
        "    input_drop_cols = ['NUTS3_ID', 'COUNTRY']\n",
        "\n",
        "  # Add numeric id columns : DEKADAL\n",
        "  dekadal_df = dekadal_df.join(input_id_df, input_join_cols).drop(*input_drop_cols)\n",
        "  dekadal_df = dekadal_df.select(input_num_id_cols + \n",
        "                                 [c for c in dekadal_df.columns if c not in input_num_id_cols])\n",
        "  if (print_debug):\n",
        "    print('\\n')\n",
        "    print('DEKADAL')\n",
        "    dekadal_df.orderBy(input_num_id_cols + ['FYEAR', 'DEKAD']).show(10)\n",
        "\n",
        "  # Add numeric id columns : TREND\n",
        "  trend_ft_df = trend_ft_df.join(label_id_df, label_join_cols).drop(*label_drop_cols)\n",
        "  trend_ft_df = trend_ft_df.select(label_num_id_cols +\n",
        "                                   [c for c in trend_ft_df.columns if c not in label_num_id_cols])\n",
        "  if (print_debug):\n",
        "    print('\\n')\n",
        "    print('TREND')\n",
        "    trend_ft_df.orderBy(label_num_id_cols + ['FYEAR']).show(10)\n",
        "\n",
        "  # Add numeric id columns : YIELD\n",
        "  label_df = label_df.drop('COUNTRY')\n",
        "  label_df = label_df.join(label_id_df, label_join_cols).drop(*label_drop_cols)\n",
        "  label_df = label_df.select(label_num_id_cols +\n",
        "                             [c for c in label_df.columns if c not in label_num_id_cols])\n",
        "  if (print_debug):\n",
        "    print('\\n')\n",
        "    print('YIELD')\n",
        "    label_df.orderBy(label_num_id_cols + ['FYEAR']).show(10)\n",
        "\n",
        "  # Add numeric id columns : STATIC\n",
        "  # We drop COUNTRY later after getting one-hot encondings\n",
        "  static_df = static_df.join(input_id_df, input_join_cols).drop(*['NUTS2_ID', 'NUTS3_ID'])\n",
        "  static_df = static_df.select(input_num_id_cols +\n",
        "                               [c for c in static_df.columns if c not in input_num_id_cols])\n",
        "  pd_static_df = static_df.toPandas()\n",
        "  if (len(pd_static_df['COUNTRY'].unique()) > 1):\n",
        "    pd_sel_static_df = pd_static_df[['COUNTRY', 'AEZ_ID']].astype({ 'AEZ_ID' : 'str' })\n",
        "    pd_onehot_df = pd.get_dummies(pd_sel_static_df, prefix=['CN', 'AEZ'])\n",
        "  else:\n",
        "    pd_sel_static_df = pd_static_df[['AEZ_ID']].astype({ 'AEZ_ID' : 'str' })\n",
        "    pd_onehot_df = pd.get_dummies(pd_sel_static_df, prefix=['AEZ'])\n",
        "\n",
        "  pd_static_df = pd.concat([pd_static_df, pd_onehot_df], axis=1).drop(columns=['COUNTRY', 'AEZ_ID'])\n",
        "  if (print_debug):\n",
        "    print('\\n')\n",
        "    print('STATIC')\n",
        "    print(pd_static_df.sort_values(by=input_num_id_cols).head(10).to_string(index=False))\n",
        "\n",
        "  if (print_debug):\n",
        "    print('\\n')\n",
        "    print('NUMERIC_IDS')\n",
        "    input_id_df.orderBy(input_num_id_cols).show(10)\n",
        "    label_id_df.orderBy(label_num_id_cols).show(10)\n",
        "\n",
        "  pd_dekadal_df = dekadal_df.toPandas()\n",
        "  pd_trend_df = trend_ft_df.toPandas()\n",
        "  pd_label_df = label_df.toPandas()\n",
        "  pd_input_id_df = input_id_df.toPandas()\n",
        "  pd_label_id_df = label_id_df.toPandas()\n",
        "\n",
        "  combined_dfs = {\n",
        "      'DEKADAL' : pd_dekadal_df,\n",
        "      'STATIC' : pd_static_df,\n",
        "      'YIELD_TREND' : pd_trend_df,\n",
        "      'YIELD' : pd_label_df,\n",
        "      'INPUT_NUMERIC_IDS' : pd_input_id_df,\n",
        "      'LABEL_NUMERIC_IDS' : pd_label_id_df\n",
        "  }\n",
        "\n",
        "  return combined_dfs, country_years"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKWsYBQbquRs"
      },
      "source": [
        "## Training, Validation and Test Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiIg70PEq076"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class CYPTrainTestSplitter:\n",
        "  def __init__(self, verbose=False):\n",
        "    self.verbose = verbose\n",
        "\n",
        "  def getTestYears(self, all_years, test_fraction=None, use_yield_trend=None):\n",
        "    num_years = len(all_years)\n",
        "    test_years = []\n",
        "    if (test_fraction is None):\n",
        "      test_fraction = self.test_fraction\n",
        "\n",
        "    if (use_yield_trend is None):\n",
        "      use_yield_trend = self.use_yield_trend\n",
        "\n",
        "    if (use_yield_trend):\n",
        "      # If test_year_start 15, years with index >= 15 are added to the test set\n",
        "      test_year_start = num_years - np.floor(num_years * test_fraction).astype('int')\n",
        "      test_years = all_years[test_year_start:]\n",
        "    else:\n",
        "      # If test_year_pos = 5, every 5th year is added to test set.\n",
        "      # indices start with 0, so test_year_pos'th year has index (test_year_pos - 1)\n",
        "      test_year_pos = np.floor(1/test_fraction).astype('int')\n",
        "      test_years = all_years[test_year_pos - 1::test_year_pos]\n",
        "\n",
        "    return test_years\n",
        "\n",
        "  # NOTE Y_train should include region_id, FYEAR as first two columns.\n",
        "  def getCustomKFoldValidationYears(self, all_years, num_folds=5, num_valid_years=1):\n",
        "    \"\"\"\n",
        "    Custom K-fold Validation Splits:\n",
        "    When using yield trend, we cannot do k-fold cross-validation. The custom\n",
        "    K-Fold validation splits data in time-ordered fashion. The test data\n",
        "    always comes after the training data.\n",
        "    \"\"\"\n",
        "    num_years = len(all_years)\n",
        "    num_train_years = num_years - (num_valid_years * num_folds)\n",
        "\n",
        "    custom_split_info = '\\nCustom sliding validation train, test splits'\n",
        "    custom_split_info += '\\n----------------------------------------------'\n",
        "\n",
        "    cv_valid_years = []\n",
        "    for k in range(num_folds):\n",
        "      test_years_start = num_train_years + (k * num_valid_years)\n",
        "      k_train_years = all_years[:test_years_start]\n",
        "      k_val_years = all_years[test_years_start:test_years_start + num_valid_years]\n",
        "      cv_valid_years.append(k_val_years)\n",
        "      k_train_years = [str(y) for y in k_train_years]\n",
        "      k_val_years = [str(y) for y in k_val_years]\n",
        "      custom_split_info += '\\nValidation set ' + str(k + 1) + ' training years: ' + ', '.join(k_train_years)\n",
        "      custom_split_info += '\\nValidation set ' + str(k + 1) + ' test years: ' + ', '.join(k_val_years)\n",
        "\n",
        "    custom_split_info += '\\n'\n",
        "    if (self.verbose):\n",
        "      print(custom_split_info)\n",
        "\n",
        "    return cv_valid_years"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDxuCEObX58A"
      },
      "source": [
        "## Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aRh3ijm5dy_"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "# A dataset class for crop yield forecasting data\n",
        "# A dataset class for crop yield forecasting data\n",
        "class CYPMLDataset(Dataset):\n",
        "  \"\"\"\n",
        "  Dataset class used to load features and labels for training and testing.\n",
        "  For more info about writing custom datasets classes check\n",
        "  https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  data_dfs : dictionary of input dataframes\n",
        "  yield_trend : data uses yield trend\n",
        "  early_season_end : early season prediction dekad (relative to harvest)\n",
        "  is_train : bool to differentiate training, validation and test sets\n",
        "  is_validation : bool to differentiate between training and validation sets\n",
        "  test_fraction : fraction of years to use for held-out testing\n",
        "  num_folds : number of folds for custom cv\n",
        "  num_valid_years : number of years in validation set\n",
        "  fold_iter : iteration number of cv\n",
        "  scaler_args : mean, std values (for each indicator) calculated using training dataset\n",
        "  country : country code\n",
        "  \"\"\"\n",
        "  def __init__(self, data_dfs, country_years,\n",
        "               yield_trend=True, early_season_end=None,\n",
        "               is_train=True, is_validation=False, test_fraction=0.3,\n",
        "               num_folds=1, num_valid_years=5, fold_iter=0,\n",
        "               test_years=None, scaler_args=None,\n",
        "               print_debug=False, log_fh=None):\n",
        "\n",
        "    if (is_train and print_debug):\n",
        "      print('\\n----------------')\n",
        "      print('Training data')\n",
        "      print('----------------')\n",
        "    elif (is_validation and print_debug):\n",
        "      print('\\n------------------')\n",
        "      print('Validation data')\n",
        "      print('------------------')\n",
        "    elif (print_debug):\n",
        "      print('\\n-------------')\n",
        "      print('Test data:')\n",
        "      print('-------------')\n",
        "\n",
        "    pd_dekadal_df = data_dfs['DEKADAL']\n",
        "    pd_label_df = data_dfs['YIELD']\n",
        "    pd_trend_df = data_dfs['YIELD_TREND']\n",
        "    pd_static_df = data_dfs['STATIC']\n",
        "    label_id_df = data_dfs['LABEL_NUMERIC_IDS']\n",
        "\n",
        "    # Static data\n",
        "    static_excl_cols = [ 'id0', 'id_x', 'id_y', 'AEZ_ID', 'TOTAL_AREA']\n",
        "    static_feature_cols = [ c for c in pd_static_df.columns if c not in static_excl_cols]\n",
        "    static_sel_cols =  ['id0', 'id_y', 'id_x', 'TOTAL_AREA'] + static_feature_cols\n",
        "\n",
        "    # Dekadal data should have id0, id_y, id_x, FYEAR, DEKAD, ...\n",
        "    dekadal_feature_cols = list(pd_dekadal_df.columns.values)[5:]\n",
        "    self.dekadal_feature_cols = dekadal_feature_cols\n",
        "    max_dekad = 36\n",
        "    if (early_season_end is not None):\n",
        "      # early_season_end is relative to harvest (so 0 or negative)\n",
        "      max_dekad += early_season_end\n",
        "\n",
        "    self.max_dekad = max_dekad\n",
        "    # Trend data should have id0, id_y, FYEAR, ...\n",
        "    trend_feature_cols = list(pd_trend_df.columns.values)[3:]\n",
        "    # For labels, we want to keep all columns. CROP_AREA is included in pd_label_df\n",
        "    label_cols = list(pd_label_df.columns.values)\n",
        "\n",
        "    if (print_debug):\n",
        "      assert (log_fh is not None)\n",
        "      feature_cols_info = '\\n'\n",
        "      feature_cols_info += '\\nDekadal features: ' + ', '.join(dekadal_feature_cols)\n",
        "      feature_cols_info += '\\nOther features: ' + ', '.join(static_feature_cols)\n",
        "      feature_cols_info += '\\nTrend features: ' + ', '.join(trend_feature_cols)\n",
        "      feature_cols_info += '\\nLabel columns: ' + ', '.join(label_cols[3:])\n",
        "      print(feature_cols_info)\n",
        "      log_fh.write(feature_cols_info + '\\n')\n",
        "\n",
        "    self.Y = None\n",
        "    self.pd_dekadal_df = None\n",
        "    self.pd_trend_df = None\n",
        "    self.pd_static_df = None\n",
        "    self.input_id_df = data_dfs['INPUT_NUMERIC_IDS']\n",
        "    for cn in country_years:\n",
        "      cn_id = label_id_df[label_id_df['COUNTRY'] == cn]['id0'].values[0]\n",
        "      cn_all_years = sorted(country_years[cn])\n",
        "      if (is_train or is_validation or (test_years is None)):\n",
        "        # in the baseline, test years are determined based on all years available.\n",
        "        select_years = self.selectYears(cn_all_years, is_train, is_validation, test_fraction,\n",
        "                                        num_folds, num_valid_years, fold_iter,\n",
        "                                        yield_trend)\n",
        "      else:\n",
        "        select_years = test_years\n",
        "\n",
        "      pd_cn_label_df = pd_label_df[pd_label_df['id0'] == cn_id]\n",
        "      pd_cn_trend_df = pd_trend_df[pd_trend_df['id0'] == cn_id]\n",
        "      pd_cn_dekadal_df = pd_dekadal_df[pd_dekadal_df['id0'] == cn_id]\n",
        "      min_trend_year = pd_cn_trend_df['FYEAR'].min()\n",
        "      min_dek_year = pd_cn_dekadal_df['FYEAR'].min()\n",
        "      if (is_train):\n",
        "        # if using given test years, use all previous years for training\n",
        "        if (test_years is not None):\n",
        "          select_years = [ yr for yr in cn_all_years if yr < test_years[0]]\n",
        "\n",
        "        # Filter earlier years not in trend and dekadal data.\n",
        "        select_years = [ yr for yr in select_years if ((yr >= min_trend_year) and (yr >= min_dek_year))]\n",
        "\n",
        "      if (is_train and print_debug):\n",
        "        train_info = '\\n' + cn + ' Training years: ' + ', '.join([str(yr) for yr in select_years])\n",
        "        log_fh.write(train_info + '\\n')\n",
        "        print(train_info)\n",
        "      elif (is_validation and print_debug):\n",
        "        valid_info = '\\n' + cn + ' Validation years: ' + ', '.join([str(yr) for yr in select_years])\n",
        "        log_fh.write(valid_info + '\\n')\n",
        "        print(valid_info)\n",
        "      elif (print_debug):\n",
        "        test_info = '\\n' + cn + ' Test years: ' + ', '.join([str(yr) for yr in select_years])\n",
        "        log_fh.write(test_info + '\\n')\n",
        "        print(test_info)\n",
        "\n",
        "      # filter by selected years\n",
        "      pd_cn_label_df = pd_cn_label_df[pd_cn_label_df['FYEAR'].isin(select_years)]\n",
        "      pd_cn_trend_df = pd_cn_trend_df[pd_cn_trend_df['FYEAR'].isin(select_years)]\n",
        "      pd_cn_dekadal_df = pd_cn_dekadal_df[pd_cn_dekadal_df['FYEAR'].isin(select_years)]\n",
        "      pd_cn_static_df = pd_static_df[pd_static_df['id0'] == cn_id]\n",
        "\n",
        "      if (self.Y is None):\n",
        "        self.Y = pd_cn_label_df[label_cols].values\n",
        "        self.pd_dekadal_df = pd_cn_dekadal_df\n",
        "        self.pd_trend_df = pd_cn_trend_df\n",
        "        self.pd_static_df = pd_cn_static_df[static_sel_cols]\n",
        "      else:\n",
        "        self.Y = np.append(self.Y, pd_cn_label_df[label_cols].values, axis=0)\n",
        "        self.pd_dekadal_df = pd.concat([self.pd_dekadal_df, pd_cn_dekadal_df], axis=0)\n",
        "        self.pd_trend_df = pd.concat([self.pd_trend_df, pd_cn_trend_df], axis=0)\n",
        "        self.pd_static_df = pd.concat([self.pd_static_df, pd_cn_static_df[static_sel_cols]], axis=0)\n",
        "\n",
        "    # Normalize data\n",
        "    if (is_train and (scaler_args is not None)):\n",
        "      for dek_col in dekadal_feature_cols:\n",
        "        scaler_args[dek_col] = [self.pd_dekadal_df[dek_col].mean(), self.pd_dekadal_df[dek_col].std()]\n",
        "\n",
        "      for trend_col in trend_feature_cols:\n",
        "        scaler_args[trend_col] = [self.pd_trend_df[trend_col].mean(), self.pd_trend_df[trend_col].std()]\n",
        "\n",
        "      for st_col in static_feature_cols:\n",
        "        scaler_args[st_col] = [self.pd_static_df[st_col].mean(), self.pd_static_df[st_col].std()]\n",
        "\n",
        "    if (scaler_args is not None):\n",
        "      for dek_col in dekadal_feature_cols:\n",
        "        avg_val, std_val = scaler_args[dek_col][0], scaler_args[dek_col][1]\n",
        "        self.pd_dekadal_df[dek_col] = (self.pd_dekadal_df[dek_col] - avg_val)/std_val\n",
        "\n",
        "      for trend_col in trend_feature_cols:\n",
        "        avg_val, std_val = scaler_args[trend_col][0], scaler_args[trend_col][1]\n",
        "        self.pd_trend_df[trend_col] = (self.pd_trend_df[trend_col] - avg_val)/std_val\n",
        "\n",
        "      for st_col in static_feature_cols:\n",
        "        avg_val, std_val = scaler_args[st_col][0], scaler_args[st_col][1]\n",
        "        self.pd_static_df[st_col] = (self.pd_static_df[st_col] - avg_val)/std_val\n",
        "\n",
        "    # create pivot tables\n",
        "    self.pd_dekadal_df = self.pd_dekadal_df.pivot_table(values=dekadal_feature_cols,\n",
        "                                                        index=[\"id0\", \"id_y\", \"id_x\", \"FYEAR\"],\n",
        "                                                        columns=[\"DEKAD\"],\n",
        "                                                        fill_value=0.0)\n",
        "    self.pd_trend_df = self.pd_trend_df.pivot_table(values=trend_feature_cols,\n",
        "                                                    index=[\"id0\", \"id_y\", \"FYEAR\"],\n",
        "                                                    fill_value=0.0)\n",
        "    self.pd_total_area_df = self.pd_static_df.pivot_table(values=['TOTAL_AREA'],\n",
        "                                                      index=[\"id0\", \"id_y\", \"id_x\"],\n",
        "                                                      fill_value=0.0)\n",
        "    self.pd_static_df = self.pd_static_df.pivot_table(values=static_feature_cols,\n",
        "                                                      index=[\"id0\", \"id_y\", \"id_x\"],\n",
        "                                                      fill_value=0.0)\n",
        "\n",
        "    if ((self.Y is not None) and print_debug):\n",
        "      data_info = '\\n'\n",
        "      dekadal_data_shape = [len(self.pd_dekadal_df.index), max_dekad, len(dekadal_feature_cols)]\n",
        "      trend_data_shape = [len(self.pd_trend_df.index), len(trend_feature_cols)]\n",
        "      static_data_shape = [len(self.pd_static_df.index), len(static_feature_cols)]\n",
        "      data_info += '\\nDekadal data: ' + ', '.join([ str(x) for x in dekadal_data_shape])\n",
        "      data_info += '\\nOther feature data: ' + ', '.join([ str(x) for x in static_data_shape])\n",
        "      data_info += '\\nTrend feature data: ' + ', '.join([ str(x) for x in trend_data_shape])\n",
        "      data_info += '\\nLabel data: ' + ', '.join([ str(x) for x in self.Y.shape ])\n",
        "      print(data_info)\n",
        "      log_fh.write(data_info + '\\n')\n",
        "\n",
        "  def selectYears(self, all_years, is_train, is_validation, test_fraction,\n",
        "                  num_folds, num_valid_years, fold_iter,\n",
        "                  yield_trend=True):\n",
        "    \"\"\"Get selected train OR validation OR test years\"\"\"\n",
        "    trts_splitter = CYPTrainTestSplitter()\n",
        "    test_years = trts_splitter.getTestYears(all_years, test_fraction=test_fraction,\n",
        "                                            use_yield_trend=yield_trend)\n",
        "\n",
        "    if ((not is_train) and (not is_validation)):\n",
        "      return test_years\n",
        "\n",
        "    train_years = [yr for yr in all_years if yr not in test_years]\n",
        "    if (num_valid_years == 0):\n",
        "      return train_years\n",
        "\n",
        "    custom_valid_years = trts_splitter.getCustomKFoldValidationYears(train_years,\n",
        "                                                                     num_folds=num_folds,\n",
        "                                                                     num_valid_years=num_valid_years)\n",
        "    validation_years = custom_valid_years[fold_iter]\n",
        "    train_years = [yr for yr in train_years if yr < validation_years[0]]\n",
        "    select_years = train_years if is_train else validation_years\n",
        "\n",
        "    return select_years\n",
        "\n",
        "  def getAverageValue(self, indicator='YIELD'):\n",
        "    # self.Y has YIELD, CROP_AREA at the end\n",
        "    if (indicator == 'YIELD'):\n",
        "      return torch.from_numpy(np.array(np.mean(self.Y[:, -2]), dtype='float32'))\n",
        "    elif (indicator == 'CROP_AREA'):\n",
        "      return torch.from_numpy(np.array(np.mean(self.Y[:, -1]), dtype='float32'))\n",
        "    else:\n",
        "      return torch.zeros(1)\n",
        "\n",
        "  def __len__(self):\n",
        "    if (self.Y is None):\n",
        "      return 0\n",
        "\n",
        "    return self.Y.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\"\n",
        "    Returns data for given idx, which selects a specific id_y and FYEAR\n",
        "    sel_X_dekadal : Dekadal (time series) data\n",
        "    sel_X_static : Static feature data\n",
        "    sel_X_trend : Trend features (yields of 5 previous years)\n",
        "    sel_Y : Yield for given NUTS2 region and year\n",
        "    sel_crop_area : crop area for given NUTS2 region and year\n",
        "    sel_ids_x : NUTS3_IDs included in the selected NUTS2_ID\n",
        "    \"\"\"\n",
        "    assert (idx < self.Y.shape[0])\n",
        "    sel_cnid = self.Y[idx, 0]\n",
        "    sel_id_y = self.Y[idx, 1]\n",
        "    sel_year = self.Y[idx, 2]\n",
        "    sel_ids_x = self.input_id_df[self.input_id_df['id_y'] == sel_id_y]['id_x'].values\n",
        "    sel_X_dekadal = None\n",
        "    sel_X_static = None\n",
        "    valid_indexes = []\n",
        "    for i, id_x in enumerate(sel_ids_x):\n",
        "      try:\n",
        "        X_dek = self.pd_dekadal_df.loc[(sel_cnid, sel_id_y, id_x, sel_year)].values\n",
        "        X_dek = X_dek.reshape((len(self.dekadal_feature_cols), self.max_dekad)).T\n",
        "        X_dek = np.expand_dims(X_dek, axis=0)\n",
        "        X_static = self.pd_static_df.loc[(sel_cnid, sel_id_y, id_x)].values\n",
        "        X_static = np.expand_dims(X_static, axis=0)\n",
        "        X_total_area = self.pd_total_area_df.loc[(sel_cnid, sel_id_y, id_x)].values\n",
        "        X_total_area = np.expand_dims(X_total_area, axis=0)\n",
        "        # Add TOTAL_AREA to the front of X_static\n",
        "        X_static = np.append(X_total_area, X_static, axis=1)\n",
        "        valid_indexes.append(i)\n",
        "        if (sel_X_dekadal is None):\n",
        "          sel_X_dekadal = X_dek\n",
        "          sel_X_static = X_static\n",
        "        else:\n",
        "          sel_X_dekadal = np.append(sel_X_dekadal, X_dek, axis=0)\n",
        "          sel_X_static = np.append(sel_X_static, X_static, axis=0)\n",
        "      except KeyError:\n",
        "        continue\n",
        "\n",
        "    # print(sel_X_dekadal.shape)\n",
        "    # print(sel_X_static.shape)\n",
        "    sel_X_trend = self.pd_trend_df.loc[(sel_cnid, sel_id_y, sel_year)].values\n",
        "    # print(sel_X_trend.shape)\n",
        " \n",
        "    # NOTE: the data type needs to change if ids are 64 bit.\n",
        "    sel_X_dekadal = torch.from_numpy(np.array(sel_X_dekadal, dtype='float32'))\n",
        "    sel_X_trend = torch.from_numpy(np.array(sel_X_trend, dtype='float32'))\n",
        "    sel_X_static = torch.from_numpy(np.array(sel_X_static, dtype='float32'))\n",
        "\n",
        "    # NOTE: Label columns are id0, id_y, FYEAR, YIELD, CROP_AREA \n",
        "    sel_Y = torch.from_numpy(np.array(self.Y[idx, :-1], dtype='float32'))\n",
        "    sel_crop_area = torch.from_numpy(np.array(self.Y[idx, -1:], dtype='float32'))\n",
        "    sel_ids_x = sel_ids_x[valid_indexes]\n",
        "    sel_ids_x = torch.from_numpy(sel_ids_x.reshape((sel_ids_x.shape[0], 1)))\n",
        "\n",
        "    return sel_X_dekadal, sel_X_static, sel_X_trend, sel_Y, sel_crop_area, sel_ids_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ks9-VxhciJ9m"
      },
      "source": [
        "## Evaluation method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXnpKymzEtkQ"
      },
      "outputs": [],
      "source": [
        "def predictionError(y_hat, y_true):\n",
        "  return torch.mean((y_hat - y_true)**2)\n",
        "\n",
        "def evaluatePredictions(net, data_iter, device='cpu'):\n",
        "  \"\"\"Evaluate predictions on dataset\"\"\"\n",
        "  net.eval()\n",
        "\n",
        "  y_low_res_full = None\n",
        "  y_high_res_full = None\n",
        "  for X_ts, X_rest, X_trend, y, nuts2_crop_area, id_xs in data_iter:\n",
        "    X_ts, X_rest,  = X_ts.to(device), X_rest.to(device)\n",
        "    X_trend, y = X_trend.to(device), y.to(device)\n",
        "    nuts2_crop_area = nuts2_crop_area.to(device)\n",
        "    X_ts = torch.squeeze(X_ts, 0)\n",
        "    X_rest = torch.squeeze(X_rest, 0)\n",
        "    id_xs = torch.squeeze(id_xs, 0)\n",
        "    if (isinstance(net, CYP1DCNNModel)):\n",
        "      X_ts = X_ts.permute(0, 2, 1)\n",
        "\n",
        "    sel_year = y[0, 2].repeat(id_xs.shape[0], 1)\n",
        "    # 1st column is TOTAL_AREA\n",
        "    nuts3_total_areas = X_rest[:, 0]\n",
        "    X_rest = X_rest[:, 1:]\n",
        "    X_trend = X_trend.repeat(X_rest.shape[0], 1)\n",
        "    y_pred_nuts3 = net(X_ts, X_rest, X_trend)\n",
        "\n",
        "    nuts3_crop_area_fr_preds = torch.special.expit(y_pred_nuts3[:, 0])\n",
        "    nuts3_crop_area_preds = torch.mul(nuts3_crop_area_fr_preds, nuts3_total_areas)\n",
        "    nuts2_crop_area_pred = torch.sum(nuts3_crop_area_preds)\n",
        "    # Weights: normalize areas\n",
        "    nuts3_crop_area_pred_wts = nuts3_crop_area_preds/nuts2_crop_area_pred\n",
        "\n",
        "    yield_preds_nuts3 = y_pred_nuts3[:, 1]\n",
        "    yield_pred_nuts2 = torch.dot(yield_preds_nuts3, nuts3_crop_area_pred_wts)\n",
        "    yield_pred_nuts2 = yield_pred_nuts2.reshape(1, 1)\n",
        "    nuts2_crop_area_pred = nuts2_crop_area_pred.reshape(1, 1)\n",
        "    y_low_res_iter = torch.cat((y, yield_pred_nuts2, nuts2_crop_area, nuts2_crop_area_pred), 1)\n",
        "    yield_preds_nuts3 = yield_preds_nuts3.reshape(yield_preds_nuts3.shape[0], 1)\n",
        "    y_high_res_iter = torch.cat((id_xs, sel_year, yield_preds_nuts3), 1)\n",
        "\n",
        "    if (y_low_res_full is None):\n",
        "      y_low_res_full = y_low_res_iter\n",
        "      y_high_res_full = y_high_res_iter\n",
        "    else:\n",
        "      y_low_res_full = torch.cat((y_low_res_full, y_low_res_iter), 0)\n",
        "      y_high_res_full = torch.cat((y_high_res_full, y_high_res_iter), 0)\n",
        "\n",
        "  y_hat = y_low_res_full[:, -3]\n",
        "  y_true = y_low_res_full[:, -4]\n",
        "  if (y_hat.isnan().any() or y_true.isnan().any()):\n",
        "    print(y_hat.isnan().any(), y_true.isnan().any())\n",
        "\n",
        "  nrmse_y = torch.sqrt(predictionError(y_hat, y_true))/torch.mean(y_true)\n",
        "\n",
        "  y_low_res_full = y_low_res_full.cpu().detach().numpy()\n",
        "  y_high_res_full = y_high_res_full.cpu().detach().numpy()\n",
        "  return y_low_res_full, y_high_res_full, nrmse_y.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FGP8puEikoK"
      },
      "source": [
        "## Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-dN1ctQimUo"
      },
      "outputs": [],
      "source": [
        "def grad_clipping(net, theta):\n",
        "  \"\"\"Clip the gradient.\"\"\"\n",
        "  if isinstance(net, nn.Module):\n",
        "    params = [p for p in net.parameters() if p.requires_grad]\n",
        "  else:\n",
        "    params = net.params\n",
        "\n",
        "  norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
        "  if norm > theta:\n",
        "    for param in params:\n",
        "      param.grad[:] *= theta / norm\n",
        "\n",
        "def train_epoch(net, train_dataset, train_loader, loss, loss_lambda,\n",
        "                updater, device='cpu'):\n",
        "  # Set the model to training mode\n",
        "  if isinstance(net, torch.nn.Module):\n",
        "      net.train()\n",
        "\n",
        "  # yield loss, crop area loss, no. of examples\n",
        "  metric = d2l.Accumulator(3)\n",
        "  mean_y = train_dataset.getAverageValue('YIELD').to(device)\n",
        "  mean_crop_area = train_dataset.getAverageValue('CROP_AREA').to(device)\n",
        "  train_y_full = None\n",
        "  for X_ts, X_rest, X_trend, y, nuts2_crop_area, id_xs  in train_loader:\n",
        "    X_ts, X_rest,  = X_ts.to(device), X_rest.to(device)\n",
        "    X_trend, y,  = X_trend.to(device), y.to(device)\n",
        "    nuts2_crop_area = nuts2_crop_area.to(device)\n",
        "    X_ts = torch.squeeze(X_ts, 0)\n",
        "    X_rest = torch.squeeze(X_rest, 0)\n",
        "\n",
        "    if (isinstance(net, CYP1DCNNModel)):\n",
        "      X_ts = X_ts.permute(0, 2, 1)\n",
        "\n",
        "    # 2nd column is TOTAL_AREA\n",
        "    nuts3_total_areas = X_rest[:, 0]\n",
        "    # skip TOTAL_AREA\n",
        "    X_rest = X_rest[:, 1:]\n",
        "    X_trend = X_trend.repeat(X_rest.shape[0], 1)\n",
        "    y_pred_nuts3 = net(X_ts, X_rest, X_trend)\n",
        "    # squeeze crop area fraction predictions to [0, 1]\n",
        "    nuts3_crop_area_fr_preds = torch.special.expit(y_pred_nuts3[:, 0])\n",
        "    nuts3_crop_area_preds = torch.mul(nuts3_crop_area_fr_preds, nuts3_total_areas)\n",
        "    # Weights: normalize areas\n",
        "    nuts2_crop_area_pred = torch.sum(nuts3_crop_area_preds).flatten()\n",
        "    nuts3_crop_area_pred_wts = nuts3_crop_area_preds/nuts2_crop_area_pred\n",
        "    yield_preds_nuts3 = y_pred_nuts3[:, 1]\n",
        "    yield_pred_nuts2 = torch.dot(yield_preds_nuts3, nuts3_crop_area_pred_wts).flatten()\n",
        "    y_true = y[:, -1]\n",
        "    nuts2_crop_area = nuts2_crop_area.flatten()\n",
        "\n",
        "    # Compare crop area predictions with crop area statistics.\n",
        "    # If we use static weights, no need to compare.\n",
        "    # If we have static areas, then we can compare predicted areas with static area.\n",
        "    l_yield = torch.sqrt(loss(yield_pred_nuts2, y_true))/mean_y\n",
        "    l_crop_area = torch.sqrt(loss(nuts2_crop_area_pred, nuts2_crop_area))/mean_crop_area\n",
        "    l = loss_lambda * l_yield + (1 - loss_lambda) * l_crop_area\n",
        "    updater.zero_grad()\n",
        "    l.backward()\n",
        "    grad_clipping(net, 1)\n",
        "    updater.step()\n",
        "\n",
        "    pred_error = predictionError(yield_pred_nuts2, y_true)\n",
        "    # assert (pred_error > 0.0)\n",
        "    metric.add(float(l_yield), float(l_crop_area), 1)\n",
        "\n",
        "    yield_pred_nuts2 = yield_pred_nuts2.reshape(y.shape[0], 1)\n",
        "    y_full_iter = torch.cat((y, yield_pred_nuts2), 1)\n",
        "    if (train_y_full is None):\n",
        "      train_y_full = y_full_iter\n",
        "    else:\n",
        "      train_y_full = torch.cat((train_y_full, y_full_iter), 0)\n",
        "\n",
        "  # Return training loss and training NRMSE\n",
        "  y_hat = train_y_full[:, -1]\n",
        "  y_true = train_y_full[:, -2]\n",
        "  nrmse_y = torch.sqrt(predictionError(y_hat, y_true))/torch.mean(y_true)\n",
        "  return metric[0] / metric[2], metric[1] / metric[2], nrmse_y.item()\n",
        "\n",
        "def train(net, train_dataset, train_loader, test_iter,\n",
        "          loss, loss_lambda, updater, num_epochs,\n",
        "          early_stopping=False, device='cpu',\n",
        "          visualize=False, country=None, ymax=1.0):\n",
        "  if (visualize):\n",
        "    animator = d2l.Animator(xlabel='epoch',\n",
        "                            ylabel=('loss (' + country + ')') if country is not None else 'loss',\n",
        "                            xlim=[1, num_epochs], ylim=[0, ymax],\n",
        "                            legend=['yield loss', 'crop area loss', 'train error', 'test error'],\n",
        "                            figsize=(5,5))\n",
        "  test_error = 0\n",
        "  saved_test_errors = []\n",
        "  epochs_to_run = num_epochs\n",
        "  for epoch in range(num_epochs):\n",
        "    train_metrics = train_epoch(net, train_dataset, train_loader,\n",
        "                                loss, loss_lambda, updater, device)\n",
        "    _, _, test_error = evaluatePredictions(net, test_iter, device)\n",
        "    # Early Stopping:\n",
        "    # Check if test error is more than last two errors.\n",
        "    if (early_stopping):\n",
        "      saved_test_errors.append(test_error)\n",
        "      if ((epoch > 3) and\n",
        "          (saved_test_errors[-1] > saved_test_errors[-2]) and\n",
        "          (saved_test_errors[-2] > saved_test_errors[-3])):\n",
        "        # (epoch - 2) + 1. +1 because of range(epochs_to_run)\n",
        "        epochs_to_run = epoch - 1\n",
        "        break\n",
        "\n",
        "    if (visualize):\n",
        "      animator.add(epoch + 1, train_metrics + (test_error,))\n",
        "\n",
        "  return test_error, epochs_to_run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb2sVDTX0Pma"
      },
      "source": [
        "## 1-D CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mvaJ3eQ0RYo"
      },
      "outputs": [],
      "source": [
        "# WOFOST : Dekadal (every 10 days) : 5 variables\n",
        "# WEATHER : Dekadal : 5 variables\n",
        "# REMOTE SENSING : Dekadal (every 10 days) : 1 variable\n",
        "\n",
        "#         WOFOST_V1 WOFOST_V2 ... WEATHER_V1 WEATHER_V2 ... FAPAR\n",
        "# Dekad 1\n",
        "# Dekad 2\n",
        "# ...\n",
        "# Dekad 36\n",
        "# Yield = one value\n",
        "\n",
        "# Trend Window\n",
        "# YEAR-5, YEAR-4, YEAR-3, YEAR-2, YEAR-1\n",
        "\n",
        "def get1DCNNModel(num_inputs, num_channels, kernel_sizes, paddings, strides):\n",
        "  cnn_layers = []\n",
        "  saved_nch = num_inputs\n",
        "  for i, nch in enumerate(num_channels):\n",
        "    cnn_layers += [nn.Conv1d(in_channels = saved_nch,\n",
        "                             out_channels = nch,\n",
        "                             kernel_size = kernel_sizes[i],\n",
        "                             padding = paddings[i],\n",
        "                             stride = strides[i]),\n",
        "                   nn.BatchNorm1d(num_features=nch),\n",
        "                   nn.ReLU(),\n",
        "                   nn.Dropout(0.1),\n",
        "                   ]\n",
        "    saved_nch = nch\n",
        "\n",
        "  return nn.Sequential(*cnn_layers)\n",
        "\n",
        "class CYP1DCNNModel(nn.Module):\n",
        "  \"\"\"Use 1-D CNNs for season data and LSTM for yield trend.\"\"\"\n",
        "  def __init__(self, num_ts_inputs,\n",
        "               num_trend_features,\n",
        "               num_other_features,\n",
        "               ts_seq_len=36,\n",
        "               num_outputs=1):\n",
        "    super(CYP1DCNNModel, self).__init__()\n",
        "\n",
        "    self.num_ts_inputs = num_ts_inputs\n",
        "    num_channels = [16, 32, 8]\n",
        "\n",
        "    kernel_sizes = [3, 3, 3]\n",
        "    paddings = [1, 1, 1]\n",
        "    strides = [1, 2, 2]\n",
        "    self.cnn1d = get1DCNNModel(num_ts_inputs, num_channels, kernel_sizes,\n",
        "                               paddings, strides)\n",
        "\n",
        "    output_size = ts_seq_len\n",
        "    for i, k in enumerate(kernel_sizes):\n",
        "      output_size = int((output_size + 2 * paddings[i] - k)/strides[i] + 1)\n",
        "\n",
        "    num_ts_features = num_channels[-1] * output_size\n",
        "    num_all_features = num_ts_features + num_trend_features + num_other_features\n",
        "    self.fc = nn.Linear(num_all_features, num_outputs)\n",
        "\n",
        "  def forward(self, X_ts, X_rest, X_trend):\n",
        "    cnn1d_out = self.cnn1d(X_ts)\n",
        "    X_cnn1d = torch.flatten(cnn1d_out, 1)\n",
        "\n",
        "    all_features = torch.cat([X_cnn1d, X_trend, X_rest], 1)\n",
        "    # print(X_cnn1d.shape, X_trend.shape, X_rest.shape, all_features.shape)\n",
        "    output = self.fc(all_features)\n",
        "\n",
        "    # print(output.shape)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-yqUcALlAeF"
      },
      "source": [
        "## LSTM RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tLOxawDlDuG"
      },
      "outputs": [],
      "source": [
        "# WOFOST : Dekadal (every 10 days) : 5 variables\n",
        "# WEATHER : Dekadal : 5 variables\n",
        "# REMOTE SENSING : Dekadal (every 10 days) : 1 variable\n",
        "\n",
        "# RNN\n",
        "#         WOFOST_V1 WOFOST_V2 ... WEATHER_V1 WEATHER_V2 ... FAPAR\n",
        "# Dekad 1\n",
        "# Dekad 2\n",
        "# ...\n",
        "# Dekad 36\n",
        "# Yield = one value\n",
        "\n",
        "# Trend Window\n",
        "# YEAR-5, YEAR-4, YEAR-3, YEAR-2, YEAR-1\n",
        "\n",
        "class CYPLSTMModel(nn.Module):\n",
        "  \"\"\"The RNN model.\"\"\"\n",
        "  def __init__(self, num_ts_features,\n",
        "               num_trend_features,\n",
        "               num_other_features,\n",
        "               ts_seq_len=36,\n",
        "               num_outputs=1):\n",
        "    super(CYPLSTMModel, self).__init__()\n",
        "\n",
        "    self.ts_rnns = nn.ModuleList()\n",
        "    self.num_ts_inputs = num_ts_features\n",
        "    self.num_rnn_layers = 1\n",
        "    self.rnn_hidden_size = 64\n",
        "\n",
        "    self.num_rnn_layers = 1\n",
        "    self.rnn_hidden_size = 64\n",
        "    self.rnn = nn.LSTM(input_size=num_ts_features,\n",
        "                       hidden_size=self.rnn_hidden_size,\n",
        "                       num_layers=self.num_rnn_layers,\n",
        "                       batch_first=True)\n",
        "\n",
        "    # Comment out one of the two lines below to exclude or include yield trend features\n",
        "    # num_all_features = self.rnn_hidden_size + num_other_features\n",
        "    num_all_features = self.rnn_hidden_size + num_trend_features + num_other_features\n",
        "    self.fc = nn.Linear(num_all_features, num_outputs)\n",
        "\n",
        "  def forward(self, X_ts, X_rest, X_trend):\n",
        "    start_index = 0\n",
        "    ts_h, ts_state = self.rnn(X_ts)\n",
        "    ts_h_out = ts_state[0][self.num_rnn_layers - 1].view(-1, self.rnn.hidden_size)\n",
        "\n",
        "    # print(ts_h_out.shape, X_rest.shape, X_trend.shape)\n",
        "    # Comment out one of the two lines below to exclude or include yield trend features\n",
        "    # all_features = torch.cat([ts_h_out, X_rest], 1)\n",
        "    all_features = torch.cat([ts_h_out, X_rest, X_trend], 1)\n",
        "    output = self.fc(all_features)\n",
        "\n",
        "    # print(output.shape)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHI4XA-gsYJ7"
      },
      "source": [
        "## Run Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtMfB0oaeteg"
      },
      "source": [
        "### Unzip data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBbe-u3zddVj"
      },
      "outputs": [],
      "source": [
        "# Copy data over and unzip data\n",
        "# Expect data in /content/NUTS23-all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYWLJsQv1EGp"
      },
      "source": [
        "### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulYi8nUDnyjn",
        "outputId": "baf7db96-f827-417d-b201-7c820f865fff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Current DL Configuration\n",
            "-------------------------\n",
            "crop: soft wheat\n",
            "season_crosses_calendar_year: True\n",
            "countries: ['DE', 'ES', 'FR', 'IT']\n",
            "input_spatial_level: NUTS3\n",
            "label_spatial_level: NUTS2\n",
            "high_res_id_col: NUTS3_ID\n",
            "low_res_id_col: NUTS2_ID\n",
            "data_path: /content/NUTS23-all\n",
            "output_path: /content\n",
            "use_yield_trend: True\n",
            "early_season_end_dekad: -6\n",
            "num_cv_folds: 5\n",
            "num_valid_years: 1\n",
            "test_fraction: 0.3\n",
            "architecture: LSTM\n",
            "debug_level: 2\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if (test_env == 'notebook'):\n",
        "  cyp_config = {\n",
        "      'crop' : 'soft wheat',\n",
        "      'season_crosses_calendar_year' : True,\n",
        "      # 'countries' : ['DE', 'FR', 'HU', 'IT'], # potatoes\n",
        "      'countries' : ['DE', 'ES', 'FR', 'IT'], # soft wheat\n",
        "      # 'countries' : ['ES', 'FR', 'HU', 'IT'], # grain maize\n",
        "      'input_spatial_level' : 'NUTS3',\n",
        "      'label_spatial_level' : 'NUTS2',\n",
        "      'high_res_id_col' : 'NUTS3_ID',\n",
        "      'low_res_id_col' : 'NUTS2_ID',\n",
        "      'data_path' : '/content/NUTS23-all',\n",
        "      'output_path' : '/content',\n",
        "      'use_yield_trend' : True,\n",
        "      'early_season_end_dekad' : -6,\n",
        "      'num_cv_folds' : 5,\n",
        "      'num_valid_years' : 1,\n",
        "      'test_fraction' : 0.3,\n",
        "      'architecture' : 'LSTM',\n",
        "      'debug_level' : 2,\n",
        "  }\n",
        "\n",
        "  crop = cyp_config['crop']\n",
        "  countries = cyp_config['countries']\n",
        "  country_code = None\n",
        "  if (len(countries) == 1):\n",
        "    country_code = countries[0]\n",
        "\n",
        "  use_yield_trend = cyp_config['use_yield_trend']\n",
        "  early_season_end = cyp_config['early_season_end_dekad']\n",
        "\n",
        "  output_path = cyp_config['output_path']\n",
        "  log_file = getLogFilename(crop, use_yield_trend, early_season_end,\n",
        "                            country=country_code,\n",
        "                            architecture=cyp_config['architecture'])\n",
        "  log_fh = open(output_path + '/' + log_file, 'w+')\n",
        "\n",
        "  if (cyp_config['debug_level'] > 1):\n",
        "    printConfig(cyp_config, log_fh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgYkzkrExu9Q"
      },
      "source": [
        "### Naive Trend Model Evaluation\n",
        "\n",
        "The naive trend model assigns NUTS2-level trend to all constituent NUTS3 regions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxPWuQloxyGo",
        "outputId": "12fc8f98-b3ab-47d4-cfa4-99e0ca6e8f7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "NUTS2 TREND\n",
            "   NUTS2_ID  FYEAR  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND COUNTRY\n",
            "9      DE11   2013     7.32     7.46     6.86     6.75     6.24         6.06      DE\n",
            "10     DE11   2014     7.46     6.86     6.75     6.24     7.56         6.85      DE\n",
            "11     DE11   2015     6.86     6.75     6.24     7.56     8.29         8.24      DE\n",
            "12     DE11   2016     6.75     6.24     7.56     8.29     7.73         8.52      DE\n",
            "13     DE11   2017     6.24     7.56     8.29     7.73     6.74         7.66      DE\n",
            "23     DE12   2013     6.97     6.90     6.38     6.17     6.12         5.78      DE\n",
            "24     DE12   2014     6.90     6.38     6.17     6.12     6.72         6.27      DE\n",
            "25     DE12   2015     6.38     6.17     6.12     6.72     7.32         7.27      DE\n",
            "26     DE12   2016     6.17     6.12     6.72     7.32     6.90         7.44      DE\n",
            "27     DE12   2017     6.12     6.72     7.32     6.90     6.17         6.73      DE\n",
            "\n",
            "\n",
            "NAIVE NUTS3 TREND\n",
            "  IDREGION  FYEAR  YIELD COUNTRY NUTS2_ID  YIELD_TREND\n",
            "0    DE112   2013   8.38      DE     DE11         6.06\n",
            "1    DE113   2013   5.60      DE     DE11         6.06\n",
            "2    DE114   2013   6.45      DE     DE11         6.06\n",
            "3    DE115   2013   8.04      DE     DE11         6.06\n",
            "4    DE116   2013   7.51      DE     DE11         6.06\n",
            "5    DE118   2013   8.24      DE     DE11         6.06\n",
            "6    DE119   2013   8.04      DE     DE11         6.06\n",
            "7    DE11A   2013   7.94      DE     DE11         6.06\n",
            "8    DE11B   2013   7.19      DE     DE11         6.06\n",
            "9    DE11C   2013   6.82      DE     DE11         6.06\n",
            "\n",
            " DE Trend NRMSE: 19.01744318981854\n",
            "\n",
            "\n",
            "NUTS2 TREND\n",
            "    NUTS2_ID  FYEAR  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND COUNTRY\n",
            "476     ES11   2012     4.54     4.61     3.05     3.02     2.94         2.20      ES\n",
            "477     ES11   2013     4.61     3.05     3.02     2.94     3.16         2.45      ES\n",
            "478     ES11   2014     3.05     3.02     2.94     3.16     2.43         2.59      ES\n",
            "479     ES11   2015     3.02     2.94     3.16     2.43     3.07         2.80      ES\n",
            "480     ES11   2016     2.94     3.16     2.43     3.07     3.08         2.99      ES\n",
            "483     ES12   2012     2.00     2.00     2.14     1.00     1.10         0.94      ES\n",
            "484     ES12   2013     2.00     2.14     1.00     1.10     1.11         0.89      ES\n",
            "485     ES12   2014     2.14     1.00     1.10     1.11     1.20         0.86      ES\n",
            "486     ES12   2015     1.00     1.10     1.11     1.20     1.20         1.27      ES\n",
            "487     ES12   2016     1.10     1.11     1.20     1.20     1.20         1.25      ES\n",
            "\n",
            "\n",
            "NAIVE NUTS3 TREND\n",
            "  IDREGION  FYEAR  YIELD COUNTRY NUTS2_ID  YIELD_TREND\n",
            "0    ES111   2012   3.43      ES     ES11         2.20\n",
            "1    ES112   2012   2.43      ES     ES11         2.20\n",
            "2    ES113   2012   3.52      ES     ES11         2.20\n",
            "3    ES114   2012   2.08      ES     ES11         2.20\n",
            "4    ES111   2013   3.15      ES     ES11         2.45\n",
            "5    ES112   2013   2.00      ES     ES11         2.45\n",
            "6    ES113   2013   2.48      ES     ES11         2.45\n",
            "7    ES114   2013   2.04      ES     ES11         2.45\n",
            "8    ES111   2014   3.10      ES     ES11         2.59\n",
            "9    ES112   2014   2.65      ES     ES11         2.59\n",
            "\n",
            " ES Trend NRMSE: 39.77805638700975\n",
            "\n",
            "\n",
            "NUTS2 TREND\n",
            "    NUTS2_ID  FYEAR  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND COUNTRY\n",
            "690     FR10   2010     7.83     7.39     7.72     8.22     8.74         8.77      FR\n",
            "691     FR10   2011     7.39     7.72     8.22     8.74     8.11         8.77      FR\n",
            "692     FR10   2012     7.72     8.22     8.74     8.11     7.58         7.96      FR\n",
            "693     FR10   2013     8.22     8.74     8.11     7.58     8.13         7.75      FR\n",
            "694     FR10   2014     8.74     8.11     7.58     8.13     8.38         7.98      FR\n",
            "695     FR10   2015     8.11     7.58     8.13     8.38     8.63         8.72      FR\n",
            "696     FR10   2016     7.58     8.13     8.38     8.63     8.80         9.19      FR\n",
            "697     FR10   2017     8.13     8.38     8.63     8.80     4.34         5.51      FR\n",
            "698     FR10   2018     8.38     8.63     8.80     4.34     8.00         6.12      FR\n",
            "713     FRB0   2010     6.94     6.52     6.43     6.95     7.27         7.15      FR\n",
            "\n",
            "\n",
            "NAIVE NUTS3 TREND\n",
            "  IDREGION  FYEAR  YIELD COUNTRY NUTS2_ID  YIELD_TREND\n",
            "0    FR102   2010    8.3      FR     FR10         8.77\n",
            "1    FR103   2010    7.6      FR     FR10         8.77\n",
            "2    FR104   2010    7.6      FR     FR10         8.77\n",
            "3    FR106   2010    8.5      FR     FR10         8.77\n",
            "4    FR107   2010    7.6      FR     FR10         8.77\n",
            "5    FR108   2010    8.5      FR     FR10         8.77\n",
            "6    FR102   2011    8.0      FR     FR10         8.77\n",
            "7    FR103   2011    6.9      FR     FR10         8.77\n",
            "8    FR104   2011    6.7      FR     FR10         8.77\n",
            "9    FR106   2011    7.4      FR     FR10         8.77\n",
            "\n",
            " FR Trend NRMSE: 19.697549717307012\n",
            "\n",
            "\n",
            "NUTS2 TREND\n",
            "     NUTS2_ID  FYEAR  YIELD-5  YIELD-4  YIELD-3  YIELD-2  YIELD-1  YIELD_TREND COUNTRY\n",
            "1194     ITC1   2012     5.13     4.55     4.50     5.29     4.81         4.89      IT\n",
            "1195     ITC1   2013     4.55     4.50     5.29     4.81     5.70         5.75      IT\n",
            "1196     ITC1   2014     4.50     5.29     4.81     5.70     5.26         5.69      IT\n",
            "1197     ITC1   2015     5.29     4.81     5.70     5.26     5.66         5.70      IT\n",
            "1198     ITC1   2016     4.81     5.70     5.26     5.66     4.96         5.36      IT\n",
            "1199     ITC1   2017     5.70     5.26     5.66     4.96     6.07         5.66      IT\n",
            "1200     ITC1   2018     5.26     5.66     4.96     6.07     4.85         5.24      IT\n",
            "1213     ITC2   2012     3.00     3.00     3.00     3.00     3.00         3.00      IT\n",
            "1214     ITC2   2013     3.00     3.00     3.00     3.00     3.00         3.00      IT\n",
            "1215     ITC2   2014     3.00     3.00     3.00     3.00     3.60         3.48      IT\n",
            "\n",
            "\n",
            "NAIVE NUTS3 TREND\n",
            "  IDREGION  FYEAR  YIELD COUNTRY NUTS2_ID  YIELD_TREND\n",
            "0    ITC11   2012   7.27      IT     ITC1         4.89\n",
            "1    ITC12   2012   6.93      IT     ITC1         4.89\n",
            "2    ITC13   2012   5.45      IT     ITC1         4.89\n",
            "3    ITC14   2012   0.56      IT     ITC1         4.89\n",
            "4    ITC15   2012   4.60      IT     ITC1         4.89\n",
            "5    ITC16   2012   6.14      IT     ITC1         4.89\n",
            "6    ITC17   2012   4.87      IT     ITC1         4.89\n",
            "7    ITC18   2012   5.19      IT     ITC1         4.89\n",
            "8    ITC11   2013   5.47      IT     ITC1         5.75\n",
            "9    ITC12   2013   5.80      IT     ITC1         5.75\n",
            "\n",
            " IT Trend NRMSE: 25.65766424410381\n"
          ]
        }
      ],
      "source": [
        "def evaluateNUTS2TrendAsNUTS3Prediction(cyp_config):\n",
        "  crop_id = cropNameToID(crop_id_dict, cyp_config['crop'])\n",
        "  data_path = cyp_config['data_path']\n",
        "  nuts2_yield_df = spark.read.csv(data_path + '/' + 'YIELD_NUTS2.csv',\n",
        "                                  header = True, inferSchema = True)\n",
        "  cyp_preprocessor = CYPDataPreprocessor(spark)\n",
        "  nuts2_yield_df = cyp_preprocessor.preprocessLabels(nuts2_yield_df, 'IDREGION', crop_id)\n",
        "  trend_ft_df = getTrendWindowYields(nuts2_yield_df, 'IDREGION', 5)\n",
        "  trend_ft_df = getLinearYieldTrend(trend_ft_df.toPandas(), ['IDREGION'], 5)\n",
        "  trend_ft_df = trend_ft_df.rename(columns={'IDREGION' : 'NUTS2_ID'})\n",
        "\n",
        "  nuts3_yield_df = spark.read.csv(data_path + '/' + 'YIELD_NUTS3.csv',\n",
        "                                  header = True, inferSchema = True)\n",
        "  nuts3_yield_df = cyp_preprocessor.preprocessLabels(nuts3_yield_df, 'IDREGION', crop_id)\n",
        "  nuts3_yield_df = nuts3_yield_df.withColumn('COUNTRY', SparkF.substring(nuts3_yield_df['IDREGION'], 1, 2))\n",
        "  nuts3_yield_df = nuts3_yield_df.withColumn('NUTS2_ID', SparkF.substring(nuts3_yield_df['IDREGION'], 1, 4))\n",
        "  trend_ft_df['COUNTRY'] = trend_ft_df['NUTS2_ID'].str[:2]\n",
        "\n",
        "  pd_nuts2_yield_df = nuts2_yield_df.toPandas()\n",
        "  pd_nuts3_yield_df = nuts3_yield_df.toPandas()\n",
        "  test_fraction = 0.3\n",
        "  countries = cyp_config['countries']\n",
        "  pd_nuts3_test_df = None\n",
        "  for cn in countries:\n",
        "    cn_trend_ft_df = trend_ft_df[trend_ft_df['COUNTRY'] == cn]\n",
        "    cn_nuts3_yield_df = pd_nuts3_yield_df[pd_nuts3_yield_df['COUNTRY'] == cn]\n",
        "    cn_all_years = sorted(cn_nuts3_yield_df['FYEAR'].unique())\n",
        "    num_years = len(cn_all_years)\n",
        "    test_year_start = num_years - np.floor(num_years * test_fraction).astype('int')\n",
        "    test_years = cn_all_years[test_year_start:]\n",
        "    cn_nuts3_yield_df = cn_nuts3_yield_df[cn_nuts3_yield_df['FYEAR'].isin(test_years)]\n",
        "    cn_trend_ft_df = cn_trend_ft_df[cn_trend_ft_df['FYEAR'].isin(test_years)]\n",
        "    print('\\n')\n",
        "    print('NUTS2 TREND')\n",
        "    print(cn_trend_ft_df.head(10).to_string())\n",
        "    cn_nuts3_yield_df = cn_nuts3_yield_df.merge(cn_trend_ft_df[['NUTS2_ID', 'FYEAR', 'YIELD_TREND']],\n",
        "                                                on=['NUTS2_ID', 'FYEAR'])\n",
        "    print('\\n')\n",
        "    print('NAIVE NUTS3 TREND')\n",
        "    print(cn_nuts3_yield_df.head(10).to_string())\n",
        "    trend_nrmse = NormalizedRMSE(cn_nuts3_yield_df['YIELD'].values,\n",
        "                                 cn_nuts3_yield_df['YIELD_TREND'].values)\n",
        "    print('\\n', cn, 'Trend NRMSE:', trend_nrmse)\n",
        "    cn_nuts3_yield_df = cn_nuts3_yield_df[['IDREGION', 'FYEAR', 'YIELD_TREND', 'YIELD']]\n",
        "    if (pd_nuts3_test_df is None):\n",
        "      pd_nuts3_test_df = cn_nuts3_yield_df\n",
        "    else:\n",
        "      pd_nuts3_test_df = pd_nuts3_test_df.append(cn_nuts3_yield_df)\n",
        "\n",
        "  crop_name = cyp_config['crop'].replace(' ', '_')\n",
        "  pd_nuts3_test_df.to_csv('pred_' + crop_name + '_NUTS3_trend_early-6-NAIVE_TREND.csv', index=False)\n",
        "\n",
        "if (test_env == 'notebook'):\n",
        "  evaluateNUTS2TrendAsNUTS3Prediction(cyp_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvJm0yk71CDg"
      },
      "source": [
        "### Load and preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAqMFsXv1Fkf",
        "outputId": "3200c352-c085-4c48-9905-758c2ea8c031"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data file name \"/content/NUTS23-all/WOFOST_NUTS3.csv\"\n",
            "Data file name \"/content/NUTS23-all/METEO_DAILY_NUTS3.csv\"\n",
            "Data file name \"/content/NUTS23-all/REMOTE_SENSING_NUTS3.csv\"\n",
            "Data file name \"/content/NUTS23-all/SOIL_NUTS3.csv\"\n",
            "Data file name \"/content/NUTS23-all/GAES_NUTS3.csv\"\n",
            "Data file name \"/content/NUTS23-all/CROP_AREA_NUTS2.csv\"\n",
            "Data file name \"/content/NUTS23-all/YIELD_NUTS2.csv\"\n",
            "Loaded data: WOFOST, METEO_DAILY, REMOTE_SENSING, SOIL, GAES, CROP_AREA, YIELD\n",
            "\n",
            "\n",
            "WOFOST data available for 647 region(s)\n",
            "Season end information\n",
            "+--------+--------+-----+---------------+----------+\n",
            "|NUTS2_ID|NUTS3_ID|FYEAR|PREV_SEASON_END|SEASON_END|\n",
            "+--------+--------+-----+---------------+----------+\n",
            "|    DE11|   DE111| 1979|              0|        24|\n",
            "|    DE11|   DE111| 1980|             24|        24|\n",
            "|    DE11|   DE111| 1981|             24|        23|\n",
            "|    DE11|   DE111| 1982|             23|        23|\n",
            "|    DE11|   DE111| 1983|             23|        24|\n",
            "|    DE11|   DE111| 1984|             24|        24|\n",
            "|    DE11|   DE111| 1985|             24|        23|\n",
            "|    DE11|   DE111| 1986|             23|        24|\n",
            "|    DE11|   DE111| 1987|             24|        24|\n",
            "|    DE11|   DE111| 1988|             24|        22|\n",
            "+--------+--------+-----+---------------+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "WOFOST data\n",
            "+--------+--------+-----+-----+-------------+--------------+------+------+-------+-------+----+----+---+---+---+---+\n",
            "|NUTS2_ID|NUTS3_ID|FYEAR|DEKAD|CAMPAIGN_YEAR|CAMPAIGN_DEKAD|POT_YB|POT_YS|WLIM_YB|WLIM_YS|PLAI|WLAI|DVS|RSM|TWC|TWR|\n",
            "+--------+--------+-----+-----+-------------+--------------+------+------+-------+-------+----+----+---+---+---+---+\n",
            "|    DE11|   DE111| 1979|   25|         1980|             1|   0.0|   0.0|    0.0|    0.0| 0.0| 0.0|  0|0.0|0.0|0.0|\n",
            "|    DE11|   DE111| 1979|   26|         1980|             2|   0.0|   0.0|    0.0|    0.0| 0.0| 0.0|  0|0.0|0.0|0.0|\n",
            "|    DE11|   DE111| 1979|   27|         1980|             3|   0.0|   0.0|    0.0|    0.0| 0.0| 0.0|  0|0.0|0.0|0.0|\n",
            "|    DE11|   DE111| 1979|   28|         1980|             4|   0.0|   0.0|    0.0|    0.0| 0.0| 0.0|  0|0.0|0.0|0.0|\n",
            "|    DE11|   DE111| 1979|   29|         1980|             5|   0.0|   0.0|    0.0|    0.0| 0.0| 0.0|  0|0.0|0.0|0.0|\n",
            "|    DE11|   DE111| 1979|   30|         1980|             6|   0.0|   0.0|    0.0|    0.0| 0.0| 0.0|  0|0.0|0.0|0.0|\n",
            "|    DE11|   DE111| 1979|   31|         1980|             7|   0.0|   0.0|    0.0|    0.0| 0.0| 0.0|  0|0.0|0.0|0.0|\n",
            "|    DE11|   DE111| 1979|   32|         1980|             8|   0.0|   0.0|    0.0|    0.0| 0.0| 0.0|  0|0.0|0.0|0.0|\n",
            "|    DE11|   DE111| 1979|   33|         1980|             9|   0.0|   0.0|    0.0|    0.0| 0.0| 0.0|  0|0.0|0.0|0.0|\n",
            "|    DE11|   DE111| 1979|   34|         1980|            10|   0.0|   0.0|    0.0|    0.0| 0.0| 0.0|  0|0.0|0.0|0.0|\n",
            "+--------+--------+-----+-----+-------------+--------------+------+------+-------+-------+----+----+---+---+---+---+\n",
            "only showing top 10 rows\n",
            "\n",
            "METEO_DAILY data available for 647 region(s)\n",
            "METEO_DAILY data\n",
            "+--------+--------+-----+-----+-------------+--------------+----+----+-----+----+-----+-------+------+\n",
            "|NUTS2_ID|NUTS3_ID|FYEAR|DEKAD|CAMPAIGN_YEAR|CAMPAIGN_DEKAD|TMAX|TMIN| TAVG|PREC|  ET0|    RAD|   CWB|\n",
            "+--------+--------+-----+-----+-------------+--------------+----+----+-----+----+-----+-------+------+\n",
            "|    DE11|   DE111| 1979|   25|         1980|             1|29.5| 8.6|18.32|36.3|27.55|16263.3|  8.75|\n",
            "|    DE11|   DE111| 1979|   26|         1980|             2|25.7| 3.4|16.29|14.2|23.73|14516.6| -9.53|\n",
            "|    DE11|   DE111| 1979|   27|         1980|             3|18.7| 5.4| 10.8| 9.2|13.38| 9112.3| -4.18|\n",
            "|    DE11|   DE111| 1979|   28|         1980|             4|25.3| 4.7|14.94| 1.3|15.18|10529.8|-13.88|\n",
            "|    DE11|   DE111| 1979|   29|         1980|             5|21.7| 1.9|13.24| 7.5|10.93| 8070.8| -3.43|\n",
            "|    DE11|   DE111| 1979|   30|         1980|             6|12.5|-0.3| 6.75| 7.4|10.18|6911.45| -2.78|\n",
            "|    DE11|   DE111| 1979|   31|         1980|             7|15.4| 1.6| 7.98|56.6|  8.0| 4740.0|  48.6|\n",
            "|    DE11|   DE111| 1979|   32|         1980|             8| 9.7|-2.4| 3.21|22.0| 3.53| 3466.6| 18.47|\n",
            "|    DE11|   DE111| 1979|   33|         1980|             9|11.1|-1.0| 4.08|15.9| 3.45| 2871.2| 12.45|\n",
            "|    DE11|   DE111| 1979|   34|         1980|            10|16.0| 2.8| 9.94|20.6| 6.49| 3435.3| 14.11|\n",
            "+--------+--------+-----+-----+-------------+--------------+----+----+-----+----+-----+-------+------+\n",
            "only showing top 10 rows\n",
            "\n",
            "REMOTE_SENSING data available for 610 region(s)\n",
            "REMOTE_SENSING data\n",
            "+--------+--------+-----+-----+-------------+--------------+-----+\n",
            "|NUTS2_ID|NUTS3_ID|FYEAR|DEKAD|CAMPAIGN_YEAR|CAMPAIGN_DEKAD|FAPAR|\n",
            "+--------+--------+-----+-----+-------------+--------------+-----+\n",
            "|    DE11|   DE111| 1999|    1|         1999|            15|0.269|\n",
            "|    DE11|   DE111| 1999|    3|         1999|            17|0.216|\n",
            "|    DE11|   DE111| 1999|    4|         1999|            18|0.192|\n",
            "|    DE11|   DE111| 1999|    5|         1999|            19|0.187|\n",
            "|    DE11|   DE111| 1999|    6|         1999|            20|0.197|\n",
            "|    DE11|   DE111| 1999|    7|         1999|            21|0.215|\n",
            "|    DE11|   DE111| 1999|    8|         1999|            22|0.243|\n",
            "|    DE11|   DE111| 1999|    9|         1999|            23|0.303|\n",
            "|    DE11|   DE111| 1999|   10|         1999|            24| 0.39|\n",
            "|    DE11|   DE111| 1999|   11|         1999|            25|0.461|\n",
            "+--------+--------+-----+-----+-------------+--------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "SOIL data available for 650 region(s)\n",
            "SOIL data\n",
            "+--------+--------+------+\n",
            "|NUTS2_ID|NUTS3_ID|SM_WHC|\n",
            "+--------+--------+------+\n",
            "|    DE11|   DE111|  0.19|\n",
            "|    DE11|   DE112|  0.17|\n",
            "|    DE11|   DE113|  0.15|\n",
            "|    DE11|   DE114|  0.16|\n",
            "|    DE11|   DE115|  0.21|\n",
            "|    DE11|   DE116|  0.19|\n",
            "|    DE11|   DE117|  0.24|\n",
            "|    DE11|   DE118|  0.21|\n",
            "|    DE11|   DE119|  0.17|\n",
            "|    DE11|   DE11A|  0.16|\n",
            "+--------+--------+------+\n",
            "only showing top 10 rows\n",
            "\n",
            "GAES data available for 364 region(s)\n",
            "GAES data\n",
            "+--------+--------+------+-----------+-----------+-----------+-----------+-------------+--------------+--------------+--------------+------------+\n",
            "|NUTS2_ID|NUTS3_ID|AEZ_ID|   AVG_ELEV|   STD_ELEV|  AVG_SLOPE|  STD_SLOPE|   TOTAL_AREA|AVG_FIELD_SIZE|STD_FIELD_SIZE|IRRIG_AREA_ALL|IRRIG_AREA90|\n",
            "+--------+--------+------+-----------+-----------+-----------+-----------+-------------+--------------+--------------+--------------+------------+\n",
            "|    DE11|   DE118|  1957|234.8966165|38.67940843|1.033525944|0.712111771| 109974.85807|   6.136363636|   4.438980227|       14173.0|     5916.78|\n",
            "|    DE11|   DE119|  1957|    333.632|58.72798748|1.161122322|0.918611646|77993.0182092|          5.75|   4.655641739|       14173.0|     5916.78|\n",
            "|    DE11|   DE11B|  1957| 340.994382|58.34429044|1.216158032|0.808131516|130493.874002|   7.428571429|   4.401028018|       14173.0|     5916.78|\n",
            "|    DE11|   DE11D|  1957|512.2567568|65.12203695|1.023959994|0.821360588|151347.632375|           6.4|   4.942165517|       14173.0|     5916.78|\n",
            "|    DE12|   DE123|  2034|162.6173469|53.96685072|0.688233495|0.725002766|108505.670233|           4.9|   4.655641739|       14173.0|     5916.78|\n",
            "|    DE12|   DE127|  1957|343.8443936|63.83733303|1.060104251|0.727614582|112573.076126|   7.166666667|   4.389381126|       14173.0|     5916.78|\n",
            "|    DE12|   DE128|  1932|166.9326146|62.50935747|0.729838014|0.714358389|105750.016998|        6.5625|   4.754227442|       14173.0|     5916.78|\n",
            "|    DE13|   DE132|  1957|283.2405063|167.6303602|0.852393031|0.871295631|137886.770203|           4.0|   4.669047012|       14173.0|     5916.78|\n",
            "|    DE13|   DE134|  1957|151.6352941|27.61340841|0.342779547|0.745763123|186025.218422|           3.0|   3.937003937|       14173.0|     5916.78|\n",
            "|    DE13|   DE135|  1957|625.9733333|65.22114643| 1.09428668|0.780498803| 76921.560869|   6.214285714|   4.733366868|       14173.0|     5916.78|\n",
            "+--------+--------+------+-----------+-----------+-----------+-----------+-------------+--------------+--------------+--------------+------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "CROP_AREA data available for 97 region(s)\n",
            "CROP_AREA data\n",
            "+--------+-----+---------+\n",
            "|NUTS2_ID|FYEAR|CROP_AREA|\n",
            "+--------+-----+---------+\n",
            "|    DE11| 1991|  71822.0|\n",
            "|    DE11| 1992|  76320.0|\n",
            "|    DE11| 1993|  70330.0|\n",
            "|    DE11| 1994|  71183.0|\n",
            "|    DE11| 1995|  77611.0|\n",
            "|    DE11| 1996|  76980.0|\n",
            "|    DE11| 1997|  86636.0|\n",
            "|    DE11| 1998|  90968.0|\n",
            "|    DE11| 1999|  79364.0|\n",
            "|    DE11| 2000|  94145.0|\n",
            "+--------+-----+---------+\n",
            "only showing top 10 rows\n",
            "\n",
            "YIELD data available for 93 region(s)\n",
            "YIELD data\n",
            "+--------+-----+-----+\n",
            "|NUTS2_ID|FYEAR|YIELD|\n",
            "+--------+-----+-----+\n",
            "|    DE11| 1999| 6.19|\n",
            "|    DE11| 2000| 6.78|\n",
            "|    DE11| 2001| 6.94|\n",
            "|    DE11| 2002| 6.83|\n",
            "|    DE11| 2003| 5.85|\n",
            "|    DE11| 2004|  7.9|\n",
            "|    DE11| 2005| 6.85|\n",
            "|    DE11| 2006| 7.47|\n",
            "|    DE11| 2007| 7.19|\n",
            "|    DE11| 2008| 7.32|\n",
            "+--------+-----+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "\n",
            "\n",
            "DEKADAL\n",
            "+---+----+----+-----+-----+-------+-------+----+---+---+----+----+-----+--------+----+------+-----+\n",
            "|id0|id_y|id_x|FYEAR|DEKAD|WLIM_YB|WLIM_YS|WLAI|TWC|RSM|TMAX|TMIN| TAVG|     RAD|PREC|   CWB|FAPAR|\n",
            "+---+----+----+-----+-----+-------+-------+----+---+---+----+----+-----+--------+----+------+-----+\n",
            "|  0|   0|   0| 2004|    1|    0.0|    0.0| 0.0|0.0|0.0|29.2| 9.8|19.08|17602.82|17.8|-23.46|0.328|\n",
            "|  0|   0|   0| 2004|    2|    0.0|    0.0| 0.0|0.0|0.0|26.7| 4.0| 15.0| 14946.7|24.4|  -2.2|0.348|\n",
            "|  0|   0|   0| 2004|    3|    0.0|    0.0| 0.0|0.0|0.0|31.4| 5.1|15.85| 15024.4| 1.7|-25.02|0.383|\n",
            "|  0|   0|   0| 2004|    4|    0.0|    0.0| 0.0|0.0|0.0|28.5| 2.7|13.87| 12837.8| 9.4|-14.19|0.439|\n",
            "|  0|   0|   0| 2004|    5|    0.0|    0.0| 0.0|0.0|0.0|22.7| 4.3|11.39|  7374.5|58.7| 46.08|0.526|\n",
            "|  0|   0|   0| 2004|    6|    0.0|    0.0| 0.0|0.0|0.0|16.1|-2.4| 7.81|  8154.1|10.8|  -3.8|0.554|\n",
            "|  0|   0|   0| 2004|    7|    0.0|    0.0| 0.0|0.0|0.0|13.2|-4.0| 3.61| 6371.18|14.0|  6.11|0.551|\n",
            "|  0|   0|   0| 2004|    8|    0.0|    0.0| 0.0|0.0|0.0|14.7|-1.1| 7.57|  5799.1| 1.8| -8.08|0.523|\n",
            "|  0|   0|   0| 2004|    9|    0.0|    0.0| 0.0|0.0|0.0|12.4| 0.4| 5.85|  3415.9|16.6| 12.26|0.505|\n",
            "|  0|   0|   0| 2004|   10|    0.0|    0.0| 0.0|0.0|0.0|14.1|-0.3|  6.5|  3483.4|22.4| 18.76|0.495|\n",
            "+---+----+----+-----+-----+-------+-------+----+---+---+----+----+-----+--------+----+------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "\n",
            "\n",
            "TREND\n",
            "+---+----+-----+-------+-------+-------+-------+-------+\n",
            "|id0|id_y|FYEAR|YIELD-5|YIELD-4|YIELD-3|YIELD-2|YIELD-1|\n",
            "+---+----+-----+-------+-------+-------+-------+-------+\n",
            "|  0|   0| 2004|   6.19|   6.78|   6.94|   6.83|   5.85|\n",
            "|  0|   0| 2005|   6.78|   6.94|   6.83|   5.85|    7.9|\n",
            "|  0|   0| 2006|   6.94|   6.83|   5.85|    7.9|   6.85|\n",
            "|  0|   0| 2007|   6.83|   5.85|    7.9|   6.85|   7.47|\n",
            "|  0|   0| 2008|   5.85|    7.9|   6.85|   7.47|   7.19|\n",
            "|  0|   0| 2009|    7.9|   6.85|   7.47|   7.19|   7.32|\n",
            "|  0|   0| 2010|   6.85|   7.47|   7.19|   7.32|   7.46|\n",
            "|  0|   0| 2011|   7.47|   7.19|   7.32|   7.46|   6.86|\n",
            "|  0|   0| 2012|   7.19|   7.32|   7.46|   6.86|   6.75|\n",
            "|  0|   0| 2013|   7.32|   7.46|   6.86|   6.75|   6.24|\n",
            "+---+----+-----+-------+-------+-------+-------+-------+\n",
            "only showing top 10 rows\n",
            "\n",
            "\n",
            "\n",
            "YIELD\n",
            "+---+----+-----+-----+---------+\n",
            "|id0|id_y|FYEAR|YIELD|CROP_AREA|\n",
            "+---+----+-----+-----+---------+\n",
            "|  0|   0| 2004|  7.9|  90032.0|\n",
            "|  0|   0| 2005| 6.85|  89040.0|\n",
            "|  0|   0| 2006| 7.47|  91313.0|\n",
            "|  0|   0| 2007| 7.19|  94153.0|\n",
            "|  0|   0| 2008| 7.32|  94465.0|\n",
            "|  0|   0| 2009| 7.46|  96197.0|\n",
            "|  0|   0| 2010| 6.86|  99324.0|\n",
            "|  0|   0| 2011| 6.75|  94730.0|\n",
            "|  0|   0| 2012| 6.24|  82819.0|\n",
            "|  0|   0| 2013| 7.56|  95312.0|\n",
            "+---+----+-----+-----+---------+\n",
            "only showing top 10 rows\n",
            "\n",
            "\n",
            "\n",
            "STATIC\n",
            " id0  id_y  id_x  SM_WHC    TOTAL_AREA   AVG_ELEV   STD_ELEV  AVG_SLOPE  STD_SLOPE  AVG_FIELD_SIZE  STD_FIELD_SIZE  IRRIG_AREA_ALL  IRRIG_AREA90  CN_DE  CN_ES  CN_FR  CN_IT  AEZ_1761  AEZ_1810  AEZ_1842  AEZ_1845  AEZ_1892  AEZ_1932  AEZ_1957  AEZ_1960  AEZ_1976  AEZ_1984  AEZ_2030  AEZ_2034  AEZ_2057  AEZ_2075  AEZ_2107  AEZ_2135  AEZ_2137  AEZ_2150\n",
            "   0     0     0    0.21 109974.858070 234.896616  38.679408   1.033526   0.712112        6.136364        4.438980         14173.0       5916.78      1      0      0      0         0         0         0         0         0         0         1         0         0         0         0         0         0         0         0         0         0         0\n",
            "   0     0     1    0.17  77993.018209 333.632000  58.727987   1.161122   0.918612        5.750000        4.655642         14173.0       5916.78      1      0      0      0         0         0         0         0         0         0         1         0         0         0         0         0         0         0         0         0         0         0\n",
            "   0     0     2    0.17 130493.874002 340.994382  58.344290   1.216158   0.808132        7.428571        4.401028         14173.0       5916.78      1      0      0      0         0         0         0         0         0         0         1         0         0         0         0         0         0         0         0         0         0         0\n",
            "   0     0     3    0.16 151347.632375 512.256757  65.122037   1.023960   0.821361        6.400000        4.942166         14173.0       5916.78      1      0      0      0         0         0         0         0         0         0         1         0         0         0         0         0         0         0         0         0         0         0\n",
            "   0     1     4    0.20 108505.670233 162.617347  53.966851   0.688233   0.725003        4.900000        4.655642         14173.0       5916.78      1      0      0      0         0         0         0         0         0         0         0         0         0         0         0         1         0         0         0         0         0         0\n",
            "   0     1     5    0.18 112573.076126 343.844394  63.837333   1.060104   0.727615        7.166667        4.389381         14173.0       5916.78      1      0      0      0         0         0         0         0         0         0         1         0         0         0         0         0         0         0         0         0         0         0\n",
            "   0     1     6    0.19 105750.016998 166.932615  62.509357   0.729838   0.714358        6.562500        4.754227         14173.0       5916.78      1      0      0      0         0         0         0         0         0         1         0         0         0         0         0         0         0         0         0         0         0         0\n",
            "   0     2     7    0.16 137886.770203 283.240506 167.630360   0.852393   0.871296        4.000000        4.669047         14173.0       5916.78      1      0      0      0         0         0         0         0         0         0         1         0         0         0         0         0         0         0         0         0         0         0\n",
            "   0     2     8    0.15 186025.218422 151.635294  27.613408   0.342780   0.745763        3.000000        3.937004         14173.0       5916.78      1      0      0      0         0         0         0         0         0         0         1         0         0         0         0         0         0         0         0         0         0         0\n",
            "   0     2     9    0.16  76921.560869 625.973333  65.221146   1.094287   0.780499        6.214286        4.733367         14173.0       5916.78      1      0      0      0         0         0         0         0         0         0         1         0         0         0         0         0         0         0         0         0         0         0\n",
            "\n",
            "\n",
            "NUMERIC_IDS\n",
            "+-------+--------+--------+----+----+---+\n",
            "|COUNTRY|NUTS3_ID|NUTS2_ID|id_y|id_x|id0|\n",
            "+-------+--------+--------+----+----+---+\n",
            "|     DE|   DE118|    DE11|   0|   0|  0|\n",
            "|     DE|   DE119|    DE11|   0|   1|  0|\n",
            "|     DE|   DE11B|    DE11|   0|   2|  0|\n",
            "|     DE|   DE11D|    DE11|   0|   3|  0|\n",
            "|     DE|   DE123|    DE12|   1|   4|  0|\n",
            "|     DE|   DE127|    DE12|   1|   5|  0|\n",
            "|     DE|   DE128|    DE12|   1|   6|  0|\n",
            "|     DE|   DE132|    DE13|   2|   7|  0|\n",
            "|     DE|   DE134|    DE13|   2|   8|  0|\n",
            "|     DE|   DE135|    DE13|   2|   9|  0|\n",
            "+-------+--------+--------+----+----+---+\n",
            "only showing top 10 rows\n",
            "\n",
            "+-------+--------+----+---+\n",
            "|COUNTRY|NUTS2_ID|id_y|id0|\n",
            "+-------+--------+----+---+\n",
            "|     DE|    DE11|   0|  0|\n",
            "|     DE|    DE12|   1|  0|\n",
            "|     DE|    DE13|   2|  0|\n",
            "|     DE|    DE14|   3|  0|\n",
            "|     DE|    DE21|   4|  0|\n",
            "|     DE|    DE22|   5|  0|\n",
            "|     DE|    DE23|   6|  0|\n",
            "|     DE|    DE24|   7|  0|\n",
            "|     DE|    DE25|   8|  0|\n",
            "|     DE|    DE26|   9|  0|\n",
            "+-------+--------+----+---+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def loadAndPreprocessData(cyp_config, data_sources,\n",
        "                          dekadal_data_sources, static_data_sources):\n",
        "  \"\"\"Load, preprocess and combine data sources\"\"\"\n",
        "  crop = cyp_config['crop']\n",
        "  season_crosses = cyp_config['season_crosses_calendar_year']\n",
        "  countries = cyp_config['countries']\n",
        "  country_code = None\n",
        "  if (len(countries) == 1):\n",
        "    country_code = countries[0]\n",
        "\n",
        "  data_path = cyp_config['data_path']\n",
        "  early_season_end = cyp_config['early_season_end_dekad']\n",
        "  print_debug = (cyp_config['debug_level'] > 1)\n",
        "  high_res_id_col = cyp_config['high_res_id_col']\n",
        "  low_res_id_col = cyp_config['low_res_id_col']\n",
        "\n",
        "  data_dfs = loadAllData(spark, data_sources, data_path=data_path, country=country_code)\n",
        "  data_dfs = preprocessData(spark, data_dfs, data_sources,\n",
        "                            high_res_id_col, low_res_id_col,\n",
        "                            crop, countries, season_crosses_calyear=season_crosses,\n",
        "                            print_debug=print_debug)\n",
        "\n",
        "  combined_dfs, country_years = combineInputData(data_sources, data_dfs, countries,\n",
        "                                                 dekadal_data_sources,\n",
        "                                                 static_data_sources,\n",
        "                                                 high_res_id_col, low_res_id_col,\n",
        "                                                 early_season_end=early_season_end,\n",
        "                                                 print_debug=print_debug)\n",
        "\n",
        "  return combined_dfs, country_years\n",
        "\n",
        "if (test_env == 'notebook'):\n",
        "  crop = cyp_config['crop']\n",
        "  input_spatial_level = cyp_config['input_spatial_level']\n",
        "  label_spatial_level = cyp_config['label_spatial_level']\n",
        "  high_res_id_col = cyp_config['high_res_id_col']\n",
        "  low_res_id_col = cyp_config['low_res_id_col']\n",
        "  label_year_col = 'FYEAR'\n",
        "  input_campaign_cols = [ 'CAMPAIGN_YEAR', 'CAMPAIGN_DEKAD']\n",
        "  input_order_cols = [low_res_id_col, high_res_id_col] + input_campaign_cols\n",
        "  label_order_cols = [low_res_id_col, label_year_col]\n",
        "  wofost_indicators = ['WLIM_YB', 'WLIM_YS', 'WLAI', 'TWC', 'RSM' ]\n",
        "  meteo_indicators = ['TMAX', 'TMIN', 'TAVG', 'RAD', 'PREC', 'CWB']\n",
        "  rs_indicators = [ 'FAPAR' ]\n",
        "\n",
        "  crop_id = cropNameToID(crop_id_dict, crop)\n",
        "  gaes_features = ['AVG_ELEV', 'STD_ELEV', 'AVG_SLOPE', 'STD_SLOPE', 'AVG_FIELD_SIZE', 'STD_FIELD_SIZE']\n",
        "  gaes_features += ['IRRIG_AREA_ALL', 'IRRIG_AREA' + str(crop_id)]\n",
        "\n",
        "  data_sources = {\n",
        "      'WOFOST' : { 'spatial_level' : input_spatial_level,\n",
        "                  'order_cols' : input_order_cols,\n",
        "                  'sel_cols' : input_order_cols + wofost_indicators\n",
        "      },\n",
        "      'METEO_DAILY' : { 'spatial_level' : input_spatial_level,\n",
        "                        'order_cols' : input_order_cols,\n",
        "                        'sel_cols' : input_order_cols + meteo_indicators\n",
        "      },\n",
        "      'REMOTE_SENSING' : { 'spatial_level' : input_spatial_level,\n",
        "                          'order_cols' : input_order_cols,\n",
        "                          'sel_cols' : input_order_cols + rs_indicators\n",
        "      },\n",
        "      'SOIL' : { 'spatial_level' : input_spatial_level,\n",
        "                'order_cols' : [low_res_id_col, high_res_id_col],\n",
        "                'sel_cols' : [ low_res_id_col, high_res_id_col, 'SM_WHC' ]\n",
        "      },\n",
        "      'GAES' : { 'spatial_level' : input_spatial_level,\n",
        "                'order_cols' : [low_res_id_col, high_res_id_col],\n",
        "                'sel_cols' : [low_res_id_col, high_res_id_col, 'AEZ_ID', 'TOTAL_AREA' ] + gaes_features\n",
        "      },\n",
        "      'CROP_AREA' : { 'spatial_level' : label_spatial_level,\n",
        "                      'order_cols' : label_order_cols,\n",
        "                      'sel_cols' : label_order_cols + ['CROP_AREA']\n",
        "      },\n",
        "      'YIELD' : { 'spatial_level' : label_spatial_level,\n",
        "                  'order_cols' : label_order_cols,\n",
        "                  'sel_cols' : label_order_cols + [ 'YIELD' ]\n",
        "      },\n",
        "  }\n",
        "\n",
        "  dekadal_data_sources = ['WOFOST', 'METEO_DAILY', 'REMOTE_SENSING']\n",
        "  static_data_sources = ['SOIL', 'GAES']\n",
        "  combined_dfs, country_years = loadAndPreprocessData(cyp_config, data_sources,\n",
        "                                                      dekadal_data_sources, static_data_sources)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLJeBWYEo6nj"
      },
      "source": [
        "### Optimize Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhciRFtQo-a_"
      },
      "source": [
        "#### CV Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExkDqqwApAp6",
        "outputId": "727337f1-1983-4b7b-a24e-f86e28c79135"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "----------------\n",
            "Training data\n",
            "----------------\n",
            "\n",
            "\n",
            "Dekadal features: WLIM_YB, WLIM_YS, WLAI, TWC, RSM, TMAX, TMIN, TAVG, RAD, PREC, CWB, FAPAR\n",
            "Other features: SM_WHC, AVG_ELEV, STD_ELEV, AVG_SLOPE, STD_SLOPE, AVG_FIELD_SIZE, STD_FIELD_SIZE, IRRIG_AREA_ALL, IRRIG_AREA90, CN_DE, CN_ES, CN_FR, CN_IT, AEZ_1761, AEZ_1810, AEZ_1842, AEZ_1845, AEZ_1892, AEZ_1932, AEZ_1957, AEZ_1960, AEZ_1976, AEZ_1984, AEZ_2030, AEZ_2034, AEZ_2057, AEZ_2075, AEZ_2107, AEZ_2135, AEZ_2137, AEZ_2150\n",
            "Trend features: YIELD-5, YIELD-4, YIELD-3, YIELD-2, YIELD-1\n",
            "Label columns: YIELD, CROP_AREA\n",
            "\n",
            "DE Training years: 2004, 2005, 2006, 2007\n",
            "\n",
            "ES Training years: 2003, 2004, 2005, 2006\n",
            "\n",
            "FR Training years: 1999, 2000, 2001, 2002, 2003, 2004, 2005\n",
            "\n",
            "IT Training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006\n",
            "\n",
            "\n",
            "Dekadal data: 1882, 30, 12\n",
            "Other feature data: 357, 31\n",
            "Trend feature data: 435, 5\n",
            "Label data: 435, 5\n",
            "\n",
            "------------------\n",
            "Validation data\n",
            "------------------\n",
            "\n",
            "\n",
            "Dekadal features: WLIM_YB, WLIM_YS, WLAI, TWC, RSM, TMAX, TMIN, TAVG, RAD, PREC, CWB, FAPAR\n",
            "Other features: SM_WHC, AVG_ELEV, STD_ELEV, AVG_SLOPE, STD_SLOPE, AVG_FIELD_SIZE, STD_FIELD_SIZE, IRRIG_AREA_ALL, IRRIG_AREA90, CN_DE, CN_ES, CN_FR, CN_IT, AEZ_1761, AEZ_1810, AEZ_1842, AEZ_1845, AEZ_1892, AEZ_1932, AEZ_1957, AEZ_1960, AEZ_1976, AEZ_1984, AEZ_2030, AEZ_2034, AEZ_2057, AEZ_2075, AEZ_2107, AEZ_2135, AEZ_2137, AEZ_2150\n",
            "Trend features: YIELD-5, YIELD-4, YIELD-3, YIELD-2, YIELD-1\n",
            "Label columns: YIELD, CROP_AREA\n",
            "\n",
            "DE Validation years: 2008\n",
            "\n",
            "ES Validation years: 2007\n",
            "\n",
            "FR Validation years: 2006\n",
            "\n",
            "IT Validation years: 2007\n",
            "\n",
            "\n",
            "Dekadal data: 350, 30, 12\n",
            "Other feature data: 357, 31\n",
            "Trend feature data: 80, 5\n",
            "Label data: 80, 5\n"
          ]
        }
      ],
      "source": [
        "def getCustomCVDatasets(cyp_config, combined_dfs, log_fh):\n",
        "  \"\"\"Get training and validation datasets for custom cv\"\"\"\n",
        "  num_folds = cyp_config['num_cv_folds']\n",
        "  num_valid_years = cyp_config['num_valid_years']\n",
        "  test_fraction = cyp_config['test_fraction']\n",
        "  use_yield_trend = cyp_config['use_yield_trend']\n",
        "  early_season_end = cyp_config['early_season_end_dekad']\n",
        "\n",
        "  cv_datasets = []\n",
        "  for fold_iter in range(num_folds):\n",
        "    scaler_args = {}\n",
        "    train_dataset = CYPMLDataset(combined_dfs, country_years,\n",
        "                                 yield_trend=use_yield_trend, early_season_end=early_season_end,\n",
        "                                 is_train=True, test_fraction=test_fraction,\n",
        "                                 num_folds = num_folds, num_valid_years=num_valid_years,\n",
        "                                 fold_iter=fold_iter,\n",
        "                                 scaler_args=scaler_args,\n",
        "                                 print_debug=(fold_iter == 0),\n",
        "                                 log_fh=log_fh)\n",
        "\n",
        "    valid_dataset = CYPMLDataset(combined_dfs, country_years,\n",
        "                                 yield_trend=use_yield_trend, early_season_end=early_season_end,\n",
        "                                 is_train=False, is_validation=True,\n",
        "                                 test_fraction=test_fraction,\n",
        "                                 num_folds = num_folds, num_valid_years=num_valid_years,\n",
        "                                 fold_iter=fold_iter,\n",
        "                                 scaler_args=scaler_args,\n",
        "                                 print_debug=(fold_iter == 0),\n",
        "                                 log_fh=log_fh)\n",
        "\n",
        "    cv_datasets.append([train_dataset, valid_dataset])\n",
        "\n",
        "  return cv_datasets\n",
        "\n",
        "if (test_env == 'notebook'):\n",
        "  cv_datasets = getCustomCVDatasets(cyp_config, combined_dfs, log_fh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32i4FZVb1Odt"
      },
      "source": [
        "#### Hyperparameter Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ftz7-jbmjqai",
        "outputId": "7deee7e2-d6eb-4f36-a060-e732cecaf6a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CYPLSTMModel(\n",
            "  (ts_rnns): ModuleList()\n",
            "  (rnn): LSTM(12, 64, batch_first=True)\n",
            "  (fc): Linear(in_features=95, out_features=2, bias=True)\n",
            ")\n",
            "\n",
            "Searching for optimal hyperparameters ...\n",
            "-------------------------------------------\n",
            "yield loss weight: 0.5 , lr: 0.001 , weight decay: 0.0001 , avg cv NRMSE: 0.2039 , std cv NRMSE: 0.0122\n",
            "yield loss weight: 0.5 , lr: 0.001 , weight decay: 1e-05 , avg cv NRMSE: 0.2067 , std cv NRMSE: 0.0122\n",
            "yield loss weight: 0.5 , lr: 0.0005 , weight decay: 0.0001 , avg cv NRMSE: 0.2013 , std cv NRMSE: 0.01\n",
            "yield loss weight: 0.5 , lr: 0.0005 , weight decay: 1e-05 , avg cv NRMSE: 0.2138 , std cv NRMSE: 0.0191\n",
            "\n",
            " Yield Loss Weight  Learning Rate  Weight Decay  Avg CV NRMSE  STD CV NRMSE\n",
            "               0.5         0.0010       0.00010        0.2039        0.0122\n",
            "               0.5         0.0010       0.00001        0.2067        0.0122\n",
            "               0.5         0.0005       0.00010        0.2013        0.0100\n",
            "               0.5         0.0005       0.00001        0.2138        0.0191\n",
            "Optimal yield loss weight: 0.5\n",
            "Optimal lr: 0.0005\n",
            "Optimal weight decay lambda: 0.0001\n"
          ]
        }
      ],
      "source": [
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "def optimizeHyperparameters(cyp_config, cv_datasets, log_fh):\n",
        "  loss = nn.MSELoss()\n",
        "  # consider using Huber Loss\n",
        "  # loss = nn.HuberLoss(reduction='mean', delta=1.35)\n",
        "\n",
        "  num_folds = cyp_config['num_cv_folds']\n",
        "  debug_level = cyp_config['debug_level']\n",
        "  architecture = cyp_config['architecture']\n",
        "  best_cv_nrmse = None\n",
        "  # optimize?\n",
        "  num_epochs = 100\n",
        "\n",
        "  loss_weight_space = [0.5] #, 0.6]\n",
        "  lr_space = [1e-3, 5e-4]\n",
        "  weight_decay_space = [1e-4, 1e-5]\n",
        "  best_loss_weight = loss_weight_space[0]\n",
        "  best_lr = lr_space[0]\n",
        "  best_lambda = weight_decay_space[0]\n",
        "\n",
        "  custom_cv_params = {}\n",
        "  row_idx = 0\n",
        "  for loss_weight in loss_weight_space:\n",
        "    for lr in lr_space:\n",
        "      for weight_decay in weight_decay_space:\n",
        "        cv_nrmses = []\n",
        "        for fold_iter in range(num_folds):\n",
        "          train_dataset = cv_datasets[fold_iter][0]\n",
        "          valid_dataset = cv_datasets[fold_iter][1]\n",
        "          train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1,\n",
        "                                                    shuffle=False, num_workers=2)\n",
        "          valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1,\n",
        "                                                    shuffle=False, num_workers=2)\n",
        "\n",
        "          # train_dataset is : X_ts, X_rest, X_trend.\n",
        "          # X_rest shape is (num_regions, variables). Skip TOTAL_AREA\n",
        "          num_other_features = train_dataset[0][1].shape[1] - 1\n",
        "\n",
        "          # X_trend shape is (trend_window)\n",
        "          num_trend_features = train_dataset[0][2].shape[0]\n",
        "\n",
        "          # time series (dekadal) data is [num_regions, num_dekads, num_indicators]\n",
        "          num_ts_indicators = train_dataset[0][0].shape[2]\n",
        "          ts_seq_len = train_dataset[0][0].shape[1]\n",
        "          device = d2l.try_gpu()\n",
        "\n",
        "          if (architecture == '1DCNN'):\n",
        "            net = CYP1DCNNModel(num_ts_indicators,\n",
        "                                num_trend_features,\n",
        "                                num_other_features,\n",
        "                                ts_seq_len=ts_seq_len,\n",
        "                                num_outputs=2)\n",
        "          elif (architecture == 'LSTM'):\n",
        "            net = CYPLSTMModel(num_ts_indicators,\n",
        "                               num_trend_features,\n",
        "                               num_other_features,\n",
        "                               ts_seq_len=ts_seq_len,\n",
        "                               num_outputs=2)\n",
        "          else:\n",
        "            raise ValueError('Unsupported architecture:', architecture, 'Supported: 1DCNN, LSTM')\n",
        "\n",
        "          if ((debug_level > 1) and (row_idx == 0) and (fold_iter == 0)):\n",
        "            print(net)\n",
        "            print('\\nSearching for optimal hyperparameters ...' )\n",
        "            print('-------------------------------------------' )\n",
        "\n",
        "          net = net.to(device)\n",
        "          trainer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "          val_nrmse, _ = train(net, train_dataset, train_loader, valid_loader, loss, loss_weight,\n",
        "                               trainer, num_epochs, early_stopping=True, device=device)\n",
        "          cv_nrmses.append(val_nrmse)\n",
        "\n",
        "        avg_cv_nrmse = round(np.mean(cv_nrmses), 4)\n",
        "        std_cv_nrmse = round(np.std(cv_nrmses), 4)\n",
        "        if (debug_level > 1):\n",
        "          print(\"yield loss weight:\", loss_weight, \", lr:\", lr, \", weight decay:\", weight_decay,\n",
        "                \", avg cv NRMSE:\", avg_cv_nrmse, \", std cv NRMSE:\", std_cv_nrmse)\n",
        "\n",
        "        cv_row = [loss_weight, lr, weight_decay,  avg_cv_nrmse, std_cv_nrmse]\n",
        "        custom_cv_params['row' + str(row_idx)] = cv_row\n",
        "        row_idx += 1\n",
        "\n",
        "        if ((best_cv_nrmse is None) or (avg_cv_nrmse < best_cv_nrmse)):\n",
        "          best_cv_nrmse = avg_cv_nrmse\n",
        "          best_loss_weight = loss_weight\n",
        "          best_lr = lr\n",
        "          best_lambda = weight_decay\n",
        "\n",
        "  cv_params = ['Yield Loss Weight', 'Learning Rate', 'Weight Decay', 'Avg CV NRMSE', 'STD CV NRMSE']\n",
        "  pd_cv_params_df = pd.DataFrame.from_dict(custom_cv_params, orient='index',\n",
        "                                          columns=cv_params)\n",
        "\n",
        "  cv_info = '\\n' + pd_cv_params_df.head(row_idx).to_string(index=False)\n",
        "  cv_info += \"\\nOptimal yield loss weight: \" + str(best_loss_weight)\n",
        "  cv_info += \"\\nOptimal lr: \" + str(best_lr)\n",
        "  cv_info += \"\\nOptimal weight decay lambda: \" + str(best_lambda)\n",
        "\n",
        "  log_fh.write(cv_info)\n",
        "  if (debug_level > 1):\n",
        "    print(cv_info)\n",
        "\n",
        "  best_params = {\n",
        "      'loss_split' : best_loss_weight,\n",
        "      'lr' : best_lr,\n",
        "      'weight_decay' : best_lambda\n",
        "  }\n",
        "\n",
        "  return best_params\n",
        "\n",
        "if (test_env == 'notebook'):\n",
        "  best_params = optimizeHyperparameters(cyp_config, cv_datasets, log_fh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StPq7GJr2z0e"
      },
      "source": [
        "### Refit using Optimal Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoisXzMJ22TG"
      },
      "outputs": [],
      "source": [
        "def evaluateOptimalHyperparameters(cyp_config,\n",
        "                                   combined_dfs,\n",
        "                                   country_years,\n",
        "                                   best_params,\n",
        "                                   is_validation=False,\n",
        "                                   early_stopping=False,\n",
        "                                   visualize=False):\n",
        "  \"\"\"\n",
        "  1. Evaluate on validation data: num_valid_years = 5\n",
        "  2. Evaluate on test data: num_valid_years = 0.\n",
        "  \"\"\"\n",
        "  loss = nn.MSELoss()\n",
        "  num_epochs = best_params['num_epochs']\n",
        "  architecture = cyp_config['architecture']\n",
        "\n",
        "  test_fraction = cyp_config['test_fraction']\n",
        "  use_yield_trend = cyp_config['use_yield_trend']\n",
        "  early_season_end = cyp_config['early_season_end_dekad']\n",
        "  print_debug = cyp_config['debug_level'] > 1\n",
        "  num_valid_years = 5 if (is_validation) else 0\n",
        "  country = None\n",
        "  countries = cyp_config['countries']\n",
        "  if (len(countries) == 1):\n",
        "    country = countries[0]\n",
        "\n",
        "  scaler_args = {}\n",
        "  train_dataset = CYPMLDataset(combined_dfs, country_years,\n",
        "                               yield_trend=use_yield_trend,\n",
        "                               early_season_end=early_season_end,\n",
        "                               is_train=True,\n",
        "                               test_fraction=test_fraction,\n",
        "                               num_folds=1, fold_iter=0,\n",
        "                               num_valid_years=num_valid_years,\n",
        "                               scaler_args=scaler_args,\n",
        "                               print_debug=print_debug,\n",
        "                               log_fh=log_fh)\n",
        "\n",
        "  test_dataset = CYPMLDataset(combined_dfs, country_years,\n",
        "                              early_season_end=early_season_end,\n",
        "                              is_train=False, is_validation=is_validation,\n",
        "                              test_fraction=test_fraction,\n",
        "                              num_folds=1, fold_iter=0,\n",
        "                              num_valid_years=num_valid_years,\n",
        "                              scaler_args=scaler_args,\n",
        "                              print_debug=print_debug,\n",
        "                              log_fh=log_fh)\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1,\n",
        "                                            shuffle=False, num_workers=2)\n",
        "\n",
        "  y_max = 1.0 # torch.max(train_dataset[0][3]).item()/2\n",
        "  # train_dataset is : X_ts, X_rest, X_trend.\n",
        "  # X_rest shape is (num_regions, variables). Skip TOTAL_AREA\n",
        "  num_other_features = train_dataset[0][1].shape[1] - 1\n",
        "\n",
        "  # X_trend shape is (trend_window)\n",
        "  num_trend_features = train_dataset[0][2].shape[0]\n",
        "\n",
        "  # time series (dekadal) data is [num_regions, num_dekads, num_indicators]\n",
        "  num_ts_indicators = train_dataset[0][0].shape[2]\n",
        "  ts_seq_len = train_dataset[0][0].shape[1]\n",
        "\n",
        "  device = d2l.try_gpu()\n",
        "  if (architecture == '1DCNN'):\n",
        "    net = CYP1DCNNModel(num_ts_indicators,\n",
        "                        num_trend_features,\n",
        "                        num_other_features,\n",
        "                        ts_seq_len=ts_seq_len,\n",
        "                        num_outputs=2)\n",
        "  elif (architecture == 'LSTM'):\n",
        "    net = CYPLSTMModel(num_ts_indicators,\n",
        "                       num_trend_features,\n",
        "                       num_other_features,\n",
        "                       ts_seq_len=ts_seq_len,\n",
        "                       num_outputs=2)\n",
        "  else:\n",
        "    raise ValueError('Unsupported architecture:', architecture, 'Supported: 1DCNN, LSTM')\n",
        "\n",
        "  net = net.to(device)\n",
        "  trainer = torch.optim.Adam(net.parameters(), lr=best_params['lr'],\n",
        "                             weight_decay=best_params['weight_decay'])\n",
        "\n",
        "  test_nrmse, epochs_run = train(net, train_dataset, train_loader, test_loader,\n",
        "                                 loss, best_params['loss_split'], trainer, num_epochs,\n",
        "                                 early_stopping=early_stopping, device=device,\n",
        "                                 country=country, visualize=visualize, ymax=y_max)\n",
        "\n",
        "  if (early_stopping):\n",
        "    best_params['num_epochs'] = epochs_run\n",
        "\n",
        "  print('NRMSE:', round(test_nrmse, 4))\n",
        "  preds_l, preds_h, test_nrmse = evaluatePredictions(net, test_loader, device)\n",
        "\n",
        "  return net, preds_l, preds_h, test_nrmse\n",
        "\n",
        "if (test_env == 'notebook'):\n",
        "  if (label_spatial_level == 'NUTS2'):\n",
        "    crop_id = cropNameToID(crop_id_dict, crop)\n",
        "    yield_nuts3_file = 'YIELD_NUTS3.csv'\n",
        "    if (country_code is not None):\n",
        "      yield_nuts3_file = 'YIELD_NUTS3_' + country_code + '.csv'\n",
        "\n",
        "    data_path = cyp_config['data_path']\n",
        "    nuts3_yield_df = spark.read.csv(data_path + '/' + yield_nuts3_file,\n",
        "                                    header=True, inferSchema=True)\n",
        "    cyp_preprocessor = CYPDataPreprocessor(spark)\n",
        "    nuts3_yield_df = cyp_preprocessor.preprocessLabels(nuts3_yield_df, 'NUTS3_ID', crop_id)\n",
        "    nuts3_yield_df = nuts3_yield_df.toPandas().astype({ 'FYEAR' : str })\n",
        "    input_id_df = combined_dfs['INPUT_NUMERIC_IDS']\n",
        "    nuts3_yield_df = nuts3_yield_df.merge(input_id_df, on=['NUTS3_ID'])\n",
        "    nuts3_yield_df = nuts3_yield_df.astype({ 'FYEAR' : 'float32' })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW2dzOr98vu-"
      },
      "source": [
        "### Evaluate on validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "otD0vpdI8zW3",
        "outputId": "d6262862-7317-4faf-e645-8f770338f43c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NRMSE: 0.124\n",
            "Validation NRMSE: 12.4\n",
            "\n",
            " soft wheat DE\n",
            "Level y NRMSE: 11.344664200384635\n",
            "  COUNTRY NUTS2_ID   FYEAR  YIELD  YIELD_PRED  CROP_AREA  CROP_AREA_PRED\n",
            "0      DE     DE11  2008.0   7.32    7.577339    94465.0    76370.210938\n",
            "1      DE     DE11  2011.0   6.75    7.724831    94730.0    66652.992188\n",
            "2      DE     DE11  2012.0   6.24    7.636168    82819.0    74239.070312\n",
            "3      DE     DE11  2009.0   7.46    7.866321    96197.0    73294.351562\n",
            "4      DE     DE11  2010.0   6.86    7.770512    99324.0    81229.906250\n",
            "5      DE     DE12  2008.0   6.97    6.794579    41153.0    54134.910156\n",
            "6      DE     DE12  2012.0   6.12    6.933254    36079.0    50410.937500\n",
            "7      DE     DE12  2010.0   6.38    7.019499    42844.0    54999.476562\n",
            "8      DE     DE12  2009.0   6.90    7.047467    41907.0    50166.253906\n",
            "9      DE     DE12  2011.0   6.17    6.966794    41267.0    44678.511719\n",
            "\n",
            "Level x NRMSE: 18.09511254834656\n",
            "  COUNTRY NUTS2_ID NUTS3_ID   FYEAR  YIELD  YIELD_PRED\n",
            "0      DE     DE11    DE11D  2008.0   7.13    7.569871\n",
            "1      DE     DE11    DE11D  2011.0   7.16    7.708369\n",
            "2      DE     DE11    DE11D  2012.0   7.70    7.607800\n",
            "3      DE     DE11    DE11D  2009.0   7.39    7.841187\n",
            "4      DE     DE11    DE11D  2010.0   6.65    7.764533\n",
            "5      DE     DE11    DE11B  2008.0   7.27    7.520486\n",
            "6      DE     DE11    DE11B  2011.0   5.48    7.716370\n",
            "7      DE     DE11    DE11B  2012.0   4.91    7.622372\n",
            "8      DE     DE11    DE11B  2009.0   7.52    7.857917\n",
            "9      DE     DE11    DE11B  2010.0   6.66    7.814139\n",
            "\n",
            " soft wheat ES\n",
            "Level y NRMSE: 21.001526725151123\n",
            "    COUNTRY NUTS2_ID   FYEAR  YIELD  YIELD_PRED  CROP_AREA  CROP_AREA_PRED\n",
            "150      ES     ES13  2010.0   2.45    2.870918      462.0     6738.349609\n",
            "151      ES     ES13  2007.0   2.67    3.446425      462.0     9286.646484\n",
            "152      ES     ES13  2009.0   2.44    3.541777      346.0    15242.374023\n",
            "153      ES     ES13  2011.0   2.46    3.068988      560.0     8777.344727\n",
            "154      ES     ES13  2008.0   2.56    3.734942      935.0    22125.054688\n",
            "155      ES     ES21  2007.0   5.00    5.385686    24714.0    23314.347656\n",
            "156      ES     ES21  2010.0   5.94    5.150084    26040.0    27147.437500\n",
            "157      ES     ES21  2011.0   6.19    5.352249    25137.0    30459.429688\n",
            "158      ES     ES21  2008.0   4.60    5.483040    26580.0    29227.371094\n",
            "159      ES     ES21  2009.0   4.80    5.223264    15352.0    26756.265625\n",
            "\n",
            "Level x NRMSE: 26.869627144505184\n",
            "    COUNTRY NUTS2_ID NUTS3_ID   FYEAR  YIELD  YIELD_PRED\n",
            "709      ES     ES13    ES130  2010.0   2.45    2.870918\n",
            "710      ES     ES13    ES130  2007.0   2.67    3.446425\n",
            "711      ES     ES13    ES130  2009.0   2.44    3.541777\n",
            "712      ES     ES13    ES130  2011.0   2.46    3.068988\n",
            "713      ES     ES13    ES130  2008.0   2.56    3.734942\n",
            "714      ES     ES21    ES211  2007.0   5.00    5.385686\n",
            "715      ES     ES21    ES211  2010.0   5.95    5.150084\n",
            "716      ES     ES21    ES211  2011.0   6.20    5.352249\n",
            "717      ES     ES21    ES211  2008.0   4.60    5.483040\n",
            "718      ES     ES21    ES211  2009.0   4.80    5.223264\n",
            "\n",
            " soft wheat FR\n",
            "Level y NRMSE: 8.953122088891035\n",
            "    COUNTRY NUTS2_ID   FYEAR  YIELD  YIELD_PRED  CROP_AREA  CROP_AREA_PRED\n",
            "215      FR     FR10  2008.0   8.22    7.745522   242049.0   240035.031250\n",
            "216      FR     FR10  2007.0   7.72    7.851159   239091.0   219567.921875\n",
            "217      FR     FR10  2010.0   8.11    7.933844   230658.0   252511.187500\n",
            "218      FR     FR10  2006.0   7.39    7.847623   242580.0   248771.281250\n",
            "219      FR     FR10  2009.0   8.74    8.067863   234548.0   253125.203125\n",
            "220      FR     FRB0  2006.0   6.52    7.416641   688784.0   778032.812500\n",
            "221      FR     FRB0  2010.0   6.86    7.306544   667350.0   770575.562500\n",
            "222      FR     FRB0  2008.0   6.95    7.275434   710295.0   745869.000000\n",
            "223      FR     FRB0  2007.0   6.43    7.443842   691087.0   670532.062500\n",
            "224      FR     FRB0  2009.0   7.27    7.477880   665074.0   778888.750000\n",
            "\n",
            "Level x NRMSE: 12.866261040314631\n",
            "    COUNTRY NUTS2_ID NUTS3_ID   FYEAR  YIELD  YIELD_PRED\n",
            "909      FR     FR10    FR108  2008.0    8.3    7.709213\n",
            "910      FR     FR10    FR108  2007.0    7.6    7.740978\n",
            "911      FR     FR10    FR108  2010.0    8.5    7.874767\n",
            "912      FR     FR10    FR108  2006.0    7.5    7.888688\n",
            "913      FR     FR10    FR108  2009.0    8.9    8.025687\n",
            "914      FR     FR10    FR104  2008.0    8.3    7.804151\n",
            "915      FR     FR10    FR104  2007.0    7.6    7.927691\n",
            "916      FR     FR10    FR104  2010.0    7.6    7.838764\n",
            "917      FR     FR10    FR104  2006.0    6.6    7.860173\n",
            "918      FR     FR10    FR104  2009.0    8.3    8.009895\n",
            "\n",
            " soft wheat IT\n",
            "Level y NRMSE: 16.94781782202434\n",
            "    COUNTRY NUTS2_ID   FYEAR  YIELD  YIELD_PRED  CROP_AREA  CROP_AREA_PRED\n",
            "320      IT     ITC1  2009.0   4.50    5.107136    92105.0    71037.578125\n",
            "321      IT     ITC1  2011.0   4.81    4.894286    91073.0    63036.386719\n",
            "322      IT     ITC1  2010.0   5.29    5.223867    86515.0    95823.460938\n",
            "323      IT     ITC1  2008.0   4.55    5.199035   102711.0    98031.609375\n",
            "324      IT     ITC1  2007.0   5.13    5.109105    94458.0    97939.039062\n",
            "325      IT     ITC4  2008.0   6.02    6.132195    80908.0    61843.121094\n",
            "326      IT     ITC4  2011.0   5.08    5.765679    45050.0    43972.953125\n",
            "327      IT     ITC4  2010.0   5.84    6.237675    58015.0    64400.832031\n",
            "328      IT     ITC4  2009.0   5.58    6.071632    65715.0    55934.324219\n",
            "329      IT     ITC4  2007.0   5.27    6.094850    73672.0    58383.562500\n",
            "\n",
            "Level x NRMSE: 21.922830625015543\n",
            "     COUNTRY NUTS2_ID NUTS3_ID   FYEAR  YIELD  YIELD_PRED\n",
            "1349      IT     ITC1    ITC18  2009.0   5.07    4.987248\n",
            "1350      IT     ITC1    ITC18  2011.0   4.92    4.695778\n",
            "1351      IT     ITC1    ITC18  2010.0   5.32    5.167531\n",
            "1352      IT     ITC1    ITC18  2008.0   4.68    4.958117\n",
            "1353      IT     ITC1    ITC18  2007.0   4.66    4.920166\n",
            "1354      IT     ITC1    ITC17  2009.0   3.00    5.155692\n",
            "1355      IT     ITC1    ITC17  2011.0   4.18    4.809751\n",
            "1356      IT     ITC1    ITC17  2010.0   5.22    5.259769\n",
            "1357      IT     ITC1    ITC17  2008.0   3.00    5.050467\n",
            "1358      IT     ITC1    ITC17  2007.0   4.73    5.101955\n"
          ]
        },
        {
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"324.986562pt\" version=\"1.1\" viewBox=\"0 0 346.55 324.986562\" width=\"346.55pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 324.986562 \nL 346.55 324.986562 \nL 346.55 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 48.8975 283.559062 \nL 327.8975 283.559062 \nL 327.8975 11.759062 \nL 48.8975 11.759062 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#pbb04fbd31f)\" d=\"M 102.442955 283.559062 \nL 102.442955 11.759062 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m72782dc762\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"102.442955\" xlink:href=\"#m72782dc762\" y=\"283.559062\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(94.807955 299.677187)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#pbb04fbd31f)\" d=\"M 158.806591 283.559062 \nL 158.806591 11.759062 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"158.806591\" xlink:href=\"#m72782dc762\" y=\"283.559062\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(151.171591 299.677187)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#pbb04fbd31f)\" d=\"M 215.170227 283.559062 \nL 215.170227 11.759062 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"215.170227\" xlink:href=\"#m72782dc762\" y=\"283.559062\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(207.535227 299.677187)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#pbb04fbd31f)\" d=\"M 271.533864 283.559062 \nL 271.533864 11.759062 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"271.533864\" xlink:href=\"#m72782dc762\" y=\"283.559062\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(263.898864 299.677187)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#pbb04fbd31f)\" d=\"M 327.8975 283.559062 \nL 327.8975 11.759062 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"327.8975\" xlink:href=\"#m72782dc762\" y=\"283.559062\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(316.445 299.677187)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- epoch -->\n     <defs>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n     </defs>\n     <g transform=\"translate(170.12375 315.290937)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#pbb04fbd31f)\" d=\"M 48.8975 283.559062 \nL 327.8975 283.559062 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mf0b027c36b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.8975\" xlink:href=\"#mf0b027c36b\" y=\"283.559062\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.0 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(22.81375 288.118125)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#pbb04fbd31f)\" d=\"M 48.8975 229.199062 \nL 327.8975 229.199062 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.8975\" xlink:href=\"#mf0b027c36b\" y=\"229.199062\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.2 -->\n      <g transform=\"translate(22.81375 233.758125)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#pbb04fbd31f)\" d=\"M 48.8975 174.839062 \nL 327.8975 174.839062 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.8975\" xlink:href=\"#mf0b027c36b\" y=\"174.839062\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.4 -->\n      <g transform=\"translate(22.81375 179.398125)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#pbb04fbd31f)\" d=\"M 48.8975 120.479062 \nL 327.8975 120.479062 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.8975\" xlink:href=\"#mf0b027c36b\" y=\"120.479062\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.6 -->\n      <g transform=\"translate(22.81375 125.038125)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#pbb04fbd31f)\" d=\"M 48.8975 66.119062 \nL 327.8975 66.119062 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.8975\" xlink:href=\"#mf0b027c36b\" y=\"66.119062\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.8 -->\n      <g transform=\"translate(22.81375 70.678125)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#pbb04fbd31f)\" d=\"M 48.8975 11.759062 \nL 327.8975 11.759062 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.8975\" xlink:href=\"#mf0b027c36b\" y=\"11.759062\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1.0 -->\n      <g transform=\"translate(22.81375 16.318125)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_13\">\n     <!-- loss -->\n     <defs>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(16.318125 159.248437)rotate(-90)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_23\">\n    <path clip-path=\"url(#pbb04fbd31f)\" d=\"M 48.8975 168.424547 \nL 51.715682 244.70808 \nL 54.533864 252.519057 \nL 57.352045 255.422466 \nL 60.170227 257.144147 \nL 62.988409 257.725223 \nL 65.806591 257.92373 \nL 68.624773 258.365631 \nL 71.442955 258.754271 \nL 74.261136 259.211763 \nL 77.079318 259.318332 \nL 79.8975 259.481142 \nL 82.715682 259.604619 \nL 85.533864 259.907968 \nL 88.352045 259.861669 \nL 91.170227 260.125697 \nL 93.988409 259.799907 \nL 96.806591 260.104651 \nL 99.624773 260.147117 \nL 102.442955 260.770279 \nL 105.261136 260.55254 \nL 108.079318 260.477154 \nL 110.8975 260.844172 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_24\">\n    <path clip-path=\"url(#pbb04fbd31f)\" d=\"M 48.8975 44.444352 \nL 51.715682 184.79239 \nL 54.533864 191.056532 \nL 57.352045 197.927501 \nL 60.170227 199.240429 \nL 62.988409 205.583224 \nL 65.806591 210.44391 \nL 68.624773 208.720959 \nL 71.442955 210.398542 \nL 74.261136 211.739318 \nL 77.079318 215.400021 \nL 79.8975 212.908535 \nL 82.715682 213.930236 \nL 85.533864 213.663153 \nL 88.352045 215.576146 \nL 91.170227 218.580444 \nL 93.988409 217.599914 \nL 96.806591 218.133228 \nL 99.624773 220.269836 \nL 102.442955 219.408067 \nL 105.261136 221.437758 \nL 108.079318 219.829076 \nL 110.8975 222.910584 \n\" style=\"fill:none;stroke:#bf00bf;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_25\">\n    <path clip-path=\"url(#pbb04fbd31f)\" d=\"M 48.8975 134.988067 \nL 51.715682 231.532649 \nL 54.533864 242.633795 \nL 57.352045 246.547263 \nL 60.170227 249.135887 \nL 62.988409 249.938496 \nL 65.806591 249.463733 \nL 68.624773 249.88408 \nL 71.442955 249.976073 \nL 74.261136 250.211572 \nL 77.079318 249.963718 \nL 79.8975 249.905963 \nL 82.715682 250.423984 \nL 85.533864 250.700707 \nL 88.352045 250.650062 \nL 91.170227 250.985002 \nL 93.988409 250.771161 \nL 96.806591 250.853251 \nL 99.624773 250.817693 \nL 102.442955 251.415953 \nL 105.261136 251.389356 \nL 108.079318 251.344644 \nL 110.8975 251.590901 \n\" style=\"fill:none;stroke:#008000;stroke-dasharray:9.6,2.4,1.5,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_26\">\n    <path clip-path=\"url(#pbb04fbd31f)\" d=\"M 48.8975 228.868259 \nL 51.715682 241.774644 \nL 54.533864 246.450003 \nL 57.352045 248.146682 \nL 60.170227 248.245359 \nL 62.988409 246.612956 \nL 65.806591 249.199559 \nL 68.624773 247.905435 \nL 71.442955 249.90716 \nL 74.261136 249.265025 \nL 77.079318 249.861857 \nL 79.8975 249.859822 \nL 82.715682 250.05679 \nL 85.533864 249.215561 \nL 88.352045 250.648768 \nL 91.170227 249.518685 \nL 93.988409 250.308929 \nL 96.806591 250.031333 \nL 99.624773 250.185014 \nL 102.442955 249.277941 \nL 105.261136 250.438613 \nL 108.079318 250.53921 \nL 110.8975 250.104596 \n\" style=\"fill:none;stroke:#ff0000;stroke-dasharray:1.5,2.475;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 48.8975 283.559062 \nL 48.8975 11.759062 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 327.8975 283.559062 \nL 327.8975 11.759062 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 48.8975 283.559062 \nL 327.8975 283.559062 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 48.8975 11.759062 \nL 327.8975 11.759062 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 197.315 91.814062 \nL 319.4975 91.814062 \nQ 321.8975 91.814062 321.8975 89.414062 \nL 321.8975 20.159062 \nQ 321.8975 17.759062 319.4975 17.759062 \nL 197.315 17.759062 \nQ 194.915 17.759062 194.915 20.159062 \nL 194.915 89.414062 \nQ 194.915 91.814062 197.315 91.814062 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_27\">\n     <path d=\"M 199.715 27.477187 \nL 223.715 27.477187 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_28\"/>\n    <g id=\"text_14\">\n     <!-- yield loss -->\n     <defs>\n      <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path id=\"DejaVuSans-32\"/>\n     </defs>\n     <g transform=\"translate(233.315 31.677187)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-121\"/>\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"86.962891\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"148.486328\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"176.269531\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"239.746094\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"271.533203\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"299.316406\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"360.498047\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"412.597656\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n    <g id=\"line2d_29\">\n     <path d=\"M 199.715 45.090937 \nL 223.715 45.090937 \n\" style=\"fill:none;stroke:#bf00bf;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_30\"/>\n    <g id=\"text_15\">\n     <!-- crop area loss -->\n     <defs>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n     </defs>\n     <g transform=\"translate(233.315 49.290937)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"54.980469\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"93.84375\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"155.025391\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"218.501953\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"250.289062\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"311.568359\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"350.431641\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"411.955078\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"473.234375\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"505.021484\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"532.804688\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"593.986328\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"646.085938\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n    <g id=\"line2d_31\">\n     <path d=\"M 199.715 62.704687 \nL 223.715 62.704687 \n\" style=\"fill:none;stroke:#008000;stroke-dasharray:9.6,2.4,1.5,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_32\"/>\n    <g id=\"text_16\">\n     <!-- train error -->\n     <defs>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     </defs>\n     <g transform=\"translate(233.315 66.904687)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"232.763672\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"264.550781\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"326.074219\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"365.4375\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"404.300781\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"465.482422\" xlink:href=\"#DejaVuSans-114\"/>\n     </g>\n    </g>\n    <g id=\"line2d_33\">\n     <path d=\"M 199.715 80.318437 \nL 223.715 80.318437 \n\" style=\"fill:none;stroke:#ff0000;stroke-dasharray:1.5,2.475;stroke-dashoffset:0;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_34\"/>\n    <g id=\"text_17\">\n     <!-- test error -->\n     <g transform=\"translate(233.315 84.518437)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"192.041016\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"223.828125\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"285.351562\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"324.714844\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"363.578125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"424.759766\" xlink:href=\"#DejaVuSans-114\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pbb04fbd31f\">\n   <rect height=\"271.8\" width=\"279\" x=\"48.8975\" y=\"11.759062\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if (test_env == 'notebook'):\n",
        "  best_params['num_epochs'] = 100\n",
        "  y_num_id_df = combined_dfs['LABEL_NUMERIC_IDS']\n",
        "  x_num_id_df = combined_dfs['INPUT_NUMERIC_IDS']\n",
        "  net, preds_l, preds_h, valid_nrmse = evaluateOptimalHyperparameters(cyp_config,\n",
        "                                                                      combined_dfs,\n",
        "                                                                      country_years,\n",
        "                                                                      best_params,\n",
        "                                                                      is_validation=True,\n",
        "                                                                      early_stopping=True,\n",
        "                                                                      visualize=True)\n",
        "  print('Validation NRMSE:', round(100 * valid_nrmse, 2))\n",
        "  low_res_pred_cols =[\"id0\", \"id_y\", \"FYEAR\", \"YIELD\", \"YIELD_PRED\", \"CROP_AREA\", \"CROP_AREA_PRED\"]\n",
        "  low_res_pred_df = pd.DataFrame(data=preds_l, columns=low_res_pred_cols)\n",
        "\n",
        "  low_res_pred_df = low_res_pred_df.merge(y_num_id_df, on=['id0', 'id_y']).drop(columns=['id0', 'id_y'])\n",
        "  low_res_pred_cols =[\"COUNTRY\", low_res_id_col, \"FYEAR\", \"YIELD\", \"YIELD_PRED\", \"CROP_AREA\", \"CROP_AREA_PRED\"]\n",
        "  low_res_pred_df = low_res_pred_df[low_res_pred_cols]\n",
        "\n",
        "  high_res_pred_df = pd.DataFrame(data=preds_h, columns=[\"id_x\", \"FYEAR\", \"YIELD_PRED\"])\n",
        "  high_res_pred_df = high_res_pred_df.merge(x_num_id_df, on=['id_x'])\n",
        "  if (label_spatial_level == 'NUTS2'):\n",
        "    yield_sel_cols = ['id_x', 'FYEAR', 'YIELD']\n",
        "    high_res_pred_df = high_res_pred_df.merge(nuts3_yield_df[yield_sel_cols], on=['id_x', \"FYEAR\"])\n",
        "    high_res_pred_cols =[\"COUNTRY\", low_res_id_col, high_res_id_col, \"FYEAR\", \"YIELD\", \"YIELD_PRED\"]\n",
        "  else:\n",
        "    high_res_pred_cols =[\"COUNTRY\", low_res_id_col, high_res_id_col, \"FYEAR\", \"YIELD_PRED\"]\n",
        "\n",
        "  high_res_pred_df = high_res_pred_df[high_res_pred_cols]\n",
        "  countries = low_res_pred_df['COUNTRY'].unique()\n",
        "  valid_info = '\\nValidation Set NRMSEs'\n",
        "  valid_info += '\\n---------------------'\n",
        "  for cn in countries:\n",
        "    print('\\n', crop, cn)\n",
        "    valid_info += '\\n' + crop + '(' + cn + ')'\n",
        "    cn_low_res_df = low_res_pred_df[low_res_pred_df['COUNTRY'] == cn]\n",
        "    cn_high_res_df = high_res_pred_df[high_res_pred_df['COUNTRY'] == cn]\n",
        "    lres_nrmse = NormalizedRMSE(cn_low_res_df['YIELD'].values,\n",
        "                                cn_low_res_df['YIELD_PRED'].values)\n",
        "    print('Level y NRMSE:', lres_nrmse)\n",
        "    valid_info += '\\nLevel y NRMSE:' + str(lres_nrmse)\n",
        "    print(cn_low_res_df.head(10))\n",
        "    if (label_spatial_level == 'NUTS2'):\n",
        "      hres_nrmse = NormalizedRMSE(cn_high_res_df['YIELD'].values,\n",
        "                                  cn_high_res_df['YIELD_PRED'].values)\n",
        "      print('\\nLevel x NRMSE:', hres_nrmse)\n",
        "      valid_info += '\\nLevel x NRMSE:' + str(hres_nrmse)\n",
        "      print(cn_high_res_df.head(10))\n",
        "\n",
        "  log_fh.write(valid_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X31745bT9mqN"
      },
      "source": [
        "### Evaluate on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BOwA0Nzo9mqX",
        "outputId": "b98ddade-6cd4-4916-d4d1-3496fc86c8f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NRMSE: 0.1396\n",
            "Test NRMSE: 13.96\n",
            "\n",
            " soft wheat DE\n",
            "Level y NRMSE: 11.281173619014545\n",
            "  COUNTRY NUTS2_ID   FYEAR  YIELD  YIELD_PRED  CROP_AREA  CROP_AREA_PRED\n",
            "0      DE     DE11  2017.0   7.60    7.576547    87077.0    83729.328125\n",
            "1      DE     DE11  2016.0   6.74    7.423351    92208.0    80908.765625\n",
            "2      DE     DE11  2013.0   7.56    7.112234    95312.0    78159.281250\n",
            "3      DE     DE11  2014.0   8.29    7.117731    93025.0    67796.929688\n",
            "4      DE     DE11  2015.0   7.73    7.197464    95852.0    76455.500000\n",
            "5      DE     DE12  2016.0   6.17    6.729160    40170.0    54230.273438\n",
            "6      DE     DE12  2017.0   6.91    6.834273    37933.0    56013.847656\n",
            "7      DE     DE12  2014.0   7.32    6.530761    40525.0    45798.085938\n",
            "8      DE     DE12  2015.0   6.90    6.496878    41759.0    50462.500000\n",
            "9      DE     DE12  2013.0   6.72    6.584461    41521.0    56269.621094\n",
            "\n",
            "Level x NRMSE: 15.798565586884177\n",
            "  COUNTRY NUTS2_ID NUTS3_ID   FYEAR  YIELD  YIELD_PRED\n",
            "0      DE     DE11    DE11D  2017.0   8.44    7.558301\n",
            "1      DE     DE11    DE11D  2016.0   6.76    7.404947\n",
            "2      DE     DE11    DE11D  2013.0   8.01    7.074072\n",
            "3      DE     DE11    DE11D  2014.0   8.71    7.084116\n",
            "4      DE     DE11    DE11D  2015.0   8.52    7.154092\n",
            "5      DE     DE11    DE11B  2017.0   7.29    7.579554\n",
            "6      DE     DE11    DE11B  2016.0   6.69    7.436555\n",
            "7      DE     DE11    DE11B  2013.0   7.19    7.075249\n",
            "8      DE     DE11    DE11B  2014.0   8.00    7.114018\n",
            "9      DE     DE11    DE11B  2015.0   6.48    7.198360\n",
            "\n",
            " soft wheat ES\n",
            "Level y NRMSE: 26.2284452325617\n",
            "    COUNTRY NUTS2_ID   FYEAR  YIELD  YIELD_PRED  CROP_AREA  CROP_AREA_PRED\n",
            "151      ES     ES13  2015.0   2.45    2.244079      679.0     3952.942627\n",
            "152      ES     ES13  2013.0   2.45    2.373523      527.0     4044.742676\n",
            "153      ES     ES13  2016.0   2.45    2.191154      775.0     3547.521973\n",
            "154      ES     ES13  2012.0   1.24    2.579673      900.0     4241.998535\n",
            "155      ES     ES13  2014.0   2.45    2.257704      178.0     3732.481201\n",
            "156      ES     ES21  2014.0   5.00    5.815104    24705.0    26025.773438\n",
            "157      ES     ES21  2013.0   5.00    5.605107    24460.0    25684.117188\n",
            "158      ES     ES21  2016.0   6.44    5.634107    25007.0    24995.050781\n",
            "159      ES     ES21  2012.0   6.19    5.466123    25820.0    28069.033203\n",
            "160      ES     ES21  2015.0   5.19    5.727003    23368.0    24514.945312\n",
            "\n",
            "Level x NRMSE: 38.49389719440471\n",
            "    COUNTRY NUTS2_ID NUTS3_ID   FYEAR  YIELD  YIELD_PRED\n",
            "700      ES     ES13    ES130  2015.0   2.45    2.244079\n",
            "701      ES     ES13    ES130  2013.0   2.45    2.373523\n",
            "702      ES     ES13    ES130  2016.0   2.45    2.191154\n",
            "703      ES     ES13    ES130  2012.0   1.24    2.579673\n",
            "704      ES     ES13    ES130  2014.0   2.45    2.257704\n",
            "705      ES     ES21    ES211  2014.0   5.00    5.815104\n",
            "706      ES     ES21    ES211  2013.0   5.00    5.605107\n",
            "707      ES     ES21    ES211  2016.0   6.45    5.634107\n",
            "708      ES     ES21    ES211  2012.0   6.20    5.466123\n",
            "709      ES     ES21    ES211  2015.0   5.20    5.727003\n",
            "\n",
            " soft wheat FR\n",
            "Level y NRMSE: 12.810549111650666\n",
            "    COUNTRY NUTS2_ID   FYEAR  YIELD  YIELD_PRED  CROP_AREA  CROP_AREA_PRED\n",
            "216      FR     FR10  2016.0   4.34    8.243663   243160.0   238404.796875\n",
            "217      FR     FR10  2014.0   8.63    8.209946   239230.0   228725.953125\n",
            "218      FR     FR10  2015.0   8.80    8.101934   239790.0   239271.406250\n",
            "219      FR     FR10  2018.0   7.65    7.533105   220430.0   194267.890625\n",
            "220      FR     FR10  2017.0   8.00    7.845220   228570.0   221977.937500\n",
            "221      FR     FR10  2013.0   8.38    8.124542   237289.0   244553.500000\n",
            "222      FR     FR10  2012.0   8.13    8.124290   236553.0   232539.546875\n",
            "223      FR     FR10  2011.0   7.58    8.039555   242149.0   244319.406250\n",
            "224      FR     FRB0  2016.0   4.44    7.416522   682965.0   743911.562500\n",
            "225      FR     FRB0  2012.0   7.36    7.099205   677500.0   712353.375000\n",
            "\n",
            "Level x NRMSE: 15.862897022127303\n",
            "    COUNTRY NUTS2_ID NUTS3_ID   FYEAR  YIELD  YIELD_PRED\n",
            "900      FR     FR10    FR108  2016.0    5.0    8.191694\n",
            "901      FR     FR10    FR108  2014.0    8.3    8.183680\n",
            "902      FR     FR10    FR108  2015.0    9.1    8.117490\n",
            "903      FR     FR10    FR108  2018.0    7.7    7.527332\n",
            "904      FR     FR10    FR108  2017.0    8.2    7.807727\n",
            "905      FR     FR10    FR108  2013.0    8.5    8.153315\n",
            "906      FR     FR10    FR108  2012.0    8.2    8.084604\n",
            "907      FR     FR10    FR108  2011.0    7.4    8.024315\n",
            "908      FR     FR10    FR104  2016.0    4.5    8.261294\n",
            "909      FR     FR10    FR104  2014.0    8.5    8.228765\n",
            "\n",
            " soft wheat IT\n",
            "Level y NRMSE: 17.32692307914476\n",
            "    COUNTRY NUTS2_ID   FYEAR  YIELD  YIELD_PRED  CROP_AREA  CROP_AREA_PRED\n",
            "384      IT     ITC1  2014.0   5.66    5.352757    84632.0   101301.445312\n",
            "385      IT     ITC1  2017.0   4.85    5.595927    82156.0   103578.515625\n",
            "386      IT     ITC1  2015.0   4.96    5.329468    81826.0    92092.390625\n",
            "387      IT     ITC1  2018.0   4.40    5.514768    77580.0   116876.257812\n",
            "388      IT     ITC1  2013.0   5.26    5.140025    90810.0   113845.437500\n",
            "389      IT     ITC1  2016.0   6.07    5.457573    85805.0    98161.945312\n",
            "390      IT     ITC1  2012.0   5.70    5.000342    88982.0   100260.625000\n",
            "391      IT     ITC4  2018.0   5.46    5.842482    58761.0    73582.703125\n",
            "392      IT     ITC4  2017.0   6.41    5.768391    56556.0    68498.664062\n",
            "393      IT     ITC4  2016.0   6.12    5.659667    62027.0    61088.136719\n",
            "\n",
            "Level x NRMSE: 20.700435193082846\n",
            "     COUNTRY NUTS2_ID NUTS3_ID   FYEAR  YIELD  YIELD_PRED\n",
            "1604      IT     ITC1    ITC18  2014.0   5.80    5.279756\n",
            "1605      IT     ITC1    ITC18  2017.0   4.50    5.541934\n",
            "1606      IT     ITC1    ITC18  2015.0   4.00    5.264113\n",
            "1607      IT     ITC1    ITC18  2018.0   3.50    5.413559\n",
            "1608      IT     ITC1    ITC18  2013.0   5.39    5.065936\n",
            "1609      IT     ITC1    ITC18  2016.0   6.50    5.369278\n",
            "1610      IT     ITC1    ITC18  2012.0   5.19    4.890099\n",
            "1611      IT     ITC1    ITC17  2014.0   5.50    5.355519\n",
            "1612      IT     ITC1    ITC17  2017.0   3.50    5.593837\n",
            "1613      IT     ITC1    ITC17  2015.0   5.50    5.346605\n"
          ]
        },
        {
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"324.986562pt\" version=\"1.1\" viewBox=\"0 0 335.0975 324.986562\" width=\"335.0975pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 324.986562 \nL 335.0975 324.986562 \nL 335.0975 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 48.8975 283.559062 \nL 327.8975 283.559062 \nL 327.8975 11.759062 \nL 48.8975 11.759062 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p122cc3bc7d)\" d=\"M 102.040357 283.559062 \nL 102.040357 11.759062 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m3415b4f4d4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"102.040357\" xlink:href=\"#m3415b4f4d4\" y=\"283.559062\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(98.222857 299.677187)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p122cc3bc7d)\" d=\"M 168.468929 283.559062 \nL 168.468929 11.759062 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"168.468929\" xlink:href=\"#m3415b4f4d4\" y=\"283.559062\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(160.833929 299.677187)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p122cc3bc7d)\" d=\"M 234.8975 283.559062 \nL 234.8975 11.759062 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"234.8975\" xlink:href=\"#m3415b4f4d4\" y=\"283.559062\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 15 -->\n      <g transform=\"translate(227.2625 299.677187)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p122cc3bc7d)\" d=\"M 301.326071 283.559062 \nL 301.326071 11.759062 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"301.326071\" xlink:href=\"#m3415b4f4d4\" y=\"283.559062\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(293.691071 299.677187)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_5\">\n     <!-- epoch -->\n     <defs>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n     </defs>\n     <g transform=\"translate(170.12375 315.290937)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p122cc3bc7d)\" d=\"M 48.8975 283.559062 \nL 327.8975 283.559062 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m751537a606\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.8975\" xlink:href=\"#m751537a606\" y=\"283.559062\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0.0 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(22.81375 288.118125)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p122cc3bc7d)\" d=\"M 48.8975 229.199062 \nL 327.8975 229.199062 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.8975\" xlink:href=\"#m751537a606\" y=\"229.199062\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.2 -->\n      <g transform=\"translate(22.81375 233.758125)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p122cc3bc7d)\" d=\"M 48.8975 174.839062 \nL 327.8975 174.839062 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.8975\" xlink:href=\"#m751537a606\" y=\"174.839062\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.4 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(22.81375 179.398125)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p122cc3bc7d)\" d=\"M 48.8975 120.479062 \nL 327.8975 120.479062 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.8975\" xlink:href=\"#m751537a606\" y=\"120.479062\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.6 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(22.81375 125.038125)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p122cc3bc7d)\" d=\"M 48.8975 66.119062 \nL 327.8975 66.119062 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.8975\" xlink:href=\"#m751537a606\" y=\"66.119062\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.8 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(22.81375 70.678125)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p122cc3bc7d)\" d=\"M 48.8975 11.759062 \nL 327.8975 11.759062 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.8975\" xlink:href=\"#m751537a606\" y=\"11.759062\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 1.0 -->\n      <g transform=\"translate(22.81375 16.318125)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_12\">\n     <!-- loss -->\n     <defs>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(16.318125 159.248437)rotate(-90)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_21\">\n    <path clip-path=\"url(#p122cc3bc7d)\" d=\"M 48.8975 201.40582 \nL 62.183214 254.413998 \nL 75.468929 256.28014 \nL 88.754643 257.880099 \nL 102.040357 258.771039 \nL 115.326071 259.241155 \nL 128.611786 259.246645 \nL 141.8975 259.421671 \nL 155.183214 259.436521 \nL 168.468929 259.822848 \nL 181.754643 260.125004 \nL 195.040357 260.363073 \nL 208.326071 260.220142 \nL 221.611786 259.672112 \nL 234.8975 260.30091 \nL 248.183214 260.71344 \nL 261.468929 260.269895 \nL 274.754643 260.729752 \nL 288.040357 260.722817 \nL 301.326071 260.748945 \nL 314.611786 260.74555 \nL 327.8975 260.708266 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_22\">\n    <path clip-path=\"url(#p122cc3bc7d)\" d=\"M 48.8975 89.915856 \nL 62.183214 192.054097 \nL 75.468929 204.847398 \nL 88.754643 204.359394 \nL 102.040357 210.763229 \nL 115.326071 212.642109 \nL 128.611786 213.700859 \nL 141.8975 215.276407 \nL 155.183214 212.271432 \nL 168.468929 214.296738 \nL 181.754643 216.225204 \nL 195.040357 215.991037 \nL 208.326071 215.984541 \nL 221.611786 214.071183 \nL 234.8975 217.634755 \nL 248.183214 217.203663 \nL 261.468929 216.990599 \nL 274.754643 217.58253 \nL 288.040357 220.399534 \nL 301.326071 219.301711 \nL 314.611786 217.57848 \nL 327.8975 216.705465 \n\" style=\"fill:none;stroke:#bf00bf;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_23\">\n    <path clip-path=\"url(#p122cc3bc7d)\" d=\"M 48.8975 162.865321 \nL 62.183214 244.079357 \nL 75.468929 246.605605 \nL 88.754643 249.341002 \nL 102.040357 250.19361 \nL 115.326071 250.632541 \nL 128.611786 250.269279 \nL 141.8975 250.489817 \nL 155.183214 249.718124 \nL 168.468929 250.389458 \nL 181.754643 250.820296 \nL 195.040357 251.379463 \nL 208.326071 251.276025 \nL 221.611786 248.641819 \nL 234.8975 251.245388 \nL 248.183214 251.825007 \nL 261.468929 251.08677 \nL 274.754643 251.700961 \nL 288.040357 251.853106 \nL 301.326071 251.831021 \nL 314.611786 251.888276 \nL 327.8975 251.605449 \n\" style=\"fill:none;stroke:#008000;stroke-dasharray:9.6,2.4,1.5,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_24\">\n    <path clip-path=\"url(#p122cc3bc7d)\" d=\"M 48.8975 236.897406 \nL 62.183214 240.954718 \nL 75.468929 244.454894 \nL 88.754643 244.808078 \nL 102.040357 244.618045 \nL 115.326071 244.523232 \nL 128.611786 245.032455 \nL 141.8975 244.522644 \nL 155.183214 243.791785 \nL 168.468929 245.375251 \nL 181.754643 245.674912 \nL 195.040357 245.483936 \nL 208.326071 240.003374 \nL 221.611786 245.257797 \nL 234.8975 245.440948 \nL 248.183214 238.894783 \nL 261.468929 245.895491 \nL 274.754643 245.8772 \nL 288.040357 245.792832 \nL 301.326071 245.452195 \nL 314.611786 245.907447 \nL 327.8975 245.626598 \n\" style=\"fill:none;stroke:#ff0000;stroke-dasharray:1.5,2.475;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 48.8975 283.559062 \nL 48.8975 11.759062 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 327.8975 283.559062 \nL 327.8975 11.759062 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 48.8975 283.559062 \nL 327.8975 283.559062 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 48.8975 11.759062 \nL 327.8975 11.759062 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 197.315 91.814062 \nL 319.4975 91.814062 \nQ 321.8975 91.814062 321.8975 89.414062 \nL 321.8975 20.159062 \nQ 321.8975 17.759062 319.4975 17.759062 \nL 197.315 17.759062 \nQ 194.915 17.759062 194.915 20.159062 \nL 194.915 89.414062 \nQ 194.915 91.814062 197.315 91.814062 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_25\">\n     <path d=\"M 199.715 27.477187 \nL 223.715 27.477187 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_26\"/>\n    <g id=\"text_13\">\n     <!-- yield loss -->\n     <defs>\n      <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path id=\"DejaVuSans-32\"/>\n     </defs>\n     <g transform=\"translate(233.315 31.677187)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-121\"/>\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"86.962891\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"148.486328\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"176.269531\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"239.746094\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"271.533203\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"299.316406\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"360.498047\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"412.597656\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n    <g id=\"line2d_27\">\n     <path d=\"M 199.715 45.090937 \nL 223.715 45.090937 \n\" style=\"fill:none;stroke:#bf00bf;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_28\"/>\n    <g id=\"text_14\">\n     <!-- crop area loss -->\n     <defs>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n     </defs>\n     <g transform=\"translate(233.315 49.290937)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"54.980469\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"93.84375\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"155.025391\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"218.501953\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"250.289062\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"311.568359\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"350.431641\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"411.955078\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"473.234375\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"505.021484\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"532.804688\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"593.986328\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"646.085938\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n    <g id=\"line2d_29\">\n     <path d=\"M 199.715 62.704687 \nL 223.715 62.704687 \n\" style=\"fill:none;stroke:#008000;stroke-dasharray:9.6,2.4,1.5,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_30\"/>\n    <g id=\"text_15\">\n     <!-- train error -->\n     <defs>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     </defs>\n     <g transform=\"translate(233.315 66.904687)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"232.763672\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"264.550781\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"326.074219\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"365.4375\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"404.300781\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"465.482422\" xlink:href=\"#DejaVuSans-114\"/>\n     </g>\n    </g>\n    <g id=\"line2d_31\">\n     <path d=\"M 199.715 80.318437 \nL 223.715 80.318437 \n\" style=\"fill:none;stroke:#ff0000;stroke-dasharray:1.5,2.475;stroke-dashoffset:0;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_32\"/>\n    <g id=\"text_16\">\n     <!-- test error -->\n     <g transform=\"translate(233.315 84.518437)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"192.041016\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"223.828125\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"285.351562\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"324.714844\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"363.578125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"424.759766\" xlink:href=\"#DejaVuSans-114\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p122cc3bc7d\">\n   <rect height=\"271.8\" width=\"279\" x=\"48.8975\" y=\"11.759062\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if (test_env == 'notebook'):\n",
        "  y_num_id_df = combined_dfs['LABEL_NUMERIC_IDS']\n",
        "  x_num_id_df = combined_dfs['INPUT_NUMERIC_IDS']\n",
        "  net, preds_l, preds_h, test_nrmse = evaluateOptimalHyperparameters(cyp_config,\n",
        "                                                                     combined_dfs,\n",
        "                                                                     country_years,\n",
        "                                                                     best_params,\n",
        "                                                                     is_validation=False,\n",
        "                                                                     early_stopping=False,\n",
        "                                                                     visualize=True)\n",
        "  print('Test NRMSE:', round(100 * test_nrmse, 2))\n",
        "  low_res_pred_cols =[\"id0\", \"id_y\", \"FYEAR\", \"YIELD\", \"YIELD_PRED\", \"CROP_AREA\", \"CROP_AREA_PRED\"]\n",
        "  low_res_pred_df = pd.DataFrame(data=preds_l, columns=low_res_pred_cols)\n",
        "\n",
        "  low_res_pred_df = low_res_pred_df.merge(y_num_id_df, on=['id0', 'id_y']).drop(columns=['id0', 'id_y'])\n",
        "  low_res_pred_cols =[\"COUNTRY\", low_res_id_col, \"FYEAR\", \"YIELD\", \"YIELD_PRED\", \"CROP_AREA\", \"CROP_AREA_PRED\"]\n",
        "  low_res_pred_df = low_res_pred_df[low_res_pred_cols]\n",
        "\n",
        "  high_res_pred_df = pd.DataFrame(data=preds_h, columns=[\"id_x\", \"FYEAR\", \"YIELD_PRED\"])\n",
        "  high_res_pred_df = high_res_pred_df.merge(x_num_id_df, on=['id_x'])\n",
        "  if (label_spatial_level == 'NUTS2'):\n",
        "    yield_sel_cols = ['id_x', 'FYEAR', 'YIELD']\n",
        "    high_res_pred_df = high_res_pred_df.merge(nuts3_yield_df[yield_sel_cols], on=['id_x', \"FYEAR\"])\n",
        "    high_res_pred_cols =[\"COUNTRY\", low_res_id_col, high_res_id_col, \"FYEAR\", \"YIELD\", \"YIELD_PRED\"]\n",
        "  else:\n",
        "    high_res_pred_cols =[\"COUNTRY\", low_res_id_col, high_res_id_col, \"FYEAR\", \"YIELD_PRED\"]\n",
        "\n",
        "  high_res_pred_df = high_res_pred_df[high_res_pred_cols]\n",
        "  countries = low_res_pred_df['COUNTRY'].unique()\n",
        "  test_info = '\\nTest Set NRMSEs'\n",
        "  test_info += '\\n---------------'\n",
        "  for cn in countries:\n",
        "    print('\\n', crop, cn)\n",
        "    test_info += '\\n' + crop + '(' + cn + ')'\n",
        "    cn_low_res_df = low_res_pred_df[low_res_pred_df['COUNTRY'] == cn]\n",
        "    cn_high_res_df = high_res_pred_df[high_res_pred_df['COUNTRY'] == cn]\n",
        "    lres_nrmse = NormalizedRMSE(cn_low_res_df['YIELD'].values,\n",
        "                                cn_low_res_df['YIELD_PRED'].values)\n",
        "    print('Level y NRMSE:', lres_nrmse)\n",
        "    test_info += '\\nLevel y NRMSE:' + str(lres_nrmse)\n",
        "    print(cn_low_res_df.head(10))\n",
        "    if (label_spatial_level == 'NUTS2'):\n",
        "      hres_nrmse = NormalizedRMSE(cn_high_res_df['YIELD'].values,\n",
        "                                  cn_high_res_df['YIELD_PRED'].values)\n",
        "      print('\\nLevel x NRMSE:', hres_nrmse)\n",
        "      test_info += '\\nLevel x NRMSE:' + str(hres_nrmse)\n",
        "      print(cn_high_res_df.head(10))\n",
        "\n",
        "  output_path = cyp_config['output_path']\n",
        "  high_res_pred_file = getPredictionFilename(cyp_config['crop'],\n",
        "                                             cyp_config['use_yield_trend'],\n",
        "                                             cyp_config['early_season_end_dekad'],\n",
        "                                             country=country_code,\n",
        "                                             spatial_level=cyp_config['input_spatial_level'],\n",
        "                                             architecture=cyp_config['architecture'])\n",
        "  high_res_pred_df.to_csv(output_path + '/' + high_res_pred_file + '.csv', index=False)\n",
        "\n",
        "  low_res_pred_file = getPredictionFilename(cyp_config['crop'],\n",
        "                                            cyp_config['use_yield_trend'],\n",
        "                                            cyp_config['early_season_end_dekad'],\n",
        "                                            country=country_code,\n",
        "                                            spatial_level=cyp_config['label_spatial_level'],\n",
        "                                            architecture=cyp_config['architecture'])\n",
        "\n",
        "  low_res_pred_df.to_csv(output_path + '/' + low_res_pred_file + '.csv', index=False)\n",
        "\n",
        "  log_fh.write(test_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q41axxFLsBww"
      },
      "source": [
        "### Evaluate Multiple Runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEM92cRP5bQx"
      },
      "source": [
        "#### Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bb4tUKBw5bQz",
        "outputId": "824f9cf5-af65-4967-f7ec-30b1b7e1a59c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "----------------\n",
            "Training data\n",
            "----------------\n",
            "\n",
            "\n",
            "Dekadal features: WLIM_YB, WLIM_YS, WLAI, TWC, RSM, TMAX, TMIN, TAVG, RAD, PREC, CWB, FAPAR\n",
            "Other features: SM_WHC, AVG_ELEV, STD_ELEV, AVG_SLOPE, STD_SLOPE, AVG_FIELD_SIZE, STD_FIELD_SIZE, IRRIG_AREA_ALL, IRRIG_AREA90, CN_DE, CN_ES, CN_FR, CN_IT, AEZ_1761, AEZ_1810, AEZ_1842, AEZ_1845, AEZ_1892, AEZ_1932, AEZ_1957, AEZ_1960, AEZ_1976, AEZ_1984, AEZ_2030, AEZ_2034, AEZ_2057, AEZ_2075, AEZ_2107, AEZ_2135, AEZ_2137, AEZ_2150\n",
            "Trend features: YIELD-5, YIELD-4, YIELD-3, YIELD-2, YIELD-1\n",
            "Label columns: YIELD, CROP_AREA\n",
            "\n",
            "DE Training years: 2004, 2005, 2006, 2007\n",
            "\n",
            "ES Training years: 2003, 2004, 2005, 2006\n",
            "\n",
            "FR Training years: 1999, 2000, 2001, 2002, 2003, 2004, 2005\n",
            "\n",
            "IT Training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006\n",
            "\n",
            "\n",
            "Dekadal data: 1882, 30, 12\n",
            "Other feature data: 357, 31\n",
            "Trend feature data: 435, 5\n",
            "Label data: 435, 5\n",
            "\n",
            "------------------\n",
            "Validation data\n",
            "------------------\n",
            "\n",
            "\n",
            "Dekadal features: WLIM_YB, WLIM_YS, WLAI, TWC, RSM, TMAX, TMIN, TAVG, RAD, PREC, CWB, FAPAR\n",
            "Other features: SM_WHC, AVG_ELEV, STD_ELEV, AVG_SLOPE, STD_SLOPE, AVG_FIELD_SIZE, STD_FIELD_SIZE, IRRIG_AREA_ALL, IRRIG_AREA90, CN_DE, CN_ES, CN_FR, CN_IT, AEZ_1761, AEZ_1810, AEZ_1842, AEZ_1845, AEZ_1892, AEZ_1932, AEZ_1957, AEZ_1960, AEZ_1976, AEZ_1984, AEZ_2030, AEZ_2034, AEZ_2057, AEZ_2075, AEZ_2107, AEZ_2135, AEZ_2137, AEZ_2150\n",
            "Trend features: YIELD-5, YIELD-4, YIELD-3, YIELD-2, YIELD-1\n",
            "Label columns: YIELD, CROP_AREA\n",
            "\n",
            "DE Validation years: 2008, 2009, 2010, 2011, 2012\n",
            "\n",
            "ES Validation years: 2007, 2008, 2009, 2010, 2011\n",
            "\n",
            "FR Validation years: 2006, 2007, 2008, 2009, 2010\n",
            "\n",
            "IT Validation years: 2007, 2008, 2009, 2010, 2011\n",
            "\n",
            "\n",
            "Dekadal data: 1750, 30, 12\n",
            "Other feature data: 357, 31\n",
            "Trend feature data: 400, 5\n",
            "Label data: 400, 5\n",
            "\n",
            "----------------\n",
            "Training data\n",
            "----------------\n",
            "\n",
            "\n",
            "Dekadal features: WLIM_YB, WLIM_YS, WLAI, TWC, RSM, TMAX, TMIN, TAVG, RAD, PREC, CWB, FAPAR\n",
            "Other features: SM_WHC, AVG_ELEV, STD_ELEV, AVG_SLOPE, STD_SLOPE, AVG_FIELD_SIZE, STD_FIELD_SIZE, IRRIG_AREA_ALL, IRRIG_AREA90, CN_DE, CN_ES, CN_FR, CN_IT, AEZ_1761, AEZ_1810, AEZ_1842, AEZ_1845, AEZ_1892, AEZ_1932, AEZ_1957, AEZ_1960, AEZ_1976, AEZ_1984, AEZ_2030, AEZ_2034, AEZ_2057, AEZ_2075, AEZ_2107, AEZ_2135, AEZ_2137, AEZ_2150\n",
            "Trend features: YIELD-5, YIELD-4, YIELD-3, YIELD-2, YIELD-1\n",
            "Label columns: YIELD, CROP_AREA\n",
            "\n",
            "DE Training years: 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012\n",
            "\n",
            "ES Training years: 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011\n",
            "\n",
            "FR Training years: 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010\n",
            "\n",
            "IT Training years: 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011\n",
            "\n",
            "\n",
            "Dekadal data: 3632, 30, 12\n",
            "Other feature data: 357, 31\n",
            "Trend feature data: 835, 5\n",
            "Label data: 835, 5\n",
            "\n",
            "-------------\n",
            "Test data:\n",
            "-------------\n",
            "\n",
            "\n",
            "Dekadal features: WLIM_YB, WLIM_YS, WLAI, TWC, RSM, TMAX, TMIN, TAVG, RAD, PREC, CWB, FAPAR\n",
            "Other features: SM_WHC, AVG_ELEV, STD_ELEV, AVG_SLOPE, STD_SLOPE, AVG_FIELD_SIZE, STD_FIELD_SIZE, IRRIG_AREA_ALL, IRRIG_AREA90, CN_DE, CN_ES, CN_FR, CN_IT, AEZ_1761, AEZ_1810, AEZ_1842, AEZ_1845, AEZ_1892, AEZ_1932, AEZ_1957, AEZ_1960, AEZ_1976, AEZ_1984, AEZ_2030, AEZ_2034, AEZ_2057, AEZ_2075, AEZ_2107, AEZ_2135, AEZ_2137, AEZ_2150\n",
            "Trend features: YIELD-5, YIELD-4, YIELD-3, YIELD-2, YIELD-1\n",
            "Label columns: YIELD, CROP_AREA\n",
            "\n",
            "DE Test years: 2013, 2014, 2015, 2016, 2017\n",
            "\n",
            "ES Test years: 2012, 2013, 2014, 2015, 2016\n",
            "\n",
            "FR Test years: 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018\n",
            "\n",
            "IT Test years: 2012, 2013, 2014, 2015, 2016, 2017, 2018\n",
            "\n",
            "\n",
            "Dekadal data: 2171, 30, 12\n",
            "Other feature data: 357, 31\n",
            "Trend feature data: 499, 5\n",
            "Label data: 499, 5\n"
          ]
        }
      ],
      "source": [
        "test_fraction = cyp_config['test_fraction']\n",
        "use_yield_trend = cyp_config['use_yield_trend']\n",
        "early_season_end = cyp_config['early_season_end_dekad']\n",
        "print_debug = cyp_config['debug_level'] > 1\n",
        "num_valid_years = 5\n",
        "\n",
        "scaler_args = {}\n",
        "train_dataset = CYPMLDataset(combined_dfs, country_years,\n",
        "                             yield_trend=use_yield_trend,\n",
        "                             early_season_end=early_season_end,\n",
        "                             is_train=True,\n",
        "                             test_fraction=test_fraction,\n",
        "                             num_folds=1, fold_iter=0,\n",
        "                             num_valid_years=num_valid_years,\n",
        "                             scaler_args=scaler_args,\n",
        "                             print_debug=print_debug,\n",
        "                             log_fh=log_fh)\n",
        "\n",
        "valid_dataset = CYPMLDataset(combined_dfs, country_years,\n",
        "                             yield_trend=use_yield_trend,\n",
        "                             early_season_end=early_season_end,\n",
        "                             is_train=False, is_validation=True,\n",
        "                             test_fraction=test_fraction,\n",
        "                             num_folds=1, fold_iter=0,\n",
        "                             num_valid_years=num_valid_years,\n",
        "                             scaler_args=scaler_args,\n",
        "                             print_debug=print_debug,\n",
        "                             log_fh=log_fh)\n",
        "\n",
        "scaler_args = {}\n",
        "train_dataset2 = CYPMLDataset(combined_dfs, country_years,\n",
        "                              yield_trend=use_yield_trend,\n",
        "                              early_season_end=early_season_end,\n",
        "                              is_train=True,\n",
        "                              test_fraction=test_fraction,\n",
        "                              num_folds=1, fold_iter=0,\n",
        "                              num_valid_years=0,\n",
        "                              scaler_args=scaler_args,\n",
        "                              print_debug=print_debug,\n",
        "                              log_fh=log_fh)\n",
        "\n",
        "test_dataset = CYPMLDataset(combined_dfs, country_years,\n",
        "                            yield_trend=use_yield_trend,\n",
        "                            early_season_end=early_season_end,\n",
        "                            is_train=False, is_validation=False,\n",
        "                            test_fraction=test_fraction,\n",
        "                            num_folds=1, fold_iter=0,\n",
        "                            num_valid_years=0,\n",
        "                            scaler_args=scaler_args,\n",
        "                            print_debug=print_debug,\n",
        "                            log_fh=log_fh)\n",
        "\n",
        "datasets = {\n",
        "    'valid' : [train_dataset, valid_dataset],\n",
        "    'test' : [train_dataset2, test_dataset]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fibd2TgP5np3"
      },
      "source": [
        "#### Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7fyEZxN5nqI"
      },
      "outputs": [],
      "source": [
        "def trainAndTest(cyp_config, best_params,\n",
        "                 train_dataset, test_dataset,\n",
        "                 early_stopping=False,\n",
        "                 visualize=False, country=None):\n",
        "  \"\"\"\n",
        "  1. Evaluate on validation data: num_valid_years = 5\n",
        "  2. Evaluate on test data: num_valid_years = 0.\n",
        "  \"\"\"\n",
        "  loss = nn.MSELoss()\n",
        "  num_epochs = best_params['num_epochs']\n",
        "  architecture = cyp_config['architecture']\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1,\n",
        "                                            shuffle=False, num_workers=2)\n",
        "\n",
        "  y_max = 1.0 # torch.max(train_dataset[0][3]).item()/2\n",
        "  # train_dataset is : X_ts, X_rest, X_trend.\n",
        "  # X_rest shape is (num_regions, variables). Skip TOTAL_AREA\n",
        "  num_other_features = train_dataset[0][1].shape[1] - 1\n",
        "\n",
        "  # X_trend shape is (trend_window)\n",
        "  num_trend_features = train_dataset[0][2].shape[0]\n",
        "\n",
        "  # time series (dekadal) data is [num_regions, num_dekads, num_indicators]\n",
        "  num_ts_indicators = train_dataset[0][0].shape[2]\n",
        "  ts_seq_len = train_dataset[0][0].shape[1]\n",
        "\n",
        "  device = d2l.try_gpu()\n",
        "  assert architecture in ['1DCNN', 'LSTM'], 'Architecture is not supported. Must be 1DCNN or LSTM.'\n",
        "  if (architecture == '1DCNN'):\n",
        "    net = CYP1DCNNModel(num_ts_indicators,\n",
        "                        num_trend_features,\n",
        "                        num_other_features,\n",
        "                        ts_seq_len=ts_seq_len,\n",
        "                        num_outputs=2)\n",
        "  elif (architecture == 'LSTM'):\n",
        "    net = CYPLSTMModel(num_ts_indicators,\n",
        "                       num_trend_features,\n",
        "                       num_other_features,\n",
        "                       ts_seq_len=ts_seq_len,\n",
        "                       num_outputs=2)\n",
        "\n",
        "  net = net.to(device)\n",
        "  trainer = torch.optim.Adam(net.parameters(), lr=best_params['lr'],\n",
        "                             weight_decay=best_params['weight_decay'])\n",
        "\n",
        "  test_nrmse, epochs_run = train(net, train_dataset, train_loader, test_loader,\n",
        "                                 loss, best_params['loss_split'], trainer, num_epochs,\n",
        "                                 early_stopping=early_stopping, device=device,\n",
        "                                 country=country, visualize=visualize, ymax=y_max)\n",
        "\n",
        "  if (early_stopping):\n",
        "    best_params['num_epochs'] = epochs_run\n",
        "\n",
        "  print('NRMSE:', round(test_nrmse, 4))\n",
        "  preds_l, preds_h, test_nrmse = evaluatePredictions(net, test_loader, device)\n",
        "\n",
        "  return net, preds_h, preds_l, test_nrmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBrlsTmc5zTl"
      },
      "source": [
        "#### Evaluate Multiple Runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5v5FuUh5zTm",
        "outputId": "3ac11f6a-8f7e-40f5-9e13-1edd62a5cf7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 1\n",
            "-------------------\n",
            "\n",
            "\n",
            "Validation Set Evaluation\n",
            "----------------------------\n",
            "NRMSE: 0.1266\n",
            "\n",
            "Validation NRMSE:12.655\n",
            "Validation Set NRMSEs\n",
            "---------------------\n",
            "soft wheat(DE)\n",
            "Level y NRMSE:11.724637376053066\n",
            "Level x NRMSE:18.044598359535293\n",
            "soft wheat(ES)\n",
            "Level y NRMSE:21.253909624560702\n",
            "Level x NRMSE:26.345081982416442\n",
            "soft wheat(FR)\n",
            "Level y NRMSE:8.900899631409343\n",
            "Level x NRMSE:12.995312901957906\n",
            "soft wheat(IT)\n",
            "Level y NRMSE:17.301289110481395\n",
            "Level x NRMSE:20.453443308763287\n",
            "\n",
            "\n",
            "Test Set Evaluation\n",
            "----------------------------\n",
            "NRMSE: 0.141\n",
            "\n",
            "Test NRMSE: 14.1\n",
            "Test Set NRMSEs\n",
            "---------------------\n",
            "soft wheat(DE)\n",
            "Level y NRMSE:12.635944314122039\n",
            "Level x NRMSE:17.571843628560565\n",
            "soft wheat(ES)\n",
            "Level y NRMSE:22.113162367201337\n",
            "Level x NRMSE:31.589012732826287\n",
            "soft wheat(FR)\n",
            "Level y NRMSE:12.559984883849141\n",
            "Level x NRMSE:14.972598844972117\n",
            "soft wheat(IT)\n",
            "Level y NRMSE:17.14023355498051\n",
            "Level x NRMSE:20.43041259723727\n",
            "\n",
            "Iteration 2\n",
            "-------------------\n",
            "\n",
            "\n",
            "Validation Set Evaluation\n",
            "----------------------------\n",
            "NRMSE: 0.123\n",
            "\n",
            "Validation NRMSE:12.297\n",
            "Validation Set NRMSEs\n",
            "---------------------\n",
            "soft wheat(DE)\n",
            "Level y NRMSE:11.308794254358558\n",
            "Level x NRMSE:18.121115848822612\n",
            "soft wheat(ES)\n",
            "Level y NRMSE:19.84948143679838\n",
            "Level x NRMSE:25.388262545632784\n",
            "soft wheat(FR)\n",
            "Level y NRMSE:9.219871826766386\n",
            "Level x NRMSE:13.304523290082937\n",
            "soft wheat(IT)\n",
            "Level y NRMSE:16.587687867890153\n",
            "Level x NRMSE:21.00851241815213\n",
            "\n",
            "\n",
            "Test Set Evaluation\n",
            "----------------------------\n",
            "NRMSE: 0.1407\n",
            "\n",
            "Test NRMSE: 14.07\n",
            "Test Set NRMSEs\n",
            "---------------------\n",
            "soft wheat(DE)\n",
            "Level y NRMSE:11.759172812284154\n",
            "Level x NRMSE:16.610121625575204\n",
            "soft wheat(ES)\n",
            "Level y NRMSE:24.581182382035152\n",
            "Level x NRMSE:36.26633332015436\n",
            "soft wheat(FR)\n",
            "Level y NRMSE:12.888903612935488\n",
            "Level x NRMSE:15.704129999465058\n",
            "soft wheat(IT)\n",
            "Level y NRMSE:17.402631126870087\n",
            "Level x NRMSE:21.571743197126448\n",
            "\n",
            "Iteration 3\n",
            "-------------------\n",
            "\n",
            "\n",
            "Validation Set Evaluation\n",
            "----------------------------\n",
            "NRMSE: 0.1259\n",
            "\n",
            "Validation NRMSE:12.587\n",
            "Validation Set NRMSEs\n",
            "---------------------\n",
            "soft wheat(DE)\n",
            "Level y NRMSE:11.458429521732215\n",
            "Level x NRMSE:18.091384819791234\n",
            "soft wheat(ES)\n",
            "Level y NRMSE:20.22869014925993\n",
            "Level x NRMSE:27.5678380018694\n",
            "soft wheat(FR)\n",
            "Level y NRMSE:9.639193182688986\n",
            "Level x NRMSE:13.760557432268772\n",
            "soft wheat(IT)\n",
            "Level y NRMSE:17.081580119499296\n",
            "Level x NRMSE:21.925040879730428\n",
            "\n",
            "\n",
            "Test Set Evaluation\n",
            "----------------------------\n",
            "NRMSE: 0.1452\n",
            "\n",
            "Test NRMSE: 14.52\n",
            "Test Set NRMSEs\n",
            "---------------------\n",
            "soft wheat(DE)\n",
            "Level y NRMSE:12.851594525689867\n",
            "Level x NRMSE:17.65119318459216\n",
            "soft wheat(ES)\n",
            "Level y NRMSE:23.028293162355016\n",
            "Level x NRMSE:31.674417532270848\n",
            "soft wheat(FR)\n",
            "Level y NRMSE:12.87356327880128\n",
            "Level x NRMSE:15.22933705126433\n",
            "soft wheat(IT)\n",
            "Level y NRMSE:18.127486656864544\n",
            "Level x NRMSE:20.98894258230429\n",
            "\n",
            "Iteration 4\n",
            "-------------------\n",
            "\n",
            "\n",
            "Validation Set Evaluation\n",
            "----------------------------\n",
            "NRMSE: 0.1209\n",
            "\n",
            "Validation NRMSE:12.089\n",
            "Validation Set NRMSEs\n",
            "---------------------\n",
            "soft wheat(DE)\n",
            "Level y NRMSE:11.119866743865465\n",
            "Level x NRMSE:18.00889768194056\n",
            "soft wheat(ES)\n",
            "Level y NRMSE:19.72218886207903\n",
            "Level x NRMSE:25.93598327816553\n",
            "soft wheat(FR)\n",
            "Level y NRMSE:8.777307904488206\n",
            "Level x NRMSE:12.827578132575313\n",
            "soft wheat(IT)\n",
            "Level y NRMSE:16.71544792452853\n",
            "Level x NRMSE:20.694391018072412\n",
            "\n",
            "\n",
            "Test Set Evaluation\n",
            "----------------------------\n",
            "NRMSE: 0.1423\n",
            "\n",
            "Test NRMSE: 14.23\n",
            "Test Set NRMSEs\n",
            "---------------------\n",
            "soft wheat(DE)\n",
            "Level y NRMSE:12.290279435813817\n",
            "Level x NRMSE:17.096708355455178\n",
            "soft wheat(ES)\n",
            "Level y NRMSE:22.896718844991515\n",
            "Level x NRMSE:33.93598610593483\n",
            "soft wheat(FR)\n",
            "Level y NRMSE:12.852625612045713\n",
            "Level x NRMSE:15.544385385160362\n",
            "soft wheat(IT)\n",
            "Level y NRMSE:17.860278809406108\n",
            "Level x NRMSE:20.896258472908464\n",
            "\n",
            "Iteration 5\n",
            "-------------------\n",
            "\n",
            "\n",
            "Validation Set Evaluation\n",
            "----------------------------\n",
            "NRMSE: 0.1335\n",
            "\n",
            "Validation NRMSE:13.349\n",
            "Validation Set NRMSEs\n",
            "---------------------\n",
            "soft wheat(DE)\n",
            "Level y NRMSE:11.031569239246355\n",
            "Level x NRMSE:17.907331111444226\n",
            "soft wheat(ES)\n",
            "Level y NRMSE:20.236513363152255\n",
            "Level x NRMSE:27.8898178459076\n",
            "soft wheat(FR)\n",
            "Level y NRMSE:8.683687548143954\n",
            "Level x NRMSE:13.989133054088327\n",
            "soft wheat(IT)\n",
            "Level y NRMSE:24.66313715922109\n",
            "Level x NRMSE:23.421434656657038\n",
            "\n",
            "\n",
            "Test Set Evaluation\n",
            "----------------------------\n",
            "NRMSE: 0.1409\n",
            "\n",
            "Test NRMSE: 14.09\n",
            "Test Set NRMSEs\n",
            "---------------------\n",
            "soft wheat(DE)\n",
            "Level y NRMSE:11.79664520026578\n",
            "Level x NRMSE:16.9505864210409\n",
            "soft wheat(ES)\n",
            "Level y NRMSE:22.955118039109294\n",
            "Level x NRMSE:33.51685895461861\n",
            "soft wheat(FR)\n",
            "Level y NRMSE:12.889871388755402\n",
            "Level x NRMSE:15.792685987433563\n",
            "soft wheat(IT)\n",
            "Level y NRMSE:18.175814691144783\n",
            "Level x NRMSE:22.14511064408192\n",
            "\n",
            "Iteration 6\n",
            "-------------------\n",
            "\n",
            "\n",
            "Validation Set Evaluation\n",
            "----------------------------\n",
            "NRMSE: 0.131\n",
            "\n",
            "Validation NRMSE:13.104\n",
            "Validation Set NRMSEs\n",
            "---------------------\n",
            "soft wheat(DE)\n",
            "Level y NRMSE:11.084940879796177\n",
            "Level x NRMSE:17.74275965494903\n",
            "soft wheat(ES)\n",
            "Level y NRMSE:23.517240554041415\n",
            "Level x NRMSE:31.466894453036012\n",
            "soft wheat(FR)\n",
            "Level y NRMSE:10.003686269752867\n",
            "Level x NRMSE:15.6120450626131\n",
            "soft wheat(IT)\n",
            "Level y NRMSE:19.164480582113953\n",
            "Level x NRMSE:24.41459906606899\n",
            "\n",
            "\n",
            "Test Set Evaluation\n",
            "----------------------------\n",
            "NRMSE: 0.1397\n",
            "\n",
            "Test NRMSE: 13.97\n",
            "Test Set NRMSEs\n",
            "---------------------\n",
            "soft wheat(DE)\n",
            "Level y NRMSE:11.486643566649999\n",
            "Level x NRMSE:15.805376504076271\n",
            "soft wheat(ES)\n",
            "Level y NRMSE:22.217218420678623\n",
            "Level x NRMSE:34.07564246858396\n",
            "soft wheat(FR)\n",
            "Level y NRMSE:13.008036091441259\n",
            "Level x NRMSE:16.454024335143664\n",
            "soft wheat(IT)\n",
            "Level y NRMSE:18.134675354487417\n",
            "Level x NRMSE:22.177679153080557\n",
            "\n",
            "Iteration 7\n",
            "-------------------\n",
            "\n",
            "\n",
            "Validation Set Evaluation\n",
            "----------------------------\n",
            "NRMSE: 0.1298\n",
            "\n",
            "Validation NRMSE:12.984\n",
            "Validation Set NRMSEs\n",
            "---------------------\n",
            "soft wheat(DE)\n",
            "Level y NRMSE:10.909050486470962\n",
            "Level x NRMSE:17.680416779202908\n",
            "soft wheat(ES)\n",
            "Level y NRMSE:19.769092102060988\n",
            "Level x NRMSE:25.08228420246695\n",
            "soft wheat(FR)\n",
            "Level y NRMSE:7.574784062676028\n",
            "Level x NRMSE:11.752232827118116\n",
            "soft wheat(IT)\n",
            "Level y NRMSE:24.52736961874006\n",
            "Level x NRMSE:22.617670538690692\n",
            "\n",
            "\n",
            "Test Set Evaluation\n",
            "----------------------------\n",
            "NRMSE: 0.1389\n",
            "\n",
            "Test NRMSE: 13.89\n",
            "Test Set NRMSEs\n",
            "---------------------\n",
            "soft wheat(DE)\n",
            "Level y NRMSE:11.555381066737846\n",
            "Level x NRMSE:16.44330386351075\n",
            "soft wheat(ES)\n",
            "Level y NRMSE:23.545976217456587\n",
            "Level x NRMSE:34.78775166533458\n",
            "soft wheat(FR)\n",
            "Level y NRMSE:12.809171752805844\n",
            "Level x NRMSE:16.119732124156403\n",
            "soft wheat(IT)\n",
            "Level y NRMSE:17.404499783905845\n",
            "Level x NRMSE:22.31714489132393\n",
            "\n",
            "Iteration 8\n",
            "-------------------\n",
            "\n",
            "\n",
            "Validation Set Evaluation\n",
            "----------------------------\n",
            "NRMSE: 0.126\n",
            "\n",
            "Validation NRMSE:12.595\n",
            "Validation Set NRMSEs\n",
            "---------------------\n",
            "soft wheat(DE)\n",
            "Level y NRMSE:11.917826176181405\n",
            "Level x NRMSE:18.272115269687458\n",
            "soft wheat(ES)\n",
            "Level y NRMSE:19.725331844117317\n",
            "Level x NRMSE:25.47371664162172\n",
            "soft wheat(FR)\n",
            "Level y NRMSE:9.114193010178825\n",
            "Level x NRMSE:13.173901866987535\n",
            "soft wheat(IT)\n",
            "Level y NRMSE:16.71785031955001\n",
            "Level x NRMSE:20.837865068484245\n",
            "\n",
            "\n",
            "Test Set Evaluation\n",
            "----------------------------\n",
            "NRMSE: 0.1417\n",
            "\n",
            "Test NRMSE: 14.17\n",
            "Test Set NRMSEs\n",
            "---------------------\n",
            "soft wheat(DE)\n",
            "Level y NRMSE:12.383575869585481\n",
            "Level x NRMSE:17.390123830519787\n",
            "soft wheat(ES)\n",
            "Level y NRMSE:22.43090958334774\n",
            "Level x NRMSE:31.635009173154923\n",
            "soft wheat(FR)\n",
            "Level y NRMSE:12.721688624545536\n",
            "Level x NRMSE:15.266973849816903\n",
            "soft wheat(IT)\n",
            "Level y NRMSE:17.78202915705719\n",
            "Level x NRMSE:21.137357223114446\n",
            "\n",
            "Iteration 9\n",
            "-------------------\n",
            "\n",
            "\n",
            "Validation Set Evaluation\n",
            "----------------------------\n",
            "NRMSE: 0.1216\n",
            "\n",
            "Validation NRMSE:12.156\n",
            "Validation Set NRMSEs\n",
            "---------------------\n",
            "soft wheat(DE)\n",
            "Level y NRMSE:11.19337592819795\n",
            "Level x NRMSE:17.73559310505591\n",
            "soft wheat(ES)\n",
            "Level y NRMSE:21.79174521718871\n",
            "Level x NRMSE:27.934511824234953\n",
            "soft wheat(FR)\n",
            "Level y NRMSE:8.018046197131538\n",
            "Level x NRMSE:12.692806091417614\n",
            "soft wheat(IT)\n",
            "Level y NRMSE:16.846845294153262\n",
            "Level x NRMSE:21.372588426729855\n",
            "\n",
            "\n",
            "Test Set Evaluation\n",
            "----------------------------\n",
            "NRMSE: 0.1413\n",
            "\n",
            "Test NRMSE: 14.13\n",
            "Test Set NRMSEs\n",
            "---------------------\n",
            "soft wheat(DE)\n",
            "Level y NRMSE:11.58133394408287\n",
            "Level x NRMSE:16.27132093922808\n",
            "soft wheat(ES)\n",
            "Level y NRMSE:23.100369611489032\n",
            "Level x NRMSE:34.3044149585777\n",
            "soft wheat(FR)\n",
            "Level y NRMSE:12.606255985620953\n",
            "Level x NRMSE:15.559395590104247\n",
            "soft wheat(IT)\n",
            "Level y NRMSE:19.560150192659407\n",
            "Level x NRMSE:21.507961588343225\n",
            "\n",
            "Iteration 10\n",
            "-------------------\n",
            "\n",
            "\n",
            "Validation Set Evaluation\n",
            "----------------------------\n",
            "NRMSE: 0.1286\n",
            "\n",
            "Validation NRMSE:12.862\n",
            "Validation Set NRMSEs\n",
            "---------------------\n",
            "soft wheat(DE)\n",
            "Level y NRMSE:11.838032300282563\n",
            "Level x NRMSE:18.265784074377596\n",
            "soft wheat(ES)\n",
            "Level y NRMSE:21.14045590252027\n",
            "Level x NRMSE:30.079421796218263\n",
            "soft wheat(FR)\n",
            "Level y NRMSE:9.061380386357389\n",
            "Level x NRMSE:13.819736681069687\n",
            "soft wheat(IT)\n",
            "Level y NRMSE:18.16149040210967\n",
            "Level x NRMSE:22.705724234590217\n",
            "\n",
            "\n",
            "Test Set Evaluation\n",
            "----------------------------\n",
            "NRMSE: 0.1399\n",
            "\n",
            "Test NRMSE: 13.99\n",
            "Test Set NRMSEs\n",
            "---------------------\n",
            "soft wheat(DE)\n",
            "Level y NRMSE:11.679646751802977\n",
            "Level x NRMSE:16.16527222072058\n",
            "soft wheat(ES)\n",
            "Level y NRMSE:24.385281361871492\n",
            "Level x NRMSE:36.445712061145485\n",
            "soft wheat(FR)\n",
            "Level y NRMSE:13.009549373856293\n",
            "Level x NRMSE:16.72893193114777\n",
            "soft wheat(IT)\n",
            "Level y NRMSE:16.802146708581578\n",
            "Level x NRMSE:21.733258348253507\n",
            "\n",
            "\n",
            "Average Validation Set NRMSEs\n",
            "----------------------------\n",
            "soft wheat, DE(NUTS2): 11.359\n",
            "soft wheat, DE(NUTS3): 17.987\n",
            "soft wheat, ES(NUTS2): 20.723\n",
            "soft wheat, ES(NUTS3): 27.316\n",
            "soft wheat, FR(NUTS2): 8.899\n",
            "soft wheat, FR(NUTS3): 13.393\n",
            "soft wheat, IT(NUTS2): 18.777\n",
            "soft wheat, IT(NUTS3): 21.945\n",
            "\n",
            "\n",
            "Average Test Set NRMSEs\n",
            "----------------------------\n",
            "soft wheat, DE(NUTS2): 12.002\n",
            "soft wheat, DE(NUTS3): 16.796\n",
            "soft wheat, ES(NUTS2): 23.125\n",
            "soft wheat, ES(NUTS3): 33.823\n",
            "soft wheat, FR(NUTS2): 12.822\n",
            "soft wheat, FR(NUTS3): 15.737\n",
            "soft wheat, IT(NUTS2): 17.839\n",
            "soft wheat, IT(NUTS3): 21.491\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import statistics as stat\n",
        "\n",
        "num_iters = 10\n",
        "valid_set_lres_nrmses = {}\n",
        "valid_set_hres_nrmses = {}\n",
        "test_set_lres_nrmses = {}\n",
        "test_set_hres_nrmses = {}\n",
        "\n",
        "for i in range(1, num_iters + 1):\n",
        "  \"\"\"\n",
        "  Evaluate on validation data with early stopping\n",
        "  \"\"\"\n",
        "  iter_info = '\\n' + 'Iteration ' + str(i)\n",
        "  iter_info += '\\n-------------------'\n",
        "  print(iter_info)\n",
        "  log_fh.write(iter_info)\n",
        "\n",
        "  best_params['num_epochs'] = 100\n",
        "  valid_info = '\\n\\nValidation Set Evaluation'\n",
        "  valid_info += '\\n----------------------------'\n",
        "  print(valid_info)\n",
        "\n",
        "  y_num_id_df = combined_dfs['LABEL_NUMERIC_IDS']\n",
        "  x_num_id_df = combined_dfs['INPUT_NUMERIC_IDS']\n",
        "  net, preds_h, preds_l, valid_nrmse = trainAndTest(cyp_config, best_params,\n",
        "                                                    datasets['valid'][0], datasets['valid'][1],\n",
        "                                                    early_stopping=True, visualize=False, country=country_code)\n",
        "\n",
        "  valid_info = '\\nValidation NRMSE:' + str(round(100 * valid_nrmse, 3))\n",
        "  low_res_pred_cols =[\"id0\", \"id_y\", \"FYEAR\", \"YIELD\", \"YIELD_PRED\", \"CROP_AREA\", \"CROP_AREA_PRED\"]\n",
        "  pd_lres_cv_preds = pd.DataFrame(data=preds_l, columns=low_res_pred_cols)\n",
        "\n",
        "  pd_lres_cv_preds = pd_lres_cv_preds.merge(y_num_id_df, on=['id0', 'id_y']).drop(columns=['id0', 'id_y'])\n",
        "  low_res_pred_cols =[\"COUNTRY\", low_res_id_col, \"FYEAR\", \"YIELD\", \"YIELD_PRED\", \"CROP_AREA\", \"CROP_AREA_PRED\"]\n",
        "  pd_lres_cv_preds = pd_lres_cv_preds[low_res_pred_cols]\n",
        "\n",
        "  pd_hres_cv_preds = pd.DataFrame(data=preds_h, columns=[\"id_x\", \"FYEAR\", \"YIELD_PRED\"])\n",
        "  pd_hres_cv_preds = pd_hres_cv_preds.merge(x_num_id_df, on=['id_x'])\n",
        "  if (label_spatial_level == 'NUTS2'):\n",
        "    yield_sel_cols = ['id_x', 'FYEAR', 'YIELD']\n",
        "    pd_hres_cv_preds = pd_hres_cv_preds.merge(nuts3_yield_df[yield_sel_cols], on=['id_x', \"FYEAR\"])\n",
        "    high_res_pred_cols =[\"COUNTRY\", low_res_id_col, high_res_id_col, \"FYEAR\", \"YIELD\", \"YIELD_PRED\"]\n",
        "  else:\n",
        "    high_res_pred_cols =[\"COUNTRY\", low_res_id_col, high_res_id_col, \"FYEAR\", \"YIELD_PRED\"]\n",
        "\n",
        "  pd_hres_cv_preds = pd_hres_cv_preds[high_res_pred_cols]\n",
        "  countries = pd_lres_cv_preds['COUNTRY'].unique()\n",
        "  valid_info += '\\nValidation Set NRMSEs'\n",
        "  valid_info += '\\n---------------------'\n",
        "  for cn in countries:\n",
        "    # print('\\n', crop, cn)\n",
        "    valid_info += '\\n' + crop + '(' + cn + ')'\n",
        "    cn_low_res_df = pd_lres_cv_preds[pd_lres_cv_preds['COUNTRY'] == cn]\n",
        "    cn_high_res_df = pd_hres_cv_preds[pd_hres_cv_preds['COUNTRY'] == cn]\n",
        "    lres_nrmse = NormalizedRMSE(cn_low_res_df['YIELD'].values,\n",
        "                                cn_low_res_df['YIELD_PRED'].values)\n",
        "    # print('Level y NRMSE:', lres_nrmse)\n",
        "    valid_info += '\\nLevel y NRMSE:' + str(lres_nrmse)\n",
        "    if (cn in valid_set_lres_nrmses):\n",
        "      nrmse_list = valid_set_lres_nrmses[cn]\n",
        "    else:\n",
        "      nrmse_list = []\n",
        " \n",
        "    nrmse_list.append(lres_nrmse)\n",
        "    valid_set_lres_nrmses[cn] = nrmse_list\n",
        "\n",
        "    # print(cn_low_res_df.head(10))\n",
        "    if (label_spatial_level == 'NUTS2'):\n",
        "      hres_nrmse = NormalizedRMSE(cn_high_res_df['YIELD'].values,\n",
        "                                  cn_high_res_df['YIELD_PRED'].values)\n",
        "      # print('\\nLevel x NRMSE:', hres_nrmse)\n",
        "      valid_info += '\\nLevel x NRMSE:' + str(hres_nrmse)\n",
        "      # print(cn_high_res_df.head(10))\n",
        "      if (cn in valid_set_hres_nrmses):\n",
        "        nrmse_list = valid_set_hres_nrmses[cn]\n",
        "      else:\n",
        "        nrmse_list = []\n",
        " \n",
        "      nrmse_list.append(hres_nrmse)\n",
        "      valid_set_hres_nrmses[cn] = nrmse_list\n",
        "\n",
        "  print(valid_info)\n",
        "  log_fh.write(valid_info)\n",
        "\n",
        "  \"\"\"\n",
        "  Evaluate on test data with early stopping epochs from above\n",
        "  \"\"\"\n",
        "  y_num_id_df = combined_dfs['LABEL_NUMERIC_IDS']\n",
        "  x_num_id_df = combined_dfs['INPUT_NUMERIC_IDS']\n",
        "  test_info = '\\n\\nTest Set Evaluation'\n",
        "  test_info += '\\n----------------------------'\n",
        "  print(test_info)\n",
        "  net, preds_h, preds_l, test_nrmse = trainAndTest(cyp_config, best_params,\n",
        "                                                   datasets['test'][0], datasets['test'][1],\n",
        "                                                   early_stopping=False, visualize=False, country=country_code)\n",
        "  test_info = '\\nTest NRMSE: ' + str(round(100 * test_nrmse, 2))\n",
        "  low_res_pred_cols =[\"id0\", \"id_y\", \"FYEAR\", \"YIELD\", \"YIELD_PRED\", \"CROP_AREA\", \"CROP_AREA_PRED\"]\n",
        "  pd_lres_test_preds = pd.DataFrame(data=preds_l, columns=low_res_pred_cols)\n",
        "\n",
        "  pd_lres_test_preds = pd_lres_test_preds.merge(y_num_id_df, on=['id0', 'id_y']).drop(columns=['id0', 'id_y'])\n",
        "  low_res_pred_cols =[\"COUNTRY\", low_res_id_col, \"FYEAR\", \"YIELD\", \"YIELD_PRED\", \"CROP_AREA\", \"CROP_AREA_PRED\"]\n",
        "  pd_lres_test_preds = pd_lres_test_preds[low_res_pred_cols]\n",
        "\n",
        "  pd_hres_test_preds = pd.DataFrame(data=preds_h, columns=[\"id_x\", \"FYEAR\", \"YIELD_PRED\"])\n",
        "  pd_hres_test_preds = pd_hres_test_preds.merge(x_num_id_df, on=['id_x'])\n",
        "  if (label_spatial_level == 'NUTS2'):\n",
        "    yield_sel_cols = ['id_x', 'FYEAR', 'YIELD']\n",
        "    pd_hres_test_preds = pd_hres_test_preds.merge(nuts3_yield_df[yield_sel_cols], on=['id_x', \"FYEAR\"])\n",
        "    high_res_pred_cols =[\"COUNTRY\", low_res_id_col, high_res_id_col, \"FYEAR\", \"YIELD\", \"YIELD_PRED\"]\n",
        "  else:\n",
        "    high_res_pred_cols =[\"COUNTRY\", low_res_id_col, high_res_id_col, \"FYEAR\", \"YIELD_PRED\"]\n",
        "\n",
        "  pd_hres_test_preds = pd_hres_test_preds[high_res_pred_cols]\n",
        "  countries = pd_lres_test_preds['COUNTRY'].unique()\n",
        "  test_info += '\\nTest Set NRMSEs'\n",
        "  test_info += '\\n---------------------'\n",
        "  for cn in countries:\n",
        "    # print('\\n', crop, cn)\n",
        "    test_info += '\\n' + crop + '(' + cn + ')'\n",
        "    cn_low_res_df = pd_lres_test_preds[pd_lres_test_preds['COUNTRY'] == cn]\n",
        "    cn_high_res_df = pd_hres_test_preds[pd_hres_test_preds['COUNTRY'] == cn]\n",
        "    lres_nrmse = NormalizedRMSE(cn_low_res_df['YIELD'].values,\n",
        "                                cn_low_res_df['YIELD_PRED'].values)\n",
        "    # print('Level y NRMSE:', lres_nrmse)\n",
        "    test_info += '\\nLevel y NRMSE:' + str(lres_nrmse)\n",
        "    # print(cn_low_res_df.head(10))\n",
        "    if (cn in test_set_lres_nrmses):\n",
        "      nrmse_list = test_set_lres_nrmses[cn]\n",
        "    else:\n",
        "      nrmse_list = []\n",
        " \n",
        "    nrmse_list.append(lres_nrmse)\n",
        "    test_set_lres_nrmses[cn] = nrmse_list\n",
        "\n",
        "    if (label_spatial_level == 'NUTS2'):\n",
        "      hres_nrmse = NormalizedRMSE(cn_high_res_df['YIELD'].values,\n",
        "                                  cn_high_res_df['YIELD_PRED'].values)\n",
        "      # print('\\nLevel x NRMSE:', hres_nrmse)\n",
        "      test_info += '\\nLevel x NRMSE:' + str(hres_nrmse)\n",
        "      # print(cn_high_res_df.head(10))\n",
        "      if (cn in test_set_hres_nrmses):\n",
        "        nrmse_list = test_set_hres_nrmses[cn]\n",
        "      else:\n",
        "        nrmse_list = []\n",
        " \n",
        "      nrmse_list.append(hres_nrmse)\n",
        "      test_set_hres_nrmses[cn] = nrmse_list\n",
        "\n",
        "  print(test_info)\n",
        "  log_fh.write(test_info)\n",
        "\n",
        "  output_path = cyp_config['output_path']\n",
        "  high_res_pred_file = getPredictionFilename(cyp_config['crop'],\n",
        "                                             cyp_config['use_yield_trend'],\n",
        "                                             cyp_config['early_season_end_dekad'],\n",
        "                                             country=country_code,\n",
        "                                             spatial_level=cyp_config['input_spatial_level'],\n",
        "                                             architecture=cyp_config['architecture'])\n",
        "  pd_hres_test_preds.to_csv(output_path + '/' + high_res_pred_file + '-' + str(i) + '.csv', index=False)\n",
        "\n",
        "  low_res_pred_file = getPredictionFilename(cyp_config['crop'],\n",
        "                                            cyp_config['use_yield_trend'],\n",
        "                                            cyp_config['early_season_end_dekad'],\n",
        "                                            country=country_code,\n",
        "                                            spatial_level=cyp_config['label_spatial_level'],\n",
        "                                            architecture=cyp_config['architecture'])\n",
        "\n",
        "  pd_lres_test_preds.to_csv(output_path + '/' + low_res_pred_file + '-' + str(i) + '.csv', index=False)\n",
        "\n",
        "valid_info = '\\n\\nAverage Validation Set NRMSEs'\n",
        "valid_info += '\\n----------------------------'\n",
        "for cn in valid_set_lres_nrmses:\n",
        "  cn_avg_lres_nrmse = round(stat.fmean(valid_set_lres_nrmses[cn]), 3)\n",
        "  cn_avg_hres_nrmse = round(stat.fmean(valid_set_hres_nrmses[cn]), 3)\n",
        "  valid_info += '\\n' + crop + ', '  + cn + '(' + cyp_config['label_spatial_level'] + '): ' + str(cn_avg_lres_nrmse)\n",
        "  valid_info += '\\n' + crop + ', '  + cn + '(' + cyp_config['input_spatial_level'] + '): ' + str(cn_avg_hres_nrmse)\n",
        "\n",
        "print(valid_info)\n",
        "log_fh.write(valid_info)\n",
        "\n",
        "test_info = '\\n\\nAverage Test Set NRMSEs'\n",
        "test_info += '\\n----------------------------'\n",
        "for cn in test_set_lres_nrmses:\n",
        "  cn_avg_lres_nrmse = round(stat.fmean(test_set_lres_nrmses[cn]), 3)\n",
        "  cn_avg_hres_nrmse = round(stat.fmean(test_set_hres_nrmses[cn]), 3)\n",
        "  test_info += '\\n' + crop + ', '  + cn + '(' + cyp_config['label_spatial_level'] + '): ' + str(cn_avg_lres_nrmse)\n",
        "  test_info += '\\n' + crop + ', '  + cn + '(' + cyp_config['input_spatial_level'] + '): ' + str(cn_avg_hres_nrmse)\n",
        "\n",
        "log_fh.write(test_info)\n",
        "print(test_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnQj-gNfKRL8"
      },
      "source": [
        "### Close file handle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROkMjuoBKRL-"
      },
      "outputs": [],
      "source": [
        "log_fh.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AutMojRSAEkD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "cJgYxi8L1w_9",
        "91RKRiDfsGdZ",
        "xKWsYBQbquRs",
        "2FGP8puEikoK",
        "gb2sVDTX0Pma",
        "sgYkzkrExu9Q",
        "fibd2TgP5np3"
      ],
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}